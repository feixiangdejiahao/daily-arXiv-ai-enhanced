{"id": "2511.00074", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00074", "abs": "https://arxiv.org/abs/2511.00074", "authors": ["Richard Osuagwu", "Thomas Cook", "Maraim Masoud", "Koustav Ghosal", "Riccardo Mattivi"], "title": "ScaleCall - Agentic Tool Calling at Scale for Fintech: Challenges, Methods, and Deployment Insights", "comment": null, "summary": "While Large Language Models (LLMs) excel at tool calling, deploying these\ncapabilities in regulated enterprise environments such as fintech presents\nunique challenges due to on-premises constraints, regulatory compliance\nrequirements, and the need to disambiguate large, functionally overlapping\ntoolsets. In this paper, we present a comprehensive study of tool retrieval\nmethods for enterprise environments through the development and deployment of\nScaleCall, a prototype tool-calling framework within Mastercard designed for\norchestrating internal APIs and automating data engineering workflows. We\nsystematically evaluate embedding-based retrieval, prompt-based listwise\nranking, and hybrid approaches, revealing that method effectiveness depends\nheavily on domain-specific factors rather than inherent algorithmic\nsuperiority. Through empirical investigation on enterprise-derived benchmarks,\nwe find that embedding-based methods offer superior latency for large tool\nrepositories, while listwise ranking provides better disambiguation for\noverlapping functionalities, with hybrid approaches showing promise in specific\ncontexts. We integrate our findings into ScaleCall's flexible architecture and\nvalidate the framework through real-world deployment in Mastercard's regulated\nenvironment. Our work provides practical insights into the trade-offs between\nretrieval accuracy, computational efficiency, and operational requirements,\ncontributing to the understanding of tool-calling system design for enterprise\napplications in regulated industries.", "AI": {"tldr": "The paper investigates and compares multiple tool retrieval approaches for LLM-powered systems in fintech, building a practical framework (ScaleCall) and showing that the best method depends on the specific enterprise context, regulatory needs, and tool complexities. Results guide future enterprise LLM application design.", "motivation": "Despite the strong tool-calling capabilities of Large Language Models (LLMs), deploying them in regulated enterprise environments like fintech is challenging due to regulatory, technical, and operational requirements as well as complexities in managing large, overlapping toolsets.", "method": "The paper develops and deploys ScaleCall, a prototype tool-calling framework within Mastercard for orchestrating internal APIs and automating workflows. Various tool retrieval methods are systematically evaluated, including embedding-based retrieval, prompt-based listwise ranking, and hybrid techniques, using enterprise-derived benchmarks.", "result": "Embedding-based methods yielded superior latency for large tool repositories, while listwise ranking provided better disambiguation for overlapping functionalities. Hybrid approaches showed potential in specific situations. The framework was successfully validated in Mastercard's regulated environment.", "conclusion": "The study highlights that there is no one-size-fits-all algorithmic solution for enterprise tool retrieval; effectiveness depends on domain-specific requirements. Practical insights are provided for balancing retrieval accuracy, computational efficiency, and operational needs in regulated industries."}}
{"id": "2511.00087", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00087", "abs": "https://arxiv.org/abs/2511.00087", "authors": ["Anshu Dubey", "Akash Dhruv"], "title": "Adding New Capability in Existing Scientific Application with LLM Assistance", "comment": "8 pages, 4 figures, submitted to The 1st International Workshop on\n  Foundational large Language Models Advances for HPC in Asia", "summary": "With the emergence and rapid evolution of large language models (LLM),\nautomating coding tasks has become an im- portant research topic. Many efforts\nare underway and liter- ature abounds about the efficacy of models and their\nability to generate code. A less explored aspect of code generation is for new\nalgorithms, where the training data-set would not have included any previous\nexample of similar code. In this paper we propose a new methodology for writing\ncode from scratch for a new algorithm using LLM assistance, and describe\nenhancement of a previously developed code- translation tool, Code-Scribe, for\nnew code generation.", "AI": {"tldr": "This paper proposes a method to use LLMs and an enhanced version of Code-Scribe to help write code from scratch for new algorithms, addressing the limitation of LLMs when generating code for algorithms with no prior training data examples.", "motivation": "While large language models have demonstrated impressive abilities for code generation, a largely overlooked challenge is generating code for entirely new algorithms that lack training examples. This paper addresses that gap.", "method": "The authors introduce a new methodology for generating code from scratch for novel algorithms with the help of large language models (LLMs). They also enhance an existing code-translation tool, Code-Scribe, to better support this new code generation scenario.", "result": "The paper presents a workflow leveraging LLMs and improved code-translation tools for the challenging problem of coding new, previously unseen algorithms. Specific implementation or evaluation details are not given in the abstract.", "conclusion": "Using LLMs, combined with specialized tooling (Code-Scribe), provides a promising approach for writing code for novel algorithms that do not exist in LLM training data."}}
{"id": "2511.00125", "categories": ["cs.SE", "cs.AI", "cs.LO", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.00125", "abs": "https://arxiv.org/abs/2511.00125", "authors": ["\u00c1lvaro Silva", "Alexandra Mendes", "Ruben Martins"], "title": "Inferring multiple helper Dafny assertions with LLMs", "comment": null, "summary": "The Dafny verifier provides strong correctness guarantees but often requires\nnumerous manual helper assertions, creating a significant barrier to adoption.\nWe investigate the use of Large Language Models (LLMs) to automatically infer\nmissing helper assertions in Dafny programs, with a primary focus on cases\ninvolving multiple missing assertions. To support this study, we extend the\nDafnyBench benchmark with curated datasets where one, two, or all assertions\nare removed, and we introduce a taxonomy of assertion types to analyze\ninference difficulty. Our approach refines fault localization through a hybrid\nmethod that combines LLM predictions with error-message heuristics. We\nimplement this approach in a new tool called DAISY (Dafny Assertion Inference\nSYstem). While our focus is on multiple missing assertions, we also evaluate\nDAISY on single-assertion cases. DAISY verifies 63.4% of programs with one\nmissing assertion and 31.7% with multiple missing assertions. Notably, many\nprograms can be verified with fewer assertions than originally present,\nhighlighting that proofs often admit multiple valid repair strategies and that\nrecovering every original assertion is unnecessary. These results demonstrate\nthat automated assertion inference can substantially reduce proof engineering\neffort and represent a step toward more scalable and accessible formal\nverification.", "AI": {"tldr": "The paper introduces DAISY, a tool that uses large language models to automatically add missing helper assertions in Dafny programs, greatly reducing manual effort and helping make formal verification more accessible.", "motivation": "The Dafny verifier needs many manual helper assertions, which creates a significant barrier to its adoption. There's a need to ease this process and lower the manual effort required to use Dafny for formal verification.", "method": "The authors investigate using Large Language Models (LLMs) to automatically infer missing helper assertions in Dafny programs. They extend the DafnyBench benchmark to create datasets with varying numbers of missing assertions, introduce an assertion taxonomy, and develop a hybrid approach that combines LLM predictions with error-message heuristics. This method is implemented as a new tool called DAISY.", "result": "DAISY successfully verifies 63.4% of programs with one missing assertion and 31.7% with multiple missing assertions. The study also finds that many programs can be verified with fewer assertions than originally present, implying that multiple proof repair strategies are valid.", "conclusion": "Automated inference of helper assertions using LLMs, as realized in DAISY, significantly reduces the manual proof effort in Dafny. This approach improves the scalability and accessibility of formal verification, moving toward more widespread adoption."}}
{"id": "2511.00160", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00160", "abs": "https://arxiv.org/abs/2511.00160", "authors": ["Katherine A. Rosenfeld", "Cliff C. Kerr", "Jessica Lundin"], "title": "What a diff makes: automating code migration with large language models", "comment": "10 pages, 8 figures", "summary": "Modern software programs are built on stacks that are often undergoing\nchanges that introduce updates and improvements, but may also break any project\nthat depends upon them. In this paper we explore the use of Large Language\nModels (LLMs) for code migration, specifically the problem of maintaining\ncompatibility with a dependency as it undergoes major and minor semantic\nversion changes. We demonstrate, using metrics such as test coverage and change\ncomparisons, that contexts containing diffs can significantly improve\nperformance against out of the box LLMs and, in some cases, perform better than\nusing code. We provide a dataset to assist in further development of this\nproblem area, as well as an open-source Python package, AIMigrate, that can be\nused to assist with migrating code bases. In a real-world migration of\nTYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of\nrequired changes in a single run, increasing to 80% with multiple runs, with\n47% of changes generated perfectly.", "AI": {"tldr": "The paper shows that using LLMs with diff contexts improves automated code migration for software dependencies, outperforming standard methods. Their open-source tool AIMigrate achieved notable accuracy in real-world migrations, offering valuable support for dependency updates.", "motivation": "Software stacks frequently change, causing compatibility issues for dependent projects. Addressing code migration due to updates is critical for software reliability and maintainability.", "method": "The paper investigates leveraging Large Language Models (LLMs) for automating code migration when dependencies undergo semantic version changes. Contexts containing diffs are compared against out-of-the-box LLMs and code-based contexts using metrics like test coverage and change comparisons. The authors also introduce a dataset and an open-source Python tool called AIMigrate.", "result": "Contexts with diffs significantly enhance migration performance, sometimes outperforming code-only approaches. In a case study of migrating TYPHOIDSIM to a new version of STARSIM, AIMigrate correctly identified 65% of required changes in one run and 80% with multiple runs, with 47% of changes being perfectly generated.", "conclusion": "LLMs can substantially assist in code migration tasks, especially when utilizing context-rich information like diffs. Tools like AIMigrate show promise in automating dependency upgrade processes."}}
{"id": "2511.00403", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.00403", "abs": "https://arxiv.org/abs/2511.00403", "authors": ["Wentao Peng", "Ruyi Ji", "Yingfei Xiong"], "title": "Equality Saturation Guided by Large Language Models", "comment": "presented at EGRAPHS 2025", "summary": "One critical issue with large language models (LLMs) is their inability to\nguarantee correctness. Although this problem can be addressed by applying LLMs\nto formal rewrite systems, current LLMs are still far from adequate to generate\nsound rewrite chains. To bridge this gap, this paper proposes LLM-guided\nequality saturation, dubbed LGuess, by incorporating e-graphs as an\nintermediate layer between LLMs and rewrite systems. LGuess queries LLMs only\nfor high-level rewrite checkpoints and uses e-graphs to supply low-level\nrewrite chains between these checkpoints. The key technical challenge in this\nprocedure lies in effectively extracting a suitable checkpoint from a saturated\ne-graph, which LGuess addresses by learning a probabilistic model from the LLM.\nThe model predicts probable checkpoints while remaining simple enough for\neffective extraction. We implement a prototype of LGuess and evaluate it on the\nproblem of factorizing multivariable polynomials. The results demonstrate a\nsignificant advantage of LGuess compared to both straightforward equality\nsaturation and the approach that queries the LLM directly for the rewrite\nchain.", "AI": {"tldr": "LGuess (LLM-guided equality saturation) combines e-graphs with LLMs to address correctness issues in rewrite chain generation. It achieves better results in polynomial factorization tasks than current alternatives by strategically querying LLMs and using a learned model for checkpoints.", "motivation": "Current LLMs struggle to guarantee correctness in generating rewrite chains for formal systems. Improving their reliability and soundness is crucial, especially for mathematical transformations like factorization.", "method": "The authors introduce LGuess, a framework that inserts e-graphs between LLMs and formal rewrite systems. LGuess queries LLMs for high-level rewrite checkpoints and employs e-graphs to generate low-level rewrite chains. A learned probabilistic model aids in extracting optimal checkpoints from the e-graph.", "result": "The prototype of LGuess, evaluated on multivariable polynomial factorization, shows a significant performance advantage over straightforward equality saturation and direct LLM query approaches.", "conclusion": "LGuess significantly outperforms both traditional equality saturation and direct LLM-based rewrite chain generation in multivariable polynomial factorization, providing better correctness guarantees."}}
{"id": "2511.00197", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00197", "abs": "https://arxiv.org/abs/2511.00197", "authors": ["Oorja Majgaonkar", "Zhiwei Fei", "Xiang Li", "Federica Sarro", "He Ye"], "title": "Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories", "comment": null, "summary": "The increasing deployment of Large Language Model (LLM) agents for complex\nsoftware engineering tasks has created a need to understand their\nproblem-solving behaviours beyond simple success metrics. While these agents\ndemonstrate impressive capabilities in automated issue resolution, their\ndecision-making processes remain largely opaque. This paper presents an\nempirical study of agent trajectories, namely the execution traces capturing\nthe steps agents take when attempting to resolve software issues. We analyse\ntrajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and\nPrometheus) on the SWE-Bench benchmark, examining both successful and failed\nattempts. Our investigation reveals several key insights into agent behaviour.\nFirst, we identify how distinct problem-solving strategies, such as defensive\nprogramming and context gathering, enable success in different scenarios.\nSecond, we find that failed trajectories are consistently longer and exhibit\nhigher variance than successful ones, with failure patterns differing\nsignificantly between agents. Third, our fault localisation analysis shows that\nwhile most trajectories correctly identify problematic files (72-81\\% even in\nfailures), success depends more on achieving approximate rather than exact code\nmodifications. These and other findings unveiled by our study, provide a\nfoundation for understanding agent behaviour through trajectory analysis,\ncontributing to the development of more robust and interpretable autonomous\nsoftware engineering systems.", "AI": {"tldr": "This paper analyzes the detailed behaviors of leading code-generating LLM agents on software tasks, showing that their problem-solving paths and strategies hold the key to both their successes and failures. Understanding these trajectories can help design better, more reliable AI agents for software engineering.", "motivation": "There is a growing use of Large Language Model (LLM) agents in complex software engineering, but their problem-solving processes are not well understood beyond whether they succeed or fail. Understanding their behaviors and trajectories can help improve their effectiveness and interpretability.", "method": "The paper conducts an empirical study by analyzing execution traces (trajectories) from three advanced code agents\u2014OpenHands, SWE-agent, and Prometheus\u2014using the SWE-Bench benchmark. Both successful and failed attempts at issue resolution are examined for patterns and strategies.", "result": "The study finds that successful agent strategies include defensive programming and context gathering, which are effective in varying circumstances. Failed attempts have longer, more variable trajectories, and the nature of these failures differs across agents. Importantly, most trajectories (even failures) accurately localize faults, but overall success depends more on making approximately correct code changes than exact ones.", "conclusion": "Trajectory analysis offers valuable insights into LLM agent behavior, revealing actionable strategies and failure patterns. This helps pave the way for more interpretable and robust autonomous software engineering agents by moving beyond simple success/failure definitions."}}
{"id": "2511.00488", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00488", "abs": "https://arxiv.org/abs/2511.00488", "authors": ["Jun Gao", "Yun Peng", "Xiaoxue Ren"], "title": "\\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress in\ncode-related tasks. Despite their advancement, empirical evidence reveals that\nthey still struggle with \\emph{deductive code reasoning}, the ability to reason\nabout the program execution process. While prior studies have recognized this\nlimitation, the underlying causes remain largely underexplored. In this paper,\nwe begin by presenting a comprehensive empirical study that reveals three key\nchallenges undermining deductive code reasoning: (1) an intrinsic gap between\ngeneration and reasoning abilities, (2) a consistent bias towards code sources,\nand (3) weak zero-shot generalization on complex benchmarks. In light of these\nchallenges, we propose \\texttt{ReMind}, a multi-agent framework composed of\n\\texttt{Mutator}, \\texttt{Executor}, and \\texttt{Inspector}. The\n\\texttt{Mutator} generates code variants to mitigate bias towards code sources,\nthe \\texttt{Executor} traces variable states step-by-step to expose\ninconsistency, and the \\texttt{Inspector} identifies problematic reasoning\nsteps and provides control-flow refinement to bridge the intrinsic reasoning\ngap. Through their coordinated collaboration, \\texttt{ReMind} systematically\nidentifies and refines reasoning flaws, achieving outstanding performance and\nenabling robust zero-shot generalization. Extensive experiments on two\nbenchmarks with five LLMs demonstrate the superior advantages of\n\\texttt{ReMind} compared to baseline approaches in deductive code reasoning.", "AI": {"tldr": "LLMs have difficulty with deductive code reasoning due to intrinsic limitations. The proposed ReMind framework uses a collaborative multi-agent approach to diagnose and address these problems, leading to improved, generalizable performance over existing methods.", "motivation": "LLMs are good at code-related tasks but struggle with deductive code reasoning, which involves understanding and reasoning about program execution. The reasons for this limitation are not well understood, and current solutions are inadequate.", "method": "The authors conduct a comprehensive empirical study to identify key challenges in deductive code reasoning for LLMs. Based on these insights, they introduce ReMind, a multi-agent framework with three components: Mutator (generates code variants to reduce source code bias), Executor (traces variable states to reveal inconsistencies), and Inspector (detects and refines flawed reasoning steps).", "result": "ReMind systematically identifies and improves reasoning errors, resulting in robust zero-shot generalization and outstanding performance on deductive code reasoning tasks. It outperforms baseline methods across two benchmarks and five LLMs.", "conclusion": "ReMind significantly enhances deductive code reasoning in LLMs by addressing intrinsic capability gaps, source bias, and generalization issues through a collaborative multi-agent approach."}}
{"id": "2511.00202", "categories": ["cs.SE", "cs.LG", "cs.LO", "F.3.1; I.2.5"], "pdf": "https://arxiv.org/pdf/2511.00202", "abs": "https://arxiv.org/abs/2511.00202", "authors": ["Jacqueline Mitchell", "Yasser Shaaban"], "title": "Position: Vibe Coding Needs Vibe Reasoning: Improving Vibe Coding with Formal Verification", "comment": "7 pages, 3 figures, In Proceedings of the 1st ACM SIGPLAN\n  International Workshop on Language Models and Programming Languages\n  (LMPL'25), October 12-18, 2025, Singapore, Singapore. ACM, New York, NY, USA", "summary": "``Vibe coding'' -- the practice of developing software through iteratively\nconversing with a large language model (LLM) -- has exploded in popularity\nwithin the last year. However, developers report key limitations including the\naccumulation of technical debt, security issues, and code churn to achieve\nsatisfactory results. We argue that these pitfalls result from LLMs' inability\nto reconcile accumulating human-imposed constraints during vibe coding, with\ndevelopers inadvertently failing to resolve contradictions because LLMs\nprioritize user commands over code consistency. Given LLMs' receptiveness to\nverification-based feedback, we argue that formal methods can mitigate these\npitfalls, making vibe coding more reliable. However, we posit that integrating\nformal methods must transcend existing approaches that combine formal methods\nand LLMs. We advocate for a side-car system throughout the vibe coding process\nwhich: (1) \\emph{Autoformalizes} specifications (2) Validates against targets,\n(3) Delivers \\emph{actionable} feedback to the LLM, and (4) Allows intuitive\ndeveloper influence on specifications.", "AI": {"tldr": "Vibe coding with LLMs suffers from code inconsistency and technical problems due to poor management of evolving constraints. This paper suggests that integrating formal verification through a dedicated side-car system can overcome these issues and lead to more dependable software development.", "motivation": "Vibe coding with LLMs is increasingly popular, but developers face issues like technical debt, security problems, and unreliable code results. The motivation is to address these pitfalls caused by the disconnect between iterative human constraints and LLMs' focus on immediate user commands.", "method": "The paper proposes a new 'side-car system' integrated throughout vibe coding, which autoformalizes specifications, validates outputs against targets, provides actionable feedback to the LLM, and allows developers to intuitively adjust specifications.", "result": "The implementation of the proposed side-car system aims to improve reliability and mitigate technical pitfalls in vibe coding by systematically reconciling human-imposed constraints and verification using formal methods.", "conclusion": "Using formal methods, delivered via a seamless side-car system, can substantially reduce the pitfalls of vibe coding with LLMs and make the process more reliable, maintainable, and secure than current approaches."}}
{"id": "2511.00592", "categories": ["cs.PL", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.00592", "abs": "https://arxiv.org/abs/2511.00592", "authors": ["Massinissa Merouani", "Islem Kara Bernou", "Riyadh Baghdadi"], "title": "Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization", "comment": "Accepted at the 34th International Conference on Parallel\n  Architectures and Compilation Techniques (PACT 2025). 12 pages, plus appendix", "summary": "Automatic code optimization remains a difficult challenge, particularly for\ncomplex loop nests on modern hardware. This paper investigates a novel approach\nto code optimization where Large Language Models (LLMs) guide the process\nthrough a closed-loop interaction with a compiler. We present ComPilot, an\nexperimental framework that leverages off-the-shelf LLMs, without any\ntask-specific fine-tuning, as interactive optimization agents. ComPilot\nestablishes a feedback loop where an LLM proposes transformations for a given\nloop nest to a compiler. The compiler attempts the transformations, reporting\nback legality status and measured speedup or slowdown. The LLM utilizes this\nconcrete feedback to iteratively refine its optimization strategy. Our\nextensive evaluation across the PolyBench benchmark suite demonstrates the\neffectiveness of this zero-shot approach. ComPilot achieves geometric mean\nspeedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original\ncode. Furthermore, ComPilot demonstrates competitive performance against the\nstate-of-the-art Pluto polyhedral optimizer, outperforming it in many cases.\nThis experimental study demonstrates that general-purpose LLMs can effectively\nguide the code optimization process when grounded by compiler feedback, opening\npromising research directions for agentic AI in code optimization.", "AI": {"tldr": "ComPilot uses standard LLMs and compiler feedback in a feedback loop to propose and refine code optimizations, achieving significant speedups and competitive results versus traditional optimizers, with no task-specific fine-tuning required.", "motivation": "Optimizing complex loop nests on modern hardware is challenging, and existing tools often require task-specific configuration or tuning. The motivation here is to explore whether large language models (LLMs) can be leveraged in a generalized, flexible manner to guide automatic code optimization without the need for specialized fine-tuning.", "method": "The authors introduce ComPilot, a framework using off-the-shelf LLMs (without specific fine-tuning) as agents that interact with a compiler. In a closed-loop system, the LLM suggests code transformations, which the compiler attempts. The compiler feeds back legality and performance data (speedup/slowdown), which the LLM uses to further refine its suggestions iteratively.", "result": "Extensive testing on the PolyBench benchmark suite shows ComPilot achieves significant geometric mean speedups: 2.66x for a single run and 3.54x for the best of five runs. ComPilot performs competitively and even outperforms the Pluto polyhedral optimizer in several cases.", "conclusion": "General-purpose LLMs, when integrated with compiler feedback, can effectively guide code optimization, matching or exceeding traditional state-of-the-art methods. This approach suggests new directions for AI agents in program optimization."}}
{"id": "2511.00215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00215", "abs": "https://arxiv.org/abs/2511.00215", "authors": ["Xiaomeng Xu", "Zahin Wahab", "Reid Holmes", "Caroline Lemieux"], "title": "DocPrism: Local Categorization and External Filtering to Identify Relevant Code-Documentation Inconsistencies", "comment": null, "summary": "Code-documentation inconsistencies are common and undesirable: they can lead\nto developer misunderstandings and software defects. This paper introduces\nDocPrism, a multi-language, code-documentation inconsistency detection tool.\nDocPrism uses a standard large language model (LLM) to analyze and explain\ninconsistencies. Plain use of LLMs for this task yield unacceptably high false\npositive rates: LLMs identify natural gaps between high-level documentation and\ndetailed code implementations as inconsistencies. We introduce and apply the\nLocal Categorization, External Filtering (LCEF) methodology to reduce false\npositives. LCEF relies on the LLM's local completion skills rather than its\nlong-term reasoning skills. In our ablation study, LCEF reduces DocPrism's\ninconsistency flag rate from 98% to 14%, and increases accuracy from 14% to\n94%. On a broad evaluation across Python, TypeScript, C++, and Java, DocPrism\nmaintains a low flag rate of 15%, and achieves a precision of 0.62 without\nperforming any fine-tuning.", "AI": {"tldr": "DocPrism is a multi-language tool that uses LLMs to spot code-documentation inconsistencies. By adding a novel LCEF filtering method, it significantly improves accuracy and reduces false alarms. It works well across various languages and doesn't need fine-tuning.", "motivation": "Code-documentation inconsistencies frequently occur in software projects and can result in developer confusion and increase the risk of software errors, hence there is a need for effective tools that detect and explain such inconsistencies.", "method": "The paper introduces DocPrism, a tool that uses a standard LLM to detect code-documentation inconsistencies in multiple languages. To address the challenge of high false positive rates in LLM-based detection, the authors propose the Local Categorization, External Filtering (LCEF) methodology, which leverages the LLM's local completion skills and applies external filtering to reduce misclassifications.", "result": "Using LCEF, DocPrism reduced the inconsistency flag rate from 98% to 14% in ablation studies and raised detection accuracy from 14% to 94%. In wider evaluation across Python, TypeScript, C++, and Java, DocPrism maintained a low flag rate (15%) and achieved a precision score of 0.62 without any fine-tuning.", "conclusion": "DocPrism demonstrates the effectiveness of combining LLMs with the LCEF methodology to accurately detect code-documentation inconsistencies across several programming languages, greatly reducing false positives with high accuracy and without needing fine-tuning."}}
{"id": "2511.00740", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.00740", "abs": "https://arxiv.org/abs/2511.00740", "authors": ["Igor Engel", "Ekaterina Verbitskaia"], "title": "Typed Embedding of miniKanren for Functional Conversion", "comment": null, "summary": "Relational programming enables program synthesis through a verifier-to-solver\napproach. An earlier paper introduced a functional conversion that mitigated\nsome of the inherent performance overhead. However, the conversion was\ninelegant: it was oblivious to types, demanded determinism annotations, and\nimplicit generator threading. In this paper, we address these issues by\nproviding a typed tagless-final embedding of miniKanren into Haskell. This\nimprovement significantly reduces boilerplate while preserving, and sometimes\nenhancing, earlier speedups.", "AI": {"tldr": "This paper refines relational programming in Haskell by integrating miniKanren via a typed tagless-final approach, removing much of the manual overhead and retaining or improving performance.", "motivation": "Previous relational programming approaches suffered from performance issues and inelegant abstractions, requiring manual determinism annotations and implicit generator threading.", "method": "The authors present a typed tagless-final embedding of miniKanren into Haskell, restructuring the implementation to be more type-aware and reducing boilerplate.", "result": "The new embedding not only reduces programming boilerplate but also retains or improves performance gains achieved in earlier work.", "conclusion": "The proposed approach streamlines relational programming in Haskell, making it more efficient and convenient without sacrificing speed."}}
{"id": "2511.00262", "categories": ["cs.SE", "D.2; I.2"], "pdf": "https://arxiv.org/pdf/2511.00262", "abs": "https://arxiv.org/abs/2511.00262", "authors": ["Romina Etezadi", "Sallam Abualhaija", "Chetan Arora", "Lionel Briand"], "title": "LLM-Driven Cost-Effective Requirements Change Impact Analysis", "comment": "28 pages, 6 figures", "summary": "Requirements are inherently subject to changes throughout the software\ndevelopment lifecycle. Within the limited budget available to requirements\nengineers, manually identifying the impact of such changes on other\nrequirements is both error-prone and effort-intensive. That might lead to\noverlooked impacted requirements, which, if not properly managed, can cause\nserious issues in the downstream tasks. Inspired by the growing potential of\nlarge language models (LLMs) across diverse domains, we propose ProReFiCIA, an\nLLM-driven approach for automatically identifying the impacted requirements\nwhen changes occur. We conduct an extensive evaluation of ProReFiCIA using\nseveral LLMs and prompts variants tailored to this task. Using the best\ncombination of an LLM and a prompt variant, ProReFiCIA achieves a recall of\n93.3% on a benchmark dataset and 95.8% on a newly created industry dataset,\ndemonstrating its strong effectiveness in identifying impacted requirements.\nFurther, the cost of applying ProReFiCIA remains small, as the engineer only\nneeds to review the generated results, which represent between 2.1% and 8.5% of\nthe entire set of requirements.", "AI": {"tldr": "Manual impact analysis of requirements is inefficient and error-prone. ProReFiCIA, an LLM-powered system, automatically detects impacted requirements with high recall (>93%) and minimal review effort, making requirements change management much more effective.", "motivation": "Manual identification of impacted requirements after changes in the software development process is error-prone and resource-intensive due to budget and human limitations. This can cause overlooked requirements and serious issues in subsequent development stages.", "method": "The authors propose ProReFiCIA, a Large Language Model (LLM)-based automatic approach to identify impacted requirements, experimenting with several LLMs and customized prompts to optimize performance.", "result": "ProReFiCIA achieves a recall of 93.3% on a benchmark dataset and 95.8% on a new industry dataset. It requires engineers to review only 2.1% to 8.5% of all requirements, minimizing effort.", "conclusion": "ProReFiCIA is highly effective in automatically identifying impacted requirements with low manual effort, leveraging the capabilities of LLMs. This approach can substantially improve requirements traceability and change management in software engineering."}}
{"id": "2511.01736", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.01736", "abs": "https://arxiv.org/abs/2511.01736", "authors": ["Charles Yuan"], "title": "Cobble: Compiling Block Encodings for Quantum Computational Linear Algebra", "comment": "20 pages, 12 figures", "summary": "Quantum algorithms for computational linear algebra promise up to exponential\nspeedups for applications such as simulation and regression, making them prime\ncandidates for hardware realization. But these algorithms execute in a model\nthat cannot efficiently store matrices in memory like a classical algorithm\ndoes, instead requiring developers to implement complex expressions for matrix\narithmetic in terms of correct and efficient quantum circuits. Among the\nchallenges for the developer is navigating a cost model in which conventional\noptimizations for linear algebra, such as subexpression reuse, can be\ninapplicable or unprofitable.\n  In this work, we present Cobble, a language for programming with quantum\ncomputational linear algebra. Cobble enables developers to express and\nmanipulate the quantum representations of matrices, known as block encodings,\nusing high-level notation that automatically compiles to correct quantum\ncircuits. Cobble features analyses that estimate leading factors in time and\nspace usage of programs, as well as optimizations that reduce overhead and\ngenerate efficient circuits using leading techniques such as the quantum\nsingular value transformation. We evaluate Cobble on benchmark kernels for\nsimulation, regression, search, and other applications, showing 2.6x-25.4x\nspeedups not achieved by existing circuit optimizers on these benchmarks.", "AI": {"tldr": "Cobble is a new high-level language for quantum linear algebra programming that automatically compiles concise matrix operations into optimized quantum circuits. It provides resource analysis and state-of-the-art optimizations, demonstrating significant speedups over existing circuit optimizers in common quantum algorithm benchmarks.", "motivation": "Quantum linear algebra algorithms can offer exponential speedups for tasks like simulation and regression, but developing these algorithms for quantum hardware is challenging. Classical memory and optimization strategies can't be easily transferred to quantum systems, requiring complex and specialized quantum programming techniques.", "method": "The authors introduce Cobble, a high-level programming language for quantum computational linear algebra. Cobble lets developers express quantum matrix operations (block encodings) in concise notation, which is automatically compiled into efficient quantum circuits. It includes tools for resource analysis and leading-edge circuit optimizations, like quantum singular value transformation.", "result": "Cobble was tested on benchmark applications such as simulation, regression, and search. It produced quantum circuits with speedups ranging from 2.6x to 25.4x over existing quantum circuit optimization tools on these benchmarks.", "conclusion": "Cobble substantially simplifies the development of quantum linear algebra programs, allowing for more efficient circuit generation and resource analysis. It leads to better performance compared to current tools, and can accelerate practical applications of quantum computational linear algebra."}}
{"id": "2511.00417", "categories": ["cs.SE", "cs.AI", "cs.HC", "H.5.3; D.2.9; I.2.0"], "pdf": "https://arxiv.org/pdf/2511.00417", "abs": "https://arxiv.org/abs/2511.00417", "authors": ["Marcel Valovy"], "title": "Human-AI Programming Role Optimization: Developing a Personality-Driven Self-Determination Framework", "comment": "PhD Dissertation, Prague University of Economics and Business, 2025.\n  323 pages. ACM CCS 2012: Human-computer interaction, Collaborative\n  interaction, Human-AI collaborative systems, Pair programming, AI-assisted\n  software engineering", "summary": "As artificial intelligence transforms software development, a critical\nquestion emerges: how can developers and AI systems collaborate most\neffectively? This dissertation optimizes human-AI programming roles through\nself-determination theory and personality psychology, introducing the Role\nOptimization Motivation Alignment (ROMA) framework.\n  Through Design Science Research spanning five cycles, this work establishes\nempirically-validated connections between personality traits, programming role\npreferences, and collaborative outcomes, engaging 200 experimental participants\nand 46 interview respondents.\n  Key findings demonstrate that personality-driven role optimization\nsignificantly enhances self-determination and team dynamics, yielding 23%\naverage motivation increases among professionals and up to 65% among\nundergraduates. Five distinct personality archetypes emerge: The Explorer (high\nOpenness/low Agreeableness), The Orchestrator (high\nExtraversion/Agreeableness), The Craftsperson (high Neuroticism/low\nExtraversion), The Architect (high Conscientiousness), and The Adapter\n(balanced profile). Each exhibits distinct preferences for programming roles\n(Co-Pilot, Co-Navigator, Agent), with assignment modes proving crucial for\nsatisfaction.\n  The dissertation contributes: (1) an empirically-validated framework linking\npersonality traits to role preferences and self-determination outcomes; (2) a\ntaxonomy of AI collaboration modalities mapped to personality profiles while\npreserving human agency; and (3) an ISO/IEC 29110 extension enabling Very Small\nEntities to implement personality-driven role optimization within established\nstandards.\n  Keywords: artificial intelligence, human-computer interaction, behavioral\nsoftware engineering, self-determination theory, personality psychology,\nphenomenology, intrinsic motivation, pair programming, design science research,\nISO/IEC 29110", "AI": {"tldr": "Optimizing software development team roles based on personality traits, using the ROMA framework, substantially increases motivation and collaboration when working with AI. The work offers practical, standards-aligned tools to implement this approach, validated through extensive research.", "motivation": "As AI becomes increasingly embedded in software development, there is a need to understand and optimize how developers and AI systems work together. The goal is to maximize developer motivation, satisfaction, and productivity by aligning human roles with inherent personality traits.", "method": "The dissertation employs Design Science Research over five iterative cycles, involving 200 experimental participants and 46 interviewees. It develops and validates the Role Optimization Motivation Alignment (ROMA) framework, combining self-determination theory and personality psychology.", "result": "The study empirically demonstrates that aligning programming roles with individual personality traits greatly increases motivation (by 23% for professionals and up to 65% for undergraduates) and improves team dynamics. Five personality archetypes and their preferred collaborative AI roles are identified, and their assignment modes are shown to be crucial for satisfaction.", "conclusion": "Personality-driven role assignment in human-AI software development teams significantly boosts motivation and team functioning. The research delivers an empirically-backed framework, a taxonomy for AI collaboration tailored to personalities, and an extension to ISO/IEC 29110 for small entities to utilize personality-based role optimization."}}
{"id": "2511.00450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00450", "abs": "https://arxiv.org/abs/2511.00450", "authors": ["Vahid Etemadi", "Gregorio Robles"], "title": "SmartDoc: A Context-Aware Agentic Method Comment Generation Plugin", "comment": "6 pages, Already submitted to The 3rd International Workshop on\n  Integrated Development Environments (the IDE Workshop)", "summary": "Context: The software maintenance phase involves many activities such as code\nrefactoring, bug fixing, code review or testing. Program comprehension is key\nto all these activities, as it demands developers to grasp the knowledge (e.g.,\nimplementation details) required to modify the codebase. Methods as main\nbuilding blocks in a program can offer developers this knowledge source for\ncode comprehension. However, reading entire method statements can be\nchallenging, which necessitates precise and up-to-date comments. Objective: We\npropose a solution as an IntelliJ IDEA plugin, named SmartDoc, that assists\ndevelopers in generating context-aware method comments. Method: This plugin\nacts as an Artificial Intelligence (AI) agent that has its own memory and is\naugmented by target methods' context. When a request is initiated by the\nend-user, the method content and all its nested method calls are used in the\ncomment generation. At the beginning, these nested methods are visited and a\ncall graph is generated. This graph is then traversed using depth-first search\n(DFS), enabling the provision of full-context to enrich Large Language Model\n(LLM) prompts. Result: The product is a software, as a plugin, developed for\nJava codebase and installable on IntelliJ IDEA. This plugin can serve\nconcurrently for methods whose comments are being updated , and it shares\nmemory across all flows to avoid redundant calls. o measure the accuracy of\nthis solution, a dedicated test case is run to record SmartDoc generated\ncomments and their corresponding ground truth. For each collected result-set,\nthree metrics are computed, BERTScore, BLEU and ROUGE-1. These metrics will\ndetermine how accurate the generated comments are in comparison to the ground\ntruth. Result: The obtained accuracy, in terms of the precision, recall and F1,\nis promising, and lies in the range of 0.80 to 0.90 for BERTScore.", "AI": {"tldr": "SmartDoc, an IntelliJ plugin, uses AI and method call context to generate precise comments for Java methods, achieving high accuracy (BERTScore 0.80-0.90), thus aiding maintenance tasks.", "motivation": "Software maintenance requires extensive program comprehension for activities like bug fixing and refactoring. Reading complete method bodies is challenging for developers, so precise, contextual comments are needed to aid understanding.", "method": "The authors developed SmartDoc, an IntelliJ IDEA plugin that uses AI to generate context-aware method comments. It builds a call graph of nested methods using DFS and incorporates their contents to enrich LLM (Large Language Model) prompts for comment generation. The plugin supports parallel updates and shares memory to optimize performance.", "result": "SmartDoc was tested on Java codebases. Its generated comments were compared against ground truth using BERTScore, BLEU, and ROUGE-1 metrics. The plugin achieved promising accuracy, with BERTScore values between 0.80 and 0.90 for precision, recall, and F1.", "conclusion": "SmartDoc effectively generates accurate, context-aware comments for Java methods, streamlining software maintenance tasks by improving code comprehension."}}
{"id": "2511.01529", "categories": ["cs.SE", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.01529", "abs": "https://arxiv.org/abs/2511.01529", "authors": ["Murali Sridharan", "Mikel Robredo", "Leevi Rantala", "Matteo Esposito", "Valentina Lenarduzzi", "Mika Mantyla"], "title": "Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt", "comment": null, "summary": "Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for\nproactive software maintenance. Previous research has primarily targeted\ndetecting and prioritizing SATD, with little focus on the source code afflicted\nwith SATD. Our goal in this work is to connect the SATD comments with source\ncode constructs that surround them.\n  Method. We leverage the extensive SATD dataset PENTACET, containing code\ncomments from over 9000 Java Open Source Software (OSS) repositories. We\nquantitatively infer where SATD most commonly occurs and which code\nconstructs/statements it most frequently affects.\n  Results and Conclusions. Our large-scale study links over 225,000 SATD\ncomments to their surrounding code, showing that SATD mainly arises in inline\ncode near definitions, conditionals, and exception handling, where developers\nface uncertainty and trade-offs, revealing it as an intentional signal of\nawareness during change rather than mere neglect.", "AI": {"tldr": "This paper analyzes over 225,000 instances of self-admitted technical debt in Java repositories, finding that SATD often occurs around definitions, conditionals, and exception handling, and serves as a marker of developer awareness during complex changes, not just neglect.", "motivation": "Detecting Self-Admitted Technical Debt (SATD) is essential for effective software maintenance, but prior research has largely ignored the specific code regions affected by SATD. This paper aims to bridge that gap by directly connecting SATD comments to their associated code constructs.", "method": "The authors utilized the PENTACET dataset, which includes SATD-related comments from over 9000 Java open source repositories. They quantitatively analyzed where SATD occurs most frequently and which types of code constructs are most often affected by these comments.", "result": "Through examining over 225,000 SATD comments, the study discovered that SATD is most frequently found in inline code adjacent to definitions, conditional statements, and exception handling sections. These are areas where developers experience uncertainty and must make trade-offs.", "conclusion": "SATD comments primarily signal developers' awareness and intentional communication around uncertain or complex code changes, rather than simply reflecting carelessness or neglect. The study deepens understanding of where and why SATD surfaces in code, informing better maintenance practices."}}
{"id": "2511.00467", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00467", "abs": "https://arxiv.org/abs/2511.00467", "authors": ["Liu Wang", "Dong Wang", "Shidong Pan", "Zheng Jiang", "Haoyu Wang", "Yi Wang"], "title": "A Big Step Forward? A User-Centric Examination of iOS App Privacy Report and Enhancements", "comment": "Accepted to S&P 2025", "summary": "The prevalent engagement with mobile apps underscores the importance of\nunderstanding their data practices. Transparency plays a crucial role in this\ncontext, ensuring users to be informed and give consent before any data access\noccurs. Apple introduced a new feature since iOS 15.2, App Privacy Report, to\ninform users about detailed insights into apps' data access and sharing. This\nfeature continues Apple's trend of privacy-focused innovations (following\nPrivacy Nutrition Labels), and has been marketed as a big step forward in user\nprivacy. However, its real-world impacts on user privacy and control remain\nunexamined. We thus proposed an end-to-end study involving systematic\nassessment of the App Privacy Report's real-world benefits and limitations,\nLLM-enabled and multi-technique synthesized enhancements, and comprehensive\nevaluation from both system and user perspectives. Through a structured focus\ngroup study with twelve everyday iOS users, we explored their experiences,\nunderstanding, and perceptions of the feature, suggesting its limited practical\nimpact resulting from missing important details. We identified two primary user\nconcerns: the clarity of data access purpose and domain description. In\nresponse, we proposed enhancements including a purpose inference framework and\ndomain clarification pipeline. We demonstrated the effectiveness and benefits\nof such enhancements for mobile app users. This work provides practical\ninsights that could help enhance user privacy transparency and discusses areas\nfor future research.", "AI": {"tldr": "Apple's App Privacy Report aims to improve app data transparency but falls short in user clarity and practical impact. The paper identifies shortcomings and proposes new technical enhancements, demonstrating their effectiveness in helping users better understand how apps access and use their data.", "motivation": "The massive use of mobile apps makes understanding their data practices crucial. Apple's App Privacy Report aims to increase transparency and user control, but its real-world impact and limitations are not yet fully understood.", "method": "The study involved a systematic assessment of the App Privacy Report, the synthesis of LLM-enabled and multi-technique enhancements, and comprehensive evaluations from both system and user perspectives, including a structured focus group study with twelve iOS users.", "result": "The focus group highlighted limited practical impact of the App Privacy Report due to missing details, particularly around data access purpose and domain description. The researchers developed enhancements\u2014a purpose inference framework and domain clarification pipeline\u2014and showed these improvements benefit users.", "conclusion": "The App Privacy Report in its current form has limited effectiveness for informing users about app data practices. The proposed enhancements improve clarity and user understanding, offering practical insights to refine privacy mechanisms and guide future research."}}
{"id": "2511.00517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00517", "abs": "https://arxiv.org/abs/2511.00517", "authors": ["Shuochuan Li", "Dong Wang", "Patanamon Thongtanunam", "Zan Wang", "Jiuqiao Yu", "Junjie Chen"], "title": "Issue-Oriented Agent-Based Framework for Automated Review Comment Generation", "comment": null, "summary": "Code review (CR) is a crucial practice for ensuring software quality. Various\nautomated review comment generation techniques have been proposed to streamline\nthe labor-intensive process. However, existing approaches heavily rely on a\nsingle model to identify various issues within the code, limiting the model's\nability to handle the diverse, issue-specific nature of code changes and\nleading to non-informative comments, especially in complex scenarios such as\nbug fixes. To address these limitations, we propose RevAgent, a novel\nagent-based issue-oriented framework, decomposes the task into three stages:\n(1) Generation Stage, where five category-specific commentator agents analyze\ncode changes from distinct issue perspectives and generate candidate comments;\n(2) Discrimination Stage, where a critic agent selects the most appropriate\nissue-comment pair; and (3) Training Stage, where all agents are fine-tuned on\ncurated, category-specific data to enhance task specialization. Evaluation\nresults show that RevAgent significantly outperforms state-of-the-art PLM- and\nLLM-based baselines, with improvements of 12.90\\%, 10.87\\%, 6.32\\%, and 8.57\\%\non BLEU, ROUGE-L, METEOR, and SBERT, respectively. It also achieves relatively\nhigher accuracy in issue-category identification, particularly for challenging\nscenarios. Human evaluations further validate the practicality of RevAgent in\ngenerating accurate, readable, and context-aware review comments. Moreover,\nRevAgent delivers a favorable trade-off between performance and efficiency.", "AI": {"tldr": "RevAgent is a new multi-agent framework for automated code review comments. It uses category-specific agents and a critic agent to generate more accurate, informative, and context-aware review comments than state-of-the-art methods. It outperforms others in both automatic and human assessments, especially for complex scenarios, and offers good efficiency.", "motivation": "Current automated code review comment generation methods use a single model to identify multiple types of coding issues, resulting in poor handling of diverse issue categories and uninformative comments, especially in complex cases such as bug fixes.", "method": "The authors propose RevAgent, an agent-based framework that decomposes the code review comment generation into three stages: (1) multiple issue-specific agents generate candidate comments from distinct perspectives, (2) a critic agent selects the most suitable comment, and (3) agents are fine-tuned on category-specific datasets for specialization.", "result": "RevAgent surpasses leading PLM and LLM baselines with notable improvements across four key metrics (BLEU, ROUGE-L, METEOR, SBERT). It provides higher accuracy for issue-category identification, particularly in complex scenarios, and human evaluations confirm its practical utility and readability of comments. RevAgent balances performance and efficiency well.", "conclusion": "RevAgent, with its issue-oriented multi-agent design, effectively enhances automated code review comment generation in terms of quality, relevance, and efficiency, outperforming existing single-model approaches."}}
{"id": "2511.00527", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00527", "abs": "https://arxiv.org/abs/2511.00527", "authors": ["Robab Aghazadeh-Chakherlou", "Qing Guo", "Siddartha Khastgir", "Peter Popov", "Xiaoge Zhang", "Xingyu Zhao"], "title": "HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models", "comment": "under review", "summary": "Large Language Models (LLMs) are increasingly deployed across diverse\ndomains, raising the need for rigorous reliability assessment methods. Existing\nbenchmark-based evaluations primarily offer descriptive statistics of model\naccuracy over datasets, providing limited insight into the probabilistic\nbehavior of LLMs under real operational conditions. This paper introduces\nHIP-LLM, a Hierarchical Imprecise Probability framework for modeling and\ninferring LLM reliability. Building upon the foundations of software\nreliability engineering, HIP-LLM defines LLM reliability as the probability of\nfailure-free operation over a specified number of future tasks under a given\nOperational Profile (OP). HIP-LLM represents dependencies across (sub-)domains\nhierarchically, enabling multi-level inference from subdomain to system-level\nreliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty\nand incorporates OPs to reflect usage contexts. It derives posterior\nreliability envelopes that quantify uncertainty across priors and data.\nExperiments on multiple benchmark datasets demonstrate that HIP-LLM offers a\nmore accurate and standardized reliability characterization than existing\nbenchmark and state-of-the-art approaches. A publicly accessible repository of\nHIP-LLM is provided.", "AI": {"tldr": "HIP-LLM proposes a hierarchical imprecise probability framework to better assess LLM reliability in real-world use, outperforming standard benchmark approaches and offering a public implementation.", "motivation": "Traditional benchmark methods for evaluating LLMs only yield descriptive accuracy metrics, lacking probabilistic characterization of reliability under real-world operational profiles. There is a need for rigorous reliability assessment methods for practical deployment.", "method": "The paper develops HIP-LLM, a Hierarchical Imprecise Probability model that represents reliability through hierarchical subdomain dependencies, incorporates imprecise priors for epistemic uncertainty, and Operational Profiles to model actual usage contexts. It performs multi-level inference and derives posterior reliability envelopes.", "result": "Experiments on multiple benchmark datasets show HIP-LLM delivers improved reliability estimation and standardization compared with existing techniques. The framework, along with code, is made publicly available.", "conclusion": "HIP-LLM framework provides more accurate and standardized reliability assessment for LLMs compared to existing benchmark-based and state-of-the-art methods."}}
{"id": "2511.00528", "categories": ["cs.SE", "D.2.9"], "pdf": "https://arxiv.org/pdf/2511.00528", "abs": "https://arxiv.org/abs/2511.00528", "authors": ["Muhammad Hamid Raza Mookadam", "Ridewaan Hanslo"], "title": "Employee Performance when Implementing Agile Practices in an IT Workforce", "comment": "11 pages, 1 figure, 1 table, 7th World Symposium on Software\n  Engineering (WSSE 2025)", "summary": "Adoption of agile practices has increased in IT workforces. However, there is\na lack of comprehensive studies in the African context on employee performance\nwhen implementing agile practices. This study addresses this gap by exploring\nemployee performance in agile environments for IT workforces in South Africa.\nAn interpretivist mono-method qualitative approach was used, with the use of\ninterviews as a research strategy. Seventeen semi-structured interviews were\nconducted with agile practitioners from various roles. Our results indicated\nthat agile practices influence employee performance significantly, with\nparticipants reporting on aspects which included planning, communication,\nemployee development and well-being, collaboration, team culture and progress.\nAdditionally, our results reported obstacles when using agile practices that\nincluded adoption, team engagement, leadership and instilling an agile mindset.\nAgile practices influence employee performance in IT workforces by fostering\nimproved team dynamics, enhanced collaboration, improved efficiencies, risk\nmanagement, planning, continuous improvement, learning, personal development\nand well-being. Conclusively, our findings suggest that if agile challenges are\naddressed and additional support is provided, employee performance can be\nsignificantly improved.", "AI": {"tldr": "This study finds that agile practices generally boost employee performance in South African IT, especially in teamwork, planning, and well-being, but overcoming adoption and cultural challenges is essential for maximizing results.", "motivation": "Agile methodologies are widely adopted globally in IT, but there is little research on how they affect employee performance specifically in the African, and more precisely, South African IT context. Existing studies do not comprehensively address local factors or challenges.", "method": "The study utilizes an interpretivist mono-method qualitative approach. Researchers conducted seventeen semi-structured interviews with agile practitioners from diverse roles in the South African IT sector to gather in-depth insights.", "result": "Agile practices significantly influence employee performance, particularly in improving planning, communication, employee development, well-being, collaboration, team culture, and progress. Despite these positives, challenges such as adoption issues, team engagement difficulties, leadership gaps, and resistance to the agile mindset impede effectiveness. Addressing these obstacles and increasing support for employees could further enhance performance outcomes.", "conclusion": "Agile practices have a substantial positive impact on employee performance in South Africa\u2019s IT workforce. However, to fully realize the benefits, issues related to implementation and employee support must be tackled."}}
{"id": "2511.00619", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00619", "abs": "https://arxiv.org/abs/2511.00619", "authors": ["Huaijin Ran", "Haoyi Zhang", "Xunzhu Tang"], "title": "GDPR-Bench-Android: A Benchmark for Evaluating Automated GDPR Compliance Detection in Android", "comment": null, "summary": "Automating the detection of EU General Data Protection Regulation (GDPR)\nviolations in source code is a critical but underexplored challenge. We\nintroduce \\textbf{GDPR-Bench-Android}, the first comprehensive benchmark for\nevaluating diverse automated methods for GDPR compliance detection in Android\napplications. It contains \\textbf{1951} manually annotated violation instances\nfrom \\textbf{15} open-source repositories, covering 23 GDPR articles at file-,\nmodule-, and line-level granularities. To enable a multi-paradigm evaluation,\nwe contribute \\textbf{Formal-AST}, a novel, source-code-native formal method\nthat serves as a deterministic baseline. We define two tasks: (1)\n\\emph{multi-granularity violation localization}, evaluated via\nAccuracy@\\textit{k}; and (2) \\emph{snippet-level multi-label classification},\nassessed by macro-F1 and other classification metrics. We benchmark 11 methods,\nincluding eight state-of-the-art LLMs, our Formal-AST analyzer, a\nretrieval-augmented (RAG) method, and an agentic (ReAct) method. Our findings\nreveal that no single paradigm excels across all tasks. For Task 1, the ReAct\nagent achieves the highest file-level Accuracy@1 (17.38%), while the\nQwen2.5-72B LLM leads at the line level (61.60%), in stark contrast to the\nFormal-AST method's 1.86%. For the difficult multi-label Task 2, the\nClaude-Sonnet-4.5 LLM achieves the best Macro-F1 (5.75%), while the RAG method\nyields the highest Macro-Precision (7.10%). These results highlight the\ntask-dependent strengths of different automated approaches and underscore the\nvalue of our benchmark in diagnosing their capabilities. All resources are\navailable at: https://github.com/Haoyi-Zhang/GDPR-Bench-Android.", "AI": {"tldr": "The paper introduces GDPR-Bench-Android, a large annotated dataset for benchmarking GDPR violation detection methods in Android apps. It evaluates 11 automated approaches, finding that performance varies by task and method\u2014no single method dominates. The work provides key insights and resources for progressing automated GDPR compliance detection.", "motivation": "Detecting violations of the EU GDPR in software source code is important for data protection but lacks comprehensive automated tools and benchmarks, especially for Android applications.", "method": "The authors present GDPR-Bench-Android, a dataset with 1951 manually annotated violation instances from 15 open-source repositories, mapped to 23 GDPR articles. They offer granular annotations (file, module, line) and propose two evaluation tasks: violation localization (Accuracy@k metric) and snippet-level multi-label classification (macro-F1 and precision metrics). Eleven methods are benchmarked, including eight LLMs, a Formal-AST baseline, a RAG method, and an agentic ReAct method.", "result": "No single approach excels at all tasks. ReAct agent is best at file-level localization, Qwen2.5-72B is best at line-level, while Formal-AST's performance is low. For multi-label classification, Claude-Sonnet-4.5 leads on Macro-F1 and RAG on Macro-Precision. Results show that different methods have specific strengths depending on the task.", "conclusion": "GDPR-Bench-Android enables thorough, fair benchmarking of automated GDPR detection methods, revealing clear task-dependent strengths and weaknesses. This resource is essential for guiding further research and development for GDPR compliance automation in Android apps."}}
{"id": "2511.00624", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00624", "abs": "https://arxiv.org/abs/2511.00624", "authors": ["Haoyi Zhang", "Huaijin Ran", "Xunzhu Tang"], "title": "Can Large Language Models Detect Real-World Android Software Compliance Violations?", "comment": null, "summary": "The rapid development of Large Language Models (LLMs) has transformed\nsoftware engineering, showing promise in tasks like code generation, bug\ndetection, and compliance checking. However, current models struggle to detect\ncompliance violations in Android applications across diverse legal frameworks.\nWe propose \\emph{CompliBench}, a novel evaluation framework for assessing LLMs'\nability to detect compliance violations under regulations like LGPD, PDPA, and\nPIPEDA. The framework defines two tasks: Task 1 evaluates \\emph{retrieval and\nlocalization} at file, module, and line granularities, and Task 2 assesses\n\\emph{multi-label judgment} for code snippets. These tasks mirror the audit\nprocess, where auditors locate problematic code and determine implicated\nprovisions. Traditional metrics fail to capture important aspects like\ncross-granularity stability and jurisdictional consistency. Thus, we introduce\nstability-aware composites (SGS, RCS, CRGS, and OCS) for a more comprehensive\nassessment. Experiments with six models, including GPT-4O and Claude-3.5, show\n\\emph{CompliBench} improves compliance detection, with\nClaude-3.5-sonnet-20241022 achieving the highest OCS score (0.3295), and\nGemini-2.5-pro the lowest (0.0538). This work demonstrates \\emph{CompliBench}'s\npotential for improving LLM performance in compliance tasks and provides a\nfoundation for future tools aligned with data protection standards. Our project\nis available at https://github.com/Haoyi-Zhang/CompliBench.", "AI": {"tldr": "CompliBench is a new evaluation benchmark for detecting compliance violations in Android apps using LLMs, featuring tailored tasks and metrics for regulatory requirements. Testing with top models shows it improves assessment accuracy, paving the way for smarter compliance tools.", "motivation": "Existing Large Language Models (LLMs) have shown promise in software engineering tasks but struggle with detecting compliance violations in Android apps across different legal frameworks, necessitating better evaluation tools.", "method": "CompliBench is introduced as a novel evaluation framework to assess LLMs on compliance violation detection. It defines two tasks: one focused on retrieval and localization of problematic code, and another on multi-label judgment for legal implications. New composite metrics are introduced to evaluate cross-granularity stability and jurisdictional consistency.", "result": "Through experiments with six models (including GPT-4O and Claude-3.5), CompliBench was shown to improve compliance detection. Claude-3.5-sonnet-20241022 achieved the best performance, highlighting the effectiveness of the framework.", "conclusion": "CompliBench offers a more thorough method for evaluating LLMs in compliance tasks, addressing gaps in current metrics and providing a foundation for future improvements in data protection compliance tools."}}
{"id": "2511.00658", "categories": ["cs.SE", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.00658", "abs": "https://arxiv.org/abs/2511.00658", "authors": ["Guilherme H. Travassos", "Sabrina Rocha", "Rodrigo Feitosa", "Felipe Assis", "Patricia Goncalves", "Andre Gheventer", "Larissa Galeno", "Arthur Sasse", "Julio Cesar Guimaraes", "Carlos Brito", "Joao Pedro Wieland"], "title": "Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare", "comment": "11 pages, 2 figures, in Portuguese language", "summary": "The advances and availability of technologies involving Generative Artificial\nIntelligence (AI) are evolving clearly and explicitly, driving immediate\nchanges in various work activities. Software Engineering (SE) is no exception\nand stands to benefit from these new technologies, enhancing productivity and\nquality in its software development processes. However, although the use of\nGenerative AI in SE practices is still in its early stages, considering the\nlack of conclusive results from ongoing research and the limited technological\nmaturity, we have chosen to incorporate these technologies in the development\nof a web-based software system to be used in clinical trials by a thoracic\ndiseases research group at our university. For this reason, we decided to share\nthis experience report documenting our development team's learning journey in\nusing Generative AI during the software development process. Project\nmanagement, requirements specification, design, development, and quality\nassurance activities form the scope of observation. Although we do not yet have\ndefinitive technological evidence to evolve our development process\nsignificantly, the results obtained and the suggestions shared here represent\nvaluable insights for software organizations seeking to innovate their\ndevelopment practices to achieve software quality with generative AI.", "AI": {"tldr": "Generative AI is being explored in software development for clinical trials; although results are not yet definitive due to technological immaturity and ongoing research, the team's insights and documented learning journey offer practical suggestions for organizations aiming to enhance software quality with generative AI.", "motivation": "Generative AI technologies are rapidly evolving and impacting various work activities, including software engineering. The potential to enhance productivity and quality in software development motivates the exploration of its application, despite limited conclusive research and technological maturity.", "method": "The authors incorporated generative AI technologies in the development process of a web-based software system intended for clinical trials. They documented an experience report covering the team's learning journey, focusing on project management, requirements specification, design, development, and quality assurance.", "result": "There is no definitive technological evidence yet of significant process evolution, but the team's experiences yielded useful results and insights. These suggestions and results may benefit other software organizations looking to innovate their practices with generative AI to improve software quality.", "conclusion": "While the use of generative AI in software engineering, especially in clinical trial systems, is still immature and lacks conclusive evidence for substantial improvement, early experiences and observations provide valuable guidance for other organizations seeking to innovate."}}
{"id": "2511.00678", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00678", "abs": "https://arxiv.org/abs/2511.00678", "authors": ["Tasmia Zerin", "Moumita Asad", "B. M. Mainul Hossain", "Kazi Sakib"], "title": "Repairing Responsive Layout Failures Using Retrieval Augmented Generation", "comment": "Accepted at the 41st IEEE International Conference on Software\n  Maintenance and Evolution 2025 (ICSME'25)", "summary": "Responsive websites frequently experience distorted layouts at specific\nscreen sizes, called Responsive Layout Failures (RLFs). Manually repairing\nthese RLFs involves tedious trial-and-error adjustments of HTML elements and\nCSS properties. In this study, an automated repair approach, leveraging LLM\ncombined with domain-specific knowledge is proposed. The approach is named\nReDeFix, a Retrieval-Augmented Generation (RAG)-based solution that utilizes\nStack Overflow (SO) discussions to guide LLM on CSS repairs. By augmenting\nrelevant SO knowledge with RLF-specific contexts, ReDeFix creates a prompt that\nis sent to the LLM to generate CSS patches. Evaluation demonstrates that our\napproach achieves an 88\\% accuracy in repairing RLFs. Furthermore, a study from\nsoftware engineers reveals that generated repairs produce visually correct\nlayouts while maintaining aesthetics.", "AI": {"tldr": "Responsive layout failures (RLFs) in websites are difficult to fix manually. This paper proposes an automated repair approach (ReDeFix) that uses LLMs and Stack Overflow expertise to guide CSS repairs, achieving 88% accuracy and producing visually pleasing results.", "motivation": "Responsive websites often suffer from distorted layouts on certain screen sizes, called Responsive Layout Failures (RLFs), and current manual repair methods are tedious and inefficient.", "method": "The paper introduces ReDeFix, a Retrieval-Augmented Generation (RAG) approach that uses Large Language Models (LLMs) and incorporates relevant domain-specific knowledge from Stack Overflow discussions to generate prompts for automated CSS repairs.", "result": "ReDeFix achieves 88% accuracy in repairing RLFs. The study also finds that the generated CSS patches result in visually correct layouts that preserve aesthetic quality, as confirmed by software engineers.", "conclusion": "ReDeFix efficiently automates the repair of responsive layout failures using LLMs and crowdsourced CSS knowledge, providing accurate and aesthetically pleasing fixes with high reliability."}}
{"id": "2511.00706", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00706", "abs": "https://arxiv.org/abs/2511.00706", "authors": ["Marcos Vinicius Cruz", "Pragya Verma", "Grischa Liebel"], "title": "An Empirical Investigation of the Experiences of Dyslexic Software Engineers", "comment": null, "summary": "Dyslexia is a common learning disorder that primarily impairs an individual's\nreading and writing abilities. In adults, dyslexia can affect both professional\nand personal lives, often leading to mental challenges and difficulties\nacquiring and keeping work. In Software Engineering (SE), reading and writing\ndifficulties appear to pose substantial challenges for core tasks such as\nprogramming. However, initial studies indicate that these challenges may not\nsignificantly affect their performance compared to non-dyslexic colleagues.\nConversely, strengths associated with dyslexia could be particularly valuable\nin areas like programming and design. However, there is currently no work that\nexplores the experiences of dyslexic software engineers, and puts their\nstrengths into relation with their difficulties. To address this, we present a\nqualitative study of the experiences of dyslexic individuals in SE. We followed\nthe basic stage of the Socio-Technical Grounded Theory method and base our\nfindings on data collected through 10 interviews with dyslexic software\nengineers, 3 blog posts and 153 posts on the social media platform Reddit. We\nfind that dyslexic software engineers especially struggle at the programming\nlearning stage, but can succeed and indeed excel at many SE tasks once they\nmaster this step. Common SE-specific support tools, such as code completion and\nlinters are especially useful to these individuals and mitigate many of the\nexperienced difficulties. Finally, dyslexic software engineers exhibit\nstrengths in areas such as visual thinking and creativity. Our findings have\nimplications to SE practice and motivate several areas of future research in\nSE, such as investigating what makes code less/more understandable to dyslexic\nindividuals.", "AI": {"tldr": "Dyslexic software engineers face initial learning challenges in programming, but excel at SE tasks once they overcome these. They benefit from SE support tools and display strengths in creativity and visual thinking. Future research should focus on improving code accessibility for dyslexic professionals.", "motivation": "There is little research on the experiences of dyslexic software engineers and how their strengths and difficulties interact in software engineering tasks. The study aims to fill this gap.", "method": "A qualitative study using the Socio-Technical Grounded Theory method. Data were collected from 10 interviews with dyslexic software engineers, 3 blog posts, and 153 Reddit posts.", "result": "Dyslexic software engineers mainly struggle at the programming learning stage, but are able to succeed and excel at many tasks once past this hurdle. Support tools like code completion and linters are especially helpful. Dyslexic individuals show strengths in visual thinking and creativity.", "conclusion": "Dyslexic software engineers experience unique challenges mainly during learning, but possess valuable strengths that aid success in SE. Support tools mitigate difficulties, and further research into code understandability for dyslexic individuals is needed."}}
{"id": "2511.00776", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00776", "abs": "https://arxiv.org/abs/2511.00776", "authors": ["Cuiyun Gao", "Guodong Fan", "Chun Yong Chong", "Shizhan Chen", "Chao Liu", "David Lo", "Zibin Zheng", "Qing Liao"], "title": "A Systematic Literature Review of Code Hallucinations in LLMs: Characterization, Mitigation Methods, Challenges, and Future Directions for Reliable AI", "comment": null, "summary": "Model hallucination is one of the most critical challenges faced by Large\nLanguage Models (LLMs), especially in high-stakes code intelligence tasks. As\nLLMs become increasingly integrated into software engineering tasks,\nunderstanding and mitigating hallucination in code becomes essential. In this\nsurvey, we provide a systematic review of hallucination phenomena in\ncode-oriented LLMs from four key perspectives. First, we begin by surveying 60\npapers to define hallucination in the context of code and summarize its primary\ncauses, such as data noise, exposure bias, and insufficient semantic grounding,\nwhile also tracing recent trends in literature across natural language\nprocessing (NLP) and software engineering communities. Second, we review model\nhallucination surveys in a broader span and summarize representative\nhallucination mitigation strategies, such as knowledge-enhanced generation,\nconstrained decoding, and post-editing. Third, we review approaches targeted\nfor code intelligence and highlight code-specific challenges that aggravate\nhallucination, including syntax sensitivity, strict type systems, and\ndependence on external libraries. Meanwhile, we analyze how emerging code\nintelligence tasks, e.g., program analysis, symbolic execution, and unit\ntesting, are utilized to detect and mitigate hallucinations. Fourth, we\nsummarize current evaluation benchmarks, ranging from static metrics to dynamic\nchecks, e.g., compilation and execution correctness, and emphasize the need for\nhallucination-oriented benchmarks.", "AI": {"tldr": "This survey analyzes the causes, challenges, mitigation strategies, and evaluation methods for model hallucination in code-oriented large language models. It finds unique issues for code, reviews the latest research, and calls for better benchmarks and targeted solutions.", "motivation": "Model hallucination poses severe risks in applying Large Language Models (LLMs) to code intelligence tasks, which are increasingly critical in modern software engineering. Understanding and reducing hallucinations in this domain is vital for the safe and reliable automation of coding tasks.", "method": "The paper conducts a systematic survey of 60 papers, defining hallucination in code-oriented LLMs, analyzing its causes, reviewing mitigation strategies across both NLP and code domains, assessing code-specific challenges and detection methods, and summarizing current evaluation benchmarks.", "result": "The survey clarifies primary causes of hallucination (e.g., data noise, exposure bias, insufficient semantic grounding), compares mitigation approaches (e.g., knowledge-enhanced generation, constrained decoding, post-editing), identifies unique challenges for code intelligence (syntax, type systems, library dependencies), discusses how tasks like program analysis help in detection and mitigation, and highlights the limitations of current evaluation benchmarks.", "conclusion": "Current approaches to LLM hallucination in code are varied but not sufficient. There are unique challenges in code intelligence compared to general NLP, and more targeted mitigation strategies along with improved hallucination-specific evaluation benchmarks are needed for practical, reliable use."}}
{"id": "2511.00780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00780", "abs": "https://arxiv.org/abs/2511.00780", "authors": ["Chenyu Zhao", "Shenglin Zhang", "Zeshun Huang", "Weilin Jin", "Yongqian Sun", "Dan Pei", "Chaoyun Zhang", "Qingwei Lin", "Chetan Bansal", "Saravan Rajmohan", "Minghua Ma"], "title": "Can Language Models Go Beyond Coding? Assessing the Capability of Language Models to Build Real-World Systems", "comment": null, "summary": "Large language models (LLMs) have shown growing potential in software\nengineering, yet few benchmarks evaluate their ability to repair software\nduring migration across instruction set architectures (ISAs). Cross-ISA\nmigration, such as between x86_64 and aarch64, requires handling complex\ndependencies, heterogeneous toolchains, and long build logs while ensuring\nexecutable verification. To address this challenge, we present Build-bench, an\nend-to-end benchmark that systematically evaluates the capability of LLMs to\nrepair build failures in cross-ISA settings. Build-bench collects 268\nreal-world failed packages and integrates auxiliary tools including Structure\nExtraction, File Content Extraction, Content Modification, and Build\nVerification to support autonomous, tool-augmented reasoning. The repair\nprocess operates in an iterative loop where, upon failure, the model receives\nupdated build logs and previous repair outcomes to refine subsequent attempts.\nThrough a comparative evaluation of six representative LLMs, Build-bench\nreveals that current models achieve a maximum build success rate of 63% and\ntool usage patterns differ significantly across models. By coupling real build\nenvironments with verifiable outcomes, Build-bench establishes the first\narchitecture-aware benchmark for studying LLM-based software build and repair.", "AI": {"tldr": "Build-bench is a new benchmark to test how well LLMs fix build failures during software migration across CPU architectures. Studying 268 real failed builds, the tool finds current LLMs have a top 63% success rate, and provides the first systematic way to evaluate and improve LLM-assisted software repair in complex, real-world scenarios.", "motivation": "Large language models (LLMs) are increasingly used in software engineering, but their effectiveness in repairing software during migration between different instruction set architectures (ISAs) has been underexplored. This task is complex due to dependencies, toolchains, and build verification requirements.", "method": "The paper introduces Build-bench, a benchmark suite that evaluates LLMs' abilities to repair build failures caused by cross-ISA migration. It collects 268 real failed software packages and uses auxiliary tools for structure extraction, content extraction, modification, and automated build verification. The repair is modeled as an iterative process, where models receive feedback from build logs and attempts to improve repairs.", "result": "When tested on six major LLMs, Build-bench finds a maximum build success rate of 63%. The study also observes significant differences in how various LLMs utilize supporting tools during the repair process.", "conclusion": "Build-bench enables systematic, reproducible, architecture-aware evaluation of LLM-based repair tools in real build environments, filling a crucial gap in benchmarking LLMs for cross-ISA software migration."}}
{"id": "2511.00802", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00802", "abs": "https://arxiv.org/abs/2511.00802", "authors": ["Jie JW Wu", "Ayanda Patrick Herlihy", "Ahmad Saleem Mirza", "Ali Afoud", "Fatemeh Fard"], "title": "GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents", "comment": null, "summary": "With the software industry shifting toward a data-driven culture, online A/B\ntesting is a key tool for evaluating new technologies. However, deploying such\nexperiments requires substantial resources, may negatively impact users, and\ninvolves long data collection periods. To address this, \\textit{off-policy\nevaluation (OPE)}, or offline A/B testing, uses logged data to assess\ntechnologies and is fundamental in Reinforcement Learning, making it crucial in\ndomains where online testing is costly or risky, such as healthcare,\nrecommender systems, education, dialog systems, and robotics. Despite advances\nin coding LLMs and agentic AI, little is known about leveraging them to\noptimize OPE results. We investigate whether LLMs and LLM-based agents can\nimprove OPE performance via code optimization. We propose\n\\textit{GrowthHacker}, a benchmark with agent and baseline methods on\nlarge-scale real-world datasets, which iteratively optimizes code, evaluates\nresults, and begins new optimization cycles. We collected datasets, established\nprotocols, implemented baselines for OPE on the Open Bandit Pipeline\n(OBP)~\\cite{saito2021openbanditdatasetpipeline} and\nScope-RL~\\cite{kiyohara2023scope}, and developed the \\textit{two_agent}\nframework, which reduces system complexity while preserving optimization\neffectiveness. Results show the two_agent framework achieves 100% reliability\nand the highest average improvement of 106.7% among positive outcomes. Both\ntwo_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.\nThese findings demonstrate the feasibility of LLM-based agents as automated\n\"growth hackers\" to enhance OPE systems, with implications for scaling\ndata-driven decision-making in production.", "AI": {"tldr": "This paper proposes GrowthHacker, a benchmark framework that uses LLM-based agents to optimize code for off-policy evaluation. The two_agent system reliably enhances OPE performance on large datasets, outperforming baseline methods and other agent frameworks, indicating a promising future for scalable agentic optimization in data-driven industries.", "motivation": "Online A/B testing, although crucial for evaluating new technologies, is resource-intensive, risks negative user impact, and requires long data collection periods. Off-policy evaluation (OPE) offers an alternative using logged data, which is especially valuable in domains where online testing is costly or risky. Leveraging advances in Large Language Models (LLMs) and agentic AI for optimizing OPE has received little attention.", "method": "The authors introduce GrowthHacker, a benchmark with agent and baseline methods applied to large-scale real-world datasets. The approach involves iterative code optimization, result evaluation, and repeated improvement cycles. Protocols and baselines for OPE are established using Open Bandit Pipeline and Scope-RL datasets. The two_agent framework is developed to reduce system complexity while maintaining optimization effectiveness.", "result": "The two_agent framework achieved 100% reliability and demonstrated the highest average improvement of 106.7% among positive outcomes. Both two_agent and CrewAI frameworks attained a 45% success rate, exceeding AutoGen\u2019s 34%.", "conclusion": "LLM-based agents can serve as automated 'growth hackers' to improve OPE systems efficiently, supporting scalable and more effective data-driven decision making in production environments."}}
{"id": "2511.00839", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00839", "abs": "https://arxiv.org/abs/2511.00839", "authors": ["John Yang", "Kilian Lieret", "Joyce Yang", "Carlos E. Jimenez", "Ofir Press", "Ludwig Schmidt", "Diyi Yang"], "title": "CodeClash: Benchmarking Goal-Oriented Software Engineering", "comment": null, "summary": "Current benchmarks for coding evaluate language models (LMs) on concrete,\nwell-specified tasks such as fixing specific bugs or writing targeted tests.\nHowever, human programmers do not spend all day incessantly addressing isolated\ntasks. Instead, real-world software development is grounded in the pursuit of\nhigh-level goals, like improving user retention or reducing costs. Evaluating\nwhether LMs can also iteratively develop code to better accomplish open-ended\nobjectives without any explicit guidance remains an open challenge. To address\nthis, we introduce CodeClash, a benchmark where LMs compete in multi-round\ntournaments to build the best codebase for achieving a competitive objective.\nEach round proceeds in two phases: agents edit their code, then their codebases\ncompete head-to-head in a code arena that determines winners based on\nobjectives like score maximization, resource acquisition, or survival. Whether\nit's writing notes, scrutinizing documentation, analyzing competition logs, or\ncreating test suites, models must decide for themselves how to improve their\ncodebases both absolutely and against their opponents. We run 1680 tournaments\n(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal\nthat while models exhibit diverse development styles, they share fundamental\nlimitations in strategic reasoning. Models also struggle with long-term\ncodebase maintenance, as repositories become progressively messy and redundant.\nThese limitations are stark: top models lose every round against expert human\nprogrammers. We open-source CodeClash to advance the study of autonomous,\ngoal-oriented code development.", "AI": {"tldr": "CodeClash, a new benchmark for language models, tests their ability to autonomously build codebases toward open-ended goals. Eight models competed in thousands of rounds, but all showed fundamental weaknesses in strategy and code maintenance, being consistently outperformed by expert humans. The benchmark sets the stage for better evaluation and development of autonomous coding agents.", "motivation": "Existing code evaluation benchmarks for LMs focus on isolated, well-defined tasks, which does not reflect the high-level, open-ended goals of real-world software development. There is a need to assess whether LMs can autonomously and iteratively develop code toward complex objectives without explicit step-by-step guidance.", "method": "The authors introduce CodeClash, a benchmark where LMs compete in multi-round tournaments to build and improve codebases with competitive objectives. Each round includes two phases: code editing and direct competition in a code arena. Models self-decide how to enhance their codebases, aiming to maximize specific objectives. The benchmark involves running 1,680 tournaments (25,200 rounds) across six competitive arenas, evaluating eight different LMs.", "result": "Models demonstrated diverse approaches to code development, but exhibited significant limitations in strategic reasoning and long-term maintenance. As projects progressed, codebases generated by LMs became increasingly disorganized and redundant. Top-performing LMs consistently failed to surpass expert human programmers in every round.", "conclusion": "Current language models, even at their best, are notably deficient in autonomous, goal-oriented software development tasks. They struggle particularly with strategic planning and maintaining clean codebases over time. The open-source CodeClash benchmark provides a new standard for evaluating these capabilities and exposes key areas for improvement in future LMs."}}
{"id": "2511.00872", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00872", "abs": "https://arxiv.org/abs/2511.00872", "authors": ["Zhuowen Yin", "Cuifeng Gao", "Chunsong Fan", "Wenzhang Yang", "Yinxing Xue", "Lijun Zhang"], "title": "A Comprehensive Empirical Evaluation of Agent Frameworks on Code-centric Software Engineering Tasks", "comment": null, "summary": "Unlike traditional automation tools or static LLM-based systems, agents\ncombine decision-making and tool utilization to accomplish complex tasks,\nshowing great potential in software engineering. However, existing studies\nlargely focus on specific tasks or isolated aspects, providing an incomplete\npicture of agents' practical capabilities. To address this, we conduct a\ncomprehensive empirical study evaluating seven general-purpose agent frameworks\nacross three representative code-centric tasks: software development,\nvulnerability detection, and program repair. Each task is assessed using\nstandard, widely adopted benchmarks to ensure objective and comparable\nevaluation. Agent performance is systematically analyzed from three\ncomplementary perspectives: effectiveness (task success), efficiency (execution\nprocess), and overhead (token consumption). Our findings reveal distinct\ncapability patterns and trade-offs among the evaluated frameworks. In terms of\neffectiveness, agents achieve moderate overall performance. Regarding\nefficiency, AgentOrchestra tends to exhibit the longest trajectories and the\nmost correction attempts due to coordination overhead, whereas OpenHands\ndemonstrate stronger reflective reasoning abilities. For overhead, software\ndevelopment incurs the highest monetary cost, while GPTswarm remains the most\ncost-efficient. Furthermore, we conduct an in-depth cross-analysis of the\nrelationship between effectiveness and efficiency, exploring the underlying\nreasons behind their interplay. These findings guide both practical adoption\nand future research toward more efficient software engineering agents.", "AI": {"tldr": "This paper compares seven agent frameworks for software engineering tasks, revealing varied strengths in effectiveness, efficiency, and cost. No framework is best overall; trade-offs exist, informing future adoption and research.", "motivation": "Traditional automation and LLM-based systems have limited scope in handling complex software engineering tasks, often focusing only on specific or isolated problem areas. There's a need for a holistic understanding of how agent frameworks perform across diverse, realistic scenarios.", "method": "A comprehensive empirical study was conducted comparing seven general-purpose agent frameworks on three representative code-centric tasks: software development, vulnerability detection, and program repair. Standard benchmarks were used to ensure fair evaluation, and agent performance was analyzed from perspectives of effectiveness, efficiency, and overhead.", "result": "The study identified distinct patterns and trade-offs among agents. Overall effectiveness was moderate; AgentOrchestra showed longest task trajectories and most corrections (high overhead), while OpenHands excelled in reflective reasoning. Software development was the most costly in terms of token consumption, but GPTswarm was the most cost-efficient. Detailed cross-analysis explained how effectiveness and efficiency interrelate.", "conclusion": "Agents present promising, but varied, capabilities for software engineering tasks. Understanding their trade-offs in effectiveness, efficiency, and cost is crucial for both practical deployment and guiding research towards more efficient agent designs."}}
{"id": "2511.00901", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00901", "abs": "https://arxiv.org/abs/2511.00901", "authors": ["Vincenzo De Martino", "Stefano Lambiase", "Fabiano Pecorelli", "Willem-Jan van den Heuvel", "Filomena Ferrucci", "Fabio Palomba"], "title": "Sustainability of Machine Learning-Enabled Systems: The Machine Learning Practitioner's Perspective", "comment": null, "summary": "Software sustainability is a key multifaceted non-functional requirement that\nencompasses environmental, social, and economic concerns, yet its integration\ninto the development of Machine Learning (ML)-enabled systems remains an open\nchallenge. While previous research has explored high-level sustainability\nprinciples and policy recommendations, limited empirical evidence exists on how\nsustainability is practically managed in ML workflows. Existing studies\npredominantly focus on environmental sustainability, e.g., carbon footprint\nreduction, while missing the broader spectrum of sustainability dimensions and\nthe challenges practitioners face in real-world settings. To address this gap,\nwe conduct an empirical study to characterize sustainability in ML-enabled\nsystems from a practitioner's perspective. We investigate (1) how ML engineers\nperceive and describe sustainability, (2) the software engineering practices\nthey adopt to support it, and (3) the key challenges hindering its adoption. We\nfirst perform a qualitative analysis based on interviews with eight experienced\nML engineers, followed by a large-scale quantitative survey with 203 ML\npractitioners. Our key findings reveal a significant disconnection between\nsustainability awareness and its systematic implementation, highlighting the\nneed for more structured guidelines, measurement frameworks, and regulatory\nsupport.", "AI": {"tldr": "Software sustainability in ML systems is recognized but poorly implemented, mainly due to missing guidelines, measurement tools, and regulations. This study\u2014via interviews and surveys\u2014reveals practitioners understand sustainability broadly but struggle to apply it. More structure and support are needed to close the awareness-implementation gap.", "motivation": "There is a growing recognition of the importance of software sustainability\u2014including environmental, social, and economic dimensions\u2014in ML-enabled systems, but its practical integration and management remain underexplored. Existing research mostly focuses on environmental aspects, neglecting other key sustainability concerns and real-world practitioner challenges.", "method": "The authors conducted a mixed-methods empirical study, including qualitative interviews with eight experienced ML engineers and a large-scale quantitative survey of 203 ML practitioners, to explore how sustainability is perceived, described, and managed in ML workflows.", "result": "The study found a significant gap between practitioners' awareness of sustainability and its practical implementation. Key challenges include the lack of structured guidelines, measurement frameworks, and regulatory support for systematically integrating sustainability into ML-enabled system development.", "conclusion": "There is an urgent need for clearer guidelines, standardized measurement frameworks, and better regulatory support to bridge the gap between awareness and actionable sustainability in ML-enabled systems. Addressing these needs will help practitioners systematically integrate diverse sustainability concerns into their workflows."}}
{"id": "2511.00915", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00915", "abs": "https://arxiv.org/abs/2511.00915", "authors": ["Jukka Ruohonen", "Abhishek Tiwari"], "title": "Empirical Derivations from an Evolving Test Suite", "comment": "Submitted", "summary": "The paper presents a longitudinal empirical analysis of the automated,\ncontinuous, and virtualization-based software test suite of the NetBSD\noperating system. The longitudinal period observed spans from the initial roll\nout of the test suite in the early 2010s to late 2025. According to the\nresults, the test suite has grown continuously, currently covering over ten\nthousand individual test cases. Failed test cases exhibit overall stability,\nalthough there have been shorter periods marked with more frequent failures. A\nsimilar observation applies to build failures, failures of the test suite to\ncomplete, and installation failures, all of which are also captured by the\nNetBSD's testing framework. Finally, code churn and kernel modifications do not\nprovide longitudinally consistent statistical explanations for the failures.\nAlthough some periods exhibit larger effects, including particularly with\nrespect to the kernel modifications, the effects are small on average. Even\nthough only in an exploratory manner, these empirical observations contribute\nto efforts to draw conclusions from large-scale and evolving software test\nsuites.", "AI": {"tldr": "This paper analyzes NetBSD's test suite over more than a decade, finding its coverage has expanded and failure rates have remained largely stable. Kernel and code changes are not strong long-term predictors of failures. The work provides exploratory insights into the management and stability of large, continuously evolving test suites.", "motivation": "The motivation is to understand the effectiveness and stability of the NetBSD operating system's automated test suite over a long period. This study seeks to empirically analyze how test coverage and failure rates evolve in a large-scale, continuously growing software test suite.", "method": "The paper uses a longitudinal empirical analysis, examining test suite records and results from the early 2010s to late 2025. It analyzes the frequency and nature of test case failures, build failures, installation failures, and relates these to code churn and kernel modifications.", "result": "The test suite has grown to cover over ten thousand test cases. The frequency and numbers of failed test cases, build failures, installation failures, and test suite completion issues are generally stable, with some shorter periods of increased failures. Code churn and kernel modifications are not consistent predictors of failures, though spikes are observed at times.", "conclusion": "On average, kernel and code changes do not have a substantial long-term impact on test suite failures. The empirical findings enhance the understanding of evolving, large-scale test suites, though the analysis is mainly exploratory."}}
{"id": "2511.01043", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.01043", "abs": "https://arxiv.org/abs/2511.01043", "authors": ["Zihan Fang", "Yifan Zhang", "Yueke Zhang", "Kevin Leach", "Yu Huang"], "title": "DPO-F+: Aligning Code Repair Feedback with Developers' Preferences", "comment": "10 pages, 2 figures", "summary": "Large Language Models (LLMs) are increasingly applied to software engineering\ntasks, especially code repair. However, developers often struggle to interpret\nmodel outputs, limiting effective human-AI teaming. Prior work largely\noptimizes repaired code while under-addressing the natural-language feedback\nthat enables comprehension and iterative improvement. We present DPO-f+, a\nnovel framework that aligns code-repair feedback with developer needs and\nprofiles. It (1) formalizes developer-profiled, domain-specific metrics for\nfeedback alignment; (2) automatically constructs pairwise preference datasets\nfrom code-repair tasks; (3) fine-tunes using Direct Preference Optimization\n(DPO) augmented with a lightweight margin signal; and (4) provides an automated\nfeedback evaluation protocol. Empirically, DPO-f+ outperforms both the baseline\nand standard DPO on generated-code accuracy and overall feedback alignment. On\nnovice programming tasks, DPO-f+ raises the top-1 pass rate by 5.71 percentage\npoints (pp) over the baseline and by 3.30 pp over DPO. On the more challenging\nSWE-bench Lite benchmark, it increases the issue-resolution rate by 1.67 pp\nover DPO and by 4.67 pp over the baseline. It also achieves the largest\nimprovement in feedback alignment, outperforming DPO and the baseline. By\naligning feedback more closely with developer needs, DPO-f+ turns LLM-assisted\nrepair from one-shot outputs into a collaborative sensemaking workflow,\nproviding a practical approach to enhancing code comprehension and fostering\nmore effective human-AI teaming in software engineering.", "AI": {"tldr": "DPO-f+ framework makes code-repair feedback from LLMs more understandable and helpful for developers, leading to improved accuracy and collaboration compared to previous methods.", "motivation": "Current LLM-based code repair tools do not adequately support developers in interpreting model outputs, particularly in terms of natural-language feedback needed for iterative improvement and comprehension.", "method": "The paper introduces DPO-f+, which formalizes developer-profiled, domain-specific metrics for feedback; auto-generates preference datasets from code-repair tasks; fine-tunes LLMs using Direct Preference Optimization with a margin signal; and includes an automated feedback evaluation protocol.", "result": "Empirical evaluation shows DPO-f+ boosts top-1 pass rates and feedback alignment, outperforming baseline and standard DPO approaches on both novice and complex software engineering tasks.", "conclusion": "DPO-f+ significantly improves code-repair accuracy and the alignment of model feedback with developer needs, resulting in better code comprehension and enhanced human-AI collaboration."}}
{"id": "2511.01047", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01047", "abs": "https://arxiv.org/abs/2511.01047", "authors": ["Yu Shi", "Hao Li", "Bram Adams", "Ahmed E. Hassan"], "title": "HAFixAgent: History-Aware Automated Program Repair Agent", "comment": "31 pages, 6 figures", "summary": "Automated program repair (APR) has recently shifted toward large language\nmodels and agent-based systems, yet most systems rely on local snapshot\ncontext, overlooking repository history. Prior work shows that repository\nhistory helps repair single-line bugs, since the last commit touching the buggy\nline is often the bug-introducing one. In this paper, we investigate whether\nrepository history can also improve agentic APR systems at scale, especially\nfor complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing\nAgent that injects blame-derived repository heuristics into its repair loop. A\npreliminary study of all 854 real-world bugs from Defects4J motivates our\ndesign, showing that bug-relevant history is both widely available and highly\nconcentrated. Empirical comparison of HAFixAgent with two state-of-the-art\nbaselines shows: (1) Effectiveness: HAFixAgent significantly improves over the\nagent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)\nEfficiency: history does not significantly increase agent steps and keeps token\ncosts comparable, with notably lower median costs for complex\nmulti-file-multi-hunk bugs. (3) Practicality: combining different historical\nheuristics repairs more bugs, offering a clear cost-benefit trade-off.\nHAFixAgent offers a practical recipe for history-aware agentic APR: ground the\nagent in version control history, prioritize diff-based historical context, and\nintegrate complementary heuristics when needed.", "AI": {"tldr": "HAFixAgent, a history-aware automated repair agent, uses repository history to fix bugs more effectively and efficiently than current methods, with especially strong results on complex bugs by integrating blame-derived heuristics.", "motivation": "Most current automated program repair systems using large language models and agents ignore repository history, even though prior work shows it is useful for fixing single-line bugs.", "method": "The authors design HAFixAgent, a history-aware bug-fixing agent that incorporates repository 'blame' information and heuristics derived from version control history into its agentic repair loop. They analyze 854 bugs from Defects4J to justify their approach and empirically compare their agent with two state-of-the-art baselines.", "result": "HAFixAgent outperforms agent-based and multi-hunk baselines, improving repair effectiveness by 212.3% and 29.9%, respectively. It achieves these gains without significant increases in agent repair steps or token costs, and even lowers costs for complex bug cases. Combining different historical heuristics repairs more bugs.", "conclusion": "Integrating repository history, specifically blame-derived heuristics, into agent-based APR systems significantly improves both effectiveness and practicality, particularly for complex bugs. The approach is efficient and cost-effective, providing clear guidance on leveraging version control history in automated repairs."}}
{"id": "2511.01104", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01104", "abs": "https://arxiv.org/abs/2511.01104", "authors": ["Yujian Liu", "Jiabao Ji", "Yang Zhang", "Wenbo Guo", "Tommi Jaakkola", "Shiyu Chang"], "title": "HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning", "comment": null, "summary": "Existing LLM-based automatic test generation methods mainly produce input and\nexpected output pairs to categorize the intended behavior of correct programs.\nAlthough straightforward, these methods have limited diversity in generated\ntests and cannot provide enough debugging information. We propose HarnessLLM, a\ntwo-stage training pipeline that enables LLMs to write harness code for\ntesting. Particularly, LLMs generate code that synthesizes inputs and validates\nthe observed outputs, allowing complex test cases and flexible output\nvalidation such as invariant checking. To achieve this, we train LLMs with SFT\nfollowed by RLVR with a customized reward design. Experiments show that\nHarnessLLM outperforms input-output-based testing in bug finding and testing\nstrategy diversity. HarnessLLM further benefits the code generation performance\nthrough test-time scaling with our generated test cases as inference-phase\nvalidation. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/HarnessLLM.git.", "AI": {"tldr": "HarnessLLM overcomes limitations in existing LLM-based test generation by training LLMs to create harness code for more diverse and effective testing, leading to better bug finding and code generation.", "motivation": "Existing LLM-based automatic test generation primarily focuses on generating input-output pairs, which limits test diversity and debugging capabilities.", "method": "HarnessLLM introduces a two-stage training pipeline for LLMs: first using supervised fine-tuning (SFT), then Reinforcement Learning with Validation Rewards (RLVR) and customized reward design. This enables LLMs to generate harness code that synthesizes inputs and validates outputs, supporting complex test cases and flexible validation methods like invariant checking.", "result": "HarnessLLM produces more diverse and effective test cases, finds more bugs, and improves code generation performance by using generated test cases for inference-phase validation. It outperforms traditional input-output-based testing methods in both bug finding and testing strategy diversity.", "conclusion": "HarnessLLM advances LLM-based automatic test generation by enabling harness code generation, improving debugging support, test diversity, and code generation accuracy."}}
{"id": "2511.01176", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.01176", "abs": "https://arxiv.org/abs/2511.01176", "authors": ["Wenqing Zhu", "Norihiro Yoshida", "Eunjong Choi", "Yutaka Matsubara", "Hiroaki Takada"], "title": "An Empirical Study of LLM-Based Code Clone Detection", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious software engineering tasks, such as code generation and debugging,\nbecause of their ability to translate between programming languages and natural\nlanguages. Existing studies have demonstrated the effectiveness of LLMs in code\nclone detection. However, two crucial issues remain unaddressed: the ability of\nLLMs to achieve comparable performance across different datasets and the\nconsistency of LLMs' responses in code clone detection. To address these\nissues, we constructed seven code clone datasets and then evaluated five LLMs\nin four existing prompts with these datasets. The datasets were created by\nsampling code pairs using their Levenshtein ratio from two different code\ncollections, CodeNet and BigCloneBench. Our evaluation revealed that although\nLLMs perform well in CodeNet-related datasets, with o3-mini achieving a 0.943\nF1 score, their performance significantly decreased in BigCloneBench-related\ndatasets. Most models achieved a high response consistency, with over 90\\% of\njudgments remaining consistent across all five submissions. The fluctuations of\nthe F1 score affected by inconsistency are also tiny; their variations are less\nthan 0.03.", "AI": {"tldr": "LLMs are reliable and consistent for code clone detection in some datasets, but their effectiveness varies with dataset differences, particularly struggling on BigCloneBench compared to CodeNet.", "motivation": "Despite the effectiveness of large language models (LLMs) in code clone detection, their performance consistency across different datasets and the reliability of their responses remain unexplored.", "method": "The authors constructed seven code clone datasets by sampling code pairs using the Levenshtein ratio from two code collections, CodeNet and BigCloneBench, and evaluated five LLMs using four established prompts.", "result": "LLMs performed well on CodeNet datasets (with o3-mini reaching 0.943 F1 score) but saw significant performance drops on BigCloneBench datasets. Most models exhibited high response consistency (over 90%), with minor F1 score variation (< 0.03) due to inconsistency.", "conclusion": "LLMs show strong code clone detection performance and consistent responses on certain datasets (like CodeNet), but performance does not generalize across all datasets, specifically declining on BigCloneBench."}}
{"id": "2511.01252", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.01252", "abs": "https://arxiv.org/abs/2511.01252", "authors": ["Siyuan Li", "Yaowen Zheng", "Hong Li", "Jingdong Guo", "Chaopeng Dong", "Chunpeng Yan", "Weijie Wang", "Yimo Ren", "Limin Sun", "Hongsong Zhu"], "title": "Lares: LLM-driven Code Slice Semantic Search for Patch Presence Testing", "comment": null, "summary": "In modern software ecosystems, 1-day vulnerabilities pose significant\nsecurity risks due to extensive code reuse. Identifying vulnerable functions in\ntarget binaries alone is insufficient; it is also crucial to determine whether\nthese functions have been patched. Existing methods, however, suffer from\nlimited usability and accuracy. They often depend on the compilation process to\nextract features, requiring substantial manual effort and failing for certain\nsoftware. Moreover, they cannot reliably differentiate between code changes\ncaused by patches or compilation variations. To overcome these limitations, we\npropose Lares, a scalable and accurate method for patch presence testing. Lares\nintroduces Code Slice Semantic Search, which directly extracts features from\nthe patch source code and identifies semantically equivalent code slices in the\npseudocode of the target binary. By eliminating the need for the compilation\nprocess, Lares improves usability, while leveraging large language models\n(LLMs) for code analysis and SMT solvers for logical reasoning to enhance\naccuracy. Experimental results show that Lares achieves superior precision,\nrecall, and usability. Furthermore, it is the first work to evaluate patch\npresence testing across optimization levels, architectures, and compilers. The\ndatasets and source code used in this article are available at\nhttps://github.com/Siyuan-Li201/Lares.", "AI": {"tldr": "Lares introduces an LLM- and SMT-driven method to accurately and scalably detect patch presence in binaries, outperforming previous approaches and bypassing the need for compilation-derived features.", "motivation": "1-day vulnerabilities remain a significant risk due to widespread code reuse. Simply detecting vulnerable functions in binaries isn't sufficient; it's essential to know if these vulnerabilities are patched. Existing solutions are inaccurate, labor-intensive, and often depend on the compilation process, which isn't always feasible or reliable.", "method": "Lares is proposed: a method that extracts features directly from patch source code and finds semantically equivalent code in the pseudocode of the target binary, skipping the compilation dependency. It utilizes large language models (LLMs) for code analysis and SMT solvers for logical reasoning.", "result": "Lares demonstrates higher precision, recall, and usability compared to prior methods, and is the first to validate its effectiveness across different architectures, optimization levels, and compilers.", "conclusion": "Lares offers a scalable, accurate, and practical solution for patch presence testing in binaries without relying on the compilation process, significantly improving upon previous techniques."}}
{"id": "2511.01316", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01316", "abs": "https://arxiv.org/abs/2511.01316", "authors": ["Chong Wang", "Chen Zhang", "Jiajun Wu", "Wunan Guo", "Jianfeng Qu", "Yewen Tian", "Yang Liu"], "title": "Exploringand Unleashing the Power of Large Language Models in CI/CD Configuration Translation", "comment": null, "summary": "Continuous Integration (CI) is a cornerstone of modern collaborative software\ndevelopment, and numerous CI platforms are available. Differences in\nmaintenance overhead, reliability, and integration depth with code-hosting\nplatforms make migration between CI platforms a common practice. A central step\nin migration is translating CI configurations, which is challenging due to the\nintrinsic complexity of CI configurations and the need to understand semantic\ndifferences and relationships across CI platforms.\n  With the advent of large language models (LLMs), recent advances in software\nengineering highlight their potential for CI configuration translation. In this\npaper, we present a study on LLM-based CI configuration translation, focusing\non the migration from Travis CI to GitHub Actions. First, using 811 migration\nrecords, we quantify the effort involved and find that developers read an\naverage of 38 lines of Travis configuration and write 58 lines of GitHub\nActions configuration, with nearly half of the migrations requiring multiple\ncommits. We further analyze translations produced by each of the four LLMs and\nidentify 1,121 issues grouped into four categories: logic inconsistencies\n(38%), platform discrepancies (32%), environment errors (25%), and syntax\nerrors (5%). Finally, we evaluate three enhancement strategies and show that\ncombining guideline-based prompting with iterative refinement achieves the best\nperformance, reaching a Build Success Rate of 75.5%-nearly a threefold\nimprovement over GPT-4o with a basic prompt.", "AI": {"tldr": "Migrating CI configurations between platforms like Travis CI and GitHub Actions is common but complex. LLMs can help but make many mistakes. Combining guidelines and iterative refinement boosts success rates significantly, demonstrating a practical way to automate and improve CI migration.", "motivation": "Migrating CI configurations between different platforms (e.g., Travis CI to GitHub Actions) is a complex and frequent necessity due to platform differences. This process is challenging because of semantic differences, configuration complexity, and the need to minimize developer effort.", "method": "The study analyzes actual migration records (811 cases) to quantify effort, then systematically evaluates translations produced by four different large language models (LLMs). They categorize 1,121 issues found in these translations, and test three enhancement strategies to improve translation quality.", "result": "Typical migrations require reading 38 lines and writing 58 lines of configuration, half involve multiple commits. LLM-generated translations show substantial issues: logic inconsistencies (38%), platform discrepancies (32%), environment errors (25%), syntax errors (5%). Combining guideline-based prompting with iterative refinement significantly improves results, achieving a 75.5% Build Success Rate, almost three times higher than GPT-4o with a basic prompt.", "conclusion": "LLMs can be leveraged for CI configuration translation, but naive application yields many errors. Guided, iterative approaches greatly enhance accuracy and reliability, making LLM-assisted migration a promising direction for reducing developer effort."}}
{"id": "2511.01324", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.01324", "abs": "https://arxiv.org/abs/2511.01324", "authors": ["Lekshmi Murali Rani", "Richard Berntsson Svensson", "Robert Feldt"], "title": "AI for Requirements Engineering: Industry adoption and Practitioner perspectives", "comment": "Accepted at the Intelligent Software Engineering (ISE) 2025 Workshop\n  at the Automated Software Engineering (ASE) 2025 Conference", "summary": "The integration of AI for Requirements Engineering (RE) presents significant\nbenefits but also poses real challenges.Although RE is fundamental to software\nengineering, limited research has examined AI adoption in RE.We surveyed 55\nsoftware practitioners to map AI usage across four RE phases:Elicitation,\nAnalysis, Specification, and Validation, and four approaches for decision\nmaking: human only decisions, AI validation, Human AI Collaboration (HAIC), and\nfull AI automation.Participants also shared their perceptions, challenges, and\nopportunities when applying AI for RE tasks.Our data show that 58.2% of\nrespondents already use AI in RE, and 69.1% view its impact as positive or very\npositive.HAIC dominates practice, accounting for 54.4% of all RE techniques,\nwhile full AI automation remains minimal at 5.4%.Passive AI validation (4.4 to\n6.2%) lags even further behind, indicating that practitioners value AI's active\nsupport over passive oversight.These findings suggest that AI is most effective\nwhen positioned as a collaborative partner rather than a replacement for human\nexpertise.It also highlights the need for RE specific HAIC frameworks along\nwith robust and responsible AI governance as AI adoption in RE grows.", "AI": {"tldr": "AI adoption in requirements engineering is already significant and well-perceived, with practitioners favoring collaborative (HAIC) models over full automation or passive AI support. This underscores the importance of developing collaborative frameworks and responsible governance for AI in RE.", "motivation": "Despite the fundamental role of Requirements Engineering (RE) in software engineering, there is limited research on how AI is adopted in RE practices. Understanding this integration is crucial as AI presents both opportunities and challenges for improving RE processes.", "method": "The authors conducted a survey with 55 software practitioners to investigate AI usage across four RE phases (Elicitation, Analysis, Specification, and Validation) and four decision-making approaches (human only, AI validation, Human-AI Collaboration - HAIC, and full AI automation). They also collected perceptions, challenges, and opportunities regarding AI in RE.", "result": "58.2% of respondents already use AI in RE, and 69.1% perceive its impact as positive or very positive. Human-AI Collaboration (HAIC) is the predominant approach, making up 54.4% of RE techniques. Full AI automation and passive AI validation are used minimally, indicating practitioners' preference for AI as an active collaborative tool rather than a passive observer or full replacement for humans.", "conclusion": "AI is most effective in RE when used as a collaborative partner alongside human expertise, rather than a complete replacement or solely as passive validation. There is a clear need for RE-specific HAIC frameworks and strong AI governance as adoption expands."}}
{"id": "2511.01348", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01348", "abs": "https://arxiv.org/abs/2511.01348", "authors": ["Robin Gr\u00f6pler", "Steffen Klepke", "Jack Johns", "Andreas Dreschinski", "Klaus Schmid", "Benedikt Dornauer", "Eray T\u00fcz\u00fcn", "Joost Noppen", "Mohammad Reza Mousavi", "Yongjian Tang", "Johannes Viehmann", "Selin \u015eirin Aslang\u00fcl", "Beum Seuk Lee", "Adam Ziolkowski", "Eric Zie"], "title": "The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project", "comment": "Submitted to 2nd IEEE/ACM International Conference on AI-powered\n  Software (AIware 2025)", "summary": "Generative AI (GenAI) has recently emerged as a groundbreaking force in\nSoftware Engineering, capable of generating code, suggesting fixes, and\nsupporting quality assurance. While its use in coding tasks shows considerable\npromise, applying GenAI across the entire Software Development Life Cycle\n(SDLC) has not yet been fully explored. Critical uncertainties in areas such as\nreliability, accountability, security, and data privacy demand deeper\ninvestigation and coordinated action. The GENIUS project, comprising over 30\nEuropean industrial and academic partners, aims to address these challenges by\nadvancing AI integration across all SDLC phases. It focuses on GenAI's\npotential, the development of innovative tools, and emerging research\nchallenges, actively shaping the future of software engineering. This vision\npaper presents a shared perspective on the future of GenAI-based software\nengineering, grounded in cross-sector dialogue and experience within the GENIUS\nconsortium, supported by an exploratory literature review. The paper explores\nfour central elements: (1) a structured overview of current challenges in GenAI\nadoption across the SDLC; (2) a forward-looking vision outlining key\ntechnological and methodological advances expected over the next five years;\n(3) anticipated shifts in the roles and required skill sets of software\nprofessionals; and (4) the contribution of GENIUS in realizing this\ntransformation through practical tools and industrial validation. By aligning\ntechnical innovation with business relevance, this paper aims to inform both\nresearch agendas and industrial strategies, providing a foundation for\nreliable, scalable, and industry-ready GenAI solutions for software engineering\nteams.", "AI": {"tldr": "This vision paper provides a roadmap for integrating Generative AI throughout the Software Development Life Cycle, identifying current challenges and future opportunities, and advancing both research and industrial practice through the GENIUS consortium's initiatives.", "motivation": "Generative AI holds significant promise for software engineering, especially in coding tasks. However, its application throughout the entire Software Development Life Cycle (SDLC) remains underexplored, particularly in terms of reliability, accountability, security, and data privacy.", "method": "The authors present a vision paper that integrates cross-sector dialogue, experiences from the multinational GENIUS consortium, and an exploratory literature review. They examine current challenges, forecast future advances, analyze shifts in professional roles, and outline GENIUS's contributions through tools and industrial validation.", "result": "The paper delivers a structured overview of GenAI's adoption challenges, offers a forward-looking vision for technological and methodological advances, anticipates the evolution of software professionals' roles, and details how GENIUS's efforts can drive this comprehensive transformation.", "conclusion": "Aligning technical innovation with business needs, the paper argues that a collaborative, holistic approach is critical for developing scalable, reliable, and industry-ready GenAI solutions for software engineering teams."}}
{"id": "2511.01395", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.01395", "abs": "https://arxiv.org/abs/2511.01395", "authors": ["Maimouna Tamah Diao", "Moustapha Awwalou Diouf", "Iyiola Emmanuel Olatunji", "Abdoul Kader Kabor\u00e9", "Gervais Mendy", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Characterizing Build Compromises Through Vulnerability Disclosure Analysis", "comment": null, "summary": "The software build process transforms source code into deployable artifacts,\nrepresenting a critical yet vulnerable stage in software development. Build\ninfrastructure security poses unique challenges: the complexity of\nmulti-component systems (source code, dependencies, build tools), the\ndifficulty of detecting intrusions during compilation, and prevalent build\nnon-determinism that masks malicious modifications. Despite these risks, the\nsecurity community lacks a systematic understanding of build-specific attack\nvectors, hindering effective defense design.\n  This paper presents an empirically-derived taxonomy of attack vectors\ntargeting the build process, constructed through a large-scale CVE mining (of\n621 vulnerability disclosures from the NVD database). We categorize attack\nvectors by their injection points across the build pipeline, from source code\nmanipulation to compiler compromise. To validate our taxonomy, we analyzed 168\ndocumented software supply chain attacks, identifying 40 incidents specifically\ntargeting build phases. Our analysis reveals that 23.8\\% of supply chain\nattacks exploit build vulnerabilities, with dependency confusion and build\nscript injection representing the most prevalent vectors.\n  Dataset available at:\nhttps://anonymous.4open.science/r/Taxonomizing-Build-Attacks-8BB0.", "AI": {"tldr": "This paper analyzes vulnerabilities in the software build process, creating an attack taxonomy using large-scale CVE mining and supply chain attack analysis. Nearly a quarter of supply chain attacks exploit build vulnerabilities, mainly via dependency confusion and build script injection. The results point to urgent needs for security improvements in build infrastructure.", "motivation": "The build process is essential in software development but is vulnerable to unique security threats, which are not systematically understood by the security community.", "method": "The authors mined 621 CVEs from the NVD database to empirically derive a taxonomy of build process attack vectors. They further validated this taxonomy by analyzing 168 documented software supply chain attacks.", "result": "The study found that 23.8% of supply chain attacks target build vulnerabilities, with dependency confusion and build script injection being the most common attack vectors.", "conclusion": "The systematic taxonomy and empirical evidence reveal significant gaps in build infrastructure security, highlighting the need for targeted defenses and better understanding of build-specific attacks."}}
{"id": "2511.01417", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.01417", "abs": "https://arxiv.org/abs/2511.01417", "authors": ["Bassel Rafie", "Christian Schindler", "Andreas Rausch"], "title": "VeriODD: From YAML to SMT-LIB - Automating Verification of Operational Design Domains", "comment": null, "summary": "Operational Design Domains (ODDs) define the conditions under which an\nAutomated Driving System (ADS) is allowed to operate, while Current Operational\nDomains (CODs) capture the actual runtime situation. Ensuring that a COD\ninstance lies within the ODD is a crucial step in safety assurance. Today, ODD\nand COD specifications are frequently expressed in YAML to remain accessible\nfor stakeholders, but such descriptions are not directly suitable for\nsolver-based verification. Manual translation into formal languages such as\nSMT-LIB is slow and error-prone. We present VeriODD, a tool that automates this\ntranslation. VeriODD uses ANTLR-based compiler technology to transform\nYAML-based ODD/COD specifications into both human-readable propositional logic,\nfor lightweight review on a simple basis, and solver-ready SMT-LIB. The tool\nintegrates with SMT solvers such as Z3 to provide automated consistency checks\nof ODD specifications and verification of COD conformance. A graphical user\ninterface supports editing specifications, inspecting generated formulas, and\nperforming verification with a single click. VeriODD thereby closes the gap\nbetween stakeholder-friendly ODD/COD notations and formal verification,\nenabling scalable and automated assurance of operational boundaries in\nautonomous driving. Video demonstration: https://youtu.be/odRacNoL_Pk Tool\navailable at: https://github.com/BasselRafie/VeriODD", "AI": {"tldr": "Manual translation of human-friendly ODD/COD safety specs for autonomous vehicles into formal logic is slow and error-prone. VeriODD automates this process, generating both readable logic and solver-ready formats for efficient, scalable safety verification, complete with a GUI for easy stakeholder use.", "motivation": "Ensuring the safety of automated driving systems requires verifying that the runtime situation (COD) remains within the predetermined safe boundaries (ODD). Current descriptions are often done in human-friendly (YAML) form, which is not suitable for automated, formal verification, leading to manual, error-prone, and inefficient processes.", "method": "The authors developed VeriODD, a tool that automatically translates YAML-based ODD/COD specifications into human-readable propositional logic and solver-ready SMT-LIB formats, using ANTLR-based compiler technology. The tool also integrates with SMT solvers (like Z3) for automated consistency and conformance checks, and provides a graphical user interface for specifications editing and verification.", "result": "VeriODD successfully streamlines the process of translating stakeholder-friendly, YAML-form ODD/COD descriptions into formal representations suitable for automated verification. This enables scalable, automated assurance of operational boundaries for autonomous driving systems, bridging the gap between readable specifications and robust, solver-based safety checks.", "conclusion": "VeriODD facilitates reliable, automated verification of ODD/COD boundaries in autonomous driving by converting accessible YAML specifications into formal logic and enabling integrated, user-friendly consistency and conformance checks."}}
{"id": "2511.01423", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.01423", "abs": "https://arxiv.org/abs/2511.01423", "authors": ["Ruidi He", "Yu Zhang", "Meng Zhang", "Andreas Rausch"], "title": "LLM-Assisted Tool for Joint Generation of Formulas and Functions in Rule-Based Verification of Map Transformations", "comment": null, "summary": "High-definition map transformations are essential in autonomous driving\nsystems, enabling interoperability across tools. Ensuring their semantic\ncorrectness is challenging, since existing rule-based frameworks rely on\nmanually written formulas and domain-specific functions, limiting scalability.\n  In this paper, We present an LLM-assisted pipeline that jointly generates\nlogical formulas and corresponding executable predicates within a computational\nFOL framework, extending the map verifier in CommonRoad scenario designer with\nelevation support. The pipeline leverages prompt-based LLM generation to\nproduce grammar-compliant rules and predicates that integrate directly into the\nexisting system.\n  We implemented a prototype and evaluated it on synthetic bridge and slope\nscenarios. The results indicate reduced manual engineering effort while\npreserving correctness, demonstrating the feasibility of a scalable,\nsemi-automated human-in-the-loop approach to map-transformation verification.", "AI": {"tldr": "The paper demonstrates a scalable, LLM-assisted method for generating logic formulas and predicates to verify high-definition map transformations in autonomous driving, reducing manual work while maintaining correctness.", "motivation": "Existing rule-based frameworks for high-definition map transformations in autonomous driving rely heavily on manually written formulas, limiting their scalability and making semantic correctness difficult to ensure.", "method": "The paper introduces an LLM-assisted pipeline that jointly generates logical formulas and corresponding executable predicates within a computational first-order logic (FOL) framework. This extends the existing map verifier in the CommonRoad scenario designer with support for elevation features, leveraging prompt-based LLMs to generate grammar-compliant rules integrated into the system.", "result": "The proposed system was prototyped and evaluated on synthetic bridge and slope scenarios. The evaluation showed that it reduces the need for manual engineering while preserving the semantic correctness of map transformations.", "conclusion": "The LLM-assisted pipeline makes map-transformation verification more scalable and feasible through a semi-automated, human-in-the-loop approach, decreasing manual effort and maintaining correctness."}}
{"id": "2511.01545", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01545", "abs": "https://arxiv.org/abs/2511.01545", "authors": ["Ronivaldo Ferreira", "Guilherme da Silva", "Carla Rocha", "Gustavo Pinto"], "title": "From Pre-labeling to Production: Engineering Lessons from a Machine Learning Pipeline in the Public Sector", "comment": "11 pages, 2 figures, 4 tables", "summary": "Machine learning is increasingly being embedded into government digital\nplatforms, but public-sector constraints make it difficult to build ML systems\nthat are accurate, auditable, and operationally sustainable. In practice, teams\nface not only technical issues like extreme class imbalance and data drift, but\nalso organizational barriers such as bureaucratic data access, lack of\nversioned datasets, and incomplete governance over provenance and monitoring.\nOur study of the Brasil Participativo (BP) platform shows that common\nengineering choices -- like using LLMs for pre-labeling, splitting models into\nrouted classifiers, and generating synthetic data -- can speed development but\nalso introduce new traceability, reliability, and cost risks if not paired with\ndisciplined data governance and human validation. This means that, in the\npublic sector, responsible ML is not just a modeling problem but an\ninstitutional engineering problem, and ML pipelines must be treated as civic\ninfrastructure. Ultimately, this study shows that the success of machine\nlearning in the public sector will depend less on breakthroughs in model\naccuracy and more on the ability of institutions to engineer transparent,\nreproducible, and accountable data infrastructures that citizens can trust.", "AI": {"tldr": "Building ML for government is not just a technical challenge. Study of Brazil's BP platform shows that quick engineering solutions (like using LLMs, routed classifiers, and synthetic data) can save time but bring new risks if not supported by strong data governance and human checks. ML in the public sector needs good institutional practices and robust civic data infrastructure to be trustworthy and sustainable.", "motivation": "The paper is motivated by the challenges of implementing machine learning (ML) systems in the public sector, where there are unique operational, technical, and organizational constraints. These include issues like extreme data imbalance, data drift, bureaucratic obstacles, lack of versioned datasets, and gaps in governance and monitoring.", "method": "The study analyzes the Brasil Participativo (BP) platform, examining the engineering choices made when implementing ML, such as using large language models (LLMs) for pre-labeling, splitting models into routed classifiers, and generating synthetic data. The paper considers both technical and organizational factors that influence ML system reliability and sustainability.", "result": "The study finds that while common engineering choices can speed up the development of ML systems, they also introduce risks related to traceability, reliability, and cost if not accompanied by strong data governance and human involvement. It highlights that responsible ML in the public sector requires treating ML systems as civic infrastructure and emphasizes institutional practices over technical advancement alone.", "conclusion": "The paper concludes that success in public-sector ML depends not on model accuracy but on the institution's ability to build transparent, reproducible, and accountable data infrastructures. Responsible ML is primarily an institutional engineering challenge rather than just a technical one."}}
{"id": "2511.01757", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.01757", "abs": "https://arxiv.org/abs/2511.01757", "authors": ["Shamse Tasnim Cynthia", "Banani Roy"], "title": "Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for Galaxy", "comment": null, "summary": "Scientific Workflow Management Systems (SWfMSs) such as Galaxy have become\nessential infrastructure in bioinformatics, supporting the design, execution,\nand sharing of complex multi-step analyses. Despite hosting hundreds of\nreusable workflows across domains, Galaxy's current keyword-based retrieval\nsystem offers limited support for semantic query interpretation and often fails\nto surface relevant workflows when exact term matches are absent. To address\nthis gap, we propose a task-aware, two-stage retrieval framework that\nintegrates dense vector search with large language model (LLM)-based reranking.\nOur system first retrieves candidate workflows using state-of-the-art embedding\nmodels and then reranks them using instruction-tuned generative LLMs (GPT-4o,\nMistral-7B) based on semantic task alignment. To support robust evaluation, we\nconstruct a benchmark dataset of Galaxy workflows annotated with semantic\ntopics via BERTopic and synthesize realistic task-oriented queries using LLMs.\nWe conduct a comprehensive comparison of lexical, dense, and reranking models\nusing standard IR metrics, presenting the first systematic evaluation of\nretrieval performance in the Galaxy ecosystem. Results show that our approach\nsignificantly improves top-k accuracy and relevance, particularly for long or\nunder-specified queries. We further integrate our system as a prototype tool\nwithin Galaxy, providing a proof-of-concept for LLM-enhanced workflow search.\nThis work advances the usability and accessibility of scientific workflows,\nespecially for novice users and interdisciplinary researchers.", "AI": {"tldr": "This paper presents a novel workflow search framework for Galaxy. By combining dense vector retrieval and LLM-based reranking, their approach surpasses traditional keyword search in both relevance and accuracy, particularly for complex or underspecified queries. The solution is validated through benchmark testing and integrated as a prototype, promising improved usability for a wide user base.", "motivation": "Galaxy\u2019s workflow search is limited: traditional keyword matching fails when queries don\u2019t correspond exactly to workflow terms, making it difficult for users to find relevant workflows in complex or unfamiliar domains.", "method": "The authors propose a two-stage retrieval framework: (1) workflows are initially retrieved using dense embedding models; (2) top candidates are reranked with instruction-tuned large language models (GPT-4o, Mistral-7B) based on semantic task alignment. The system is evaluated using a benchmark set of workflows annotated via BERTopic and realistic queries generated by LLMs.", "result": "Their system outperforms traditional keyword search and other baselines, offering improved accuracy and relevance for top-k results, especially with long or vaguely-worded queries. It is deployed as a prototype in Galaxy.", "conclusion": "Integrating dense retrieval with LLM-based reranking significantly enhances workflow search in Galaxy, broadening accessibility for less experienced users and multidisciplinary researchers."}}
{"id": "2511.01763", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01763", "abs": "https://arxiv.org/abs/2511.01763", "authors": ["Xiaohan Wang", "Yuxin Hu", "Kevin Leach"], "title": "Context-Guided Decompilation: A Step Towards Re-executability", "comment": null, "summary": "Binary decompilation plays an important role in software security analysis,\nreverse engineering, and malware understanding when source code is unavailable.\nHowever, existing decompilation techniques often fail to produce source code\nthat can be successfully recompiled and re-executed, particularly for optimized\nbinaries. Recent advances in large language models (LLMs) have enabled neural\napproaches to decompilation, but the generated code is typically only\nsemantically plausible rather than truly executable, limiting their practical\nreliability. These shortcomings arise from compiler optimizations and the loss\nof semantic cues in compiled code, which LLMs struggle to recover without\ncontextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid\ndecompilation framework that leverages in-context learning (ICL) to guide LLMs\ntoward generating re-executable source code. We evaluate our method across\nmultiple datasets, optimization levels, and compilers, demonstrating around\n40\\% improvement in re-executability over state-of-the-art decompilation\nmethods while maintaining robustness.", "AI": {"tldr": "ICL4Decomp uses in-context learning to help LLMs produce source code from binaries that can actually be recompiled and run, improving re-executability by about 40% over current methods.", "motivation": "Existing binary decompilation techniques struggle to generate source code that can be recompilable and executable, especially for optimized binaries; using LLMs often produces code that is only semantically plausible, not reliably executable. This limitation is due to compiler optimizations and the loss of semantic information, which LLMs cannot recover effectively without more guidance.", "method": "The authors propose ICL4Decomp, a hybrid decompilation framework that utilizes in-context learning (ICL) to better guide large language models (LLMs) towards generating source code that can be successfully re-executed. The method is evaluated across various datasets, optimization levels, and compilers.", "result": "ICL4Decomp demonstrates around 40% improvement in re-executability compared to state-of-the-art decompilation methods, while maintaining robustness.", "conclusion": "ICL4Decomp significantly enhances the reliability and practicality of neural binary decompilation by guiding LLMs via in-context learning, which results in higher rates of generating re-executable source code, overcoming challenges posed by compiler optimizations."}}
{"id": "2511.01850", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01850", "abs": "https://arxiv.org/abs/2511.01850", "authors": ["Jiawei Jin", "Yingxin Su", "Xiaotong Zhu"], "title": "SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring", "comment": null, "summary": "The rapid expansion of artificial intelligence and machine learning (ML)\napplications has intensified the demand for integrated environments that unify\nmodel development, deployment, and monitoring. Traditional Integrated\nDevelopment Environments (IDEs) focus primarily on code authoring, lacking\nintelligent support for the full ML lifecycle, while existing MLOps platforms\nremain detached from the coding workflow. To address this gap, this study\nproposes the design of an LLM-Integrated IDE with automated MLOps pipelines\nthat enables continuous model development and monitoring within a single\nenvironment. The proposed system embeds a Large Language Model (LLM) assistant\ncapable of code generation, debugging recommendation, and automatic pipeline\nconfiguration. The backend incorporates automated data validation, feature\nstorage, drift detection, retraining triggers, and CI/CD deployment\norchestration. This framework was implemented in a prototype named SmartMLOps\nStudio and evaluated using classification and forecasting tasks on the UCI\nAdult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio\nreduces pipeline configuration time by 61%, improves experiment reproducibility\nby 45%, and increases drift detection accuracy by 14% compared to traditional\nworkflows. By bridging intelligent code assistance and automated operational\npipelines, this research establishes a novel paradigm for AI engineering -\ntransforming the IDE from a static coding tool into a dynamic, lifecycle-aware\nintelligent platform for scalable and efficient model development.", "AI": {"tldr": "A new LLM-powered IDE, SmartMLOps Studio, streamlines ML model development and monitoring by combining intelligent code assistance and automated MLOps pipelines. It significantly improves workflow efficiency and experiment reproducibility compared to traditional methods.", "motivation": "AI and ML applications are growing rapidly, creating a need for environments that unify model development, deployment, and monitoring. Existing IDEs focus mainly on coding, while MLOps platforms are separated from development workflows, leaving a gap in supporting the complete ML lifecycle.", "method": "The study designed and implemented an LLM-Integrated IDE called SmartMLOps Studio. This IDE includes a Large Language Model assistant for code generation, debugging, and automated MLOps pipeline configuration. The backend provides features such as automated data validation, feature storage, drift detection, retraining triggers, and CI/CD deployment orchestration. The prototype was evaluated using classification and forecasting tasks on standard datasets.", "result": "SmartMLOps Studio reduces pipeline configuration time by 61%, improves experiment reproducibility by 45%, and increases drift detection accuracy by 14% compared to traditional workflows.", "conclusion": "Integrating LLM-powered intelligent code assistance and automated MLOps pipelines within an IDE establishes a new paradigm for AI engineering, making model development more scalable, efficient, and lifecycle-aware."}}
