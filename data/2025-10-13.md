<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 25]
- [cs.PL](#cs.PL) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions](https://arxiv.org/abs/2510.08576)
*Justus Flerlage,Alexander Acker,Odej Kao*

Main category: cs.SE

TL;DR: This paper compares open-source, locally deployable language models with proprietary GPT-4 systems for user intent resolution, demonstrating the promise and challenges of open LLMs in privacy-focused, decentralized AI workflows.


<details>
  <summary>Details</summary>
Motivation: Current LLM-powered workflows often depend on cloud-based, proprietary models, which raise concerns about privacy, autonomy, and scalability. There is a need to explore whether open-source, locally deployable LLMs can serve as viable alternatives, especially for intent-based OS operations.

Method: The study conducts a comparative analysis of several open-source/open-access LLMs against proprietary GPT-4-based solutions. The models are tested on their ability to resolve user intentions and generate cross-application workflows.

Result: Empirical evidence is presented regarding the performance, trade-offs, and feasibility of using open LLMs locally for dynamic user intent resolution. The analysis reveals practical strengths and weaknesses and informs the discussion of AI decentralization.

Conclusion: Local, open-access LLMs show potential as core components for future operating systems prioritizing privacy, autonomy, and seamless user interaction. The work highlights their practical viability and possible limitations compared to current proprietary solutions.

Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural
language understanding and user intent resolution, enabling tasks such as
translation, summarization, and, increasingly, the orchestration of complex
workflows. This development signifies a paradigm shift from conventional,
GUI-driven user interfaces toward intuitive, language-first interaction
paradigms. Rather than manually navigating applications, users can articulate
their objectives in natural language, enabling LLMs to orchestrate actions
across multiple applications in a dynamic and contextual manner. However,
extant implementations frequently rely on cloud-based proprietary models, which
introduce limitations in terms of privacy, autonomy, and scalability. For
language-first interaction to become a truly robust and trusted interface
paradigm, local deployment is not merely a convenience; it is an imperative.
This limitation underscores the importance of evaluating the feasibility of
locally deployable, open-source, and open-access LLMs as foundational
components for future intent-based operating systems. In this study, we examine
the capabilities of several open-source and open-access models in facilitating
user intention resolution through machine assistance. A comparative analysis is
conducted against OpenAI's proprietary GPT-4-based systems to assess
performance in generating workflows for various user intentions. The present
study offers empirical insights into the practical viability, performance
trade-offs, and potential of open LLMs as autonomous, locally operable
components in next-generation operating systems. The results of this study
inform the broader discussion on the decentralization and democratization of AI
infrastructure and point toward a future where user-device interaction becomes
more seamless, adaptive, and privacy-conscious through locally embedded
intelligence.

</details>


### [2] [Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?](https://arxiv.org/abs/2510.08609)
*Imranur Rahman,Jill Marley,William Enck,Laurie Williams*

Main category: cs.SE

TL;DR: The paper investigates how different dependency version constraints (like pinning or floating) influence the likelihood of dependencies becoming outdated or vulnerable. Floating constraints tend to keep dependencies more up-to-date and secure compared to pinning, which often leads to outdated software.


<details>
  <summary>Details</summary>
Motivation: Dependency version constraints are crucial in software development for managing updates and risks. Pinning can protect against changes but increases maintenance, while floating reduces manual work but could introduce breaking changes or vulnerabilities. Despite recommendations, the true implications of different version constraints on outdatedness or vulnerability are not well understood.

Method: The study empirically evaluates how likely dependencies become outdated or vulnerable across different version constraint types. Researchers analyzed version constraint usage and change patterns in the npm, PyPI, and Cargo ecosystems, and applied survival analysis to model state transitions and estimate risks.

Result: Floating-minor is the most common among outdated and vulnerable dependencies, followed by pinning. Floating-major is least likely to result in outdated dependencies, and floating-minor is least likely to result in vulnerable dependencies.

Conclusion: Choice of version constraint significantly affects the risk of outdated or vulnerable dependencies. Floating-major and floating-minor constraints are recommended to mitigate these risks, as pinning tends to create more outdated dependencies.

Abstract: Developers consistently use version constraints to specify acceptable
versions of the dependencies for their project. \emph{Pinning} dependencies can
reduce the likelihood of breaking changes, but comes with a cost of manually
managing the replacement of outdated and vulnerable dependencies. On the other
hand, \emph{floating} can be used to automatically get bug fixes and security
fixes, but comes with the risk of breaking changes. Security practitioners
advocate \emph{pinning} dependencies to prevent against software supply chain
attacks, e.g., malicious package updates. However, since \emph{pinning} is the
tightest version constraint, \emph{pinning} is the most likely to result in
outdated dependencies. Nevertheless, how the likelihood of becoming outdated or
vulnerable dependencies changes across version constraint types is unknown. The
goal of this study is to aid developers in making an informed dependency
version constraint choice by empirically evaluating the likelihood of
dependencies becoming outdated or vulnerable across version constraint types at
scale. In this study, we first identify the trends in dependency version
constraint usage and the patterns of version constraint type changes made by
developers in the npm, PyPI, and Cargo ecosystems. We then modeled the
dependency state transitions using survival analysis and estimated how the
likelihood of becoming outdated or vulnerable changes when using \emph{pinning}
as opposed to the rest of the version constraint types. We observe that among
outdated and vulnerable dependencies, the most commonly used version constraint
type is \emph{floating-minor}, with \emph{pinning} being the next most common.
We also find that \emph{floating-major} is the least likely to result in
outdated and \emph{floating-minor} is the least likely to result in vulnerable
dependencies.

</details>


### [3] [Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model](https://arxiv.org/abs/2510.08610)
*Imranur Rahman,Md Rayhanur Rahman*

Main category: cs.SE

TL;DR: This paper introduces a method for improving code completion in IDEs by preprocessing code into smaller chunks and selecting relevant ones using similarity and positioning, resulting in better performance of large language models.


<details>
  <summary>Details</summary>
Motivation: Current research does not sufficiently address what constitutes an effective context for code completion using large language models (LLMs) with information available to IDEs.

Method: The authors propose a strategy that preprocesses a code repository into smaller chunks, then retrieves these chunks for code completion tasks based on syntactic and semantic similarity, along with their relative positions.

Result: Using code chunking and relative chunk positioning in context retrieval enhances the performance of LLMs on code completion tasks.

Conclusion: An effective context collection strategy utilizing code chunking and relative positioning leads to better code completion by LLMs.

Abstract: Code completion can help developers improve efficiency and ease the
development lifecycle. Although code completion is available in modern
integrated development environments (IDEs), research lacks in determining what
makes a good context for code completion based on the information available to
the IDEs for the large language models (LLMs) to perform better. In this paper,
we describe an effective context collection strategy to assist the LLMs in
performing better at code completion tasks. The key idea of our strategy is to
preprocess the repository into smaller code chunks and later use syntactic and
semantic similarity-based code chunk retrieval with relative positioning. We
found that code chunking and relative positioning of the chunks in the final
context improve the performance of code completion tasks.

</details>


### [4] [Literate Tracing](https://arxiv.org/abs/2510.09073)
*Matthew Sotoudeh*

Main category: cs.SE

TL;DR: Literate tracing is a new way to document software using annotated execution traces, making explanations clearer and more accurate than traditional comments or design docs. The TReX tool helps create interactive and precise traces, which have successfully documented parts of major systems like Linux, Git, and GCC.


<details>
  <summary>Details</summary>
Motivation: Effective communication about complex software systems between experts and novices is challenging. Existing documentation methods either lack global context or connections to actual code.

Method: The author introduces 'literate tracing,' a documentation paradigm using annotated, concrete execution traces. They describe TReX, a tool for interactive and visual literate traces that are guaranteed to reflect true program execution.

Result: Literate tracing offers documentation that bridges gaps between code comments and design documents. TReX enables creation of interactive, visual, and semantically faithful traces. The tool has been used for components of major software like the Linux kernel, Git, and GCC.

Conclusion: Literate tracing, supported by TReX, improves program understanding and documentation, especially for substantial and complex systems. It enhances the clarity and fidelity of communication between developers.

Abstract: As computer systems grow ever larger and more complex, a crucial task in
software development is for one person (the system expert) to communicate to
another (the system novice) how a certain program works. This paper reports on
the author's experiences with a paradigm for program documentation that we call
literate tracing. A literate trace explains a software system using annotated,
concrete execution traces of the system. Literate traces complement both
in-code comments (which often lack global context) and out-of-band design docs
(which often lack a concrete connection to the code). We also describe TReX,
our tool for making literate traces that are interactive, visual, and
guaranteed by construction to be faithful to the program semantics. We have
used TReX to write literate traces explaining components of large systems
software including the Linux kernel, Git source control system, and GCC
compiler.

</details>


### [5] [Impact of LLMs on Team Collaboration in Software Development](https://arxiv.org/abs/2510.08612)
*Devang Dhanuka*

Main category: cs.SE

TL;DR: This paper analyzes how LLMs impact teamwork in software development, finding they boost productivity and communication but also introduce challenges like privacy and model reliability. Further research is needed to tailor LLMs for specific development contexts and ensure secure, trustworthy integration into workflows.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly impacting software development teams, but their effects on collaboration throughout the SDLC are not well understood. The paper aims to update prior research with new evidence and clarify how LLMs can address collaboration hurdles.

Method: The paper uses a literature review, analysis of industry examples, surveys of team experiences, and two case studies to investigate the impact of LLM-assisted tools on software engineering collaboration.

Result: LLMs can improve team efficiency by automating repetitive tasks and documentation, clarify communication, and facilitate cross-functional collaboration. However, they present new challenges, such as model limitations and privacy concerns.

Conclusion: LLMs have the potential to reshape collaborative practices in software engineering, enhancing productivity and communication but requiring careful consideration of security, trust, and domain-specific adaptation. Future directions include customizing models for different domains and strengthening their integration and security.

Abstract: Large Language Models (LLMs) are increasingly being integrated into software
development processes, with the potential to transform team workflows and
productivity. This paper investigates how LLMs affect team collaboration
throughout the Software Development Life Cycle (SDLC). We reframe and update a
prior study with recent developments as of 2025, incorporating new literature
and case studies. We outline the problem of collaboration hurdles in SDLC and
explore how LLMs can enhance productivity, communication, and decision-making
in a team context. Through literature review, industry examples, a team survey,
and two case studies, we assess the impact of LLM-assisted tools (such as code
generation assistants and AI-powered project management agents) on
collaborative software engineering practices. Our findings indicate that LLMs
can significantly improve efficiency (by automating repetitive tasks and
documentation), enhance communication clarity, and aid cross-functional
collaboration, while also introducing new challenges like model limitations and
privacy concerns. We discuss these benefits and challenges, present research
questions guiding the investigation, evaluate threats to validity, and suggest
future research directions including domain-specific model customization,
improved integration into development tools, and robust strategies for ensuring
trust and security.

</details>


### [6] [Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools](https://arxiv.org/abs/2510.08640)
*Ha Min Son,Huan Ren,Xin Liu,Zhe Zhao*

Main category: cs.SE

TL;DR: The paper introduces a new benchmark for Android build errors and presents GradleFixer, an LLM agent with specialized tools, which outperforms general solutions by guiding LLMs with domain-aware actions. Domain-specific tools enable LLMs to bridge high-level reasoning with effective low-level execution for build error repair.


<details>
  <summary>Details</summary>
Motivation: Automatically building Android applications is challenging, and current methods—even those leveraging powerful Large Language Models (LLMs)—are not yet effective at fixing specific Android build errors. The paper identifies the gap in applying LLMs to resolve Android-specific build failures.

Method: The authors introduce AndroidBuildBench, a curated benchmark of 1,019 real-world build failures from open-source Android projects, each paired with a verified fix. They also present GradleFixer, an LLM agent equipped with domain-specific tools designed to inspect and manipulate the Gradle build environment, leveraging a strategy called Tool Bridging by replacing generic shell commands with Android-specific abstractions.

Result: GradleFixer achieves an 81.4% resolution rate (pass@1), significantly outperforming a general-purpose coding agent. The tool’s effectiveness stems from providing LLMs with an API-like interface and narrowing down action choices to domain-relevant operations.

Conclusion: LLMs have the high-level knowledge needed for Android build error resolution, but require domain-aware tooling to execute fixes effectively. Tool Bridging, offering domain-specific abstractions, allows LLMs to better translate knowledge into actionable repairs, outperforming generic agents.

Abstract: Android is the largest mobile platform, yet automatically building
applications remains a practical challenge. While Large Language Models (LLMs)
show promise for code repair, their use for fixing Android build errors remains
underexplored. To address this gap, we first introduce AndroidBuildBench, a
benchmark of 1,019 build failures curated from the commit histories of 43
open-source Android projects. Each problem is paired with a verified solution
from a subsequent commit, ensuring that fixes are feasible. Second, we propose
GradleFixer, an LLM agent with domain-specific tools for inspecting and
manipulating the Gradle build environment. GradleFixer achieves a resolve rate
of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent
that relies on a general-purpose shell. GradleFixer's success suggests that
while LLMs possess the high-level knowledge to solve these failures, they
struggle to translate this knowledge into effective low-level actions using a
general-purpose shell. We demonstrate the effectiveness of a strategy we term
Tool Bridging, which replaces general-purpose shell commands with domain-aware
abstractions. We hypothesize this approach works through two mechanisms: 1) it
provides tools in an API-like format that LLMs use more reliably, and 2) it
constrains the action space to relevant operations. This approach bridges the
gap between the model's high-level reasoning and effective low-level execution.

</details>


### [7] [Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware](https://arxiv.org/abs/2510.08664)
*Jianan Mu,Mingyu Shi,Yining Wang,Tianmeng Yang,Bin Sun,Xing Hu,Jing Ye,Huawei Li*

Main category: cs.SE

TL;DR: LLM-generated RTL code struggles with accuracy due to semantic gaps and limited data; Faver, a new middleware, streamlines verification and boosts accuracy by up to 14% in experiments.


<details>
  <summary>Details</summary>
Motivation: LLM-based generation of register-transfer level (RTL) code could automate one of the least automated stages in chip design, but faces significant challenges from a semantic gap between input languages and RTL, as well as limited training and test data.

Method: The paper introduces Faver, a function-abstracted verifiable middleware that combines LLM-friendly code structures with rule-based templates. This middleware streamlines circuit verification within LLM-based RTL generation workflows by decoupling verification details and allowing LLMs to better focus on functional correctness.

Result: Experiments conducted on both SFT and open-source models show that Faver improved the generation accuracy of RTL code by up to 14%.

Conclusion: The Faver middleware helps overcome several core challenges in LLM-based RTL generation, specifically boosting verification capability and generation accuracy despite the semantic gap and data limitations.

Abstract: LLM-based RTL generation is an interesting research direction, as it holds
the potential to liberate the least automated stage in the current chip design.
However, due to the substantial semantic gap between high-level specifications
and RTL, coupled with limited training data, existing models struggle with
generation accuracy. Drawing on human experience, design with verification
helps improving accuracy. However, as the RTL testbench data are even more
scarce, it is not friendly for LLMs. Although LLMs excel at higher-level
languages like Python/C, they have a huge semantic gap from RTL. When
implementing the same functionality, Python/C code and hardware code differ
significantly in the spatiotemporal granularity, requiring the LLM not only to
consider high-level functional semantics but also to ensure the low-level
details align with the circuit code. It is not an easy task. In this paper, we
propose a function abstracted verifiable middleware (Faver) that streamlines
RTL verification in LLM-based workflows. By mixing LLM-friendly code structures
with a rule-based template, Faver decouples the details of circuit
verification, allowing the LLM to focus on the functionality itself. In our
experiments on the SFT model and open-source models, Faver improved the model's
generation accuracy by up to 14%.

</details>


### [8] [RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution](https://arxiv.org/abs/2510.08665)
*Aofan Liu,Haoxuan Li,Bin Wang,Ao Yang,Hui Li*

Main category: cs.SE

TL;DR: A new multi-agent, ReAct-based code generation framework improves safety, accuracy, and user control compared to existing LLM solutions, achieving high performance and security rates in experiments.


<details>
  <summary>Details</summary>
Motivation: Current code generation models based on large language models face challenges with safety, accuracy, and controllability, especially for complex coding tasks. Existing solutions lack dynamic integration with external tools, transparent reasoning, and user control over safety aspects.

Method: The paper proposes a controllable code generation framework that uses a multi-agent system and the ReAct paradigm. Four specialized agents are employed: a Planner (task decomposition), a Searcher (reasoning and tool integration via ReAct), a CodeGen (code generation), and an Extractor (structured data retrieval). The Searcher agent dynamically alternates between reasoning and executing actions, effectively integrating internal knowledge with external resources.

Result: The framework demonstrated effectiveness across multiple programming languages and achieved a 94.8% security rate on the SVEN dataset using CodeQL, surpassing existing methods. The transparent reasoning process enhances user trust and control.

Conclusion: The proposed controllable code generation framework significantly improves code safety, accuracy, and user controllability through dynamic multi-agent collaboration and transparent reasoning, outperforming current approaches.

Abstract: Code generation models based on large language models (LLMs) have gained wide
adoption, but challenges remain in ensuring safety, accuracy, and
controllability, especially for complex tasks. Existing methods often lack
dynamic integration of external tools, transparent reasoning, and user control
over safety. To address these issues, we propose a controllable code generation
framework utilizing the ReAct paradigm for multi-agent task execution. This
framework is a multi-agent system designed to enable efficient, precise, and
interpretable code generation through dynamic interactions between LLMs and
external resources. The framework adopts a collaborative architecture
comprising four specialized agents: a Planner for task decomposition, a
Searcher that leverages the ReAct framework for reasoning and tool integration,
a CodeGen agent for accurate code generation, and an Extractor for structured
data retrieval. The ReAct-based Searcher alternates between generating
reasoning traces and executing actions, facilitating seamless integration of
internal knowledge with external tools (such as search engines) to enhance
accuracy and user control. Experimental results show the framework's
effectiveness across multiple languages, achieving a 94.8% security rate on the
SVEN dataset with CodeQL, outperforming existing approaches. Its transparent
reasoning process fosters user trust and improves controllability.

</details>


### [9] [RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data](https://arxiv.org/abs/2510.08667)
*Mohammad Baqar*

Main category: cs.SE

TL;DR: A novel system combining semantic embedding, vector search, and language models is introduced to mine and recommend solutions for software tickets by unifying JIRA and GitHub data, demonstrating substantial performance improvements in DevOps workflows.


<details>
  <summary>Details</summary>
Motivation: Software teams often face delays in resolving recurring or related issues due to scattered information across JIRA tickets, developer discussions, and GitHub pull requests.

Method: The authors propose a Retrieval-Augmented Generation (RAG) framework that uses Sentence-Transformers for semantic embeddings and FAISS-based vector search. This system embeds data from JIRA and GitHub to retrieve similar past cases, which are then synthesized by a Large Language Model (LLM) to generate resolution suggestions.

Result: Experimental evaluation indicates significant improvements in resolution accuracy, fix quality, and knowledge reuse, as measured by precision, recall, resolution time reduction, and developer acceptance.

Conclusion: The proposed unified RAG framework effectively leverages linked JIRA and GitHub data to deliver context-aware and explainable ticket resolution recommendations, improving the efficiency and quality of software issue resolution.

Abstract: Modern software teams frequently encounter delays in resolving recurring or
related issues due to fragmented knowledge scattered across JIRA tickets,
developer discussions, and GitHub pull requests (PRs). To address this
challenge, we propose a Retrieval-Augmented Generation (RAG) framework that
integrates Sentence-Transformers for semantic embeddings with FAISS-based
vector search to deliver context-aware ticket resolution recommendations. The
approach embeds historical JIRA tickets, user comments, and linked PR metadata
to retrieve semantically similar past cases, which are then synthesized by a
Large Language Model (LLM) into grounded and explainable resolution
suggestions. The framework contributes a unified pipeline linking JIRA and
GitHub data, an embedding and FAISS indexing strategy for heterogeneous
software artifacts, and a resolution generation module guided by retrieved
evidence. Experimental evaluation using precision, recall, resolution time
reduction, and developer acceptance metrics shows that the proposed system
significantly improves resolution accuracy, fix quality, and knowledge reuse in
modern DevOps environments.

</details>


### [10] [BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution](https://arxiv.org/abs/2510.08697)
*Terry Yue Zhuo,Xiaolong Jin,Hange Liu,Juyong Jiang,Tianyang Liu,Chen Gong,Bhupesh Bishnoi,Vaisakhi Mishra,Marek Suppa,Noah Ziems,Saiteja Utpala,Ming Xu,Guangyu Song,Kaixin Li,Yuhan Cao,Bo Liu,Zheng Liu,Sabina Abdurakhmanova,Wenhao Yu,Mengzhao Jia,Jihan Yao,Kenneth Hamilton,Kumar Shridhar,Minh Chien Vu,Dingmin Wang,Jiawei Liu,Zijian Wang,Qian Liu,Binyuan Hui,Meg Risdal,Ahsen Khaliq,Atin Sood,Zhenchang Xing,Wasi Uddin Ahmad,John Grundy,David Lo,Banghua Zhu,Xiaoning Du,Torsten Scholak,Leandro von Werra*

Main category: cs.SE

TL;DR: BigCodeArena is a new platform for human-involved and automatic code generation evaluation with live code execution, leading to better benchmarks and revealing proprietary LLMs' superior coding performance.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLM-generated code is challenging due to the need to understand complex code and simulate execution, and existing crowdsourced evaluation platforms lack support for interactive code execution.

Method: The authors introduce BigCodeArena, an open human evaluation platform for code generation with real-time execution capabilities. They collected extensive conversational data from multiple LLMs, analyzed human preferences in coding scenarios, and created two benchmarks: BigCodeReward (for reward model alignment) and AutoCodeArena (an automatic Elo rating system).

Result: Analysis of over 14,000 code-centric sessions revealed nuanced human preferences and demonstrated that LLMs perform better in code judgment tasks when execution results are available. Proprietary LLMs, such as GPT-5 and Claude models, show leading performance in code generation according to their new benchmarks.

Conclusion: BigCodeArena provides an effective, execution-driven platform for real-time code evaluation, offering new benchmarks and insights into LLM performance. The tools and data curated facilitate improved assessment methods for code generation models, reinforcing the strengths of state-of-the-art proprietary LLMs.

Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable
real-time evaluation from human perspectives to assess the quality of model
responses. In the coding domain, manually examining the quality of
LLM-generated content is extremely challenging, as it requires understanding
long chunks of raw code and deliberately simulating code execution. To this
end, we introduce BigCodeArena, an open human evaluation platform for code
generation backed by a comprehensive and on-the-fly execution environment.
Built on top of Chatbot Arena, BigCodeArena enables the execution of
LLM-generated code and allows humans to interact with the execution process and
outcomes. We collected over 14,000 raw code-centric conversation sessions
across 10 widely used LLMs, spanning 10 languages and 8 types of execution
environments. Among these conversations, we identified more than 4,700
multi-turn samples with pairwise human preferences. Further analysis uncovers
underexplored preferences of LLMs in fine-grained domains characterized by
tasks, languages, and frameworks. To systematically examine code understanding
and generation capabilities of frontier LLMs, we curated two benchmarks based
on the collected data, namely BigCodeReward and AutoCodeArena. For
BigCodeReward, we post-processed the 4,700 conversations and evaluated the
consistency between reward models and human preferences. The evaluation shows
that most LLMs have superior performance in judging coding preferences when the
execution results are available. Inspired by these findings, we propose
AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding
quality of LLMs without human involvement. We find that proprietary LLMs like
GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation
performance among recent emerging models.

</details>


### [11] [Search-based Hyperparameter Tuning for Python Unit Test Generation](https://arxiv.org/abs/2510.08716)
*Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: Using differential evolution to tune hyperparameters in test-generation algorithms improves test coverage and is more efficient than grid search.


<details>
  <summary>Details</summary>
Motivation: Users typically do not adjust the many configuration options available in search-based test-generation algorithms, relying on default values that may not provide optimal results. Tuning these options can improve outcomes but is usually resource-intensive.

Method: The paper explores differential evolution, a meta-heuristic search algorithm, to tune the hyperparameters of DynaMOSA and MIO many-objective search algorithms, using the Pynguin framework for implementation.

Result: Tuned DynaMOSA algorithm yields significantly better test suite coverage. Differential evolution is more efficient at finding improved configurations than basic grid search.

Conclusion: Differential evolution is an effective and efficient approach for tuning hyperparameters in search-based test-generation algorithms, leading to improved test suite coverage while reducing resource demands compared to grid search.

Abstract: Search-based test-generation algorithms have countless configuration options.
Users rarely adjust these options and usually stick to the default values,
which may not lead to the best possible results. Tuning an algorithm's
hyperparameters is a method to find better hyperparameter values, but it
typically comes with a high demand of resources. Meta-heuristic search
algorithms -- that effectively solve the test-generation problem -- have been
proposed as a solution to also efficiently tune parameters. In this work we
explore the use of differential evolution as a means for tuning the
hyperparameters of the DynaMOSA and MIO many-objective search algorithms as
implemented in the Pynguin framework. Our results show that significant
improvement of the resulting test suite's coverage is possible with the tuned
DynaMOSA algorithm and that differential evolution is more efficient than basic
grid search.

</details>


### [12] [PyMigTool: a tool for end-to-end Python library migration](https://arxiv.org/abs/2510.08810)
*Mohayeminul Islam,Ajay Kumar Jha,May Mahmoud,Sarah Nadi*

Main category: cs.SE

TL;DR: This paper introduces PyMigTool, an automated tool using Large Language Models, static analysis, and dynamic analysis to migrate code between any Python libraries. It significantly reduces migration effort, performing fully accurate migrations in 32% of real-world cases and leaving only minor fixes for developers in most of the rest.


<details>
  <summary>Details</summary>
Motivation: Library migration is challenging and labor-intensive for developers, involving understanding and mapping APIs and transforming code. Existing automated techniques are limited, often stopping at API mapping or supporting only specific libraries. There is a need for a more effective, automatic, and generalizable solution for library migration.

Method: The paper investigates the effectiveness of Large Language Models (LLMs) in automating library migration by analyzing 321 real-world library migrations. It then builds PyMigTool, a command line tool that leverages LLMs along with static and dynamic code analysis to automate the migration between arbitrary Python libraries.

Result: PyMigTool is evaluated on 717 real-world Python applications (not part of the initial benchmark). The tool achieves completely correct migration for 32% of cases. Furthermore, for more than half of the remaining projects, only 14% of the migration-related changes require manual intervention by developers.

Conclusion: The use of LLMs, combined with static and dynamic analysis, enables effective and automated migration between Python libraries. PyMigTool significantly reduces developer effort, although some migration tasks may still require minimal manual fixes.

Abstract: Library migration is the process of replacing a library with a similar one in
a software project. Manual library migration is time consuming and error prone,
as it requires developers to understand the Application Programming Interfaces
(API) of both libraries, map equivalent APIs, and perform the necessary code
transformations. Due to the difficulty of the library migration process, most
of the existing automated techniques and tooling stop at the API mapping stage
or support a limited set of libraries and code transformations. In this paper,
we develop an end-to-end solution that can automatically migrate code between
any arbitrary pair of Python libraries that provide similar functionality. Due
to the promising capabilities of Large Language Models (LLMs) in code
generation and transformation, we use LLMs as the primary engine for migration.
Before building the tool, we first study the capabilities of LLMs for library
migration on a benchmark of 321 real-world library migrations. We find that
LLMs can effectively perform library migration, but some post-processing steps
can further improve the performance. Based on this, we develop PyMigTool, a
command line application that combines the power of LLMs, static analysis, and
dynamic analysis to provide accurate library migration. We evaluate PyMigTool
on 717 real-world Python applications that are not from our benchmark. We find
that PyMigTool can migrate 32% of the migrations with complete correctness. Of
the remaining migrations, only 14% of the migration-related changes are left
for developers to fix for more than half of the projects.

</details>


### [13] [McMining: Automated Discovery of Misconceptions in Student Code](https://arxiv.org/abs/2510.08827)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.SE

TL;DR: The paper introduces McMining, a new task and dataset for discovering student programming misconceptions, and shows that LLMs can effectively identify such misconceptions in code.


<details>
  <summary>Details</summary>
Motivation: Students frequently form misconceptions when learning programming, which hinders their progress and leads to flawed code. Addressing these misconceptions is crucial for effective learning and better code.

Method: The paper proposes McMining, a novel task focused on automatically identifying programming misconceptions in student code. It includes the construction of a benchmark dataset of misconceptions, paired with code samples, and evaluates two Large Language Model (LLM)-based approaches (McMiner) using prominent LLMs such as Gemini, Claude, and GPT.

Result: The LLM-based McMiner models, using Gemini, Claude, and GPT, successfully identify misconceptions in student code, demonstrating the effectiveness of LLMs for this purpose.

Conclusion: The study establishes McMining as a viable task, provides resources for the research community, and shows that current LLMs are able to effectively mine misconceptions from student code.

Abstract: When learning to code, students often develop misconceptions about various
programming language concepts. These can not only lead to bugs or inefficient
code, but also slow down the learning of related concepts. In this paper, we
introduce McMining, the task of mining programming misconceptions from samples
of code from a student. To enable the training and evaluation of McMining
systems, we develop an extensible benchmark dataset of misconceptions together
with a large set of code samples where these misconceptions are manifested. We
then introduce two LLM-based McMiner approaches and through extensive
evaluations show that models from the Gemini, Claude, and GPT families are
effective at discovering misconceptions in student code.

</details>


### [14] [Identifying Video Game Debugging Bottlenecks: An Industry Perspective](https://arxiv.org/abs/2510.08834)
*Carlos Pinto Gomez,Fabio Petrillo*

Main category: cs.SE

TL;DR: Video game debugging requires unique methods and tools beyond traditional software debugging. By analyzing real studio debugging sessions, the study found that much developer time is used inspecting artifacts and reproducing bugs, relying on specialized tools and multidisciplinary teamwork.


<details>
  <summary>Details</summary>
Motivation: Traditional debugging techniques are used in video games, but the unique nature of video game development may require specialized debugging methods. The goal is to understand how video game developers actually debug in practice, and what techniques and tools are most effective.

Method: The study involved observing and analyzing recordings of debugging sessions from a video game studio, focusing on 20 experienced developers. The sessions addressed critical bugs such as crashes, object behaviors, and object persistence. Thematic analysis was used to identify bottlenecks and categorize debugging activities, tools, and interdisciplinary collaboration.

Result: The analysis found that game developers spend about 36.6% of their debugging time inspecting game artifacts and 35.1% reproducing the bug locally. The study also highlighted the use of specialized debugging tools (e.g., On-Screen Console, Debug Draws, Debug Camera, Cheats, In-Game Menus, and Data Scrubbing) and revealed the importance of collaboration among different disciplines, with technical roles being central to debugging.

Conclusion: Debugging in video games significantly relies on unique tools and cross-disciplinary collaboration, with substantial developer time allocated to inspecting artifacts and reproducing bugs. Traditional debugging techniques are complemented by game-specific methods to address the complex nature of video game development.

Abstract: Conventional debugging techniques used in traditional software are similarly
used when debugging video games. However, the reality of video games require
its own set of unique debugging techniques such as On-Screen Console, Debug
Draws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this
article, we provide insights from a video game studio on how 20 seasoned
industry game developers debug during the production of a game. Our experiments
rely on the recordings of debugging sessions for the most critical bugs
categorized as Crashes, Object Behaviors, and Object Persistence. In this
paper, we focus on identifying the debugging activities that bottleneck bug
resolution. We also identify the debugging tools used to perform debugging
techniques. Lastly, we present how different disciplines collaborate during
debugging and how technical roles are at the core of debugging. Our thematic
analysis has identified game developers spend 36.6\% of their time inspecting
game artifacts and 35.1\% of their time reproducing the bug locally.

</details>


### [15] [Repository-Aware File Path Retrieval via Fine-Tuned LLMs](https://arxiv.org/abs/2510.08850)
*Vasudha Yanuganti,Ishaan Puri,Swapnil Chhatre,Mantinder Singh,Ashok Jallepalli,Hritvik Shrivastava,Pradeep Kumar Sharma*

Main category: cs.SE

TL;DR: The paper introduces a method for training LLMs to retrieve relevant source file paths from natural language queries by leveraging multiple code-aware strategies. The approach achieves high accuracy and recall on several Python projects, demonstrating scalability and effectiveness over traditional code search techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional code search methods often fail to capture semantic context and links between files, making it difficult for developers and AI to answer repository-specific questions. LLMs excel at understanding natural language but lack context about specific repositories.

Method: The authors fine-tune a Large Language Model (Qwen3-8B) using QLoRA and Unsloth optimizations to directly retrieve relevant file paths from natural language queries. They devise six code-aware training strategies leveraging AST structure and repository content to create realistic question-file path datasets.

Result: The fine-tuned model achieved up to 91% exact match accuracy and 93% recall on test queries, significantly outperforming models trained with a single strategy. On the large PyTorch codebase (around 4,000 Python files), it achieved 59% recall, indicating good scalability.

Conclusion: Multi-strategy, code-aware fine-tuning enables LLMs to more accurately retrieve relevant file paths across codebases, surpassing traditional and single-strategy approaches. The limitations include context length for very large repositories, but the approach is promising for future integration into code intelligence tools.

Abstract: Modern codebases make it hard for developers and AI coding assistants to find
the right source files when answering questions like "How does this feature
work?" or "Where was the bug introduced?" Traditional code search (keyword or
IR based) often misses semantic context and cross file links, while large
language models (LLMs) understand natural language but lack repository specific
detail. We present a method for file path retrieval that fine tunes a strong
LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file
paths directly from a natural language query. To build training data, we
introduce six code aware strategies that use abstract syntax tree (AST)
structure and repository content to generate realistic question-answer pairs,
where answers are sets of file paths. The strategies range from single file
prompts to hierarchical repository summaries, providing broad coverage. We fine
tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,
and obtain high retrieval accuracy: up to 91\% exact match and 93\% recall on
held out queries, clearly beating single strategy training. On a large codebase
like PyTorch (about 4,000 Python files), the model reaches 59\% recall, showing
scalability. We analyze how multi level code signals help the LLM reason over
cross file context and discuss dataset design, limits (for example, context
length in very large repos), and future integration of retrieval with LLM based
code intelligence.

</details>


### [16] [Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval](https://arxiv.org/abs/2510.08876)
*Kostiantyn Bevziuk,Andrii Fatula,Svetozar Lashin Yaroslav Opanasenko,Anna Tukhtarova,Ashok Jallepalli Pradeepkumar Sharma,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: The paper introduces a system that transforms large software repositories into knowledge graphs enriched with semantic information and supports advanced retrieval and human-friendly explanations, facilitating automated understanding and development.


<details>
  <summary>Details</summary>
Motivation: Large software repositories contain complex, interrelated code. Understanding and developing them further requires semantic structuring and better automation. The paper aims to improve repository comprehension and enable advanced tooling by converting repositories into structured, semantically rich graphs.

Method: The system decomposes repositories into knowledge graphs that capture syntactic relations (containment, references, calls, inheritance) and semantic content (LLM-derived summaries, vector embeddings). A hybrid retrieval approach fuses semantic retrieval with graph-based context expansion. An LLM-based assistant generates read-only graph queries and explains results to users.

Result: The system builds detailed, vectorized knowledge graphs mirroring both architectural and semantic code relations. Retrieval combines semantic search and structure-aware expansion, while a conversational assistant supports user interaction and comprehension.

Conclusion: The proposed decomposition and retrieval system automates the understanding and further development of large code repositories by structuring them as semantic knowledge graphs, augmented with LLM summaries and embeddings, and making them accessible via a hybrid retriever and LLM-powered assistant.

Abstract: We present a repository decomposition system that converts large software
repositories into a vectorized knowledge graph which mirrors project
architectural and semantic structure, capturing semantic relationships and
allowing a significant level of automatization of further repository
development. The graph encodes syntactic relations such as containment,
implementation, references, calls, and inheritance, and augments nodes with
LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline
combines semantic retrieval with graph-aware expansion, and an LLM-based
assistant formulates constrained, read-only graph requests and produces
human-oriented explanations.

</details>


### [17] [SEER: Sustainability Enhanced Engineering of Software Requirements](https://arxiv.org/abs/2510.08981)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: The paper introduces SEER, a framework using AI methods to embed sustainability into software requirements from the start. It identifies, evaluates, and optimizes sustainability requirements, proving effective across multiple domains and helping developers address global sustainability goals efficiently.


<details>
  <summary>Details</summary>
Motivation: There is a need for software development practices that address sustainability from the earliest stages, as current approaches are either too general or only focus on later design/implementation phases. Early assessment of sustainability in requirements engineering is essential for meeting global goals like the United Nations Sustainable Development Goals (SDGs).

Method: The paper presents SEER, a framework used during the requirements engineering phase of software development. SEER works in three steps: 1) It identifies relevant sustainability requirements using a general taxonomy, 2) evaluates these requirements for sustainability, and 3) optimizes the requirements that do not meet sustainability standards. The technical implementation employs the reasoning abilities of large language models combined with an agentic Retrieval Augmented Generation (RAG) approach.

Result: SEER was tested on four projects in different domains, leveraging the Gemini 2.5 reasoning model. The results show SEER can accurately identify a wide range of sustainability requirements and concerns early in the software development process.

Conclusion: Implementing SEER in the requirements engineering phase helps ensure sustainability is embedded early, allowing software teams to address a wide variety of sustainability issues more effectively and efficiently across domains.

Abstract: The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.

</details>


### [18] [Towards a Taxonomy of Sustainability Requirements for Software Design](https://arxiv.org/abs/2510.08990)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: The paper presents a comprehensive taxonomy of sustainability requirements in software engineering, derived from a systematic literature review. It covers four sustainability dimensions, provides definitions, metrics, measures, and highlights their inter-relations, aiding practitioners and researchers in achieving sustainable software development.


<details>
  <summary>Details</summary>
Motivation: Existing sustainability requirements in software engineering are fragmented, domain-specific, or focused on particular sustainability dimensions. There is no unified taxonomy covering all relevant aspects, which hampers systematic consideration of sustainability from the requirements phase.

Method: The authors conduct a Systematic Literature Review (SLR) to identify, extract, and organize sustainability requirements from current literature. They then synthesize these requirements into a comprehensive taxonomy spanning environmental, technical, social, and economic dimensions.

Result: The research provides a unified taxonomy of sustainability requirements for software engineering, complete with definitions, metrics, and measures for each category. Additionally, a correlation matrix is presented to illustrate synergies and conflicts among categories across different sustainability dimensions.

Conclusion: The systematized taxonomy and correlation matrix offer a valuable reference for software engineers and researchers, enabling more holistic and effective sustainable software development by supporting the formulation, management, and reconciliation of sustainability-related trade-offs.

Abstract: Software systems are a significant contributor to global sustainability
concerns, demanding that environmental, social, technical, and economic factors
be systematically addressed from the initial requirements engineering phase.
Although existing research provides various sustainability requirements (SRs),
these contributions are often fragmented, specific to certain dimensions, or
limited to particular application domains, resulting in a critical lack of a
unified, comprehensive taxonomy for the software engineering community. To
address this gap, this research conducts a Systematic Literature Review (SLR)
to extract and organize sustainability requirements from the state-of-the-art.
The primary contribution is a comprehensive taxonomy of SRs across the four
dimensions of sustainability (environmental, technical, social, and economic).
For each identified category, we provide clear definitions, associated metrics,
and measures. Furthermore, we depict a correlation matrix that projects the
positive and negative influences (synergies and conflicts) among categories
across different dimensions. This systematized reference assists both software
developers and researchers in effectively formulating, managing, and
reconciling trade-offs within sustainable software development.

</details>


### [19] [Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation](https://arxiv.org/abs/2510.08996)
*Spandan Garg,Ben Steenhoek,Yufan Huang*

Main category: cs.SE

TL;DR: Current benchmarks give an inflated sense of chat-based coding assistant abilities. The paper presents a method to make benchmarks more realistic using real interaction data, showing that agent capabilities were previously overestimated by up to 50%. This new approach allows more reliable assessment of such tools.


<details>
  <summary>Details</summary>
Motivation: Current benchmarking approaches for software engineering agents do not accurately reflect real-world developer interaction, particularly in chat-based IDE environments. This results in an overestimation of agent capabilities.

Method: The authors propose a novel benchmarking framework that transforms formal benchmarks (like GitHub issue descriptions) into realistic user queries by analyzing actual developer interaction patterns with chat-based coding assistants. This is achieved through telemetry analysis and systematic mutation of benchmark tasks.

Result: When applying this framework to several benchmarks, including SWE-Bench Verified and private datasets, the authors found that existing benchmarks overestimated agent performance by more than 50% for public datasets and 10-16% for their internal benchmark.

Conclusion: There is a significant mismatch between current benchmark tasks and real developer-chat agent interactions. The proposed benchmark mutation technique enables more realistic evaluation and establishes a new standard for assessing chat-based software engineering agents.

Abstract: Current benchmarks for evaluating software engineering agents, such as
SWE-Bench Verified, are predominantly derived from GitHub issues and fail to
accurately reflect how developers interact with chat-based coding assistants in
integrated development environments (IDEs). We posit that this mismatch leads
to a systematic overestimation of agent's capabilities in real-world scenarios,
especially bug fixing. We introduce a novel benchmarking framework that
transforms existing formal benchmarks into realistic user queries through
systematic analysis of developer interaction patterns with chat-based agents.
Our methodology is flexible and can be easily extended to existing benchmarks.
In this paper, we apply our testing framework to SWE-Bench Verified, the
TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and
transform formal GitHub issue descriptions into realistic user-style queries
based on telemetry analysis of a popular chat-based agent interactions. Our
findings reveal that existing benchmarks significantly overestimate agent
capabilities for some models by >50% over baseline performance for public
benchmarks and ~10-16% for our internal benchmark. This work establishes a new
paradigm for evaluating interactive chat-based software engineering agents
through benchmark mutation techniques.

</details>


### [20] [Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements](https://arxiv.org/abs/2510.09045)
*Manojit Chakraborty,Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: This paper presents a zero-shot code translation technique for LLMs that substitutes lengthy identifiers with placeholders, improving translation accuracy and efficiency for long source codes by reducing token usage and preserving code structure.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges translating long source codes because their context window size is limited, resulting in accuracy issues.

Method: The proposed method performs zero-shot code translation by replacing long user-defined identifiers with generalized placeholders before feeding the code into the LLM, lowering token count and memory usage.

Result: Experiments show the method maintains code syntax and structure while reducing translation tokens required.

Conclusion: Identifier replacement enables more efficient and accurate long code translation using LLMs by focusing on code structure and lowering computational cost.

Abstract: In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don't fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.

</details>


### [21] [Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding](https://arxiv.org/abs/2510.09058)
*Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: A global survey of software professionals reveals LLMs are mainly used as coding aids, boosting productivity but raising concerns about errors, context limitations, and ethics. Most developers use them as supplementary tools, pointing to a cautious but practical adoption and underlining the need for further research and responsible practices.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are becoming increasingly prevalent in software development, but there is limited knowledge about how these models are used in practice and how software professionals perceive their value and challenges.

Method: The authors conducted a global survey involving 131 software practitioners to gather data on how LLMs are used and perceived in real development workflows.

Result: LLMs are primarily used for a variety of coding-related tasks. Practitioners report benefits such as greater productivity, lower cognitive effort, and faster learning. However, concerns are raised regarding LLMs' inaccuracy, limited context awareness, and ethical risks. Most practitioners use LLMs as assistive aids instead of fully relying on them.

Conclusion: The study provides an early, practitioner-oriented understanding of LLM adoption, emphasizing both benefits and challenges. Developers approach LLM integration cautiously, using them as support tools. These insights highlight important considerations for future research and responsible use in software engineering.

Abstract: Large Language Models have quickly become a central component of modern
software development workflows, and software practitioners are increasingly
integrating LLMs into various stages of the software development lifecycle.
Despite the growing presence of LLMs, there is still a limited understanding of
how these tools are actually used in practice and how professionals perceive
their benefits and limitations. This paper presents preliminary findings from a
global survey of 131 software practitioners. Our results reveal how LLMs are
utilized for various coding-specific tasks. Software professionals report
benefits such as increased productivity, reduced cognitive load, and faster
learning, but also raise concerns about LLMs' inaccurate outputs, limited
context awareness, and associated ethical risks. Most developers treat LLMs as
assistive tools rather than standalone solutions, reflecting a cautious yet
practical approach to their integration. Our findings provide an early,
practitioner-focused perspective on LLM adoption, highlighting key
considerations for future research and responsible use in software engineering.

</details>


### [22] [Constraint-Guided Unit Test Generation for Machine Learning Libraries](https://arxiv.org/abs/2510.09108)
*Lukas Krodinger,Altin Hajdari,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: Automated test generators struggle with ML API constraints. PynguinML uses documentation-extracted constraints to create better tests for PyTorch and TensorFlow, improving code coverage by up to 63.9%.


<details>
  <summary>Details</summary>
Motivation: Machine learning libraries require robust testing for correctness, but automated test generators fail to handle strict API input constraints, leading to low coverage.

Method: The paper proposes PynguinML, an enhanced version of Pynguin, which utilizes input constraints from API documentation to generate compliant test cases for ML libraries.

Result: PynguinML was evaluated on 165 modules from PyTorch and TensorFlow, showing up to 63.9% increase in code coverage compared to the original Pynguin tool.

Conclusion: Leveraging API constraint information substantially improves automated test case generation for ML libraries, leading to more effective and comprehensive testing.

Abstract: Machine learning (ML) libraries such as PyTorch and TensorFlow are essential
for a wide range of modern applications. Ensuring the correctness of ML
libraries through testing is crucial. However, ML APIs often impose strict
input constraints involving complex data structures such as tensors. Automated
test generation tools such as Pynguin are not aware of these constraints and
often create non-compliant inputs. This leads to early test failures and
limited code coverage. Prior work has investigated extracting constraints from
official API documentation. In this paper, we present PynguinML, an approach
that improves the Pynguin test generator to leverage these constraints to
generate compliant inputs for ML APIs, enabling more thorough testing and
higher code coverage. Our evaluation is based on 165 modules from PyTorch and
TensorFlow, comparing PynguinML against Pynguin. The results show that
PynguinML significantly improves test effectiveness, achieving up to 63.9 %
higher code coverage.

</details>


### [23] [A Semantic Framework for Patient Digital Twins in Chronic Care](https://arxiv.org/abs/2510.09134)
*Amal Elgammal,Bernd J. Krämer,Michael P. Papazoglou,Mira Raheem*

Main category: cs.SE

TL;DR: The paper introduces PMDT, a privacy-preserving, ontology-based digital twin framework for chronic care that integrates and standardizes disparate health data streams. Validated through clinical study, it supports advanced analytics and serves as a foundation for next-generation digital health systems.


<details>
  <summary>Details</summary>
Motivation: Current digital twin (DT) applications for chronic care are limited: they tend to focus on single organs or data types and do not offer a unified, privacy-preserving system to integrate multimodal health data. There is a pressing need for a framework that can combine various patient information streams (physiological, psychosocial, behavioral, genomic) into a semantically consistent, reusable, and privacy-compliant foundation.

Method: The paper presents the Patient Medical Digital Twin (PMDT), an ontology-driven framework built in OWL 2.0. PMDT organizes health data into modular 'Blueprints' covering patient profile, disease, treatment, care trajectories, safety, pathways, and adverse events. It uses conceptual views refined via expert workshops, questionnaires, and pilot studies (notably, with immunotherapy patients in the EU H2020 QUALITOP project) to validate, test, and improve the ontology.

Result: The PMDT framework demonstrates successful integration of heterogeneous health data, accurate automatic reasoning, broad ontology coverage, usability in clinical contexts, and compliance with GDPR. It can operationalize clinical queries and support a range of analytics (descriptive, predictive, prescriptive) in federated, privacy-preserving environments.

Conclusion: PMDT bridges key gaps related to fragmented data and lack of semantic standardization in chronic care management. It sets the groundwork for future digital health ecosystems that are personalized, proactive, continuously optimized, and equitable, leading to transformed chronic care delivery.

Abstract: Personalized chronic care requires the integration of multimodal health data
to enable precise, adaptive, and preventive decision-making. Yet most current
digital twin (DT) applications remain organ-specific or tied to isolated data
types, lacking a unified and privacy-preserving foundation. This paper
introduces the Patient Medical Digital Twin (PMDT), an ontology-driven in
silico patient framework that integrates physiological, psychosocial,
behavioral, and genomic information into a coherent, extensible model.
Implemented in OWL 2.0, the PMDT ensures semantic interoperability, supports
automated reasoning, and enables reuse across diverse clinical contexts. Its
ontology is structured around modular Blueprints (patient, disease and
diagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse
events), formalized through dedicated conceptual views. These were iteratively
refined and validated through expert workshops, questionnaires, and a pilot
study in the EU H2020 QUALITOP project with real-world immunotherapy patients.
Evaluation confirmed ontology coverage, reasoning correctness, usability, and
GDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous
data, operationalize competency questions, and support descriptive, predictive,
and prescriptive analytics in a federated, privacy-preserving manner. By
bridging gaps in data fragmentation and semantic standardization, the PMDT
provides a validated foundation for next-generation digital health ecosystems,
transforming chronic care toward proactive, continuously optimized, and
equitable management.

</details>


### [24] [A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms](https://arxiv.org/abs/2510.09308)
*Mira Raheem,Amal Elgammal,Michael Papazoglou,Bernd Krämer,Neamat El-Tazi*

Main category: cs.SE

TL;DR: A novel model-driven engineering framework with graphical tools and federated learning makes AI systems in healthcare easier, more consistent, and privacy-preserving, showing high accuracy and reduced coding burden in a cancer immunotherapy study.


<details>
  <summary>Details</summary>
Motivation: Despite the transformative potential of AI in healthcare, real-world implementation is hindered by fragmented data, privacy concerns, and the complexity of reliable clinical system development. Overcoming these barriers is essential for effective AI adoption in clinical settings.

Method: The authors present a Model Driven Engineering (MDE) framework tailored for healthcare AI, built on formal metamodels, domain-specific languages (DSLs), and automated software transformations. Central to this framework is the Medical Interoperability Language (MILA), a graphical DSL enabling multi-institutional collaboration via shared ontologies and federated learning without exchanging patient data. The approach is evaluated using a multicenter cancer immunotherapy study.

Result: The framework enabled strong predictive performance in the study, with support vector machines achieving up to 98.5% accuracy on key tasks. It also substantially reduced manual coding effort and ensured semantic consistency and privacy in cross-site collaboration.

Conclusion: MDE principles—hybrid metamodeling, semantic integration, and automated code generation—offer a practical solution for building interoperable, reproducible, and trustworthy digital health platforms, facilitating AI's effective and privacy-preserving use in healthcare.

Abstract: Artificial intelligence (AI) has the potential to transform healthcare by
supporting more accurate diagnoses and personalized treatments. However, its
adoption in practice remains constrained by fragmented data sources, strict
privacy rules, and the technical complexity of building reliable clinical
systems. To address these challenges, we introduce a model driven engineering
(MDE) framework designed specifically for healthcare AI. The framework relies
on formal metamodels, domain-specific languages (DSLs), and automated
transformations to move from high level specifications to running software. At
its core is the Medical Interoperability Language (MILA), a graphical DSL that
enables clinicians and data scientists to define queries and machine learning
pipelines using shared ontologies. When combined with a federated learning
architecture, MILA allows institutions to collaborate without exchanging raw
patient data, ensuring semantic consistency across sites while preserving
privacy. We evaluate this approach in a multi center cancer immunotherapy
study. The generated pipelines delivered strong predictive performance, with
support vector machines achieving up to 98.5 percent and 98.3 percent accuracy
in key tasks, while substantially reducing manual coding effort. These findings
suggest that MDE principles metamodeling, semantic integration, and automated
code generation can provide a practical path toward interoperable,
reproducible, and trustworthy digital health platforms.

</details>


### [25] [TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation](https://arxiv.org/abs/2510.09400)
*He Jiang,Yufu Wang,Hao Lin,Peiyu Zou,Zhide Zhou,Ang Jia,Xiaochen Li,Zhilei Ren*

Main category: cs.SE

TL;DR: TIT introduces tree-structured instruction tuning to improve LLM code translation, mitigating both syntactic confusion and semantic misalignment, and achieves much higher translation success rates than current approaches.


<details>
  <summary>Details</summary>
Motivation: Mainstream LLM-based code translation methods struggle with two main challenges: they are prone to copying source-language syntax or words (leading to syntactic confusion) and lack fine-grained semantic alignment, which causes errors in matching the meaning of code between languages.

Method: The authors introduce TIT (Tree-structured Instruction Tuning), which includes three modules: (1) a syntactic information representation module that uses structured parsing for language-agnostic features, (2) a fine-grained parallel dataset augmentation module that creates statement-level data with contrastive matching, and (3) a dual-stage tree instruction tuning module that first fine-tunes the model on syntax-aware tasks and then on code generation tasks with function-level dependencies.

Result: Experimental results show TIT delivers significantly better code translation performance across various LLMs, with a success rate 1.22x-1.75x higher than previous methods and substantially less syntactic confusion.

Conclusion: Tree-structured Instruction Tuning (TIT) effectively resolves key limitations in LLM-based code translation by introducing syntactic and semantic fine-tuning, achieving superior accuracy and reduced confusion in translated code compared to existing methods.

Abstract: Large Language Models (LLMs) have shown strong performance in automated
source-to-target code translation through pretraining on extensive code
corpora. However, mainstream LLM-based code translation methods suffer from two
critical limitations. First, they are highly sensitive to language-specific
features, which often introduce source-language syntax or lexicon into the
output, leading to syntactic confusion. Second, they lack fine-grained semantic
alignment due to an over-reliance on function-level parallel datasets,
resulting in semantic misalignment between the translated code and the original
source. To overcome these limitations, we propose TIT, a Tree-structured
Instruction Tuning paradigm for LLM-based code translation. Specifically, TIT
consists of three modules. First, to mitigate syntactic confusion, the
syntactic information representation module integrates language-agnostic
syntactic features via structured parsing. Then, to generate high-quality
fine-grained parallel data, the fine-grained parallel dataset augmentation
module aligns nodes with code segments through statement-level segmentation and
contrastive matching. Finally, we leverage the dual-stage tree instruction
tuning module to alleviate the contextual processing burden on the LLM caused
by the introduction of syntactic information. The first stage employs
syntax-aware fine-tuning to enable the LLM to autonomously comprehend
structured syntactic information, while the second stage utilizes code
generation fine-tuning to guide the model in generating accurate target code
based on function-level syntactic dependencies. The experimental results
demonstrate that the proposed method significantly outperforms existing
approaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in
code translation while markedly reducing syntactic confusion.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [26] [Neptune: Advanced ML Operator Fusion for Locality and Parallelism on GPUs](https://arxiv.org/abs/2510.08726)
*Yifan Zhao,Egan Johnson,Prasanth Chatarasi,Vikram Adve,Sasa Misailovic*

Main category: cs.PL

TL;DR: Neptune is a new tensor compiler that improves operator fusion for attention mechanisms by breaking dependencies and adding algebraic corrections, achieving up to 1.35x speedup over other compilers on multiple GPUs.


<details>
  <summary>Details</summary>
Motivation: Existing tensor compilers have difficulties fusing complex reduction computations with loop-carried dependencies, which are common in attention mechanisms in deep learning. This limitation hinders the optimization potential of operator fusion.

Method: The paper introduces Neptune, a tensor compiler designed specifically for advanced operator fusion of sequences of reduction operators. Neptune uses a new approach: it intentionally breaks certain dependencies and constructs algebraic correction expressions to ensure that the output remains correct despite the fusion.

Result: On ten benchmarks focused on attention mechanisms, Neptune-generated kernels, using simple attention code and a high-level scheduling template, outperform other compilers such as Triton, TVM, and FlexAttention (including Triton-based FlashAttention). Neptune shows an average speedup of 1.35x over the best alternative across four GPU architectures from NVIDIA and AMD.

Conclusion: Neptune presents an effective method for optimizing complex reduction operator fusion in deep learning, particularly in attention workloads, providing consistent performance improvements over existing tensor compilers.

Abstract: Operator fusion has become a key optimization for deep learning, which
combines multiple deep learning operators to improve data reuse and reduce
global memory transfers. However, existing tensor compilers struggle to fuse
complex reduction computations involving loop-carried dependencies, such as
attention mechanisms.
  The paper introduces Neptune, a tensor compiler for advanced operator fusion
for sequences of reduction operators. Neptune presents a new approach for
advanced operator fusion, which intentionally breaks some existing dependencies
and compensates by constructing algebraic correction expressions that allow the
kernel to produce the correct result.
  On ten attention-based benchmarks, Neptune, starting from simple attention
code and a high-level scheduling template, outperforms existing compilers like
Triton, TVM, and FlexAttention, including Triton-based implementations of
FlashAttention. Across four different GPU architectures from NVIDIA and AMD,
Neptune-generated kernels have average speedup of $1.35\times$ over the next
best alternative, demonstrating its effectiveness for deep learning workloads.

</details>


### [27] [Typestate via Revocable Capabilities](https://arxiv.org/abs/2510.08889)
*Songlin Jia,Craig Liu,Siyuan He,Haotian Deng,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: The paper presents a scalable solution for safe and expressive stateful resource management by extending existing capability mechanisms into flow-sensitive typestate tracking, implemented in Scala 3. It supports a variety of patterns and improves both safety and ergonomics with minimal language changes.


<details>
  <summary>Details</summary>
Motivation: Managing stateful resources safely is difficult, particularly when aliasing is present. Existing solutions either restrict expressiveness and parallelism (e.g., scope-based constructs) or require complex and explicit state management by programmers (e.g., imperative, flow-sensitive management with typestate). The motivation is to find an approach that combines safety with expressiveness without increasing complexity.

Method: The authors extend flow-insensitive capability mechanisms into flow-sensitive typestate tracking. Their system decouples capability lifetimes from lexical scopes, making capabilities manageable in a flow-sensitive style. The approach is implemented as an extension to the Scala 3 compiler, using path-dependent types and implicit resolution for safety and conciseness.

Result: The prototype supports various stateful usage patterns such as file operations, locking protocols, DOM construction, and session types. It achieves expressive and safe typestate management with minimal changes to existing capability-based languages.

Conclusion: Expressive and safe typestate management is possible with minimal language extension, providing both robustness and ergonomics for stateful programming.

Abstract: Managing stateful resources safely and expressively is a longstanding
challenge in programming languages, especially in the presence of aliasing.
While scope-based constructs such as Java's synchronized blocks offer ease of
reasoning, they restrict expressiveness and parallelism. Conversely,
imperative, flow-sensitive management enables fine-grained control but demands
sophisticated typestate analyses and often burdens programmers with explicit
state tracking.
  In this work, we present a novel approach that unifies the strengths of both
paradigms by extending flow-insensitive capability mechanisms into
flow-sensitive typestate tracking. Our system decouples capability lifetimes
from lexical scopes, allowing functions to provide, revoke, and return
capabilities in a flow-sensitive manner, based on the existing mechanisms
explored for the safety and ergonomics of scoped capability programming.
  We implement our approach as an extension to the Scala 3 compiler, leveraging
path-dependent types and implicit resolution to enable concise, statically
safe, and expressive typestate programming. Our prototype generically supports
a wide range of stateful patterns, including file operations, advanced locking
protocols, DOM construction, and session types. This work demonstrates that
expressive and safe typestate management can be achieved with minimal
extensions to existing capability-based languages, paving the way for more
robust and ergonomic stateful programming.

</details>


### [28] [Free to Move: Reachability Types with Flow-Sensitive Effects for Safe Deallocation and Ownership Transfer](https://arxiv.org/abs/2510.08939)
*Haotian Deng,Siyuan He,Songlin Jia,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: This paper introduces a novel effect system that allows safe, flow-sensitive manual memory management—including Rust-like move semantics—in higher-order functional languages. It tracks resource operations and ensures use-after-free safety without regions or linear types, achieving precise memory control and advancing language safety guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing systems for memory management in functional languages struggle to support explicit resource control and ownership transfer, especially in higher-order impure contexts. Traditional approaches, such as regions or linear types, can be restrictive or complicated. The motivation is to advance safer manual memory management without those limitations.

Method: The paper proposes a flow-sensitive effect system for reachability types that refines previous qualifiers with polymorphic 'use' and 'kill' effects. It formalizes a calculus with detailed typing and effect rules, and presents a compositional operational semantics, all of which are mechanized for metatheoretic guarantees like preservation, progress, and effect soundness.

Result: The system enables tracking and expression of operations on resources such as ownership transfer, freshness, and updates. It models common memory management idioms (deallocation, move semantics, swapping) and provides precise use-after-free safety guarantees, validated by mechanization.

Conclusion: Combining reachability reasoning with explicit resource management, the system supports safe manual memory management in higher-order functional languages without relying on regions or linearity, signaling a significant advancement.

Abstract: We present a flow-sensitive effect system for reachability types that
supports explicit memory management, including Rust-style move semantics, in
higher-order impure functional languages. Our system refines the existing
reachability qualifier with polymorphic \emph{use} and \emph{kill} effects that
record how references are read, written, transferred, and deallocated. The
effect discipline tracks operations performed on each resource using
qualifiers, enabling the type system to express ownership transfer, contextual
freshness, and destructive updates without regions or linearity. We formalize
the calculus, its typing and effect rules, and a compositional operational
semantics that validates use-after-free safety. All metatheoretic results,
including preservation, progress, and effect soundness, are mechanized. The
system models idioms such as reference deallocation, move semantics, reference
swapping, while exposing precise safety guarantee. Together, these
contributions integrate reachability-based reasoning with explicit resource
control, advancing the state of the art in safe manual memory management for
higher-order functional languages.

</details>


### [29] [Concept-Based Generic Programming in C++](https://arxiv.org/abs/2510.08969)
*Bjarne Stroustrup*

Main category: cs.PL

TL;DR: This paper demonstrates how C++ concepts allow for safer, more expressive generic programming by eliminating unsafe type conversions, enabling range checks, and supporting user-defined type extensions, highlighting concepts' design rationales and integration with everyday C++ programming.


<details>
  <summary>Details</summary>
Motivation: C++ generic programming has been central but challenging due to limited language features for expressing type constraints and safe type manipulation. The motivation is to demonstrate how concepts clarify and enforce constraints, improve safety, and integrate generic programming into general programming practice.

Method: The authors illustrate C++ generic programming techniques by using concepts, developing a type system that eliminates unsafe conversions and adding range checking, leveraging concepts for user-defined type system extensions, and supporting their discussion with design rationales and historical context.

Result: Through programming examples and language features like concepts, lambdas, variadic templates, and static reflection, the paper shows improved safety (elimination of narrowing conversions, range checks) and extensibility in user code, without notational or runtime overhead.

Conclusion: The paper concludes that concepts and new facilities in C++ enhance generic programming, making code more expressive, safer, and easier to extend, integrating generic programming deeply into mainstream C++ usage.

Abstract: We present programming techniques to illustrate the facilities and principles
of C++ generic programming using concepts. Concepts are C++'s way to express
constraints on generic code. As an initial example, we provide a simple type
system that eliminates narrowing conversions and provides range checking
without unnecessary notational or run-time overheads. Concepts are used
throughout to provide user-defined extensions to the type system. The aim is to
show their utility and the fundamental ideas behind them, rather than to
provide a detailed or complete explanation of C++'s language support for
generic programming or the extensive support provided by the standard library.
Generic programming is an integral part of C++, rather than an isolated
sub-language. In particular, key facilities support general programming as well
as generic programming (e.g., uniform notation for types, lambdas, variadic
templates, and C++26 static reflection). Finally, we give design rationales and
origins for key parts of the concept design, including use patterns, the
relationship to Object-Oriented Programming, value arguments, notation, concept
type-matching, and definition checking.

</details>


### [30] [A Multilingual Python Programming Language](https://arxiv.org/abs/2510.09591)
*Saad Ahmed Bazaz,Mirza Omer Beg*

Main category: cs.PL

TL;DR: Programming languages often require English proficiency, excluding non-English speakers. The authors developed UniversalPython, a Python-based transpiler allowing code to be written in various human languages. Urdu Python is demonstrated; future expansions will add more languages to improve programming accessibility. The project is open-source.


<details>
  <summary>Details</summary>
Motivation: Entry into programming is hindered for non-English speakers due to the dominance of English-based programming languages, making it difficult and costly for them to learn.

Method: Authors created UniversalPython, a language transpiler that runs on top of Python, allowing users to write Python code in their native human language. As a proof of concept, they implemented "Urdu Python."

Result: UniversalPython enables programming in Python using human languages other than English; demonstrated by supporting Urdu. The tool is open-source.

Conclusion: UniversalPython lowers the entry barrier for programming for non-English speakers by supporting native languages, with future plans to include more languages.

Abstract: All widely used and useful programming languages have a common problem. They
restrict entry on the basis of knowledge of the English language. The lack of
knowledge of English poses a major hurdle to many newcomers who do not have the
resources, in terms of time and money, to learn the English language. Studies
show that people learn better in their own language. Therefore, we propose a
language transpiler built on top of the Python programming language, called
UniversalPython, which allows one to write Python in their own human language.
We demonstrate the ability to create an "Urdu Python" with this transpiler. In
the future, we aim to scale the language to encapsulate more human languages to
increase the availability of programming. The source code for this transpiler
is open-source, and available at
https://github.com/universalpython/universalpython

</details>
