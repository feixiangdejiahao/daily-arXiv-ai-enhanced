{"id": "2510.23642", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23642", "abs": "https://arxiv.org/abs/2510.23642", "authors": ["Yuansheng Ni", "Songcheng Cai", "Xiangchao Chen", "Jiarong Liang", "Zhiheng Lyu", "Jiaqi Deng", "Kai Zou", "Ping Nie", "Fei Yuan", "Xiang Yue", "Wenhu Chen"], "title": "VisCoder2: Building Multi-Language Visualization Coding Agents", "comment": null, "summary": "Large language models (LLMs) have recently enabled coding agents capable of\ngenerating, executing, and revising visualization code. However, existing\nmodels often fail in practical workflows due to limited language coverage,\nunreliable execution, and lack of iterative correction mechanisms. Progress has\nbeen constrained by narrow datasets and benchmarks that emphasize single-round\ngeneration and single-language tasks. To address these challenges, we introduce\nthree complementary resources for advancing visualization coding agents.\nVisCode-Multi-679K is a large-scale, supervised dataset containing 679K\nvalidated and executable visualization samples with multi-turn correction\ndialogues across 12 programming languages. VisPlotBench is a benchmark for\nsystematic evaluation, featuring executable tasks, rendered outputs, and\nprotocols for both initial generation and multi-round self-debug. Finally, we\npresent VisCoder2, a family of multi-language visualization models trained on\nVisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms\nstrong open-source baselines and approaches the performance of proprietary\nmodels like GPT-4.1, with further gains from iterative self-debug, reaching\n82.4% overall execution pass rate at the 32B scale, particularly in symbolic or\ncompiler-dependent languages.", "AI": {"tldr": "This paper introduces a multi-language dataset, a new benchmark, and a family of models (VisCoder2) for visualization code generation, enabling more reliable and iterative solutions that outperform leading open-source tools and approach proprietary model performance.", "motivation": "Current coding agents powered by large language models struggle with visualization tasks due to narrow language support, unreliable execution, and lack of multi-round correction mechanisms. The progress in this domain is stunted by single-language, single-turn datasets and benchmarks.", "method": "The paper presents three new resources: (1) VisCode-Multi-679K, a large multi-language, multi-turn visualization code dataset; (2) VisPlotBench, a benchmark for multi-round evaluation with executable tasks and outputs; and (3) VisCoder2, a family of models trained on the new dataset and tested across multiple languages with iterative self-debugging.", "result": "VisCoder2, when evaluated using VisPlotBench, outperforms strong open-source baselines and gets close to the proprietary GPT-4.1 model, reaching 82.4% execution pass rate at 32B model scale, especially in symbolic or compiler-dependent languages.", "conclusion": "The new dataset, benchmark, and model advances the field of visualization coding agents, enabling more robust, multi-language, and iterative code generation and revision. Iterative correction and broader language support are key to improved performance."}}
{"id": "2510.23627", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23627", "abs": "https://arxiv.org/abs/2510.23627", "authors": ["Fred Zimmerman"], "title": "AI-Driven Development of a Publishing Imprint: Xynapse Traces", "comment": null, "summary": "Xynapse Traces is an experimental publishing imprint created via a fusion of\nhuman and algorithmic methods using a configuration-driven architecture and a\nmulti-model AI integration framework. The system achieved a remarkable 90%\nreduction in time-to-market (from a typical 6-12 months to just 2-4 weeks),\nwith 80% cost reduction compared to traditional imprint development, while\npublishing 52 books in its first year and maintaining exceptional quality\nmetrics, including 99% citation accuracy and 100% validation success after\ninitial corrections. Key technical innovations include a continuous ideation\npipeline with tournament-style evaluation, a novel codex design for\ntranscriptive meditation practice, comprehensive automation spanning from\nideation through production and distribution, and publisher personas that\ndefine and guide the imprint's mission. The system also integrates automated\nverification with human oversight, ensuring that gains in speed do not\ncompromise publishing standards. This effort has significant implications for\nthe future of book publishing, suggesting new paradigms for human-AI\ncollaboration that democratize access to sophisticated publishing capabilities\nand make previously unviable niche markets accessible.", "AI": {"tldr": "Xynapse Traces uses human-AI collaboration, automation, and innovative workflows to create a rapid, cost-effective, and high-quality publishing imprint, suggesting a future of democratized book production.", "motivation": "Traditional publishing is slow, costly, and inaccessible for niche and emerging markets. The paper aims to overcome these limitations by leveraging advanced human-AI integration.", "method": "A fusion of human and algorithmic methods under a configuration-driven architecture and multi-model AI framework, with continuous ideation, automation throughout the workflow, and human-in-the-loop verification.", "result": "Time-to-market reduced from 6-12 months to 2-4 weeks (90% decrease), costs cut by 80%, 52 books published in year one, 99% citation accuracy, and 100% validation success post-correction. The approach enables scalability, accessibility, and high quality.", "conclusion": "Xynapse Traces demonstrates that a configuration-driven, multi-model AI-powered approach can drastically improve efficiency and accessibility in book publishing, enabling democratization of production while maintaining high standards."}}
{"id": "2510.23664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23664", "abs": "https://arxiv.org/abs/2510.23664", "authors": ["Eranga Bandara", "Ross Gore", "Xueping Liang", "Sachini Rajapakse", "Isurunima Kularathne", "Pramoda Karunarathna", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Amin Hass", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "Agentsway -- Software Development Methodology for AI Agents-based Teams", "comment": null, "summary": "The emergence of Agentic AI is fundamentally transforming how software is\ndesigned, developed, and maintained. Traditional software development\nmethodologies such as Agile, Kanban, ShapeUp, etc, were originally designed for\nhuman-centric teams and are increasingly inadequate in environments where\nautonomous AI agents contribute to planning, coding, testing, and continuous\nlearning. To address this methodological gap, we present \"Agentsway\" a novel\nsoftware development framework designed for ecosystems where AI agents operate\nas first-class collaborators. Agentsway introduces a structured lifecycle\ncentered on human orchestration, and privacy-preserving collaboration among\nspecialized AI agents. The framework defines distinct roles for planning,\nprompting, coding, testing, and fine-tuning agents, each contributing to\niterative improvement and adaptive learning throughout the development process.\nBy integrating fine-tuned LLMs that leverage outputs and feedback from\ndifferent agents throughout the development cycle as part of a retrospective\nlearning process, Agentsway enhances domain-specific reasoning, and explainable\ndecision-making across the entire software development lifecycle. Responsible\nAI principles are further embedded across the agents through the coordinated\nuse of multiple fine-tuned LLMs and advanced reasoning models, ensuring\nbalanced, transparent, and accountable decision-making. This work advances\nsoftware engineering by formalizing agent-centric collaboration, integrating\nprivacy-by-design principles, and defining measurable metrics for productivity\nand trust. Agentsway represents a foundational step toward the next generation\nof AI-native, self-improving software development methodologies. To the best of\nour knowledge, this is the first research effort to introduce a dedicated\nmethodology explicitly designed for AI agent-based software engineering teams.", "AI": {"tldr": "The paper introduces Agentsway, a novel framework explicitly designed for agentic AI-driven software engineering, addressing the shortcomings of traditional human-centric methods by formalizing agent roles, integrating privacy and responsible AI, and enabling self-improving, transparent software development teams.", "motivation": "Traditional software development methods are human-centered and inadequate for environments where autonomous AI agents are core contributors. A new methodology is needed for agent-centric collaboration.", "method": "Agentsway introduces a structured software development lifecycle where specialized AI agents carry out distinct roles (planning, coding, testing, etc.) with human orchestration and privacy-preserving collaboration, using fine-tuned LLMs and advanced reasoning models.", "result": "Agentsway enables iterative improvement, adaptive learning, and explainable decision-making in AI agent-centric software projects, with improved domain-specific reasoning, transparency, and accountability.", "conclusion": "Agentsway establishes a foundational methodology for AI agent-based software engineering teams, integrating privacy-by-design, ethical AI, and measurable productivity/trust metrics."}}
{"id": "2510.23674", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23674", "abs": "https://arxiv.org/abs/2510.23674", "authors": ["Bin Wang", "Hui Li", "AoFan Liu", "BoTao Yang", "Ao Yang", "YiLu Zhong", "Weixiang Huang", "Yanping Zhang", "Runhuai Huang", "Weimin Zeng"], "title": "RefleXGen:The unexamined code is not worth using", "comment": null, "summary": "Security in code generation remains a pivotal challenge when applying large\nlanguage models (LLMs). This paper introduces RefleXGen, an innovative method\nthat significantly enhances code security by integrating Retrieval-Augmented\nGeneration (RAG) techniques with guided self-reflection mechanisms inherent in\nLLMs. Unlike traditional approaches that rely on fine-tuning LLMs or developing\nspecialized secure code datasets - processes that can be resource-intensive -\nRefleXGen iteratively optimizes the code generation process through\nself-assessment and reflection without the need for extensive resources. Within\nthis framework, the model continuously accumulates and refines its knowledge\nbase, thereby progressively improving the security of the generated code.\nExperimental results demonstrate that RefleXGen substantially enhances code\nsecurity across multiple models, achieving a 13.6% improvement with GPT-3.5\nTurbo, a 6.7% improvement with GPT-4o, a 4.5% improvement with CodeQwen, and a\n5.8% improvement with Gemini. Our findings highlight that improving the quality\nof model self-reflection constitutes an effective and practical strategy for\nstrengthening the security of AI-generated code.", "AI": {"tldr": "RefleXGen boosts code security in LLM-generated code by combining retrieval augmentation with model self-reflection, showing notable security gains on multiple language models without costly fine-tuning.", "motivation": "Address persistent security vulnerabilities in code produced by LLMs, aiming for a resource-efficient technique that does not require extensive fine-tuning or the creation of secure code datasets.", "method": "RefleXGen integrates Retrieval-Augmented Generation with guided self-reflection mechanisms, allowing LLMs to iteratively self-assess and refine their code generation without relying on costly fine-tuning or specialized datasets.", "result": "RefleXGen improves code security across various models, with specific security improvements: 13.6% increase for GPT-3.5 Turbo, 6.7% for GPT-4o, 4.5% for CodeQwen, and 5.8% for Gemini.", "conclusion": "Enhancing the self-reflection abilities of large language models is a practical and effective strategy to improve code security in AI-generated outputs."}}
{"id": "2510.23761", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.23761", "abs": "https://arxiv.org/abs/2510.23761", "authors": ["Kevin Han", "Siddharth Maddikayala", "Tim Knappe", "Om Patel", "Austen Liao", "Amir Barati Farimani"], "title": "TDFlow: Agentic Workflows for Test Driven Software Engineering", "comment": null, "summary": "We introduce TDFlow, a novel test-driven agentic workflow that frames\nrepository-scale software engineering as a test-resolution task, specifically\ndesigned to solve human-written tests. Given a set of tests, TDFlow repeatedly\nproposes, revises, and debugs repository-scale patches using precisely\nengineered sub-agents and tightly constrained tools. The workflow decomposes\nsoftware engineering program repair into four components governed by respective\nsub-agents. This simple, forced decoupling of patch proposing, debugging, patch\nrevision, and optional test generation (1) reduces long-context burden on any\nindividual sub-agent, (2) focuses each sub-agent on specific, pre-defined\nsub-tasks, and (3) allows for specialized performance improvement on specific\nsub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on\nSWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and\n94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within\nSWE-Bench Lite and Verified uncover only 7 instances of test hacking, which\nwere subsequently counted as failures. Furthermore, we show that the primary\nobstacle to human-level software engineering performance lies within writing\nsuccessful reproduction tests. We envision a human-LLM interactive system\npowered by TDFlow where human developers write tests solved by LLM systems.\nTogether, these results indicate that modern LLMs, when embedded in a narrowly\nengineered, test-driven workflow, already achieve human-level test resolution\n-- with the final frontier for fully autonomous repository repair being the\naccurate generation of valid reproduction tests.", "AI": {"tldr": "TDFlow is a new agentic workflow that decomposes repository-scale program repair, leading to major improvements over prior systems on test resolution tasks. It achieves near human-level performance, with the main limitation being reliable test generation. The approach could power future human-LLM collaboration in software engineering.", "motivation": "There is an increasing need to automate repository-scale software engineering tasks, aiming to solve human-written tests effectively and improve upon existing systems.", "method": "TDFlow is proposed as a novel test-driven workflow that divides the program repair process into four sub-agent governed modules: patch proposing, debugging, patch revision, and optional test generation. Each sub-agent tackles a focused sub-task, reducing complexity and enhancing specialization. The workflow operates by repeatedly iterating proposals, revisions, and debugging using engineered tools and strategies, and it focuses on solving human-authored test cases.", "result": "TDFlow achieves an 88.8% test pass rate on SWE-Bench Lite, which is 27.8% higher than the next-best system, and a 94.3% pass rate on SWE-Bench Verified. Only 7 out of 800 test runs were flagged for test hacking. The primary challenge identified is in writing successful reproduction tests, not in passing them.", "conclusion": "Modern LLMs, when integrated within a tightly engineered and test-driven workflow like TDFlow, can reach human-level software test resolution performance. The major limitation for full autonomy lies in generating valid reproduction tests. A future human-LLM interactive workflow is envisioned, where humans author tests solved by such systems."}}
{"id": "2510.23893", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23893", "abs": "https://arxiv.org/abs/2510.23893", "authors": ["Rodrigo Falc\u00e3o", "Stefan Schweitzer", "Julien Siebert", "Emily Calvet", "Frank Elberzhager"], "title": "Evaluating the effectiveness of LLM-based interoperability", "comment": null, "summary": "Background: Systems of systems are becoming increasingly dynamic and\nheterogeneous, and this adds pressure on the long-standing challenge of\ninteroperability. Besides its technical aspect, interoperability has also an\neconomic side, as development time efforts are required to build the\ninteroperability artifacts. Objectives: With the recent advances in the field\nof large language models (LLMs), we aim at analyzing the effectiveness of\nLLM-based strategies to make systems interoperate autonomously, at runtime,\nwithout human intervention. Method: We selected 13 open source LLMs and curated\nfour versions of a dataset in the agricultural interoperability use case. We\nperformed three runs of each model with each version of the dataset, using two\ndifferent strategies. Then we compared the effectiveness of the models and the\nconsistency of their results across multiple runs. Results: qwen2.5-coder:32b\nwas the most effective model using both strategies DIRECT (average pass@1 >=\n0.99) and CODEGEN (average pass@1 >= 0.89) in three out of four dataset\nversions. In the fourth dataset version, which included an unit conversion, all\nmodels using the strategy DIRECT failed, whereas using CODEGEN\nqwen2.5-coder:32b succeeded with an average pass@1 = 0.75. Conclusion: Some\nLLMs can make systems interoperate autonomously. Further evaluation in\ndifferent domains is recommended, and further research on reliability\nstrategies should be conducted.", "AI": {"tldr": "This paper evaluates the ability of large language models to autonomously facilitate system interoperability, focusing on agricultural datasets. qwen2.5-coder:32b achieved the best performance, especially when using the CODEGEN strategy. Full autonomy was not universal, highlighting the need for further research and domain-specific validation.", "motivation": "With increasing system complexity and heterogeneity, interoperability remains a challenge, both technically and economically. Recent advances in Large Language Models (LLMs) offer a possible solution for autonomous system interoperability without ongoing human intervention.", "method": "13 open source LLMs were tested across four versions of a dataset related to agricultural interoperability. Each model underwent three runs per dataset version using two strategic approaches (DIRECT and CODEGEN). Effectiveness and result consistency were compared across models and strategies.", "result": "The qwen2.5-coder:32b model was most effective in three out of four dataset versions using both DIRECT (average pass@1 >= 0.99) and CODEGEN (average pass@1 >= 0.89) strategies. For the dataset requiring unit conversion, all models failed using DIRECT, but qwen2.5-coder:32b succeeded using CODEGEN (average pass@1 = 0.75).", "conclusion": "Some LLMs, notably qwen2.5-coder:32b, can autonomously enable system interoperability in certain cases. More comprehensive evaluation across domains and research into reliability strategies is advised."}}
{"id": "2510.23970", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23970", "abs": "https://arxiv.org/abs/2510.23970", "authors": ["Maria C. Borges", "Julian Legler", "Lucca Di Benedetto"], "title": "Validating Alerts in Cloud-Native Observability", "comment": "16th Symposium on Software Performance (SSP'25)", "summary": "Observability and alerting form the backbone of modern reliability\nengineering. Alerts help teams catch faults early before they turn into\nproduction outages and serve as first clues for troubleshooting. However,\ndesigning effective alerts is challenging. They need to strike a fine balance\nbetween catching issues early and minimizing false alarms. On top of this,\nalerts often cover uncommon faults, so the code is rarely executed and\ntherefore rarely checked. To address these challenges, several industry\npractitioners advocate for testing alerting code with the same rigor as\napplication code. Still, there's a lack of tools that support such systematic\ndesign and validation of alerts.\n  This paper introduces a new alerting extension for the observability\nexperimentation tool OXN. It lets engineers experiment with alerts early during\ndevelopment. With OXN, engineers can now tune rules at design time and\nroutinely validate the firing behavior of their alerts, avoiding future\nproblems at runtime.", "AI": {"tldr": "The paper presents an extension to the OXN tool that lets reliability engineers rigorously design, validate, and tune alerts early in development, improving alert effectiveness and reducing production issues.", "motivation": "Designing effective alerts for reliability engineering is difficult due to the need to balance early fault detection and minimizing false alarms, alongside infrequent code execution for uncommon fault coverage. Existing development lacks rigorous tools for systematic alert validation and testing.", "method": "The paper proposes a new alerting extension for the observability experimentation tool OXN. This enables engineers to experiment with, tune, and validate alert rules in the development phase, prior to production deployment.", "result": "With the OXN extension, engineers can adjust alerting rules during design and regularly verify their firing behavior, helping prevent future problems and improving reliability of alerting systems.", "conclusion": "The alerting extension for OXN allows systematic validation and tuning of alerts, strengthening reliability engineering practices and reducing the risk of undetected faults or false alarms."}}
{"id": "2510.24019", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24019", "abs": "https://arxiv.org/abs/2510.24019", "authors": ["Xing Xing", "Wei Wang", "Lipeng Ma", "Weidong Yang", "Junjie Zheng"], "title": "Lifecycle-Aware code generation: Leveraging Software Engineering Phases in LLMs", "comment": null, "summary": "Recent progress in large language models (LLMs) has advanced automatic code\ngeneration, yet most approaches rely on direct, single-step translation from\nproblem descriptions to code, disregarding structured software engineering\npractices. We introduce a lifecycle-aware framework that systematically\nincorporates intermediate artifacts such as requirements analysis, state\nmachine modeling, and pseudocode into both the training and inference stages.\nThis design aligns code generation with standard software development phases\nand enables more structured reasoning. Experiments show that lifecycle-level\nfine-tuning improves code correctness by up to 75% over the same model before\nfine-tuning, with performance gains compounding across intermediate stages.\nMulti-step inference consistently surpasses single-step generation,\ndemonstrating the effectiveness of intermediate scaffolding. Notably,\nopen-source LLMs, once fine-tuned under our framework, match or slightly\noutperform models pretrained on code. When applied to DeepSeek-Coder-1.3B, our\nframework yields relative CodeBLEU improvements of 34.3%, 20.0%, 11.2%, and\n22.3% over ChatGPT-3.5, ChatGPT-4o-mini, DeepSeek-R1, and LLaMA-8B,\nrespectively. Our pipeline also proves robust with up to 80\\% less training\ndata, confirming its resilience. Ablation studies further reveal that each\nintermediate artifact contributes distinctly to final code quality, with state\nmachine modeling yielding the most substantial impact. Our source code and\ndetailed experimental data are available at\nhttps://anonymous.4open.science/r/Lifecycle-Aware-3CCB.", "AI": {"tldr": "By aligning code generation with structured software engineering phases and using intermediate artifacts, the proposed framework makes code generated by LLMs significantly more correct and robust\u2014often surpassing single-step methods and even proprietary models when fine-tuned accordingly.", "motivation": "Existing code generation systems ignore established software engineering processes, resulting in less-structured and potentially error-prone outputs. The paper aims to bridge data-driven LLM code generation with standard software development best practices to produce more accurate and reliable code.", "method": "The paper introduces a multi-step, lifecycle-aligned approach for code generation: it formally incorporates artifacts such as requirements, state machine diagrams, and pseudocode during both training and inference stages. LLMs are fine-tuned using this structure and evaluated by comparing code correctness, CodeBLEU scores, and robustness through ablation studies.", "result": "Lifecycle-aware code generation, which incorporates intermediate software engineering artifacts (e.g., requirements analysis, state machine modeling, pseudocode), greatly enhances the effectiveness of LLM-driven code generation. Multi-step inference using these artifacts outperforms traditional single-step code generation, leading to substantial improvements in code correctness and robustness, especially for open-source LLMs.", "conclusion": "The lifecycle-aware framework leads to significant improvements in code generation, making open-source LLMs competitive or superior to models pretrained on code with much less training data required. Intermediate artifacts, especially state machine modeling, are key contributors to higher code quality."}}
{"id": "2510.24142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24142", "abs": "https://arxiv.org/abs/2510.24142", "authors": ["Joran Leest", "Ilias Gerostathopoulos", "Patricia Lago", "Claudia Raibulet"], "title": "Monitoring and Observability of Machine Learning Systems: Current Practices and Gaps", "comment": null, "summary": "Production machine learning (ML) systems fail silently -- not with crashes,\nbut through wrong decisions. While observability is recognized as critical for\nML operations, there is a lack empirical evidence of what practitioners\nactually capture. This study presents empirical results on ML observability in\npractice through seven focus group sessions in several domains. We catalog the\ninformation practitioners systematically capture across ML systems and their\nenvironment and map how they use it to validate models, detect and diagnose\nfaults, and explain observed degradations. Finally, we identify gaps in current\npractice and outline implications for tooling design and research to establish\nML observability practices.", "AI": {"tldr": "Through focus groups with ML practitioners, this study mapped real-world approaches to ML observability, highlighted key deficiencies, and called for improved tools and research to strengthen ML system reliability.", "motivation": "Production ML systems often fail silently, leading to incorrect outcomes without clear signals; understanding real-world observability practices is crucial for improving ML system reliability and operation.", "method": "The authors conducted seven focus group sessions across multiple domains to empirically investigate what observability information ML practitioners capture and utilize.", "result": "The study catalogued systematically captured information by practitioners, mapped its usage for model validation, fault detection, diagnosis, and degradation explanation, and uncovered major areas needing enhancement in tooling and normative practices.", "conclusion": "The study reveals significant gaps in current ML observability practices, indicating a need for improved tooling and research to support robust ML operations."}}
{"id": "2510.24188", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24188", "abs": "https://arxiv.org/abs/2510.24188", "authors": ["C\u00e9sar Santos", "Ermeson Andrade", "Roberto Natella"], "title": "Investigating Software Aging in LLM-Generated Software Systems", "comment": "Presented at the 17th International Workshop on Software Aging and\n  Rejuvenation (WoSAR), 2025", "summary": "Automatically generated software, especially code produced by Large Language\nModels (LLMs), is increasingly adopted to accelerate development and reduce\nmanual effort. However, little is known about the long-term reliability of such\nsystems under sustained execution. In this paper, we experimentally investigate\nthe phenomenon of software aging in applications generated by LLM-based tools.\nUsing the Bolt platform and standardized prompts from Baxbench, we generated\nfour service-oriented applications and subjected them to 50-hour load tests.\nResource usage, response time, and throughput were continuously monitored to\ndetect degradation patterns. The results reveal significant evidence of\nsoftware aging, including progressive memory growth, increased response time,\nand performance instability across all applications. Statistical analyzes\nconfirm these trends and highlight variability in the severity of aging\naccording to the type of application. Our findings show the need to consider\naging in automatically generated software and provide a foundation for future\nstudies on mitigation strategies and long-term reliability evaluation.", "AI": {"tldr": "Automatically generated software using LLMs suffers from performance degradation over time, with increased memory use, slower responses, and instability, highlighting the need for aging mitigation strategies and further reliability studies.", "motivation": "The adoption of automatically generated software using Large Language Models (LLMs) is increasing, but there is limited understanding of its long-term reliability. The motivation is to investigate how such code behaves under prolonged usage, specifically in terms of software aging.", "method": "Researchers generated four service-oriented applications using LLM-based tools (Bolt platform with Baxbench prompts) and subjected them to 50-hour load tests. They continuously monitored resource usage, response time, and throughput to detect aging-related performance degradation.", "result": "The study found significant signs of software aging in all tested applications, such as progressive memory growth, increased response time, and unstable performance. Severity of aging depended on the application type.", "conclusion": "Automatically generated software by LLMs shows measurable software aging during sustained operation, pointing to reliability concerns and the need for future research into mitigation techniques and long-term evaluation."}}
{"id": "2510.24241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24241", "abs": "https://arxiv.org/abs/2510.24241", "authors": ["Zixian Zhang", "Takfarinas Saber"], "title": "MAGNET: A Multi-Graph Attentional Network for Code Clone Detection", "comment": null, "summary": "Code clone detection is a fundamental task in software engineering that\nunderpins refactoring, debugging, plagiarism detection, and vulnerability\nanalysis. Existing methods often rely on singular representations such as\nabstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs\n(DFGs), which capture only partial aspects of code semantics. Hybrid approaches\nhave emerged, but their fusion strategies are typically handcrafted and\nineffective. In this study, we propose MAGNET, a multi-graph attentional\nframework that jointly leverages AST, CFG, and DFG representations to capture\nsyntactic and semantic features of source code. MAGNET integrates residual\ngraph neural networks with node-level self-attention to learn both local and\nlong-range dependencies, introduces a gated cross-attention mechanism for\nfine-grained inter-graph interactions, and employs Set2Set pooling to fuse\nmulti-graph embeddings into unified program-level representations. Extensive\nexperiments on BigCloneBench and Google Code Jam demonstrate that MAGNET\nachieves state-of-the-art performance with an overall F1 score of 96.5\\% and\n99.2\\% on the two datasets, respectively. Ablation studies confirm the critical\ncontributions of multi-graph fusion and each attentional component. Our code is\navailable at https://github.com/ZixianReid/Multigraph_match", "AI": {"tldr": "MAGNET, a new multi-graph neural framework, greatly improves code clone detection by combining AST, CFG, and DFG representations with attention mechanisms, achieving best-in-class accuracy and confirming the value of multi-graph fusion.", "motivation": "The motivation is to improve code clone detection, which is vital for several software engineering tasks like refactoring, debugging, plagiarism detection, and vulnerability analysis. Traditional methods rely on singular representations that miss out on many semantic features, and hybrid fusion strategies are often handcrafted and suboptimal.", "method": "The method proposed is MAGNET, a multi-graph attentional framework that utilizes AST, CFG, and DFG representations of code. It incorporates residual graph neural networks, node-level self-attention, a gated cross-attention mechanism for inter-graph interactions, and Set2Set pooling to combine multi-graph embeddings into comprehensive program representations.", "result": "MAGNET demonstrates significant improvements, achieving an overall F1 score of 96.5% on BigCloneBench and 99.2% on Google Code Jam. Ablation studies show that both multi-graph fusion and attentional components are crucial for its success.", "conclusion": "MAGNET sets a new state-of-the-art for code clone detection by effectively integrating multiple graph-based representations with advanced attentional mechanisms, addressing prior limitations in semantic capture and fusion strategies."}}
{"id": "2510.24265", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.24265", "abs": "https://arxiv.org/abs/2510.24265", "authors": ["Sadia Afroz", "Zixuan Feng", "Katie Kimura", "Bianca Trinkenreich", "Igor Steinmacher", "Anita Sarma"], "title": "Developer Productivity with GenAI", "comment": null, "summary": "Generative AI (GenAI) tools are increasingly being adopted in software\ndevelopment as productivity aids. However, evidence regarding where and when\nthese tools actually enhance productivity is unclear. In this paper, we\ninvestigate how GenAI adoption affects different dimensions of developer\nproductivity. We surveyed 415 software practitioners to capture their\nperceptions of productivity changes associated with AI-assisted development\nusing the SPACE framework - Satisfaction and well-being, Performance, Activity,\nCommunication and collaboration, and Efficiency and flow. Our results,\ndisaggregated by frequency of AI usage, reveal limited overall productivity\nchange, highlighting the productivity paradox in which developers become faster\nbut do not necessarily create better software or feel more fulfilled.", "AI": {"tldr": "The paper finds that while generative AI helps developers work faster, it does not clearly improve software quality or developer fulfillment, raising questions about real productivity gains.", "motivation": "The motivation is to clarify if and how generative AI tools truly impact productivity in software development, addressing the lack of evidence about their practical effects.", "method": "The authors surveyed 415 software practitioners using the SPACE framework, analyzing perceptions of productivity changes with GenAI across several dimensions.", "result": "The study found limited overall productivity change, with frequent AI users becoming faster but not necessarily producing better software or feeling more fulfilled.", "conclusion": "Generative AI tools do not provide clear overall productivity benefits; developers may work faster, but improvements in quality or satisfaction are uncertain, highlighting a productivity paradox."}}
{"id": "2510.24358", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24358", "abs": "https://arxiv.org/abs/2510.24358", "authors": ["Lingyue Fu", "Bolun Zhang", "Hao Guan", "Yaoming Zhu", "Lin Qiu", "Weiwen Liu", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang", "Yong Yu"], "title": "Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation", "comment": null, "summary": "Recent advances in code agents have enabled automated software development at\nthe project level, supported by large language models (LLMs) and widely adopted\ntools. However, existing benchmarks for code agent evaluation face two major\nlimitations: high annotation cost and expertise requirements, and rigid\nevaluation metrics that rely primarily on unit tests. To address these\nchallenges, we propose an agent-driven benchmark construction pipeline that\nleverages human supervision to efficiently generate diverse and challenging\nproject-level tasks. Based on this approach, we introduce PRDBench, a novel\nbenchmark comprising 50 real-world Python projects across 20 domains, each with\nstructured Product Requirement Document (PRD) requirements, comprehensive\nevaluation criteria, and reference implementations. PRDBench features rich data\nsources, high task complexity, and flexible metrics. We further employ an\nAgent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of\nvarious test types beyond unit tests. Extensive experiments on PRDBench\ndemonstrate its effectiveness in assessing the capabilities of both code agents\nand evaluation agents, providing a scalable and robust framework for annotation\nand evaluation.", "AI": {"tldr": "The paper introduces PRDBench, a scalable and robust project-level benchmark for code agents with structured requirements and diverse evaluation metrics, using agents for both generation and judging. It overcomes annotation cost and metric rigidity issues in existing benchmarks and proves effective in experiments.", "motivation": "Current code agent benchmarks suffer from high annotation costs, need for expert involvement, and limited evaluation (mainly via unit tests), hindering scalable and comprehensive assessment of code agents.", "method": "The authors propose an agent-driven benchmark creation pipeline, using human supervision to generate diverse project-level tasks. They introduce PRDBench: a benchmark with 50 real-world Python projects in 20 domains, containing structured requirements, comprehensive evaluation criteria, and reference implementations. They use an Agent-as-a-Judge paradigm to assess outputs, covering more evaluation types than just unit tests.", "result": "PRDBench offers rich and complex tasks with flexible metrics, efficiently supporting both code agent and evaluation agent assessment. Experiments show its effectiveness and scalability for annotation and evaluation.", "conclusion": "PRDBench provides a robust, scalable framework that addresses annotation and evaluation bottlenecks in code agent benchmarks, by leveraging agent-driven creation and flexible multi-type assessment processes."}}
{"id": "2510.24367", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24367", "abs": "https://arxiv.org/abs/2510.24367", "authors": ["Junda He", "Jieke Shi", "Terry Yue Zhuo", "Christoph Treude", "Jiamou Sun", "Zhenchang Xing", "Xiaoning Du", "David Lo"], "title": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead", "comment": null, "summary": "The rapid integration of Large Language Models (LLMs) into software\nengineering (SE) has revolutionized tasks like code generation, producing a\nmassive volume of software artifacts. This surge has exposed a critical\nbottleneck: the lack of scalable, reliable methods to evaluate these outputs.\nHuman evaluation is costly and time-consuming, while traditional automated\nmetrics like BLEU fail to capture nuanced quality aspects. In response, the\nLLM-as-a-Judge paradigm - using LLMs for automated evaluation - has emerged.\nThis approach leverages the advanced reasoning of LLMs, offering a path toward\nhuman-like nuance at automated scale. However, LLM-as-a-Judge research in SE is\nstill in its early stages. This forward-looking SE 2030 paper aims to steer the\ncommunity toward advancing LLM-as-a-Judge for evaluating LLM-generated software\nartifacts. We provide a literature review of existing SE studies, analyze their\nlimitations, identify key research gaps, and outline a detailed roadmap. We\nenvision these frameworks as reliable, robust, and scalable human surrogates\ncapable of consistent, multi-faceted artifact evaluation by 2030. Our work aims\nto foster research and adoption of LLM-as-a-Judge frameworks, ultimately\nimproving the scalability of software artifact evaluation.", "AI": {"tldr": "Increasing use of LLMs in software engineering makes evaluation of generated outputs a bottleneck. The paper reviews existing SE literature, exposes limitations, and proposes a roadmap for developing LLM-based automated evaluation frameworks to foster scalability and quality assessment by 2030.", "motivation": "The motivation stems from the boom in LLM-generated software artifacts and the inability of current human or traditional automated evaluation methods to scale effectively, reliably, or capture nuanced qualities.", "method": "The authors conduct a literature review of current SE studies involving LLMs for automated evaluation, analyze the limitations of existing approaches, identify key research gaps, and propose a detailed roadmap for future development.", "result": "A detailed analysis of current challenges and limitations, key gaps in the research space, and a future-focused roadmap for building robust, scalable, and nuanced LLM-as-a-Judge frameworks for software artifact evaluation.", "conclusion": "The paper concludes that fostering the research and development of LLM-as-a-Judge frameworks is crucial for improving the scalability, reliability, and nuance of software artifact evaluation in the SE domain by 2030."}}
{"id": "2510.24428", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24428", "abs": "https://arxiv.org/abs/2510.24428", "authors": ["Nguyen Hoang Anh", "Minh Le-Anh", "Bach Le", "Nghi D. Q. Bui"], "title": "CodeWiki: Automated Repository-Level Documentation at Scale", "comment": null, "summary": "Developers spend nearly 58% of their time understanding codebases, yet\nmaintaining comprehensive documentation remains challenging due to complexity\nand manual effort. While recent Large Language Models (LLMs) show promise for\nfunction-level documentation, they fail at the repository level, where\ncapturing architectural patterns and cross-module interactions is essential. We\nintroduce CodeWiki, the first open-source framework for holistic\nrepository-level documentation across seven programming languages. CodeWiki\nemploys three innovations: (i) hierarchical decomposition that preserves\narchitectural context, (ii) recursive agentic processing with dynamic\ndelegation, and (iii) synthesis of textual and visual artifacts including\narchitecture diagrams and data flows. We also present CodeWikiBench, the first\nrepository-level documentation benchmark with multi-level rubrics and agentic\nassessment. CodeWiki achieves 68.79% quality score with proprietary models and\n64.80% with open-source alternatives, outperforming existing closed-source\nsystems and demonstrating scalable, accurate documentation for real-world\nrepositories.", "AI": {"tldr": "The paper presents CodeWiki, an open-source tool for automated, holistic repository-level documentation that uses novel techniques to generate both textual and visual documentation. It outperforms prior systems and promotes scalable documentation in multiple programming languages.", "motivation": "Developers spend a significant amount of time understanding codebases, but maintaining comprehensive documentation at the repository level is challenging due to complexity and the high degree of manual effort required. Existing LLM-based methods focus mostly on function-level documentation and struggle to capture architectural patterns and cross-module interactions at the repository level.", "method": "The paper introduces CodeWiki, an open-source framework designed for holistic repository-level documentation. CodeWiki leverages three main innovations: hierarchical decomposition to maintain architectural context, recursive agentic processing with dynamic task delegation, and synthesis of both textual and visual artifacts such as architecture diagrams and data flows. Additionally, the authors present CodeWikiBench, a benchmark for evaluating repository-level documentation using multi-level rubrics and agentic assessment.", "result": "CodeWiki demonstrates strong performance, achieving a 68.79% quality score using proprietary models and 64.80% with open-source alternatives. It outperforms existing closed-source systems, proving its effectiveness in scalable and accurate repository-level documentation.", "conclusion": "CodeWiki represents the first open-source approach to comprehensive repository-level documentation, offering both qualitative and quantitative improvements over previous solutions, and enabling scalable documentation across seven programming languages and real-world codebases."}}
{"id": "2510.24483", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.24483", "abs": "https://arxiv.org/abs/2510.24483", "authors": ["Michele Lanza"], "title": "The Divine Software Engineering Comedy -- Inferno: The Okinawa Files", "comment": null, "summary": "In June 2024 I co-organized the FUture of Software Engineering symposium in\nOkinawa, Japan. Me, Andrian Marcus, Takashi Kobayashi and Shinpei Hayashi were\ngeneral chairs, Nicole Novielli, Kevin Moran, Yutaro Kashiwa and Masanari Kondo\nwere program chairs, some members of my group, Carmen Armenti, Stefano\nCampanella, Roberto Minelli, were the tables, can't have a room with only\nchairs, after all. We invited a crowd of people to discuss what future software\nengineering has. FUSE became a 3-day marathon on whether there is actually a\nfuture at all for SE. This essay is a slightly dark take about what I saw at\nthat event, very loosely based on the discussions that took place, adding some\nhealthy sarcasm and cynicism, the intellectual salt and pepper I never seem to\nrun out of. I listened to the brilliant people who gathered to talk about where\nwe're headed, and distilled three nightmares headed in our direction: software\nmakers who don't know what they're doing, but get the job done anyway, a field\nmoving so fast it can't remember its own lessons, and technologies multiplying\nlike rabbits in Spring. So, let's start. The future, eh? The future of software\nengineering looks like a car crash in slow motion: you can see it coming but\nyou can't look away. The thing is...", "AI": {"tldr": "The paper offers a darkly humorous reflection on the future of software engineering, identifying major risks: under-skilled practitioners, lost institutional memory, and uncontrollable technological growth.", "motivation": "The motivation is to critically explore and highlight the challenges and existential risks facing the future of software engineering as perceived during the symposium.", "method": "The essay is an observational reflection, based loosely on discussions from a symposium, using personal experience, sarcasm, and cynicism as stylistic devices.", "result": "The author identifies three major concerns for the future: software developers lacking deep understanding, the field evolving too quickly to retain its lessons, and the uncontrolled increase in technologies.", "conclusion": "The future of software engineering is unpredictable and potentially problematic, characterized by a lack of foundational knowledge, rapid evolution that forgets past lessons, and overwhelming technological proliferation."}}
