{"id": "2510.17839", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17839", "abs": "https://arxiv.org/abs/2510.17839", "authors": ["Johannes Schneider", "Rene Abraham"], "title": "AI Exchange Platforms", "comment": null, "summary": "The rapid integration of Artificial Intelligence (AI) into organizational\ntechnology frameworks has transformed how organizations engage with AI-driven\nmodels, influencing both operational performance and strategic innovation. With\nthe advent of foundation models, the importance of structured platforms for AI\nmodel exchange has become paramount for organizational efficacy and\nadaptability. However, a comprehensive framework to categorize and understand\nthese platforms remains underexplored. To address this gap, our taxonomy\nprovides a structured approach to categorize AI exchange platforms, examining\nkey dimensions and characteristics, as well as revealing interesting\ninteraction patterns between public research institutions and organizations:\nSome platforms leverage peer review as a mechanism for quality control, and\nprovide mechanisms for online testing, deploying, and customization of models.\nOur paper is beneficial to practitioners seeking to understand challenges and\nopportunities that arise from AI exchange platforms. For academics, the\ntaxonomy serves as a foundation for further research into the evolution,\nimpact, and best practices associated with AI model sharing and utilization in\ndifferent contexts. Additionally, our study provides insights into the evolving\nrole of AI in various industries, highlighting the importance of adaptability\nand innovation in platform design. This paper serves as a critical resource for\nunderstanding the dynamic interplay between technology, business models, and\nuser engagement in the rapidly growing domain of AI model exchanges pointing\nalso towards possible future evolution.", "AI": {"tldr": "The paper proposes a taxonomy to classify and better understand AI model exchange platforms, offering insights into their features, usage, and the challenges and opportunities they present. This aids both practitioners and researchers in navigating and advancing the field.", "motivation": "There is a gap in understanding and categorizing the rapidly evolving landscape of AI exchange platforms, which are becoming critical as foundational AI models proliferate across organizations. A comprehensive framework for these platforms is missing, hindering effective utilization and further research.", "method": "The paper develops a taxonomy\u2014a structured classification framework\u2014to categorize and analyze AI model exchange platforms. The authors examine key platform characteristics, interaction patterns (such as peer review or model customization), and differences between public and organizational use.", "result": "The taxonomy identifies important dimensions and operational features of various AI model exchange platforms, uncovering differences in quality control (e.g., peer review), model deployment, and adaptability for users. It also highlights interactions between public institutions and private organizations in platform dynamics.", "conclusion": "The taxonomy provides a foundation for practical and academic understanding of AI model exchange platforms, shedding light on industry practices and setting the stage for future research on their evolution, impact, and best practices as AI continues to transform organizations."}}
{"id": "2510.17889", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17889", "abs": "https://arxiv.org/abs/2510.17889", "authors": ["Eilene Tomkins-Flanagan", "Mary A. Kelly"], "title": "Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp", "comment": null, "summary": "Kanerva (2014) suggested that it would be possible to construct a complete\nLisp out of a vector-symbolic architecture. We present the general form of a\nvector-symbolic representation of the five Lisp elementary functions, lambda\nexpressions, and other auxiliary functions, found in the Lisp 1.5 specification\nMcCarthy (1960), which is near minimal and sufficient for Turing-completeness.\nOur specific implementation uses holographic reduced representations Plate\n(1995), with a lookup table cleanup memory. Lisp, as all Turing-complete\nlanguages, is a Cartesian closed category, unusual in its proximity to the\nmathematical abstraction. We discuss the mathematics, the purpose, and the\nsignificance of demonstrating vector-symbolic architectures' Cartesian-closure,\nas well as the importance of explicitly including cleanup memories in the\nspecification of the architecture.", "AI": {"tldr": "This paper proves that key elements of Lisp can be encoded in a vector-symbolic architecture (using holographic reduced representations), achieving Turing-completeness, and emphasizes the role of cleanup memory in such systems.", "motivation": "Kanerva (2014) proposed constructing a complete Lisp using vector-symbolic architecture. The motivation here is to demonstrate, in concrete terms, how the core elements of Lisp (elementary functions, lambda expressions, etc.) can be represented within such an architecture and achieve Turing-completeness.", "method": "The paper presents a general vector-symbolic representation for the five elementary Lisp functions, lambda expressions, and auxiliary functions, closely following the Lisp 1.5 specification. Specifically, holographic reduced representations are used along with a lookup table cleanup memory to implement this system.", "result": "They demonstrate that their vector-symbolic implementation is sufficient for Turing-completeness, aligning with the properties of a Cartesian closed category. The paper also highlights the necessity of cleanup memories for practical implementation.", "conclusion": "The study shows that a minimal Lisp, sufficient for computation, can be fully encoded within a vector-symbolic architecture, underlining the mathematical and computational relevance of such architectures. Explicit inclusion of cleanup memory is necessary for effective functioning."}}
{"id": "2510.17842", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17842", "abs": "https://arxiv.org/abs/2510.17842", "authors": ["Vinay Bamil"], "title": "Vibe Coding: Toward an AI-Native Paradigm for Semantic and Intent-Driven Programming", "comment": "10 pages, 1 figure, 2 tables", "summary": "Recent advances in large language models have enabled developers to generate\nsoftware by conversing with artificial intelligence systems rather than writing\ncode directly. This paper introduces vibe coding, an emerging AI-native\nprogramming paradigm in which a developer specifies high-level functional\nintent along with qualitative descriptors of the desired \"vibe\" (tone, style,\nor emotional resonance). An intelligent agent then transforms those\nspecifications into executable software. We formalize the definition of vibe\ncoding and propose a reference architecture that includes an intent parser, a\nsemantic embedding engine, an agentic code generator, and an interactive\nfeedback loop. A hypothetical implementation is described. We compare vibe\ncoding with declarative, functional, and prompt-based programming, and we\ndiscuss its implications for software engineering, human-AI collaboration, and\nresponsible AI practice. Finally, we examine reported productivity gains and\ndemocratizing effects, review recent studies that highlight vulnerabilities and\npotential slowdowns, identify key challenges such as alignment,\nreproducibility, bias, explainability, maintainability, and security, and\noutline future directions and open research questions.", "AI": {"tldr": "Vibe coding is a new AI-driven programming paradigm where developers specify both functional intent and qualitative 'vibes' for code generation. The paper formalizes the concept, proposes a reference architecture, explores its impacts and challenges, and sets a research agenda for addressing issues like bias, alignment, and reproducibility.", "motivation": "Recent progress in large language models enables more conversational, AI-driven software generation, motivating the search for novel programming paradigms that leverage these capabilities.", "method": "This paper formalizes the concept of 'vibe coding,' proposes a reference architecture for it, describes a hypothetical implementation, and compares it to other paradigms (declarative, functional, prompt-based). It also reviews related studies, discusses implications, reports on gains and risks, and identifies challenges and open research questions.", "result": "The paper establishes vibe coding as an AI-native programming paradigm, highlights its potential to enhance productivity and democratize software development, and identifies challenges like alignment, reproducibility, bias, explainability, maintainability, and security.", "conclusion": "Vibe coding represents a promising direction for integrating AI into software development, but considerable research is needed to address its vulnerabilities and ensure responsible, effective adoption."}}
{"id": "2510.18479", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.18479", "abs": "https://arxiv.org/abs/2510.18479", "authors": ["Samuel Chassot", "Viktor Kun\u010dak"], "title": "ZipLex: Verified Invertible Lexing with Memoized Derivatives and Zippers", "comment": null, "summary": "We present ZipLex, a verified framework for invertible lexical analysis.\nUnlike past verified lexers that focus only on satisfying the semantics of\nregular expressions and the maximal munch property, ZipLex also guarantees that\nlexing and printing are mutual inverses. Our design relies on two sets of\nideas: (1) a new abstraction of token sequences that captures the separability\nof tokens in a sequence while supporting their efficient manipulation, and (2)\na combination of verified data structures and optimizations, including Huet's\nzippers and memoized derivatives, to achieve practical performance. We\nimplemented ZipLex in Scala and verified its correctness, including\ninvertibility, using the Stainless verifier. Our evaluation demonstrates that\nZipLex supports realistic applications such as JSON processing and lexers of\nprogramming languages. In comparison to other verified lexers (which do not\nenforce invertibility), ZipLex is 4x slower than Coqlex and two orders of\nmagnitude faster than Verbatim++, showing that verified invertibility can be\nachieved without prohibitive cost.", "AI": {"tldr": "ZipLex is a verified lexer framework that guarantees the output can be inverted (printing is the inverse of lexing), using novel token sequence abstractions and verified optimizations. It achieves practical performance and substantive correctness guarantees beyond prior work.", "motivation": "Past verified lexers only guarantee regular expression semantics and maximal munch property, but do not ensure that lexing and printing are mutual inverses, which is important for reversible transformations and correctness.", "method": "ZipLex introduces a new abstraction for token sequences to model separability and efficient manipulation, uses verified data structures like Huet's zippers and memoized derivatives, and implements and verifies the framework in Scala using the Stainless verifier.", "result": "ZipLex was implemented and verified for correctness and invertibility. It supports practical tasks (e.g., JSON, programming languages), is 4x slower than Coqlex, but much faster than Verbatim++, showing verified invertibility is feasible without much overhead.", "conclusion": "ZipLex provides mutual invertibility in lexical analysis, a property previously lacking in verified lexers, and demonstrates both practical performance and full verification."}}
{"id": "2510.17865", "categories": ["cs.SE", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.17865", "abs": "https://arxiv.org/abs/2510.17865", "authors": ["Rene Davila", "Everardo Barcenas", "Rocio Aldeco-Perez"], "title": "Smart Contracts Formal Verification: A Systematic Literature Review", "comment": "in Spanish language", "summary": "Formal verification entails testing software to ensure it operates as\nspecified. Smart contracts are self-executing contracts with the terms of the\nagreement directly written into lines of code. They run on blockchain platforms\nand automatically enforce and execute the terms of an agreement when meeting\npredefined conditions. However, Smart Contracts, as software models, often\ncontain notable errors in their operation or specifications. This observation\nprompts us to conduct a focused study examining related works published across\nvarious sources. These publications detail specifications, verification tools,\nand relevant experiments. Subsequently, this survey proposes an alternative\nformal verification based on description logic.", "AI": {"tldr": "This paper surveys smart contract verification literature and introduces a new formal verification method using description logic.", "motivation": "Smart contracts, which automate agreements on blockchains, often suffer from operational or specification errors. This paper is motivated by the need for reliable verification techniques to ensure smart contracts function as intended.", "method": "The authors conduct a survey of existing publications that address the specifications, verification tools, and experiments for smart contract verification. They then propose a novel formal verification approach based on description logic.", "result": "The paper reviews a range of previous works on smart contract verification and introduces an alternative verification method utilizing description logic.", "conclusion": "A comprehensive analysis of existing smart contract verification approaches is provided, and a new description logic-based verification framework is proposed as an alternative approach."}}
{"id": "2510.18651", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18651", "abs": "https://arxiv.org/abs/2510.18651", "authors": ["Uraz Odyurt", "\u00d6mer Sayilir", "Mari\u00eblle Stoelinga", "Vadim Zaytsev"], "title": "CPSLint: A Domain-Specific Language Providing Data Validation and Sanitisation for Industrial Cyber-Physical Systems", "comment": null, "summary": "Raw datasets are often too large and unstructured to work with directly, and\nrequire a data preparation process. The domain of industrial Cyber-Physical\nSystems (CPS) is no exception, as raw data typically consists of large amounts\nof time-series data logging the system's status in regular time intervals. Such\ndata has to be sanity checked and preprocessed to be consumable by data-centric\nworkflows. We introduce CPSLint, a Domain-Specific Language designed to provide\ndata preparation for industrial CPS. We build up on the fact that many raw data\ncollections in the CPS domain require similar actions to render them suitable\nfor Machine-Learning (ML) solutions, e.g., Fault Detection and Identification\n(FDI) workflows, yet still vary enough to hope for one universally applicable\nsolution.\n  CPSLint's main features include type checking and enforcing constraints\nthrough validation and remediation for data columns, such as imputing missing\ndata from surrounding rows. More advanced features cover inference of extra\nCPS-specific data structures, both column-wise and row-wise. For instance, as\nrow-wise structures, descriptive execution phases are an effective method of\ndata compartmentalisation are extracted and prepared for ML-assisted FDI\nworkflows. We demonstrate CPSLint's features through a proof of concept\nimplementation.", "AI": {"tldr": "CPSLint is a new language that streamlines and standardizes data preparation in industrial CPS, adding robust checks and structure, especially for ML and FDI applications, with demonstrated proof-of-concept results.", "motivation": "Raw datasets in industrial Cyber-Physical Systems (CPS) are massive, unstructured, and require extensive preprocessing before being usable, especially for Machine Learning (ML) applications such as Fault Detection and Identification (FDI). There is a need for efficient, domain-specific tools to standardize and automate this data preparation process.", "method": "The authors introduce CPSLint, a Domain-Specific Language (DSL) tailored for preparing industrial CPS data. CPSLint features type checking, constraint enforcement, data validation, remediation (e.g., imputing missing values), and advanced inference of CPS-specific data structures. They present these features through a proof-of-concept implementation.", "result": "CPSLint provides a structured approach to CPS data preparation, enabling easier consumption by ML-based workflows. It facilitates sanity checks, preprocessing, type safety, and the inference of both column-wise and row-wise structures (e.g., execution phases for FDI tasks).", "conclusion": "CPSLint effectively aids in preparing and cleaning CPS data, addressing both common and domain-specific requirements, and making industrial CPS data more readily accessible for machine learning-driven fault detection and identification."}}
{"id": "2510.17868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17868", "abs": "https://arxiv.org/abs/2510.17868", "authors": ["Xinyue Zheng", "Haowei Lin", "Shaofei Cai", "Zilong Zheng", "Yitao Liang"], "title": "UniCode: A Framework for Generating High Quality Competitive Coding Problems", "comment": null, "summary": "The reliance of competitive coding benchmarks on static, human-authored\nproblems creates significant challenges, including data contamination and\nlimited scalability. To address these issues, we introduce UniCode, a novel\nframework that automatically generates high-quality algorithmic problems\nalongside robust, contamination-resistant test cases. Inspired by biological\nevolution that creates better and diverse offspring, our framework leverages\nLarge Language Models (LLMs) to systematically diversify problems through three\nstrategies: single problem extension, same-type fusion, and cross-type fusion.\nA key innovation is our stress-driven test case synthesis pipeline, which\ngenerates reliable test suites without requiring a canonical ground-truth\nsolution. This pipeline combines brute-force grounding for small-scale inputs\nwith a consensus-based validation mechanism for large-scale inputs to ensure\nhigh correctness and coverage. We demonstrate effectiveness of our framework by\ncurating a benchmark of 492 problems and evaluating 19 state-of-the-art LLMs.\nThe results reveal that UniCode is highly challenging and discriminative, with\nthe top-performing model, o4-mini, achieving a pass rate of only 70.3%. Our\nframework provides a scalable and reliable solution for generating dynamic\nevaluation datasets in coding domain.", "AI": {"tldr": "UniCode is a new LLM-powered framework for automatically creating diverse and robust algorithmic coding problems and test cases. It addresses problems of data contamination and scalability in traditional benchmarks, and produces highly challenging benchmarks that even top models struggle to solve completely.", "motivation": "Competitive coding benchmarks suffer from data contamination and are difficult to scale because they rely on static, human-authored problems.", "method": "The paper proposes UniCode, a framework using Large Language Models (LLMs) to automatically generate algorithmic problems and robust, contamination-resistant test cases. The problem generation process involves three diversification strategies: single problem extension, same-type fusion, and cross-type fusion. The framework includes a stress-driven test case synthesis pipeline, which generates reliable test suites by combining brute-force grounding on small inputs and consensus-based validation on large inputs, all without needing a canonical ground-truth solution.", "result": "UniCode generated a benchmark set of 492 problems and evaluated 19 state-of-the-art LLMs. The results show that the benchmark is challenging and discriminative: the best-performing model only achieved a 70.3% pass rate.", "conclusion": "UniCode provides a scalable, dynamic, and contamination-resistant approach to generating high-quality coding benchmark datasets, significantly improving on static, human-authored benchmarks."}}
{"id": "2510.17874", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17874", "abs": "https://arxiv.org/abs/2510.17874", "authors": ["Jason Tsay", "Zidane Wright", "Gaodan Fang", "Kiran Kate", "Saurabh Jha", "Yara Rizk"], "title": "Repairing Tool Calls Using Post-tool Execution Reflection and RAG", "comment": null, "summary": "Agentic systems interact with external systems by calling tools such as\nPython functions, REST API endpoints, or command line tools such as kubectl in\nKubernetes. These tool calls often fail for various syntactic and semantic\nreasons. Some less obvious semantic errors can only be identified and resolved\nafter analyzing the tool's response. To repair these errors, we develop a\npost-tool execution reflection component that combines large language model\n(LLM)-based reflection with domain-specific retrieval-augmented generation\n(RAG) using documents describing both the specific tool being called and\ntroubleshooting documents related to the tool. For this paper, we focus on the\nuse case of the kubectl command line tool to manage Kubernetes, a platform for\norchestrating cluster applications. Through a larger empirical study and a\nsmaller manual evaluation, we find that our RAG-based reflection will repair\nkubectl commands such that they are both more likely to successfully execute\n(pass rate) for 55% of our models evaluated and 36% more likely to correctly\nanswer the user query on average. We find that troubleshooting documents\nimprove pass rate compared to official documentation by an average of 10%.", "AI": {"tldr": "The paper introduces a system using LLMs and RAG with troubleshooting docs to better fix errors in Kubernetes tool calls, leading to higher success and accuracy rates in automated command execution.", "motivation": "Tool calls in agentic systems frequently fail due to semantic and syntactic reasons, with some errors only detected after tool execution. There is a need to repair these errors efficiently to enhance automation and reliability.", "method": "Developed a post-tool execution reflection component that combines LLM-based reflection and domain-specific retrieval-augmented generation, leveraging both official and troubleshooting documents. Evaluated through empirical and manual studies.", "result": "RAG-based reflection repairs kubectl commands, increasing successful executions (pass rate) for 55% of models and making them 36% more likely to answer user queries correctly. Troubleshooting documents further improve pass rates by 10% compared to official documentation.", "conclusion": "Reflection using RAG and troubleshooting documents significantly improves the repair and success rate of tool calls, especially with kubectl commands."}}
{"id": "2510.17891", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17891", "abs": "https://arxiv.org/abs/2510.17891", "authors": ["Jiin Woo", "Shaowei Zhu", "Allen Nie", "Zhen Jia", "Yida Wang", "Youngsuk Park"], "title": "TritonRL: Training LLMs to Think and Code Triton Without Cheating", "comment": null, "summary": "With the rapid evolution of large language models (LLMs), the demand for\nautomated, high-performance system kernels has emerged as a key enabler for\naccelerating development and deployment. We introduce TritonRL, a\ndomain-specialized LLM for Triton kernel generation, trained with a novel\ntraining framework that enables robust and automated kernel synthesis. Unlike\ngeneral-purpose programming languages, Triton kernel generation faces unique\nchallenges due to data scarcity and incomplete evaluation criteria, vulnerable\nto reward hacking. Our approach addresses these challenges end-to-end by\ndistilling Triton-specific knowledge through supervised fine-tuning on curated\ndatasets, and further improving code quality via reinforcement learning (RL)\nwith robust, verifiable rewards and hierarchical reward assignment. Our RL\nframework robustly detects reward hacking and guides both reasoning traces and\ncode tokens through fine-grained verification and hierarchical reward\ndecomposition, enabling the model to generate high-quality Triton kernels that\ncan truly replace existing modules. With robust and fine-grained evaluation,\nour experiments on KernelBench demonstrate that TritonRL achieves\nstate-of-the-art correctness and speedup, surpassing all other Triton-specific\nmodels and underscoring the effectiveness of our RL-based training paradigm.", "AI": {"tldr": "The paper introduces TritonRL, a specialized LLM for Triton kernel generation using a novel reinforcement learning framework. By tackling data scarcity and reward hacking, TritonRL achieves state-of-the-art correctness and speed, outperforming existing solutions in generating reliable, high-performance kernels.", "motivation": "Large language models are rapidly advancing, but generating system kernels\u2014especially for frameworks like Triton\u2014faces significant challenges due to scarce training data and unclear evaluation metrics, which can lead to reward hacking and unreliable models.", "method": "The authors developed TritonRL, a domain-specialized large language model for generating Triton kernels. The model is trained using a new training framework that combines supervised fine-tuning on curated datasets with reinforcement learning (RL). The RL approach includes robust, verifiable rewards and hierarchical reward assignment to detect and prevent reward hacking, ensuring the model learns effectively. The framework also uses fine-grained verification and hierarchical reward decomposition to guide model outputs.", "result": "TritonRL generated Triton kernels that achieved higher correctness and speed compared to all existing Triton-specific models. The model successfully replaced existing modules and performed robustly on KernelBench, thanks to its novel RL-based training paradigm.", "conclusion": "The proposed TritonRL and its RL-based training approach effectively address challenges in Triton kernel generation, setting a new state-of-the-art in correctness and performance, and demonstrating a robust solution for domain-specific kernel synthesis."}}
{"id": "2510.17894", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17894", "abs": "https://arxiv.org/abs/2510.17894", "authors": ["Yunhan Qiao", "Md Istiak Hossain Shihab", "Christopher Hundhausen"], "title": "A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension: Implications for Computing Education Research and Practice", "comment": null, "summary": "The ability to comprehend code has long been recognized as an essential skill\nin software engineering. As programmers lean more heavily on generative\nartificial intelligence (GenAI) assistants to develop code solutions, it is\nbecoming increasingly important for programmers to comprehend GenAI solutions\nso that they can verify their appropriateness and properly integrate them into\nexisting code. At the same time, GenAI tools are increasingly being enlisted to\nprovide programmers with tailored explanations of code written both by GenAI\nand humans. Thus, in computing education, GenAI presents new challenges and\nopportunities for learners who are trying to comprehend computer programs. To\nprovide computing educators with evidence-based guidance on the use of GenAI to\nfacilitate code comprehension and to identify directions for future research,\nwe present a systematic literature review (SLR) of state-of-the-art approaches\nand tools that leverage GenAI to enhance code comprehension. Our SLR focuses on\n31 studies published between 2022 and 2024. Despite their potential, GenAI\nassistants often yield inaccurate or unclear explanations, and novice\nprogrammers frequently struggle to craft effective prompts, thereby impeding\ntheir ability to leverage GenAI to aid code comprehension. Our review\nclassifies GenAI-based approaches and tools, identifies methods used to study\nthem, and summarizes the empirical evaluations of their effectiveness. We\nconsider the implications of our findings for computing education research and\npractice, and identify directions for future research.", "AI": {"tldr": "A review of recent literature shows that while GenAI tools can help with code comprehension, their explanations are often flawed and novices struggle with prompt creation. More research and improved tool design are needed for effective educational use.", "motivation": "With the rise of generative AI assistants in software engineering, there is a growing need to understand how these tools impact code comprehension, especially in educational contexts. Educators and learners face challenges and opportunities with GenAI's role in explaining and generating code.", "method": "The authors conducted a systematic literature review (SLR) of 31 recent studies (2022-2024) focused on GenAI tools used to enhance code comprehension. The review classified different approaches and tools, studied research methods, and summarized empirical evaluations.", "result": "The SLR found that GenAI assistants can improve code comprehension but often provide inaccurate or unclear explanations. Novice programmers have difficulty creating effective prompts. The review classified existing GenAI-based tools, outlined study methodologies, and summarized their efficacy as reported in the literature.", "conclusion": "While GenAI tools present significant potential to aid code comprehension, challenges such as explanation quality and prompt effectiveness limit their usefulness, especially for beginners. The study highlights the need for better GenAI interfaces and more research into their educational impact."}}
{"id": "2510.17925", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17925", "abs": "https://arxiv.org/abs/2510.17925", "authors": ["George Ma", "Anurag Koul", "Qi Chen", "Yawen Wu", "Sachit Kuhar", "Yu Yu", "Aritra Sengupta", "Varun Kumar", "Murali Krishna Ramanathan"], "title": "SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion", "comment": null, "summary": "Large Language Models (LLMs) excel at code-related tasks but often struggle\nin realistic software repositories, where project-specific APIs and cross-file\ndependencies are crucial. Retrieval-augmented methods mitigate this by\ninjecting repository context at inference time. The low inference-time latency\nbudget affects either retrieval quality or the added latency adversely impacts\nuser experience. We address this limitation with SpecAgent, an agent that\nimproves both latency and code-generation quality by proactively exploring\nrepository files during indexing and constructing speculative context that\nanticipates future edits in each file. This indexing-time asynchrony allows\nthorough context computation, masking latency, and the speculative nature of\nthe context improves code-generation quality. Additionally, we identify the\nproblem of future context leakage in existing benchmarks, which can inflate\nreported performance. To address this, we construct a synthetic, leakage-free\nbenchmark that enables a more realistic evaluation of our agent against\nbaselines. Experiments show that SpecAgent consistently achieves absolute gains\nof 9-11% (48-58% relative) compared to the best-performing baselines, while\nsignificantly reducing inference latency.", "AI": {"tldr": "SpecAgent is an agent for LLMs that speculatively indexes repository files to generate anticipatory context, improving code-generation quality and latency for realistic code tasks. It also introduces a new benchmark addressing context leakage, and demonstrates significant gains over existing methods.", "motivation": "Existing LLM approaches for code tasks struggle with project-specific APIs and dependencies in real-world repositories. Retrieval-augmented approaches help, but low latency budgets lead to trade-offs between retrieval quality and user experience. Additionally, current benchmarks suffer from future context leakage, falsely inflating performance metrics.", "method": "SpecAgent proactively explores repository files during indexing, constructing speculative contexts that anticipate future edits, thus masking latency by shifting context computation earlier. Additionally, the paper develops a synthetic, leakage-free benchmark for fair evaluation.", "result": "SpecAgent delivers consistent absolute improvements of 9-11% over baselines regarding code-generation quality, and substantially cuts inference latency. The improved benchmark provides more realistic evaluation.", "conclusion": "SpecAgent outperforms baselines in code generation quality and latency, with absolute gains of 9-11% and significantly reduced inference latency."}}
{"id": "2510.17932", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17932", "abs": "https://arxiv.org/abs/2510.17932", "authors": ["Jiahao Tang", "Henry Hengyuan Zhao", "Lijian Wu", "Yifei Tao", "Dongxing Mao", "Yang Wan", "Jingru Tan", "Min Zeng", "Min Li", "Alex Jinpeng Wang"], "title": "From Charts to Code: A Hierarchical Benchmark for Multimodal Models", "comment": null, "summary": "We introduce Chart2Code, a new benchmark for evaluating the chart\nunderstanding and code generation capabilities of large multimodal models\n(LMMs). Chart2Code is explicitly designed from a user-driven perspective,\ncapturing diverse real-world scenarios and progressively increasing task\ndifficulty. It consists of three levels: Level 1 (Chart Reproduction)\nreproduces charts from a reference figure and user query; Level 2 (Chart\nEditing) involves complex modifications such as changing chart types or adding\nelements; and Level 3 (Long-Table to Chart Generation) requires models to\ntransform long, information-dense tables into faithful charts following user\ninstructions. To our knowledge, this is the first hierarchical benchmark that\nreflects practical chart2code usage while systematically scaling task\ncomplexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,\npaired with multi-level evaluation metrics that assess both code correctness\nand the visual fidelity of rendered charts. We benchmark 25 state-of-the-art\n(SoTA) LMMs, including both proprietary and the latest open-source models such\nas GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental\nresults demonstrate that even the SoTA model GPT-5 averages only 0.57 on\ncode-based evaluation and 0.22 on chart-quality assessment across the editing\ntasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark\nwill drive advances in multimodal reasoning and foster the development of more\nrobust and general-purpose LMMs. Our code and data are available on Chart2Code.", "AI": {"tldr": "Chart2Code is a new benchmark for testing how well large multimodal models handle chart-related code generation and editing in real-world scenarios. It presents a significant challenge, with top models struggling to achieve high scores, highlighting the need for further progress.", "motivation": "There is a growing need for rigorous evaluation of large multimodal models (LMMs) in tasks involving chart understanding and code generation, especially considering practical, user-driven scenarios. Existing benchmarks do not sufficiently capture the complexity and variability of real-world chart-to-code tasks.", "method": "The authors constructed a hierarchical benchmark, Chart2Code, with three increasingly difficult levels: chart reproduction, chart editing, and long-table to chart generation. They designed 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics, and tested 25 state-of-the-art LMMs on these tasks.", "result": "The experimental evaluation revealed that even the best-performing model, GPT-5, scored only 0.57 on code correctness and 0.22 on chart-quality for editing tasks, indicating the substantial challenge posed by Chart2Code.", "conclusion": "Chart2Code serves as a challenging benchmark that exposes current limitations in multimodal reasoning and chart-to-code generation for LMMs. The authors expect it to encourage further research and improvements in this domain."}}
{"id": "2510.18013", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18013", "abs": "https://arxiv.org/abs/2510.18013", "authors": ["Yiran Wang", "Jos\u00e9 Antonio Hern\u00e1ndez L\u00f3pez", "Ulf Nilsson", "D\u00e1niel Varr\u00f3"], "title": "JunoBench: A Benchmark Dataset of Crashes in Python Machine Learning Jupyter Notebooks", "comment": null, "summary": "Jupyter notebooks are widely used for machine learning (ML) prototyping. Yet\nfew debugging tools are designed for ML code in notebooks, potentially due to\nthe lack of benchmarks. We introduce JunoBench, the first benchmark dataset of\nreal-world crashes in Python-based ML notebooks. JunoBench has 111 curated and\nreproducible crashes from public Kaggle notebooks, each paired with a\nverifiable fix, ranging over popular ML libraries, including TensorFlow/Keras,\nPyTorch, Scikit-learn, Pandas, and NumPy, as well as notebook-specific\nout-of-order execution issue. To support reproducibility and ease of use,\nJunoBench offers a unified execution environment where crashes and fixes can be\nreliably reproduced. By providing realistic crashes and their resolutions,\nJunoBench facilitates bug detection, localization, and repair tailored to the\ninteractive and iterative nature of notebook-based ML development.", "AI": {"tldr": "JunoBench introduces a benchmark dataset of real crash-and-fix pairs from Python ML notebooks, supporting bug detection and repair research in interactive notebook workflows.", "motivation": "There are few debugging tools for ML code in Jupyter notebooks, partly due to a lack of benchmarks for real-world crashes.", "method": "The authors created JunoBench, a curated benchmark dataset featuring 111 reproducible crashes from public Kaggle notebooks, each with a verified fix. The dataset covers popular ML libraries and notebook-specific issues.", "result": "JunoBench provides a unified environment for reliably reproducing crashes and fixes, making it easier to study bug detection, localization, and repair for notebook-based ML development.", "conclusion": "JunoBench is the first benchmark for real-world ML notebook crashes. It enables reproducible studies and tool development for debugging in notebook environments."}}
{"id": "2510.18017", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18017", "abs": "https://arxiv.org/abs/2510.18017", "authors": ["Mariana Crisostomo Martins", "Lucas Elias Cardoso Rocha", "Lucas Cordeiro Romao", "Taciana Novo Kudo", "Marcos Kalinowski", "Renato de Freitas Bulcao-Neto"], "title": "DIP-AI: A Discovery Framework for AI Innovation Projects", "comment": "Submitted and accepted at SBQS25 - Brazilian Symposium on Software\n  Quality. Presentation from November 4th to 7th, 2025 in Sao Jose dos Campos,\n  SP", "summary": "Despite the increasing development of Artificial Intelligence (AI) systems,\nRequirements Engineering (RE) activities face challenges in this new\ndata-intensive paradigm. We identified a lack of support for problem discovery\nwithin AI innovation projects. To address this, we propose and evaluate DIP-AI,\na discovery framework tailored to guide early-stage exploration in such\ninitiatives. Based on a literature review, our solution proposal combines\nelements of ISO 12207, 5338, and Design Thinking to support the discovery of AI\ninnovation projects, aiming at promoting higher quality deliveries and\nstakeholder satisfaction. We evaluated DIP-AI in an industry-academia\ncollaboration (IAC) case study of an AI innovation project, in which\nparticipants applied DIP-AI to the discovery phase in practice and provided\ntheir perceptions about the approach's problem discovery capability,\nacceptance, and suggestions. The results indicate that DIP-AI is relevant and\nuseful, particularly in facilitating problem discovery in AI projects. This\nresearch contributes to academia by sharing DIP-AI as a framework for AI\nproblem discovery. For industry, we discuss the use of this framework in a real\nIAC program that develops AI innovation projects.", "AI": {"tldr": "The paper introduces DIP-AI, a tailored framework to improve early problem discovery in AI projects, blending established standards and design thinking. Case study results show that DIP-AI effectively facilitates problem exploration and is beneficial for both academia and industry.", "motivation": "Requirements Engineering (RE) faces difficulties with early-stage problem discovery in AI projects, which are increasingly data-intensive. There is a recognized gap in structured support for identifying project problems and requirements during the exploration phase of AI initiatives.", "method": "The authors conducted a literature review and combined elements from standards ISO 12207, 5338, and Design Thinking to propose DIP-AI, a discovery framework for AI innovation projects. DIP-AI was evaluated through a case study in an industry-academia collaboration, where participants used the framework during the project discovery phase and provided feedback.", "result": "Participants found DIP-AI to be relevant and useful for facilitating problem discovery in AI projects. The results showed improved problem identification, acceptance of the framework, and valuable suggestions for its application in real settings.", "conclusion": "DIP-AI is an effective framework for guiding early-stage problem discovery in AI innovation projects. It enhances project outcomes by supporting more thorough problem identification and stakeholder satisfaction. DIP-AI is recommended for use in both academic and industry AI projects."}}
{"id": "2510.18096", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18096", "abs": "https://arxiv.org/abs/2510.18096", "authors": ["Esrat Ebtida Sakib", "MD Ahnaf Akib", "Md Muktadir Mazumder", "Maliha Noushin Raida", "Md. Mohsinul Kabir"], "title": "A Benchmark Dataset And LLMs Comparison For NFR Classification With Explainable AI", "comment": null, "summary": "Non-Functional Requirements (NFRs) play a critical role in determining the\noverall quality and user satisfaction of software systems. Accurately\nidentifying and classifying NFRs is essential to ensure that software meets\nperformance, usability, and reliability expectations. However, manual\nidentification of NFRs from documentation is time-consuming and prone to\nerrors, necessitating automated solutions. Before implementing any automated\nsolution, a robust and comprehensive dataset is essential. To build such a\ndataset, we collected NFRs from various Project Charters and Open Source\nSoftware Documentation. This enhanced the technical depth and usability of an\nalready existing NFR dataset. We categorized NFRs into sub-classes and\nidentified needs using widely used Large Language Models to facilitate\nautomation. After classifying the NFRs, we compared the classification results\nof the selected LLMs: RoBERTa, CodeBERT, Gemma-2, Phi-3, Mistral-8B, and\nLlama-3.1-8B using various evaluation metrics, including precision, recall,\nF1-score, and lime scores. Among these models, Gemma-2 achieved the best\nresults with a precision of 0.87, recall of 0.89, and F1-score of 0.88,\nalongside a lime hit score of 78 out of 80. Phi-3 closely followed with a\nprecision of 0.85, recall of 0.87, F1-score of 0.86, and the highest lime hit\nscore of 79. By improving the contextual foundation, this integration enhanced\nthe model's comprehension of technical aspects and user requirements.", "AI": {"tldr": "The paper creates an enriched NFR dataset, tests several LLMs for automated classification, and finds Gemma-2 and Phi-3 provide the best results. Automated methods are shown to be accurate and superior to manual identification for NFRs.", "motivation": "Accurate identification and classification of Non-Functional Requirements (NFRs) is crucial for ensuring software systems meet quality standards such as performance, usability, and reliability. Manual methods for extracting NFRs are inefficient and error-prone, driving the need for automated solutions.", "method": "The paper collected NFRs from Project Charters and Open Source Documentation to extend an existing NFR dataset. They further categorized NFRs into sub-classes and used multiple Large Language Models (RoBERTa, CodeBERT, Gemma-2, Phi-3, Mistral-8B, Llama-3.1-8B) to automate classification, evaluating outcomes via precision, recall, F1-score, and lime scores.", "result": "Gemma-2 performed best among the tested LLMs, achieving precision 0.87, recall 0.89, F1-score 0.88, and lime hit score of 78/80. Phi-3 was the next best, with precision 0.85, recall 0.87, F1-score 0.86, and a lime hit score of 79.", "conclusion": "Automated NFR classification, powered by improved data context and advanced LLMs, significantly outperforms manual methods and enhances both technical and user requirement comprehension. Gemma-2 and Phi-3 are particularly effective for this task."}}
{"id": "2510.18131", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18131", "abs": "https://arxiv.org/abs/2510.18131", "authors": ["Chengquan Guo", "Yuzhou Nie", "Chulin Xie", "Zinan Lin", "Wenbo Guo", "Bo Li"], "title": "BlueCodeAgent: A Blue Teaming Agent Enabled by Automated Red Teaming for CodeGen AI", "comment": null, "summary": "As large language models (LLMs) are increasingly used for code generation,\nconcerns over the security risks have grown substantially. Early research has\nprimarily focused on red teaming, which aims to uncover and evaluate\nvulnerabilities and risks of CodeGen models. However, progress on the blue\nteaming side remains limited, as developing defense requires effective semantic\nunderstanding to differentiate the unsafe from the safe. To fill in this gap,\nwe propose BlueCodeAgent, an end-to-end blue teaming agent enabled by automated\nred teaming. Our framework integrates both sides: red teaming generates diverse\nrisky instances, while the blue teaming agent leverages these to detect\npreviously seen and unseen risk scenarios through constitution and code\nanalysis with agentic integration for multi-level defense. Our evaluation\nacross three representative code-related tasks--bias instruction detection,\nmalicious instruction detection, and vulnerable code detection--shows that\nBlueCodeAgent achieves significant gains over the base models and safety\nprompt-based defenses. In particular, for vulnerable code detection tasks,\nBlueCodeAgent integrates dynamic analysis to effectively reduce false\npositives, a challenging problem as base models tend to be over-conservative,\nmisclassifying safe code as unsafe. Overall, BlueCodeAgent achieves an average\n12.7\\% F1 score improvement across four datasets in three tasks, attributed to\nits ability to summarize actionable constitutions that enhance context-aware\nrisk detection. We demonstrate that the red teaming benefits the blue teaming\nby continuously identifying new vulnerabilities to enhance defense performance.", "AI": {"tldr": "The paper introduces BlueCodeAgent, a system that combines automated red and blue teaming to detect and defend against security risks in LLM-generated code. It significantly improves risk detection accuracy and reduces false positives compared to previous methods.", "motivation": "Security concerns are increasing as LLMs are used for code generation, but most research focuses on red teaming (finding vulnerabilities) rather than blue teaming (defense). There is a need for methods that can reliably identify and defend against unsafe code outputs from these models.", "method": "The paper proposes BlueCodeAgent, an end-to-end blue teaming agent supported by automated red teaming. Red teaming generates risky instances, and the blue teaming agent uses these to detect both known and novel risks through constitution and code analysis, using agentic integration for layered defense. For vulnerable code, dynamic analysis is used to reduce false positives.", "result": "BlueCodeAgent outperforms the base models and safety-prompt defenses on three tasks: bias detection, malicious instruction detection, and vulnerable code detection, achieving an average improvement of 12.7% in F1 score across four datasets. It is particularly effective at reducing false positives in vulnerable code detection.", "conclusion": "BlueCodeAgent successfully integrates red and blue teaming to enhance LLM code generation safety, demonstrating that continuous vulnerability discovery (red teaming) strengthens defense (blue teaming) and improves risk detection effectiveness."}}
{"id": "2510.18270", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18270", "abs": "https://arxiv.org/abs/2510.18270", "authors": ["Yang Chen", "Toufique Ahmed", "Reyhaneh Jabbarvand", "Martin Hirzel"], "title": "When Old Meets New: Evaluating the Impact of Regression Tests on SWE Issue Resolution", "comment": null, "summary": "Test suites in real-world projects are often large and achieve high code\ncoverage, yet they remain insufficient for detecting all bugs. The abundance of\nunresolved issues in open-source project trackers highlights this gap. While\nregression tests are typically designed to ensure past functionality is\npreserved in the new version, they can also serve a complementary purpose:\ndebugging the current version. Specifically, regression tests can (1) enhance\nthe generation of reproduction tests for newly reported issues, and (2)\nvalidate that patches do not regress existing functionality. We present\nTestPrune, a fully automated technique that leverages issue tracker reports and\nstrategically reuses regression tests for both bug reproduction and patch\nvalidation.\n  A key contribution of TestPrune is its ability to automatically minimize the\nregression suite to a small, highly relevant subset of tests. Due to the\npredominance of LLM-based debugging techniques, this minimization is essential\nas large test suites exceed context limits, introduce noise, and inflate\ninference costs. TestPrune can be plugged into any agentic bug repair pipeline\nand orthogonally improve overall performance. As a proof of concept, we show\nthat TestPrune leads to a 6.2%-9.0% relative increase in issue reproduction\nrate within the Otter framework and a 9.4% - 12.9% relative increase in issue\nresolution rate within the Agentless framework on SWE-Bench Lite and SWE-Bench\nVerified benchmarks, capturing fixes that were correctly produced by agents but\nnot submitted as final patches. Compared to the benefits, the cost overhead of\nusing TestPrune is minimal, i.e., \\$0.02 and \\$0.05 per SWE-Bench instance,\nusing GPT-4o and Claude-3.7-Sonnet models, respectively.", "AI": {"tldr": "TestPrune is an automated tool that intelligently reduces the size of regression test suites for LLM-driven bug reproduction and patch validation. It improves both reproduction and resolution rates by 6-13% with negligible extra cost, making debugging workflows more efficient.", "motivation": "Despite having extensive test suites with high coverage, many real-world projects still fail to catch all bugs, as evidenced by unresolved issues in open-source trackers. Slow or ineffective bug reproduction and patch validation processes hinder debugging and repair pipelines.", "method": "TestPrune is a fully automated technique that utilizes issue tracker reports and reuses regression tests for bug reproduction and patch validation. It automatically minimizes regression suites to the most relevant subset of tests, making large suites manageable for LLM-based debugging agents.", "result": "TestPrune achieves a 6.2%-9.0% relative increase in issue reproduction rate in the Otter framework and a 9.4%-12.9% relative increase in issue resolution rate in the Agentless framework on SWE-Bench Lite and SWE-Bench Verified benchmarks. The additional computational cost is low ($0.02 - $0.05) per instance using GPT-4o and Claude-3.7-Sonnet.", "conclusion": "TestPrune significantly enhances the effectiveness of bug reproduction and patch validation in automated debugging workflows by pruning test suites to relevant subsets, improving performance and efficiency with minimal costs."}}
{"id": "2510.18292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18292", "abs": "https://arxiv.org/abs/2510.18292", "authors": ["Hala Abdelkader", "Mohamed Abdelrazek", "Priya Rani", "Rajesh Vasa", "Jean-Guy Schneider"], "title": "Ensuring Robustness in ML-enabled Software Systems: A User Survey", "comment": null, "summary": "Ensuring robustness in ML-enabled software systems requires addressing\ncritical challenges, such as silent failures, out-of-distribution (OOD) data,\nand adversarial attacks. Traditional software engineering practices, which rely\non predefined logic, are insufficient for ML components that depend on data and\nprobabilistic decision-making. To address these challenges, we propose the\nML-On-Rails protocol, a unified framework designed to enhance the robustness\nand trustworthiness of ML-enabled systems in production. This protocol\nintegrates key safeguards such as OOD detection, adversarial attack detection,\ninput validation, and explainability. It also includes a model-to-software\ncommunication framework using HTTP status codes to enhance transparency in\nreporting model outcomes and errors. To align our approach with real-world\nchallenges, we conducted a practitioner survey, which revealed major robustness\nissues, gaps in current solutions, and highlighted how a standardised protocol\nsuch as ML-On-Rails can improve system robustness. Our findings highlight the\nneed for more support and resources for engineers working with ML systems.\nFinally, we outline future directions for refining the proposed protocol,\nleveraging insights from the survey and real-world applications to continually\nenhance its effectiveness.", "AI": {"tldr": "ML-enabled software systems have key robustness gaps. The ML-On-Rails protocol addresses these by integrating safeguards and standardizing communication, validated through a practitioner survey. More support for ML engineers is needed.", "motivation": "ML-enabled software systems face unique robustness challenges such as silent failures, out-of-distribution data, and adversarial attacks, which traditional software engineering cannot fully address.", "method": "The authors propose ML-On-Rails, a unified protocol that incorporates OOD detection, adversarial attack detection, input validation, explainability, and a model-to-software communication framework using HTTP status codes. They also conducted a practitioner survey to validate real-world needs.", "result": "The survey identified major robustness issues and gaps in current practice, showing that ML-On-Rails can address these and improve system robustness. It also indicated a need for more support and resources for engineers working with ML systems.", "conclusion": "ML-On-Rails offers a comprehensive and standardized solution for improving the robustness and trustworthiness of ML-enabled software systems, with future development guided by practitioner feedback and real-world application."}}
{"id": "2510.18327", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18327", "abs": "https://arxiv.org/abs/2510.18327", "authors": ["Yunkun Wang", "Yue Zhang", "Guochang Li", "Chen Zhi", "Binhua Li", "Fei Huang", "Yongbin Li", "Shuiguang Deng"], "title": "InspectCoder: Dynamic Analysis-Enabled Self Repair through interactive LLM-Debugger Collaboration", "comment": null, "summary": "Large Language Models (LLMs) frequently generate buggy code with complex\nlogic errors that are challenging to diagnose. While existing LLM-based\nself-repair approaches conduct intensive static semantic analysis or reply on\nsuperficial execution logs, they miss the in-depth runtime behaviors that often\nexpose bug root causes-lacking the interactive dynamic analysis capabilities\nthat make human debugging effective. We present InspectCoder, the first agentic\nprogram repair system that empowers LLMs to actively conduct dynamic analysis\nvia interactive debugger control. Our dual-agent framework enables strategic\nbreakpoint placement, targeted state inspection, and incremental runtime\nexperimentation within stateful debugger sessions. Unlike existing methods that\nfollow fixed log collection procedures, InspectCoder adaptively inspects and\nperturbs relevant intermediate states at runtime, and leverages immediate\nprocess rewards from debugger feedback to guide multi-step reasoning,\ntransforming LLM debugging paradigm from blind trial-and-error into systematic\nroot cause diagnosis. We conduct comprehensive experiments on two challenging\nself-repair benchmarks: BigCodeBench-R and LiveCodeBench-R. InspectCoder\nachieves 5.10%-60.37% relative improvements in repair accuracy over the\nstrongest baseline, while delivering 1.67x-2.24x superior bug-fix efficiency\nrespectively. We also contribute InspectWare, an open-source middleware that\nabstracts debugger complexities and maintains stateful debugging sessions\nacross mainstream Python testing frameworks. Our work provides actionable\ninsight into the interactive LLM-debugger systems, demonstrating the\nsignificant potential of LLM-driven dynamic analysis for automated software\nengineering.", "AI": {"tldr": "InspectCoder lets LLMs use debuggers interactively for smarter bug fixing, greatly boosting both accuracy and efficiency compared to previous methods, paving the way for more advanced automated debugging tools.", "motivation": "Existing LLM-based program repair tools struggle to diagnose and fix complex bugs due to lack of dynamic analysis and interactive debugging, unlike human debuggers who can actively explore state and behavior.", "method": "A dual-agent framework where LLMs strategically place breakpoints, perform targeted inspection of program states, and conduct incremental runtime experiments using debugger feedback, rather than relying solely on static analysis or execution logs.", "result": "InspectCoder delivers 5.10%-60.37% relative improvement in repair accuracy and 1.67x-2.24x better bug-fix efficiency vs. previous state-of-the-art, validated across two challenging benchmarks.", "conclusion": "InspectCoder significantly improves LLM-based code repair by enabling dynamic, interactive debugging, outperforming existing methods in both accuracy and efficiency. Their open-source middleware, InspectWare, facilitates reproducible research and practical adoption."}}
{"id": "2510.18430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18430", "abs": "https://arxiv.org/abs/2510.18430", "authors": ["Tasha Settewong", "Youmei Fan", "Raula Gaikovina Kula", "Kenichi Matsumoto"], "title": "Human to Document, AI to Code: Three Case Studies of Comparing GenAI for Notebook Competitions", "comment": null, "summary": "Computational notebooks have become the preferred tool of choice for data\nscientists and practitioners to perform analyses and share results. Notebooks\nuniquely combine scripts with documentation. With the emergence of generative\nAI (GenAI) technologies, it is increasingly important, especially in\ncompetitive settings, to distinguish the characteristics of human-written\nversus GenAI.\n  In this study, we present three case studies to explore potential strengths\nof both humans and GenAI through the coding and documenting activities in\nnotebooks. We first characterize differences between 25 code and documentation\nfeatures in human-written, medal-winning Kaggle notebooks. We find that gold\nmedalists are primarily distinguished by longer and more detailed\ndocumentation. Second, we analyze the distinctions between human-written and\nGenAI notebooks. Our results show that while GenAI notebooks tend to achieve\nhigher code quality (as measured by metrics like code smells and technical\ndebt), human-written notebooks display greater structural diversity,\ncomplexity, and innovative approaches to problem-solving. Based on these\nresults, we envision the work as groundwork that highlight four agendas to\nfurther investigate how GenAI could be utilized in notebooks that maximizes the\npotential collaboration between human and AI.", "AI": {"tldr": "This paper compares gold medal-winning human-written and GenAI-generated computational notebooks, finding human notebooks excel in documentation detail and innovation, while GenAI notebooks demonstrate better code quality. The research sets the stage for future work on optimizing human-AI collaboration in data science notebooks.", "motivation": "The motivation behind this study is the growing use of computational notebooks by data scientists, coupled with the rising presence of Generative AI (GenAI) technologies. With increased competition and collaboration, there is a need to understand the distinguishing features of human-written and GenAI-generated notebooks.", "method": "The authors conducted three case studies focusing on coding and documenting activities in computational notebooks. They analyzed 25 features from human-written (particularly Kaggle medal-winning) and GenAI-generated notebooks, comparing aspects like documentation length, code quality, and structural diversity.", "result": "Gold medal-winning Kaggle notebooks are noted for their longer and more detailed documentation. GenAI-generated notebooks generally have higher code quality, with fewer code smells and reduced technical debt, but human-authored notebooks reveal greater structural diversity, complexity, and innovative problem-solving.", "conclusion": "The study highlights the respective strengths of humans and GenAI in notebook creation. The authors propose four research agendas to further explore how GenAI can be integrated to enhance collaboration and results in computational notebooks."}}
{"id": "2510.18448", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.18448", "abs": "https://arxiv.org/abs/2510.18448", "authors": ["Wenjing Dang", "Kaixuan Li", "Sen Chen", "Zhenwei Zhuo", "Lyuye Zhang", "Zheli Liu"], "title": "Real-World Usability of Vulnerability Proof-of-Concepts: A Comprehensive Study", "comment": null, "summary": "The Proof-of-Concept (PoC) for a vulnerability is crucial in validating its\nexistence, mitigating false positives, and illustrating the severity of the\nsecurity threat it poses. However, research on PoCs significantly lags behind\nstudies focusing on vulnerability data. This discrepancy can be directly\nattributed to several challenges, including the dispersion of real-world PoCs\nacross multiple platforms, the diversity in writing styles, and the difficulty\nassociated with PoC reproduction. To fill this gap, we conduct the first\nlarge-scale study on PoCs in the wild, assessing their report availability,\ncompleteness, reproducibility. Specifically, 1) to investigate PoC reports\navailability for CVE vulnerability, we collected an extensive dataset of\n470,921 PoCs and their reports from 13 platforms, representing the broadest\ncollection of publicly available PoCs to date. 2) To assess the completeness of\nPoC report at a fine-grained level, we proposed a component extraction method,\nwhich combines pattern-matching techniques with a fine-tuned BERT-NER model to\nextract 9 key components from PoC reports. 3) To evaluate the effectiveness of\nPoCs, we recruited 8 participants to manually reproduce 150 sampled\nvulnerabilities with 32 vulnerability types based on PoC reports, enabling an\nin-depth analysis of PoC reproducibility and the factors influencing it. Our\nfindings reveal that 78.9% of CVE vulnerabilities lack available PoCs, and\nexisting PoC reports typically miss about 30% of the essential components\nrequired for effective vulnerability understanding and reproduction, with\nvarious reasons identified for the failure to reproduce vulnerabilities using\navailable PoC reports. Finally, we proposed actionable strategies for\nstakeholders to enhance the overall usability of vulnerability PoCs in\nstrengthening software security.", "AI": {"tldr": "This paper presents the largest study on vulnerability PoC reports, revealing most CVEs lack PoCs and many reports are incomplete or hard to reproduce. It analyzes factors behind these issues and offers solutions to make PoCs more effective in real-world security practices.", "motivation": "Proof-of-Concept (PoC) reports are critical for validating vulnerabilities and informing security responses, but research on PoCs is underdeveloped compared to vulnerability data studies. Challenges include dispersed PoCs, inconsistent formats, and difficulty in reproduction.", "method": "The authors conducted the first large-scale study of PoCs, collecting 470,921 PoCs from 13 platforms. They used pattern-matching with a fine-tuned BERT-NER model to extract key report components and recruited participants to manually test the reproducibility of vulnerabilities using PoC reports.", "result": "The study found that 78.9% of CVE vulnerabilities do not have available PoCs. Additionally, existing PoC reports lack around 30% of the key information necessary for reproduction, and there are various reasons for failed reproductions. The paper identifies these factors and proposes strategies to improve PoC usability.", "conclusion": "There is a significant gap in PoC report availability and completeness, impeding effective vulnerability understanding and mitigation. The proposed strategies can help stakeholders improve PoC quality and software security."}}
{"id": "2510.18456", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18456", "abs": "https://arxiv.org/abs/2510.18456", "authors": ["Cristina Martinez Montes", "Robert Feldt", "Cristina Miguel Martos", "Sofia Ouhbi", "Shweta Premanandan", "Daniel Graziotin"], "title": "Large Language Models in Thematic Analysis: Prompt Engineering, Evaluation, and Guidelines for Qualitative Software Engineering Research", "comment": null, "summary": "As artificial intelligence advances, large language models (LLMs) are\nentering qualitative research workflows, yet no reproducible methods exist for\nintegrating them into established approaches like thematic analysis (TA), one\nof the most common qualitative methods in software engineering research.\nMoreover, existing studies lack systematic evaluation of LLM-generated\nqualitative outputs against established quality criteria. We designed and\niteratively refined prompts for Phases 2-5 of Braun and Clarke's reflexive TA,\nthen tested outputs from multiple LLMs against codes and themes produced by\nexperienced researchers. Using 15 interviews on software engineers' well-being,\nwe conducted blind evaluations with four expert evaluators who applied rubrics\nderived directly from Braun and Clarke's quality criteria. Evaluators preferred\nLLM-generated codes 61% of the time, finding them analytically useful for\nanswering the research question. However, evaluators also identified\nlimitations: LLMs fragmented data unnecessarily, missed latent interpretations,\nand sometimes produced themes with unclear boundaries. Our contributions are\nthreefold. First, a reproducible approach integrating refined, documented\nprompts with an evaluation framework to operationalize Braun and Clarke's\nreflexive TA. Second, an empirical comparison of LLM- and human-generated codes\nand themes in software engineering data. Third, guidelines for integrating LLMs\ninto qualitative analysis while preserving methodological rigour, clarifying\nwhen and how LLMs can assist effectively and when human interpretation remains\nessential.", "AI": {"tldr": "The paper presents a systematic way to integrate and test LLMs in qualitative thematic analysis. LLM outputs were often preferred for coding, but humans still did better with complex interpretations. Guidelines help ensure rigor when LLMs are used, but some human insight remains irreplaceable.", "motivation": "There is growing interest in applying large language models (LLMs) to qualitative research methods in software engineering, but reproducible integration techniques and systematic quality assessment are lacking, especially for the commonly used thematic analysis (TA).", "method": "The authors developed and refined prompts tailored to Phases 2-5 of Braun and Clarke\u2019s reflexive TA, then used 15 interviews with software engineers for blind evaluation. Outputs from multiple LLMs were compared against codes/themes generated by experienced researchers, using expert evaluators and rubrics based on established quality criteria.", "result": "Expert evaluators preferred LLM-generated codes 61% of the time, finding them useful. However, limitations were noted: LLMs sometimes fragmented data excessively, missed deeper interpretations, and produced vague or bounded themes.", "conclusion": "A reproducible framework for LLM-assisted thematic analysis was created, guidelines for effective integration were proposed, and it was clarified that while LLMs can contribute valuably to qualitative analysis, human judgment is still required for nuanced interpretation."}}
{"id": "2510.18471", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.18471", "abs": "https://arxiv.org/abs/2510.18471", "authors": ["Xue Jiang", "Yihong Dong", "Mengyang Liu", "Hongyi Deng", "Tian Wang", "Yongding Tao", "Rongyu Cao", "Binhua Li", "Zhi Jin", "Wenpin Jiao", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment", "comment": null, "summary": "While Large Language Models (LLMs) excel at code generation by learning from\nvast code corpora, a fundamental semantic gap remains between their training on\ntextual patterns and the goal of functional correctness, which is governed by\nformal execution semantics. Reinforcement Learning with Verifiable Rewards\n(RLVR) approaches attempt to bridge this gap using outcome rewards from\nexecuting test cases. However, solely relying on binary pass/fail signals is\ninefficient for establishing a well-aligned connection between the textual\nrepresentation of code and its execution semantics, especially for subtle\nlogical errors within the code. In this paper, we propose CodeRL+, a novel\napproach that integrates execution semantics alignment into the RLVR training\npipeline for code generation. CodeRL+ enables the model to infer variable-level\nexecution trajectory, providing a direct learning signal of execution\nsemantics. CodeRL+ can construct execution semantics alignment directly using\nexisting on-policy rollouts and integrates seamlessly with various RL\nalgorithms. Extensive experiments demonstrate that CodeRL+ outperforms\npost-training baselines (including RLVR and Distillation), achieving a 4.6%\naverage relative improvement in pass@1. CodeRL+ generalizes effectively to\nother coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning\nand test-output-generation benchmarks, respectively. CodeRL+ shows strong\napplicability across diverse RL algorithms and LLMs. Furthermore, probe\nanalyses provide compelling evidence that CodeRL+ strengthens the alignment\nbetween code's textual representations and its underlying execution semantics.", "AI": {"tldr": "CodeRL+ improves how LLMs learn code by aligning textual code with its execution through variable-level feedback, outperforming existing methods and showing strong generalizability.", "motivation": "Current LLM code generation relies mainly on textual pattern learning, which creates a semantic gap with functional correctness determined by code execution. RLVR uses test case outcomes but struggles with nuanced logical errors due to crude reward signals.", "method": "CodeRL+ uses execution semantics alignment in the RLVR training pipeline, enabling models to infer variable-level execution trajectories and providing direct learning signals. It leverages existing on-policy rollouts and integrates with multiple RL algorithms.", "result": "Experiments show CodeRL+ outperforms baselines (RLVR and Distillation) with a 4.6% average improvement in pass@1, 15.5% higher accuracy on code reasoning benchmarks, and 4.4% higher accuracy on test-output-generation tasks, with robust applicability across models and RL algorithms.", "conclusion": "CodeRL+ significantly enhances the alignment between code's textual representation and its execution semantics, improving code generation performance in LLMs and generalizing effectively across various tasks and RL algorithms."}}
{"id": "2510.18509", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18509", "abs": "https://arxiv.org/abs/2510.18509", "authors": ["Valtteri Ala-Salmi", "Zeeshan Rasheed", "Abdul Malik Sami", "Muhammad Waseem", "Kai-Kristian Kemell", "Jussi Rasku", "Mika Saari", "Pekka Abrahamsson"], "title": "VAPU: System for Autonomous Legacy Code Modernization", "comment": "Table 13, figure 2", "summary": "In this study, we present a solution for the modernization of legacy\napplications, an area of code generation where LLM-based multi-agent systems\nare proving essential for complex multi-phased tasks. Legacy applications often\ncontain deprecated components that create compatibility, security, and\nreliability risks, but high resource costs make companies hesitate to update.\nWe take a step forward to integrate an LLM-based multi-agent system as part of\na legacy web application update to provide a cost-effective solution to update\nlegacy applications autonomously. We propose a multi-agent system named a\nVerifying Agent Pipeline Updater (VAPU), which is designed to update code files\nin phases while simulating different roles in a software development team. In\nour previous study, we evaluated the system for legacy version updates by using\nsix legacy web application view files by resulting errors and accomplished\nrequirements. This study extends the previous evaluation of a multi-agent\npipeline system by extending the evaluation of VAPU from a single LLM to five\nLLMs and using the temperature parameter in both 0 to 1 settings. Additionally,\nwe tested the system with 20 open-source Python GitHub projects. The results of\nthe evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning\n(OSL) prompts. The extended evaluation of VAPU showed that particularly in a\nlow-temperature VAPU can get similar level of error count compared to the\nZSL/OSL prompts but with a higher level of fulfilled requirements, depending on\nthe LLM. VAPU showed up to 22.5% increase in the succeeding Python file update\nrequirements compared to ZSL/OSL prompts. The study indicates that an LLM-based\nmulti-agent system is a capable solution to update components of a legacy\napplication autonomously.", "AI": {"tldr": "This paper presents VAPU, an LLM-powered multi-agent system for modernizing legacy applications, showing that it fulfills more code update requirements than traditional prompt approaches while maintaining similar error rates.", "motivation": "Modernizing legacy applications is crucial due to compatibility, security, and reliability risks associated with outdated code, but high resource costs discourage companies from updating. LLM-based multi-agent systems have emerged as promising solutions for automating complex, multi-phased modernization tasks.", "method": "The study introduces a multi-agent system called Verifying Agent Pipeline Updater (VAPU), which simulates various software development roles to autonomously update code files in phases. The evaluation extends prior work by using five different LLMs at varying temperature settings, and compares the system's performance on 20 open-source Python projects against Zero-Shot and One-Shot Learning prompt-based methods.", "result": "VAPU performs comparably to ZSL/OSL in terms of error count, especially at low temperature settings, but achieves a significantly higher rate of fulfilled update requirements\u2014up to 22.5% more\u2014depending on the LLM used.", "conclusion": "LLM-based multi-agent systems, such as VAPU, are effective and cost-efficient solutions for autonomously updating legacy application components, achieving greater success in meeting update requirements compared to conventional prompting methods."}}
{"id": "2510.18519", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18519", "abs": "https://arxiv.org/abs/2510.18519", "authors": ["Md Arafat Hossain", "Jun Han", "Muhammad Ashad Kabir", "Steve Versteeg", "Jean-Guy Schneider", "Jiaojiao Jiang"], "title": "Mining Service Behavior for Stateful Service Emulation", "comment": "19 pages", "summary": "Enterprise software systems are increasingly integrating with diverse\nservices to meet expanding business demands. Testing these highly\ninterconnected systems presents a challenge due to the need for access to the\nconnected services. Service virtualization has emerged as a widely used\ntechnique to derive service models from recorded interactions, for service\nresponse generation during system testing. Various methods have been proposed\nto emulate actual service behavior based on these interactions, but most fail\nto account for the service's state, which reduces the accuracy of service\nemulation and the realism of the testing environment, especially when dealing\nwith stateful services. This paper proposes an approach to deriving service\nmodels from service interactions, which enhance the accuracy of response\ngeneration by considering service state. This is achieved by uncovering\ncontextual dependencies among interaction messages and analyzing the\nrelationships between message data values. The approach is evaluated using\ninteraction traces collected from both stateful and stateless services, and the\nresults reveal notable enhancements in accuracy and efficiency over existing\napproaches in service response generation.", "AI": {"tldr": "The paper introduces a new method for service virtualization that accounts for service state and dependencies in message data, resulting in better accuracy and efficiency in testing enterprise software systems.", "motivation": "Testing interconnected enterprise software systems is challenging because accessing all connected services is difficult. Service virtualization helps emulate service behaviors for testing, but current methods often ignore service state, reducing accuracy for stateful services.", "method": "The paper proposes an approach to model service interactions that includes service state. This is done by identifying contextual dependencies among interaction messages and analyzing the relationships between data values within those messages.", "result": "The approach is evaluated with traces from both stateful and stateless services, showing notable improvements in response generation accuracy and efficiency compared to previous methods.", "conclusion": "Considering service state in service virtualization models leads to more accurate and realistic test environments, especially for stateful services."}}
{"id": "2510.18534", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18534", "abs": "https://arxiv.org/abs/2510.18534", "authors": ["Uraz Odyurt", "Richard Loendersloot", "Tiedo Tinga"], "title": "Demonstrators for Industrial Cyber-Physical System Research: A Requirements Hierarchy Driven by Software-Intensive Design", "comment": null, "summary": "One of the challenges apparent in the organisation of research projects is\nthe uncertainties around the subject of demonstrators. A precise and detailed\nelicitation of the coverage for project demonstrators is often an afterthought\nand not sufficiently detailed during proposal writing. This practice leads to\ncontinuous confusion and a mismatch between targeted and achievable\ndemonstration of results, hindering progress. The reliance on the TRL scale as\na loose descriptor does not help either. We propose a demonstrator requirements\nelaboration framework aiming to evaluate the feasibility of targeted\ndemonstrations, making realistic adjustments, and assist in describing\nrequirements. In doing so, we define 5 hierarchical levels of demonstration,\nclearly connected to expectations, e.g., work package interaction, and also\nconnected to the project's industrial use-cases. The considered application\nscope in this paper is the domain of software-intensive systems and industrial\ncyber-physical systems. A complete validation is not accessible, as it would\nrequire application of our framework at the start of a project and observing\nthe results at the end, taking 4-5 years. Nonetheless, we have applied it to\ntwo research projects from our portfolio, one at the early and another at the\nfinal stages, revealing its effectiveness.", "AI": {"tldr": "The paper introduces a new framework to clarify and structure requirements for project demonstrators, helping reduce uncertainty in research projects. Although only partially validated, its use showed promising results in existing projects.", "motivation": "The motivation arises from confusion and mismatches in research projects due to poorly defined demonstrator requirements during proposal writing. Existing reliance on the TRL scale is inadequate for precisely describing what needs to be demonstrated.", "method": "The paper proposes a demonstrator requirements elaboration framework, featuring 5 hierarchical levels of demonstration connected to work package interactions and industrial use-cases. It is applied to two research projects (one early stage, one final stage) for partial validation.", "result": "Application of the framework to two projects shows its effectiveness, even though complete validation is not possible within the scope of this study.", "conclusion": "A structured framework for elaborating demonstrator requirements can clarify expectations, improve feasibility assessment, and reduce confusion in research projects focused on software-intensive and cyber-physical systems."}}
{"id": "2510.18557", "categories": ["cs.SE", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.18557", "abs": "https://arxiv.org/abs/2510.18557", "authors": ["Jianjun Zhao"], "title": "When Abstraction Breaks Physics: Rethinking Modular Design in Quantum Software", "comment": "Accepted at the NIER track of the 40th IEEE/ACM International\n  Conference on Automated Software Engineering (ASE 2025)", "summary": "Abstraction is a fundamental principle in classical software engineering,\nwhich enables modularity, reusability, and scalability. However, quantum\nprograms adhere to fundamentally different semantics, such as unitarity,\nentanglement, the no-cloning theorem, and the destructive nature of\nmeasurement, which introduce challenges to the safe use of classical\nabstraction mechanisms. This paper identifies a fundamental conflict in quantum\nsoftware engineering: abstraction practices that are syntactically valid may\nviolate the physical constraints of quantum computation. We present three\nclasses of failure cases where naive abstraction breaks quantum semantics and\npropose a set of design principles for physically sound abstraction mechanisms.\nWe further propose research directions, including quantum-specific type\nsystems, effect annotations, and contract-based module design. Our goal is to\ninitiate a systematic rethinking of abstraction in quantum software\nengineering, based on quantum semantics and considering engineering\nscalability.", "AI": {"tldr": "Classical software abstractions often break down in quantum programming, violating fundamental quantum constraints. This paper details key failure scenarios, proposes physically sound abstraction principles, and outlines research directions to invent quantum-native engineering abstractions.", "motivation": "Classical software engineering relies on abstraction for modularity, reusability, and scalability, but quantum software has fundamentally different properties that may make classical abstraction methods unsafe or invalid.", "method": "The paper analyzes the incompatibilities between classical abstraction and quantum computation by identifying fundamental conflicts and failure cases, and it proposes design principles and research directions specific to quantum software engineering.", "result": "The paper presents three classes of failure cases where classical abstraction mechanisms fail in the context of quantum programming, and introduces new design principles, as well as research directions such as quantum-specific type systems, effect annotations, and contract-based module design.", "conclusion": "Abstraction in quantum software engineering requires a rethinking of conventional methods, with new principles and tools tailored to the unique constraints of quantum computation."}}
{"id": "2510.18560", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18560", "abs": "https://arxiv.org/abs/2510.18560", "authors": ["Chunyang Li", "Yilun Zheng", "Xinting Huang", "Tianqing Fang", "Jiahao Xu", "Yangqiu Song", "Lihui Chen", "Han Hu"], "title": "WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality", "comment": null, "summary": "The paradigm of LLM-as-a-judge is emerging as a scalable and efficient\nalternative to human evaluation, demonstrating strong performance on\nwell-defined tasks. However, its reliability in open-ended tasks with dynamic\nenvironments and complex interactions remains unexplored. To bridge the gap, we\nintroduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge\nperformance in web development, with support for both non-interactive\nevaluation based on static observations and continuous interactive evaluation\nwith a dynamic web environment. WebDevJudge comprises human preference labels\nover paired web implementations, annotated with structured and query-grounded\nrubrics to ensure high-quality ground truth. Using this benchmark, we\ncomprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic\nworkflows. We systematically investigate the impact of different paradigms and\nguidance mechanisms. Our experiments reveal a significant gap between LLM\njudges and human experts. In-depth analysis indicates this gap stems from\nfundamental model limitations, including failures in recognizing functional\nequivalence, verifying task feasibility, and mitigating bias. Overall,\nWebDevJudge presents a significant challenge to LLM-as-a-judge, offering\ninsights to guide future research toward developing more reliable and capable\nautomated evaluators for complicated scenarios. Code and data are available at\nhttps://github.com/lcy2723/WebDevJudge.", "AI": {"tldr": "WebDevJudge is a new benchmark for evaluating LLMs in web development tasks, showing that current LLM 'judges' fall short compared to humans, especially in complex, dynamic scenarios. The work points out key areas for improvement and lays groundwork for future robust automated evaluation systems.", "motivation": "The motivation is to address the limitations and unexplored reliability of using LLMs as judges for open-ended, complex tasks in dynamic environments, specifically in web development. The current paradigm is successful for well-defined tasks, but its application to more complex scenarios needs systematic evaluation.", "method": "The paper introduces WebDevJudge, a comprehensive benchmark for assessing the performance of LLM-as-a-judge in web development. The method includes both static, non-interactive evaluation and dynamic, interactive evaluation, using human preference labels and structured rubrics. The authors evaluate a range of automated evaluators (LLMs, MLLMs, agentic workflows) and analyze the paradigms and guidance mechanisms.", "result": "The experiments reveal a significant gap between the performance of LLM-based evaluators and human experts. The gap is attributed to model limitations such as failure in recognizing functional equivalence, verifying task feasibility, and mitigating bias.", "conclusion": "WebDevJudge poses a formidable challenge for LLM-as-a-judge applications, highlighting their current shortcomings in complex web development scenarios and providing valuable insights for future research to enhance automated evaluator reliability and capability."}}
{"id": "2510.18590", "categories": ["cs.SE", "cs.HC", "D.2.2; D.2.11; K.6.3"], "pdf": "https://arxiv.org/pdf/2510.18590", "abs": "https://arxiv.org/abs/2510.18590", "authors": ["Antonio Lamanna"], "title": "A Structured Evaluation Framework for Low-Code Platform Selection: A Multi-Criteria Decision Model for Enterprise Digital Transformation", "comment": "15 pages, 1 figure. PDF-only submission (XeLaTeX)", "summary": "The rapid adoption of Low-Code Development Platforms (LCDPs) has created a\ncritical need for systematic evaluation methodologies that enable organizations\nto make informed platform selection decisions. This paper presents a\ncomprehensive evaluation framework based on five key criteria: Business Process\nOrchestration, UI/UX Customization, Integration and Interoperability,\nGovernance and Security, and AI-Enhanced Automation. We propose a weighted\nscoring model that allows organizations to quantitatively assess and compare\ndifferent low-code platforms based on their specific requirements and strategic\npriorities. The framework addresses the gap between marketing-driven platform\ncomparisons and rigorous, context-specific evaluation methodologies. Through\nempirical validation in enterprise environments, we demonstrate how this\nstructured approach can significantly improve decision-making outcomes and\nreduce the risk of platform lock-in or inadequate solution selection.", "AI": {"tldr": "This paper proposes a practical, criteria-based evaluation model to help businesses systematically compare and select low-code platforms, addressing key areas like orchestration, UI/UX, integration, security, and AI features. Empirical tests show it leads to smarter choices and reduces platform selection risks.", "motivation": "The widespread adoption of Low-Code Development Platforms (LCDPs) has made it essential for organizations to have systematic, reliable methods for evaluating and choosing the most suitable platform.", "method": "The paper introduces a comprehensive evaluation framework built upon five key criteria: Business Process Orchestration, UI/UX Customization, Integration and Interoperability, Governance and Security, and AI-Enhanced Automation. It utilizes a weighted scoring model to quantitatively compare platforms according to an organization's unique needs.", "result": "Empirical validation in enterprise environments shows that the proposed framework helps organizations make better decisions, reducing risks of platform lock-in and unsuitable solutions.", "conclusion": "A structured, criteria-based evaluation framework significantly enhances the platform selection process for LCDPs, improving strategic alignment and decision outcomes."}}
{"id": "2510.18596", "categories": ["cs.SE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.18596", "abs": "https://arxiv.org/abs/2510.18596", "authors": ["Haojia Lin", "Xiaoyu Tan", "Yulei Qin", "Zihan Xu", "Yuchen Shi", "Zongyi Li", "Gang Li", "Shaofei Cai", "Siqi Cai", "Chaoyou Fu", "Ke Li", "Xing Sun"], "title": "CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent", "comment": "24 pages, 6 figures", "summary": "Computer-using agents (CUAs) enable task completion through natural\ninteraction with operating systems and software interfaces. While script-based\nverifiers are widely adopted for evaluation, they suffer from limited\nscalability and inability to provide step-wise assessment. Reward models offer\npromising alternatives, but their effectiveness on CUA evaluation remains\nlargely underexplored. To address this gap, we present CUARewardBench,\ncomprising four key contributions: (1) First-ever Comprehensive CUA Reward\nBenchmark: We introduce the first benchmark for evaluating both outcome reward\nmodels (ORM) and process reward models (PRM) on CUA tasks, enabling systematic\nassessment across trajectory-level and step-level evaluation. (2) Diverse,\nPractical and Reliable Dataset: CUARewardBench encompasses trajectories from 10\nsoftware categories and 7 agent architectures with varying performance levels\n(25.9%-50.8% success rates). All trajectories are expertly annotated through\ncarefully designed protocols, with rigorous quality control to ensure\nreliability and practical applicability. (3) Comprehensive Analysis and\nInsights: Through extensive experiments across 7 vision-language models and 3\nprompt templates, we reveal critical limitations of current CUA RMs, including\ninsufficient visual reasoning capabilities, knowledge deficiencies, and the\nsuperiority of general VLMs over specialized CUA models for reward evaluation.\n(4) Unanimous Prompt Ensemble (UPE): Based on the insights from our\ncomprehensive analysis, we propose UPE, a novel ensemble method that\nsignificantly enhances reward model reliability through strict unanimous voting\nand strategic prompt-template configurations. UPE achieves 89.8% precision and\n93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially\noutperforming single VLMs and traditional ensemble approaches.", "AI": {"tldr": "This paper introduces CUARewardBench, the first comprehensive benchmark and dataset for evaluating reward models in computer-using agent tasks. Extensive experiments show general vision-language models outperform specialized ones, and highlight gaps in current model capabilities. The proposed Unanimous Prompt Ensemble (UPE) method sets a new standard for reliable reward evaluation, greatly exceeding previous approaches.", "motivation": "Script-based verifiers used for computer-using agent (CUA) evaluation are limited in scalability and do not provide detailed, step-wise assessments. Reward models are a potential alternative, but their effectiveness in evaluating CUAs is not well-studied.", "method": "The authors introduce CUARewardBench, a benchmark and dataset for evaluating reward models in CUA tasks. It includes expertly annotated trajectories from a diverse array of software and agent architectures. They perform extensive experiments with multiple vision-language models (VLMs) and prompt templates, analyze performance, and propose the Unanimous Prompt Ensemble (UPE) method to improve reward model reliability.", "result": "Comprehensive analysis reveals limitations of current CUA reward models, such as weak visual reasoning and knowledge gaps. General VLMs outperform specialized models. The proposed UPE method dramatically improves reliability and performance for both outcome and process reward model evaluation, significantly surpassing single models and conventional ensembles.", "conclusion": "CUARewardBench provides a systematic, scalable framework and dataset for evaluating reward models on computer-using agent tasks. The new UPE ensemble method sets a strong baseline for reliable reward model assessment, pushing forward research in this under-explored field."}}
{"id": "2510.18711", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18711", "abs": "https://arxiv.org/abs/2510.18711", "authors": ["Bertha Ngereja", "Magne J\u00f8rgensen"], "title": "An overview of the use of alternative funding and contracting approaches relevant for agile software development: A systematic review of real-life experiences", "comment": "48 pages, 6 tables, 6 figures and 3 appendices", "summary": "Agile software development emphasizes flexibility and iterative processes,\nwhich may conflict with the more linear, rigid, and time-consuming traditional\nfunding and contracting approaches. This review synthesizes real-life\nexperiences of using alternative (non-traditional) contracting and funding\napproaches. The focus is on identifying approaches that align better with agile\nprinciples and understanding the motivations, benefits, and challenges these\nalternatives present. A systematic literature review was conducted in SCOPUS,\nWeb of Science, and Google Scholar, where we identified 38 relevant\npeer-reviewed empirical studies from private and public sector contexts. Four\nalternative funding and four alternative contracting approaches were\nidentified. Organizations were motivated to adopt these alternative approaches\nbecause traditional approaches often proved too rigid, conflicted with agile\nprinciples, hindered effective client-contractor collaboration, and limited\nprofitability. The benefits of these alternatives included higher client\nsatisfaction, reduced contractor risk, and more efficient resource utilization.\nAdopting alternative funding and contracting approaches may promote flexibility\nand efficiency in agile projects but also presents cultural and structural\nchallenges, increases the risk of scope creep and analysis paralysis, and\nrequires additional effort in terms of time and resources. The context of the\norganization matters highly in selecting a suitable approach, such as the\norganizational readiness in terms of its leaders, people, and systems. Thus,\ninstead of wholly adopting alternative approaches and introducing changes\nabruptly, organizations may benefit from starting with hybrid approaches that\nbalance flexibility and control and progressively transition to fully flexible\napproaches tailored to their needs", "AI": {"tldr": "Traditional approaches clash with agile principles, so this review identifies alternative funding and contracting models that better support agility. These alternatives offer enhanced satisfaction and efficiency but pose challenges like scope creep and cultural resistance. Hybrid models may ease the transition to more flexible methods.", "motivation": "Traditional funding and contracting methods are often at odds with agile software development due to their rigidity, which hinders flexibility, collaboration, and profitability. The motivation is to explore alternatives that better support agile principles.", "method": "A systematic literature review analyzing 38 peer-reviewed empirical studies from both private and public sector contexts, sourced from SCOPUS, Web of Science, and Google Scholar.", "result": "Four alternative funding and four alternative contracting approaches aligning better with agile principles were identified. These alternatives brought higher client satisfaction, reduced contractor risk, and improved resource utilization. However, organizations may face cultural and structural challenges, increased risks (e.g., scope creep), and need extra resources to adopt them.", "conclusion": "Organizations should consider hybrid approaches as a starting point, balancing flexibility and control, then gradually transition to fully flexible alternatives customized to their context and readiness."}}
{"id": "2510.18719", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18719", "abs": "https://arxiv.org/abs/2510.18719", "authors": ["Chengwen Du", "Tao Chen"], "title": "Causally Perturbed Fairness Testing", "comment": "accepted by TOSEM", "summary": "To mitigate unfair and unethical discrimination over sensitive features\n(e.g., gender, age, or race), fairness testing plays an integral role in\nengineering systems that leverage AI models to handle tabular data. A key\nchallenge therein is how to effectively reveal fairness bugs under an\nintractable sample size using perturbation. Much current work has been focusing\non designing the test sample generators, ignoring the valuable knowledge about\ndata characteristics that can help guide the perturbation and hence limiting\ntheir full potential. In this paper, we seek to bridge such a gap by proposing\na generic framework of causally perturbed fairness testing, dubbed CausalFT.\nThrough causal inference, the key idea of CausalFT is to extract the most\ndirectly and causally relevant non-sensitive feature to its sensitive\ncounterpart, which can jointly influence the prediction of the label. Such a\ncausal relationship is then seamlessly injected into the perturbation to guide\na test sample generator. Unlike existing generator-level work, CausalFT serves\nas a higher-level framework that can be paired with diverse base generators.\nExtensive experiments on 1296 cases confirm that CausalFT can considerably\nimprove arbitrary base generators in revealing fairness bugs over 93% of the\ncases with acceptable extra runtime overhead. Compared with a state-of-the-art\napproach that ranks the non-sensitive features solely based on correlation,\nCausalFT performs significantly better on 64% cases while being much more\nefficient. Further, CausalFT can better improve bias resilience in nearly all\ncases.", "AI": {"tldr": "CausalFT enhances fairness testing in AI by applying causal inference to guide sample perturbations, outperforming correlation-based methods in revealing fairness bugs and improving bias resilience with minimal runtime overhead.", "motivation": "Fairness testing in AI systems is essential to prevent discrimination over sensitive features such as gender, age, or race. However, current methods often overlook the value of data characteristics in guiding perturbation, limiting their effectiveness in discovering fairness bugs.", "method": "The authors introduce CausalFT, a generic framework that leverages causal inference to identify and perturb non-sensitive features which are causally linked to sensitive ones, thereby exposing fairness bugs more effectively. CausalFT integrates with various sample generators, guiding their perturbations through the discovered causal relationships.", "result": "Extensive experiments across 1296 cases show CausalFT improves the ability of arbitrary sample generators to reveal fairness bugs in over 93% of cases, with modest additional runtime overhead. It surpasses state-of-the-art correlation-based methods in 64% of cases while being more efficient, and generally enhances bias resilience.", "conclusion": "CausalFT offers a versatile and more effective solution for fairness testing in AI models handling tabular data by integrating causal relationships into sample perturbation, outperforming existing correlation-based approaches."}}
{"id": "2510.18787", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18787", "abs": "https://arxiv.org/abs/2510.18787", "authors": ["Quim Motger", "Carlota Catot", "Xavier Franch"], "title": "ShaRE your Data! Characterizing Datasets for LLM-based Requirements Engineering", "comment": "Under review", "summary": "[Context] Large Language Models (LLMs) rely on domain-specific datasets to\nachieve robust performance across training and inference stages. However, in\nRequirements Engineering (RE), data scarcity remains a persistent limitation\nreported in surveys and mapping studies. [Question/Problem] Although there are\nmultiple datasets supporting LLM-based RE tasks (LLM4RE), they are fragmented\nand poorly characterized, limiting reuse and comparability. This research\naddresses the limited visibility and characterization of datasets used in\nLLM4RE. We investigate which public datasets are employed, how they can be\nsystematically characterized, and which RE tasks and dataset descriptors remain\nunder-represented. [Ideas/Results] To address this, we conduct a systematic\nmapping study to identify and analyse datasets used in LLM4RE research. A total\nof 62 publicly available datasets are referenced across 43 primary studies.\nEach dataset is characterized along descriptors such as artifact type,\ngranularity, RE stage, task, domain, and language. Preliminary findings show\nmultiple research gaps, including limited coverage for elicitation tasks,\nscarce datasets for management activities beyond traceability, and limited\nmultilingual availability. [Contribution] This research preview offers a public\ncatalogue and structured characterization scheme to support dataset selection,\ncomparison, and reuse in LLM4RE research. Future work will extend the scope to\ngrey literature, as well as integration with open dataset and benchmark\nrepositories.", "AI": {"tldr": "The paper systematically reviews datasets used for LLM-driven Requirements Engineering tasks, finding fragmentation and gaps, and proposes a public catalogue for better visibility and reuse.", "motivation": "Large Language Models (LLMs) need domain-specific data for strong performance, but in Requirements Engineering (RE), there is a known problem of limited and poorly characterized datasets. This limits LLM-based work in RE (LLM4RE), affecting reuse and comparability of research.", "method": "A systematic mapping study was conducted. The research identified and analyzed 62 publicly available datasets referenced in 43 primary LLM4RE studies, characterizing each dataset by artifact type, granularity, RE stage, task, domain, and language.", "result": "The study found several gaps: lack of datasets for elicitation tasks, few datasets for management activities beyond traceability, and limited multilingual resources.", "conclusion": "This work provides a public catalogue and structured dataset characterization to help researchers select, compare, and reuse datasets in LLM4RE, and suggests future extensions including integration with more repositories and grey literature."}}
{"id": "2510.18799", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18799", "abs": "https://arxiv.org/abs/2510.18799", "authors": ["Max Tiessler", "Quim Motger"], "title": "FeClustRE: Hierarchical Clustering and Semantic Tagging of App Features from User Reviews", "comment": "Under review", "summary": "[Context and motivation.] Extracting features from mobile app reviews is\nincreasingly important for multiple requirements engineering (RE) tasks.\nHowever, existing methods struggle to turn noisy, ambiguous feedback into\ninterpretable insights. [Question/problem.] Syntactic approaches lack semantic\ndepth, while large language models (LLMs) often miss fine-grained features or\nfail to structure them coherently. In addition, existing methods output flat\nlists of features without semantic organization, limiting interpretation and\ncomparability. Consequently, current feature extraction approaches do not\nprovide structured, meaningful representations of app features. As a result,\npractitioners face fragmented information that hinder requirement analysis,\nprioritization, and cross-app comparison, among other use cases. [Principal\nideas/results.] In this context, we propose FeClustRE, a framework integrating\nhybrid feature extraction, hierarchical clustering with auto-tuning and\nLLM-based semantic labelling. FeClustRE combines syntactic parsing with LLM\nenrichment, organizes features into clusters, and automatically generates\nmeaningful taxonomy labels. We evaluate FeClustRE on public benchmarks for\nextraction correctness and on a sample study of generative AI assistant app\nreviews for clustering quality, semantic coherence, and interpretability.\n[Contribution.] Overall, FeClustRE delivers (1) a hybrid framework for feature\nextraction and taxonomy generation, (2) an auto-tuning mechanism with a\ncomprehensive evaluation methodology, and (3) open-source and replicable\nimplementation. These contributions bridge user feedback and feature\nunderstanding, enabling deeper insights into current and emerging requirements.", "AI": {"tldr": "FeClustRE is a new hybrid framework that combines linguistic parsing with large language models to extract and organize features from app reviews into coherent taxonomies, improving the usefulness and structure of feedback for requirements engineering.", "motivation": "Extracting actionable features from noisy, ambiguous mobile app reviews is vital for requirements engineering (RE) tasks. However, existing methods are limited: syntactic methods miss semantic depth, LLMs struggle with fine granularity and structure, and outcomes lack semantic organization, frustrating practitioners wanting meaningful, structured representations for analysis and comparison.", "method": "The paper introduces FeClustRE, a framework that integrates syntactic parsing with LLM-based semantic enrichment for hybrid feature extraction. Features are hierarchically clustered using auto-tuning, and clusters are labelled semantically using LLMs to generate meaningful taxonomies. FeClustRE is evaluated on public extraction benchmarks and real-world app reviews for correctness, clustering quality, semantic coherence, and interpretability.", "result": "FeClustRE successfully extracts, clusters, and semantically organizes app features, outperforming traditional flat list extraction methods in accuracy, coherence, and interpretability. Evaluation confirms the effectiveness of the framework in structuring app review feedback for enhanced requirements analysis.", "conclusion": "FeClustRE provides a robust, hybrid approach to structured feature extraction from mobile app reviews, bridging gaps in current methods. Its hierarchical, semantically organized outputs enable deeper insights into user feedback, improving requirements analysis and comparison across apps. The open-source, replicable solution advances RE practices using app review data."}}
{"id": "2510.18861", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18861", "abs": "https://arxiv.org/abs/2510.18861", "authors": ["Pedro Lu\u00eds Fonseca", "Bruno Lima", "Jo\u00e3o Pascoal Faria"], "title": "Streamlining Acceptance Test Generation for Mobile Applications Through Large Language Models: An Industrial Case Study", "comment": null, "summary": "Mobile acceptance testing remains a bottleneck in modern software\ndevelopment, particularly for cross-platform mobile development using\nframeworks like Flutter. While developers increasingly rely on automated\ntesting tools, creating and maintaining acceptance test artifacts still demands\nsignificant manual effort. To help tackle this issue, we introduce AToMIC, an\nautomated framework leveraging specialized Large Language Models to generate\nGherkin scenarios, Page Objects, and executable UI test scripts directly from\nrequirements (JIRA tickets) and recent code changes. Applied to BMW's MyBMW\napp, covering 13 real-world issues in a 170+ screen codebase, AToMIC produced\nexecutable test artifacts in under five minutes per feature on standard\nhardware. The generated artifacts were of high quality: 93.3% of Gherkin\nscenarios were syntactically correct upon generation, 78.8% of PageObjects ran\nwithout manual edits, and 100% of generated UI tests executed successfully. In\na survey, all practitioners reported time savings (often a full developer-day\nper feature) and strong confidence in adopting the approach. These results\nconfirm AToMIC as a scalable, practical solution for streamlining acceptance\ntest creation and maintenance in industrial mobile projects.", "AI": {"tldr": "AToMIC uses large language models to automate mobile acceptance testing for cross-platform apps, greatly reducing manual effort. It produces accurate, usable test scripts from requirements and code changes within minutes, proving effective and time-saving in a real-world BMW app scenario.", "motivation": "Acceptance testing is a significant bottleneck in cross-platform mobile development due to the manual effort required to create and maintain acceptance test artifacts, even as automated tools become more popular.", "method": "The authors introduce AToMIC, a framework that uses specialized Large Language Models to automatically generate Gherkin scenarios, Page Objects, and executable UI test scripts directly from requirements (such as JIRA tickets) and recent code changes. The framework is evaluated on BMW's MyBMW app on 13 real-world issues across a large codebase.", "result": "AToMIC generated test artifacts for each feature in under five minutes, with 93.3% of Gherkin scenarios syntactically correct on first try, 78.8% of PageObjects running without manual edits, and 100% of UI tests executing successfully. Practitioners reported significant time savings (up to a full developer day per feature) and high confidence in the approach.", "conclusion": "AToMIC is a scalable and practical solution for automating and streamlining acceptance test creation and maintenance in industrial cross-platform mobile development projects, delivering high-quality artifacts quickly and saving considerable developer effort."}}
{"id": "2510.18863", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18863", "abs": "https://arxiv.org/abs/2510.18863", "authors": ["Yanlin Wang", "Rongyi Ou", "Yanli Wang", "Mingwei Liu", "Jiachi Chen", "Ensheng Shi", "Xilin Liu", "Yuchi Ma", "Zibin Zheng"], "title": "EffiReasonTrans: RL-Optimized Reasoning for Code Translation", "comment": null, "summary": "Code translation is a crucial task in software development and maintenance.\nWhile recent advancements in large language models (LLMs) have improved\nautomated code translation accuracy, these gains often come at the cost of\nincreased inference latency, hindering real-world development workflows that\ninvolve human-in-the-loop inspection. To address this trade-off, we propose\nEffiReasonTrans, a training framework designed to improve translation accuracy\nwhile balancing inference latency. We first construct a high-quality\nreasoning-augmented dataset by prompting a stronger language model,\nDeepSeek-R1, to generate intermediate reasoning and target translations. Each\n(source code, reasoning, target code) triplet undergoes automated syntax and\nfunctionality checks to ensure reliability. Based on this dataset, we employ a\ntwo-stage training strategy: supervised fine-tuning on reasoning-augmented\nsamples, followed by reinforcement learning to further enhance accuracy and\nbalance inference latency. We evaluate EffiReasonTrans on six translation\npairs. Experimental results show that it consistently improves translation\naccuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while\nreducing the number of generated tokens (up to -19.3%) and lowering inference\nlatency in most cases (up to -29.0%). Ablation studies further confirm the\ncomplementary benefits of the two-stage training framework. Additionally,\nEffiReasonTrans demonstrates improved translation accuracy when integrated into\nagent-based frameworks. Our code and data are available at\nhttps://github.com/DeepSoftwareAnalytics/EffiReasonTrans.", "AI": {"tldr": "EffiReasonTrans is a new training framework for code translation that enhances both accuracy and efficiency. By combining reasoning-augmented data and a two-stage training process, it surpasses previous models in accuracy and speed, with open-source code and data available.", "motivation": "Automated code translation is valuable for software development and maintenance, but improvements in accuracy using large language models (LLMs) come with higher inference latency, which negatively impacts practical workflows that require quick, human-in-the-loop inspection.", "method": "The authors propose EffiReasonTrans, a framework that first builds a high-quality, reasoning-augmented code translation dataset using a strong LLM. Each (source code, reasoning, target code) triplet is validated automatically for syntax and functionality. The model is then trained using supervised fine-tuning on this dataset, followed by reinforcement learning to boost accuracy while controlling inference latency.", "result": "EffiReasonTrans achieves higher translation accuracy (up to +49.2% CA and +27.8% CodeBLEU over the base model), reduces output token count (up to -19.3%), and decreases inference latency (up to -29.0%) on most benchmarks. Ablation studies validate the importance of each training stage, and the framework also performs well in agent-based setups.", "conclusion": "EffiReasonTrans effectively bridges the gap between translation accuracy and inference efficiency in LLM-based code translation, enabling more practical, real-time application. The approach is validated over multiple pairs and tasks, and code/data is released for the community."}}
