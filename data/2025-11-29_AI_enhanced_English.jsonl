{"id": "2511.20782", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.20782", "abs": "https://arxiv.org/abs/2511.20782", "authors": ["Russel Arbore", "Alvin Cheung", "Max Willsey"], "title": "Optimism in Equality Saturation", "comment": null, "summary": "Equality saturation is a technique for program optimization based on non-destructive rewriting and a form of program analysis called e-class analysis. The current form of e-class analysis is pessimistic and therefore ineffective at analyzing cyclic programs, such as those in SSA form. We propose an abstract interpretation algorithm that can precisely analyze cycles during equality saturation. This results in a unified algorithm for optimistic analysis and non-destructive rewriting. We instantiate this approach on a prototype abstract interpreter for SSA programs using a new semantics of SSA. Our prototype can analyze simple example programs more precisely than clang and gcc.", "AI": {"tldr": "The paper presents a new abstract interpretation method that enables optimistic and precise program analysis of cyclic SSA programs during equality saturation, outperforming clang and gcc in simple cases.", "motivation": "Existing e-class analysis in equality saturation is too pessimistic and cannot effectively handle cyclic programs, such as those in SSA form. This limits its effectiveness in optimization and program analysis.", "method": "The authors propose a new abstract interpretation algorithm capable of precisely analyzing cycles in equality saturation. They integrate optimistic analysis with non-destructive rewriting, and implement a prototype abstract interpreter for SSA programs with a new SSA semantics.", "result": "The prototype interpreter can analyze simple example SSA programs more precisely than standard compilers like clang and gcc.", "conclusion": "A unified, optimistic analysis and rewriting algorithm allows for more precise handling of cycles in equality saturation. The approach improves program analysis for SSA-style code over existing compiler technology."}}
{"id": "2511.21209", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.21209", "abs": "https://arxiv.org/abs/2511.21209", "authors": ["Yee-Jian Tan", "Andreas Nuyts", "Dominique Devriese"], "title": "Towards Computational UIP in Cubical Agda", "comment": null, "summary": "Some advantages of Cubical Type Theory, as implemented by Cubical Agda, over intensional Martin-L\u00f6f Type Theory include Quotient Inductive Types (QITs), which exist as instances of Higher Inductive Types, and functional extensionality, which is provable in Cubical Type Theory. However, HoTT features an infinite hierarchy of equalities that may become unwieldy in formalisations. Fortunately, QITs and functional extensionality are both preserved even if the equality levels of Cubical Type Theory are truncated to only homotopical Sets (h-Sets). In other words, removing the univalence axiom from Cubical Type Theory and instead postulating a conflicting axiom: the Uniqueness of Identity Proofs (UIP) postulate. Since univalence is proved in Cubical Type Theory from the so-called Glue Types, therefore, it is known that one can first remove the Glue Types (thus removing univalence) and then set-truncate all equalities (essentially assuming UIP), \u00e0 la XTT. The result is a \"h-Set Cubical Type Theory\" that retains features such as functional extensionality and QITs.\n  However, in Cubical Agda, there are currently only two unsatisfying ways to achieve h-Set Cubical Type Theory. The first is to give up on the canonicity of the theory and simply postulate the UIP axiom, while the second way is to use a standard result stating \"type formers preserve h-levels\" to manually prove UIP for every defined type. The latter is, however, laborious work best suited for an automatic implementation by the proof assistant. In this project, we analyse formulations of UIP and detail their computation rules for Cubical Agda, and evaluate their suitability for implementation. We also implement a variant of Cubical Agda without Glue, which is already compatible with postulated UIP, in anticipation of a future implementation of UIP in Cubical Agda.", "AI": {"tldr": "The paper studies how Cubical Type Theory can be simplified to h-Sets by adopting UIP instead of univalence, showing this retains key features (functional extensionality, QITs). The authors analyze and implement versions of Cubical Agda compatible with this approach, setting the stage for automated UIP support.", "motivation": "Cubical Type Theory (CTT) offers advantages over intensional Martin-L\u00f6f Type Theory, such as support for Quotient Inductive Types (QITs) and functional extensionality. However, the infinite hierarchy of equalities in Homotopy Type Theory (HoTT) can be unwieldy, and there is interest in truncating CTT's equality structure to simpler homotopical Sets (h-Sets), which essentially involves assuming the Uniqueness of Identity Proofs (UIP) rather than univalence.", "method": "The authors analyze different formulations and computation rules for the UIP axiom within Cubical Agda and evaluate how suitable these are for implementation. They also develop and discuss a variant of Cubical Agda that excludes Glue Types, making it compatible with postulated UIP and ready for future automatic UIP implementation.", "result": "The project establishes that both QITs and functional extensionality are retained in 'h-Set Cubical Type Theory' even when univalence is replaced with UIP. The study reviews limitations of the current methods to achieve h-Set CTT in Cubical Agda, and presents an implementation of Cubical Agda without Glue Types, compatible with postulated UIP.", "conclusion": "It is possible to achieve a practical version of Cubical Type Theory relevant for h-Sets retaining key advantages, by phasing out Glue Types and adopting UIP. Suitability of various UIP formulations is examined, and groundwork is prepared for future automated support in proof assistants like Cubical Agda."}}
{"id": "2511.21509", "categories": ["cs.PL", "cs.SC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21509", "abs": "https://arxiv.org/abs/2511.21509", "authors": ["Dirk Beyer", "Gidon Ernst", "Martin Jon\u00e1\u0161", "Marian Lingsch-Rosenfeld"], "title": "SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks", "comment": null, "summary": "In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.", "AI": {"tldr": "The paper introduces SV-LIB, a language-neutral exchange format for software verification, improving interoperability between tools by using familiar programming and SMT standards. SV-LIB version 1.0 is presented, supporting program expression, witness validation, and easy integration, with further features planned.", "motivation": "Existing verification tools are often tied to specific programming languages, but many verification methods are language-independent. There is a need for a common intermediate format to enhance technology transfer and tool interoperability.", "method": "The authors propose SV-LIB, an intermediate language and exchange format for software verification tasks. SV-LIB uses concepts from imperative programming and SMT-LIB for representing expressions, emphasizing ease of parsing and integration with existing SMT-based verification tools. The paper introduces the witness format for SV-LIB and methods for defining and validating witnesses.", "result": "The paper presents SV-LIB version 1.0, detailing its design goals, syntax, and informal semantics. It also introduces the witness format and outlines mechanisms for validation. The format is designed for extensibility and integration, with formal semantics and concurrency features planned for future versions.", "conclusion": "SV-LIB provides a language-agnostic exchange format for software verification tasks, enabling greater interoperability between tools and supporting independent witness validation. It is based on familiar programming and SMT concepts for ease of adoption, with future work aimed at extended formalization and concurrency support."}}
{"id": "2511.20709", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20709", "abs": "https://arxiv.org/abs/2511.20709", "authors": ["Abhijeet Pathak", "Suvadra Barua", "Dinesh Gudimetla", "Rupam Patir", "Jiawei Guo", "Hongxin Hu", "Haipeng Cai"], "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation", "comment": null, "summary": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.", "AI": {"tldr": "The paper presents DUALGAUGE, an automated framework and benchmark for jointly evaluating the security and correctness of code generated by LLMs. It reveals substantial shortcomings in current models and provides open-source tools to drive better, reproducible evaluation.", "motivation": "There is a growing need to ensure that code generated by large language models (LLMs) and autonomous coding agents is both secure and functionally correct. However, current benchmarks inadequately address the joint evaluation of these properties, often considering them separately or failing to thoroughly validate correctness and security together.", "method": "The authors introduce DUALGAUGE, an automated benchmarking framework to jointly evaluate security and correctness of LLM-generated code. They also develop DUALGAUGE-BENCH, a curated suite of coding tasks with validated test suites for both dimensions. DUALGAUGE uses an agentic program executor to run code in sandboxed environments and an LLM-based evaluator to assess both functional correctness and security vulnerabilities against specifications.", "result": "DUALGAUGE-BENCH was rigorously developed and validated, and DUALGAUGE\u2019s evaluation process was proved accurate. Using DUALGAUGE, ten leading LLMs were benchmarked across thousands of test cases, revealing significant shortcomings in the ability of these models to generate code that is both correct and secure.", "conclusion": "There are critical gaps in the ability of current LLMs to generate both correct and secure code. DUALGAUGE and DUALGAUGE-BENCH provide a scalable, reproducible, and rigorous framework for evaluation, helping to accelerate progress in secure code generation and assessment."}}
{"id": "2511.20730", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20730", "abs": "https://arxiv.org/abs/2511.20730", "authors": ["Nehal Afifi", "Christoph Wittig", "Lukas Paehler", "Andreas Lindenmann", "Kai Wolter", "Felix Leitenberger", "Melih Dogru", "Patric Grauberger", "Tobias D\u00fcser", "Albert Albers", "Sven Matthiesen"], "title": "Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities", "comment": null, "summary": "The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities.", "AI": {"tldr": "The paper systematically reviews how data-driven methods are utilized in engineering design, finding that ML and statistical methods dominate, with DL adoption rising. Most methods are applied before validation, and there are notable challenges with interpretability and real-world validation. The work lays groundwork for future design-stage guidelines.", "motivation": "The integration of data-driven methods (DDMs) into product development is fragmented due to uncertainty and a lack of clarity about which DDMs to use and when. There is a need to systematically investigate the usage of DDMs in engineering design to inform better guidelines.", "method": "The paper adopts a PRISMA systematic literature review approach. It uses the V-model as a product development framework, simplified into four stages (system design, system implementation, system integration, validation), and conducts a structured search across major databases for literature from 2014 to 2024. After screening, 114 publications were fully analyzed.", "result": "Machine learning (ML) and statistical methods are most commonly used in current practice, while deep learning (DL) is on the rise but less prevalent. Techniques such as supervised learning, clustering, regression analysis, and surrogate modeling are mainly applied in design, implementation, and integration, but rarely in validation. Major challenges include model interpretability, cross-stage traceability, and insufficient real-world validation. The paper identifies a need for interpretable hybrid models.", "conclusion": "This paper serves as an initial step in developing guidelines for integrating DDMs into product development, highlighting dominant techniques, application stages, challenges, and the need for future work mapping computer science algorithms to specific engineering design problems."}}
{"id": "2511.20813", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20813", "abs": "https://arxiv.org/abs/2511.20813", "authors": ["Simon Hacks"], "title": "Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms", "comment": "17 pages, submitted to CAiSE - International Conference on Advanced information Systems Engineering 2026", "summary": "\"Train While You Fight\" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.", "AI": {"tldr": "The paper examines how advanced distributed learning platforms can support 'Train While You Fight'\u2014continuous, operational learning\u2014by identifying seven key technical challenges and showing, through a German military use case, that software engineering patterns can address these effectively.", "motivation": "Continuous learning during operations is increasingly critical for organizations such as the military. The 'Train While You Fight' (TWYF) concept stresses the need for learning platforms that work in real-time, integrated with operational activities, rather than being separated from them. There is a gap in understanding what technical requirements are necessary and how to address them using software engineering best practices.", "method": "The paper uses a Design Science Research approach: (i) it derives technical challenges from NATO and PfPC documentation, as well as recent practical implementations, (ii) defines solution objectives for ADL platforms supporting TWYF, and (iii) maps these challenges to established software engineering patterns. The approach is complemented by illustrating these patterns with a real-world use case from the German armed forces.", "result": "Seven major technical challenges for TWYF-supportive ADL platforms are identified: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. Solutions to each are mapped to proven software engineering patterns, and their feasibility is shown with a national-level use case.", "conclusion": "Existing software engineering patterns can be systematically selected and applied to address the specific requirements of 'Train While You Fight'-compatible distributed learning platforms, making it feasible to deploy such platforms even for complex and sensitive domains like national defense."}}
{"id": "2511.20916", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20916", "abs": "https://arxiv.org/abs/2511.20916", "authors": ["Illia Khudiakov", "Vladyslav Pliuhin", "Sergiy Plankovskyy", "Yevgen Tsegelnyk"], "title": "Application of machine learning for infrastructure reconstruction programs management", "comment": "8 pages, 2 figures, 3 tables", "summary": "The purpose of this article is to describe an adaptive decision-making support model aimed at improving the efficiency of engineering infrastructure reconstruction program management in the context of developing the architecture and work breakdown structure of programs. As part of the study, the existing adaptive program management tools are analyzed, the use of infrastructure systems modelling tools is justified for program architecture and WBS creation. Existing models and modelling methods are viewed, and machine learning and artificial neural networks are selected for the model. The main components of the model are defined, which include a set of decision-maker preferences, decision-making tasks, sets of input data, and applied software components of the model. To support decision-making, the adaptive model applies the method of system modeling and predicting the value of the objective function at a given system configuration. Prediction is done using machine learning methods based on a dataset consisting of historical data related to existing engineering systems. The work describes the components of the redistribution of varied model parameters, which modify the model dataset based on the selected object type, which allows adapting the decision-making process to the existing program implementation goals. The functional composition done in Microsoft Azure Machine Learning Studio is described. The neural network parameters and evaluation results are given. The application of the developed adaptive model is possible in the management of programs for the reconstruction of such engineering systems as systems of heat, gas, electricity supply, water supply, and drainage, etc.", "AI": {"tldr": "This paper presents an adaptive decision support model using neural networks and system modeling to improve management efficiency in engineering infrastructure reconstruction projects. The solution adapts to different program types and requirements, offering data-driven predictions for program outcomes.", "motivation": "To enhance the efficiency of managing reconstruction projects for engineering infrastructure by improving program architecture development and work breakdown structure creation.", "method": "The authors analyzed current adaptive program management tools, justified infrastructure system modeling, reviewed existing models, and applied machine learning and artificial neural networks to develop an adaptive decision-support model. The model predicts outcomes based on historical data and is implemented in Microsoft Azure Machine Learning Studio.", "result": "The proposed adaptive model integrates decision-maker preferences, decision tasks, input data, and software components for tailored decision support. It demonstrates the ability to modify input parameters based on project type and successfully predicts objective outcomes using neural networks, as shown in evaluation results.", "conclusion": "The adaptive decision-making support model can be effectively used for managing reconstruction programs of various engineering systems (e.g., heat, gas, water, electricity). It adapts to different implementation goals and improves efficiency via machine learning-based predictions."}}
{"id": "2511.20933", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20933", "abs": "https://arxiv.org/abs/2511.20933", "authors": ["Mootez Saad", "Boqi Chen", "Jos\u00e9 Antonio Hern\u00e1ndez L\u00f3pez", "D\u00e1niel Varr\u00f3", "Tushar Sharma"], "title": "Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code", "comment": "18 figures", "summary": "Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \\textit{Verification} to \\textit{Guided} and \\textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \\textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.", "AI": {"tldr": "LLMs understand software cohesion/coupling well in ideal setups, but easily break down in noisy, real-world tasks, especially for coupling. They need improved robustness for autonomous software analysis.", "motivation": "Large language models are increasingly used in software engineering tasks, but it is unclear how well they understand foundational design concepts, such as cohesion and coupling, which are crucial for software quality.", "method": "The study empirically evaluates the DeepSeek-R1 LLM family (14B, 32B, 70B parameters) on understanding cohesion and coupling. This is done by generating poor code fragments and testing the models under scenarios with different guidance levels and varying noise via distractor elements. Performance is measured using F1 scores in verification, guided, and open-ended generation settings.", "result": "LLMs display a solid baseline understanding of cohesion and coupling in clean, guided contexts, but their reasoning about coupling degrades significantly in noisy, open-ended environments (F1 drops >50%), while their cohesion reasoning remains robust in guided/noisy scenarios. Even this resilience in cohesion reasoning fails without any guidance. Trace analysis shows models rely on shortcuts for coupling and exhaustive (but ultimately insufficient) reasoning for cohesion.", "conclusion": "LLMs can assist reliably in recognizing software design flaws in ideal, guided settings, but their reasoning independence is weak in realistic, noisy scenarios. Improvements are needed for more robust program understanding in practical software engineering applications."}}
{"id": "2511.20955", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20955", "abs": "https://arxiv.org/abs/2511.20955", "authors": ["Sanchit Kaul", "Kevin Nhu", "Jason Eissayou", "Ivan Eser", "Victor Borup"], "title": "SpaceX: Exploring metrics with the SPACE model for developer productivity", "comment": "Code available at https://github.com/knhu/ECS260Project", "summary": "This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.", "AI": {"tldr": "This paper creates a holistic, multi-dimensional productivity metric for developers by analyzing emotions and collaboration patterns using advanced modeling on repository data. Findings show that frustration can drive more frequent commits, and mapping collaboration networks offers better insight than commit volume alone. The Composite Productivity Score (CPS) is proposed as a superior alternative to old productivity measures.", "motivation": "Traditional productivity metrics in software development often rely on simplistic, volume-based heuristics that fail to capture the complex, multidimensional nature of developer productivity. The paper is motivated to address this gap by applying the SPACE framework and offering a more holistic understanding of productivity.", "method": "The study operationalizes the SPACE framework via repository mining and employs advanced statistical methods, including Generalized Linear Mixed Models (GLMM) and RoBERTa, a neural sentiment classifier, to examine open-source contributions. It analyzes both affective (emotion) and interactional (collaboration topology) data from large datasets.", "result": "The analysis reveals that negative affective states (e.g., frustration) positively correlate with higher commit frequency, likely reflecting ongoing remediation efforts. Also, examining the network of contributor interactions provides a more accurate picture of collaboration than simple commit counts.", "conclusion": "The research concludes that productivity in software development is multifaceted and that a Composite Productivity Score, incorporating emotional states and collaboration patterns, better represents developer efficacy than traditional metrics."}}
{"id": "2511.21022", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21022", "abs": "https://arxiv.org/abs/2511.21022", "authors": ["Guancheng Lin", "Xiao Yu", "Jacky Keung", "Xing Hu", "Xin Xia", "Alex X. Liu"], "title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations", "comment": null, "summary": "Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines \"Common API Layers\" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to \"Specific API Layers\" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.", "AI": {"tldr": "This paper systematically evaluates model editing techniques for updating LLMs' deprecated API knowledge and introduces AdaLoRA-L, which efficiently updates only relevant layers, improving specificity without compromising performance.", "motivation": "Large Language Models (LLMs) perform well on code completion tasks but often generate deprecated APIs because their knowledge is limited to outdated training data. Retraining on new data is costly, and existing model editing methods have not been systematically studied for updating knowledge of deprecated APIs.", "method": "The authors conduct a systematic study by applying 10 state-of-the-art model editing techniques to three LLMs (Qwen2.5-Coder, StarCoder2, DeepSeek-Coder). They introduce EDAPIBench, a benchmark with 70+ deprecated APIs from 8 Python libraries and 3,000+ editing instances. They find the best-performing method and propose AdaLoRA-L, a variant that restricts edits to specific layers.", "result": "AdaLoRA (a parameter-efficient fine-tuning method) achieves the best performance for updating API knowledge but struggles with specificity. The proposed AdaLoRA-L variant improves specificity by editing only API-specific layers without affecting general knowledge, maintaining strong performance in other metrics.", "conclusion": "AdaLoRA-L is an effective, targeted model editing technique that addresses the limitations of both general retraining and non-specific model editing in refreshing LLMs' outdated API knowledge."}}
{"id": "2511.21151", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21151", "abs": "https://arxiv.org/abs/2511.21151", "authors": ["M. Alecci", "P. Jim\u00e9nez", "J. Samhi", "T. Bissyand\u00e9", "J. Klein"], "title": "Exploring Hidden Geographic Disparities in Android Apps", "comment": null, "summary": "While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.\n  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.\n  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.", "AI": {"tldr": "The paper shows that identical-seeming Android apps often differ significantly across regions in terms of permissions, privacy, and functionality, leading to security and ethical concerns. The authors studied thousands of apps globally, revealing systemic geographic disparities, and released a dataset for further research.", "motivation": "While many studies focus on the evolution of mobile apps, very little research has explored how app behavior differs geographically. Understanding these differences is essential because regional variations can affect app security, privacy, and user experience, and could introduce biases or ethical concerns.", "method": "The authors built a distributed app collection pipeline that operated across multiple world regions. This pipeline was used to collect and analyze thousands of Android apps, specifically looking for functional twins (GeoTwins) that appear the same but are released with different package names and characteristics in different countries. They also analyzed the contents of base.apk files across regions and compiled a large dataset for future research.", "result": "The study uncovered significant regional differences: 1) GeoTwins\u2014apps that look the same but have different package names by country\u2014frequently diverged in permissions, libraries, and privacy disclosures. 2) Even the base.apk files, assumed to be consistent across regions, showed hidden customizations depending on location. These variations lead to inconsistencies in app assessments, undermine reproducibility, and may hide important differences behind visually identical app listings.", "conclusion": "Systemic, geographically-driven disparities exist in Android apps, affecting security, privacy, and fairness. Regional customizations can cause the same app to be assessed differently depending on its origin, raising concerns about transparency, bias, and informed consent. The authors provide a dataset to support further investigation and advocate for more awareness of these regional discrepancies among stakeholders."}}
{"id": "2511.21197", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.21197", "abs": "https://arxiv.org/abs/2511.21197", "authors": ["Paolo Buono", "Mary Cerullo", "Stefano Cirillo", "Giuseppe Desolda", "Francesco Greco", "Emanuela Guglielmi", "Grazia Margarella", "Giuseppe Polese", "Simone Scalabrino", "Cesare Tucci"], "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools", "comment": null, "summary": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.", "AI": {"tldr": "Developers see bug detectors as 'detectives' and readability tools as 'coaches.' Trust depends on clarity and control. The study suggests new design principles for human-centered AI tools in IDEs.", "motivation": "Although AI-assisted tools are increasingly sophisticated, there is limited understanding of how developers perceive these tools and how mismatches in mental models affect trust, control, and adoption.", "method": "Six co-design workshops were conducted with 58 developers to explore their mental models of AI-assisted bug detection and code readability tools.", "result": "Developers view bug detection tools as 'bug detectives' that should only warn for critical issues with transparency, actionable feedback, and confidence cues. Readability tools are seen as 'quality coaches' providing contextual, personalized, and progressive feedback. Trust depends on clarity, timing, and user control for both types of tools. The study distilled a set of Human-Centered AI design principles for IDEs.", "conclusion": "The findings clarify how developers conceptualize AI tools and suggest design principles to improve trust, usability, and adoption by balancing automation and human agency in IDE environments."}}
{"id": "2511.21380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21380", "abs": "https://arxiv.org/abs/2511.21380", "authors": ["Jingyi Chen", "Xiaoyan Guo", "Songqiang Chen", "Shing-Chi Cheung", "Jiasi Shen"], "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions", "comment": null, "summary": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.", "AI": {"tldr": "This paper empirically evaluates how well state-of-the-art multi-agent LLMs automate software engineering dataset adaptation. While these systems can partially adapt artifacts, they struggle with full correctness. Feedback-driven interventions, like error messages and reference code, greatly boost their output quality. More work is needed to develop reliable, self-correcting agent systems for practical SE research use.", "motivation": "Adapting software engineering (SE) research artifacts across various datasets is crucial for enabling research scalability and reproducibility, but this process is understudied and challenging to automate. Recent advancements in large language model (LLM)-based multi-agent systems, such as GitHub Copilot, have shown the potential to automate complex software development tasks, motivating an investigation into their capabilities and limitations in this specific domain.", "method": "The study conducts an empirical evaluation of state-of-the-art LLM-based multi-agent systems (specifically Copilot backed by GPT-4.1 and Claude Sonnet 4) on dataset adaptation tasks using SE research artifacts from benchmark repositories (ROCODE and LogHub2.0). The evaluation involves a five-stage pipeline: file comprehension, code editing, command generation, validation, and final execution. The authors measure success rates, analyze failure patterns, and assess the effects of various prompt-based interventions to enhance agent performance.", "result": "Current multi-agent LLM systems can identify essential files and make partial adaptations but rarely produce fully correct implementations on their own. Prompt-based interventions, particularly the inclusion of execution error messages and reference code, significantly increase the structural similarity of outputs to the ground truth (from 7.25% to 67.14%). This demonstrates notable improvement but also underscores the systems' present limitations.", "conclusion": "Today's LLM-based multi-agent systems show promise in automating aspects of dataset adaptation for SE artifacts but are not yet reliable for producing entirely correct results. Providing contextual feedback and reference materials can markedly improve their performance. The study points to the need for self-correcting, feedback-driven agents to advance the reliability and applicability of such tools in SE research."}}
{"id": "2511.21382", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21382", "abs": "https://arxiv.org/abs/2511.21382", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead", "comment": "33 pages, 8 figures", "summary": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.", "AI": {"tldr": "This systematic review shows LLMs are transforming unit test generation with prompt engineering and validation loops improving effectiveness, though challenges in fault detection and benchmarking remain; future research should focus on autonomous and hybrid testing systems.", "motivation": "Traditional automated unit test generation techniques lack semantic understanding, hindering their ability to produce realistic and meaningful tests. The motivation is to evaluate how LLMs, with their semantic knowledge, are changing the landscape of unit test generation.", "method": "A systematic literature review was conducted, analyzing 115 publications between May 2021 and August 2025. The review applies a taxonomy focused on the unit test generation lifecycle and examines the role of LLMs under various strategies and enhancement techniques.", "result": "The review highlights that prompt engineering dominates current usage (89% of studies) due to its flexibility, and that iterative validation and repair loops significantly improve output quality. Nevertheless, it uncovers persistent challenges, especially in fault detection and evaluation standards.", "conclusion": "The paper concludes that while LLMs are highly promising for unit test generation, key challenges such as weak fault detection and lack of standardized benchmarks persist. The authors suggest a roadmap emphasizing autonomous testing agents and hybrid approaches to address these limitations."}}
