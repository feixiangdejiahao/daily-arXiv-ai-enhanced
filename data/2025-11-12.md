<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Can Large Language Models Simulate Symbolic Execution Output Like KLEE?](https://arxiv.org/abs/2511.08530)
*Rong Feng,Vanisha Gupta,Vivek Patel,Viroopaksh Reddy Ernampati,Suman Saha*

Main category: cs.SE

TL;DR: Researchers tested if GPT-4o could mimic KLEE, a symbolic execution tool, to identify complex program paths more efficiently. GPT-4o reached about 20% accuracy, showing it can't currently replace KLEE but hints at possible future uses of LLMs for this purpose.


<details>
  <summary>Details</summary>
Motivation: Symbolic execution tools like KLEE are powerful but become prohibitively slow and resource-intensive when analyzing programs with many branching paths. The authors wanted to investigate whether large language models (LLMs) such as GPT-4o could partially replace or augment symbolic execution by predicting key outputs from these tools more efficiently.

Method: The authors tasked GPT-4o with simulating KLEE's outputs, particularly focusing on identifying the most constrained program execution path (the path with the most symbolic conditions). They evaluated GPT-4o's performance on a dataset of 100 C programs, comparing its output to that of KLEE.

Result: GPT-4o achieved about 20% accuracy both in generating outputs similar to KLEE's and in identifying the most constrained execution path.

Conclusion: While current LLMs like GPT-4o are not yet accurate enough to replace symbolic execution tools like KLEE, this preliminary evaluation demonstrates the potential and current limitations of LLMs in simulating symbolic execution tasks.

Abstract: Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.
  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.

</details>


### [2] [A Service Suite for Specifying Digital Twins for Industry 5.0](https://arxiv.org/abs/2511.07506)
*Izaque Esteves,Regina Braga,José Maria David,Victor Stroele*

Main category: cs.SE

TL;DR: This paper presents DT-Create, a service suite for specifying Digital Twins in predictive maintenance, supporting smart data processing, model selection, and adaptive decision-making, validated through case studies.


<details>
  <summary>Details</summary>
Motivation: Predictive maintenance requires rapid, accurate decisions based on extensive sensor data. Existing approaches may struggle to process and utilize this data effectively for decision-making.

Method: The authors developed a suite of services called DT-Create, which leverages intelligent techniques including semantic data processing and self-adaptation. The suite was created and refined using the Design Science Research (DSR) methodology over two development cycles and evaluated via case studies.

Result: DT-Create enables efficient specification of Digital Twins (DTs) for predictive maintenance. It facilitates collection, storage, and intelligent processing of sensor data, enriches information using machine learning and ontologies, intelligently selects predictive models matching the data, and provides robust decision support with self-adaptation capabilities.

Conclusion: DT-Create is a feasible and effective solution for specifying Digital Twins aimed at supporting agile and assertive decisions in predictive maintenance, addressing key aspects such as data processing, model selection, and adaptive decision support.

Abstract: One of the challenges of predictive maintenance is making decisions based on data in an agile and assertive way. Connected sensors and operational data favor intelligent processing techniques to enrich information and enable decision-making. Digital Twins (DTs) can be used to process information and support decision-making. DTs are a real-time representation of physical machines and generate data that predictive maintenance can use to make assertive and quick decisions. The main contribution of this work is the specification of a suite of services for specifying DTs, called DT-Create, focused on decision support in predictive maintenance. DT-Create suite is based on intelligent techniques, semantic data processing, and self-adaptation. This suite was developed using the Design Science Research (DSR) methodology through two development cycles and evaluated through case studies. The results demonstrate the feasibility of using DT-Create in specifying DTs considering the following aspects: (i) collection, storage, and intelligent processing of data generated by sensors, (ii) enrichment of information through machine learning and ontologies, (iii) use of intelligent techniques to select predictive models that adhere to the available data set, and (iv) decision support and self-adaptation.

</details>


### [3] [SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction](https://arxiv.org/abs/2511.07584)
*Wuyang Zhang,Chenkai Zhang,Zhen Luo,Jianming Ma,Wangming Yuan,Chuqiao Gu,Chenwei Feng*

Main category: cs.SE

TL;DR: SemanticForge significantly improves LLM-based code generation by tackling logical and schematic errors through semantic knowledge graphs, neural structured queries, and smarter real-time constraint handling, boosting both precision and reliability.


<details>
  <summary>Details</summary>
Motivation: LLMs greatly assist in automated code generation, but still produce systematic errors, notably logical and schematic hallucinations due to lack of strong semantic understanding at the repository level.

Method: SemanticForge introduces four advances: (1) automatic reconciliation for static/dynamic knowledge graphs, (2) neural generation of structured graph queries from natural language, (3) beam search with integrated SMT solving for real-time constraint validation, and (4) incremental algorithms for efficient semantic knowledge graph updating.

Result: SemanticForge achieves 73% precision in generating graph queries (compared to 51% for traditional methods), enables real-time constraint checks during code generation, and supports efficient semantic knowledge updating.

Conclusion: The four SemanticForge innovations substantially advance semantically-aware code generation by mitigating common LLM failure modes and improving accuracy, reliability, and efficiency.

Abstract: Large language models (LLMs) have transformed software development by enabling automated code generation, yet they frequently suffer from systematic errors that limit practical deployment. We identify two critical failure modes: \textit{logical hallucination} (incorrect control/data-flow reasoning) and \textit{schematic hallucination} (type mismatches, signature violations, and architectural inconsistencies). These errors stem from the absence of explicit, queryable representations of repository-wide semantics.
  This paper presents \textbf{SemanticForge}, which introduces four fundamental algorithmic advances for semantically-aware code generation: (1) a novel automatic reconciliation algorithm for dual static-dynamic knowledge graphs, unifying compile-time and runtime program semantics; (2) a neural approach that learns to generate structured graph queries from natural language, achieving 73\% precision versus 51\% for traditional retrieval; (3) a novel beam search algorithm with integrated SMT solving, enabling real-time constraint verification during generation rather than post-hoc validation; and (4) an incremental maintenance algorithm that updates knowledge graphs in $O(|ΔR| \cdot \log n)$ time while maintaining semantic equivalence.

</details>


### [4] [An Exploratory Eye Tracking Study on How Developers Classify and Debug Python Code in Different Paradigms](https://arxiv.org/abs/2511.07612)
*Samuel W. Flint,Jigyasa Chauhan,Niloofar Mansoor,Bonita Sharif,Robert Dyer*

Main category: cs.SE

TL;DR: This study used eye-tracking to see how Python's programming paradigms affect code comprehension and debugging. Developers found Functional and Procedural code harder to classify than Object-Oriented, took longer with Functional code, and felt less confident debugging it, but their actual debugging ability didn't change. Reading patterns differed, especially with Functional code, indicating some paradigm features impact how code is read and understood.


<details>
  <summary>Details</summary>
Motivation: Previous research found that some programming paradigms are harder to comprehend, but it's unclear which specific features affect comprehension. There is a lack of research on the impact of paradigm-specific language features on developers' understanding and debugging abilities, especially in multi-paradigm languages like Python.

Method: An exploratory empirical eye-tracking study was conducted with 29 developers (mostly students). Participants performed four code classification and four debugging tasks in Python, with their eye movements tracked to analyze how they interact with paradigm-specific language features.

Result: The study found that developers had difficulty labeling Functional and Procedural code accurately, but not Object-Oriented code. Functional code tasks took the longest to complete, and while changing the paradigm did not affect actual debugging performance, developers felt less confident with Functional code. Significant differences were observed in reading patterns during debugging, particularly for Functional code. During classification, paradigm-relevant token types were not consistently read by developers.

Conclusion: Paradigm-specific features, especially those of the Functional paradigm, impact code comprehension and confidence, but do not necessarily affect debugging effectiveness. Developers struggle more with Functional and Procedural paradigms compared to Object-Oriented, and their reading behavior differs according to the paradigm.

Abstract: Modern programming languages, such as Python, support language features from several paradigms, such as object-oriented, procedural, and functional. Research has shown that code written in some paradigms can be harder to comprehend, but to date, no research has looked at which paradigm-specific language features impact comprehension. To this end, this study seeks to uncover which paradigm-specific features impactcomprehension and debugging of code or how multi-paradigm code might affect a developer's ability to do so. We present an exploratory empirical eye-tracking study to investigate 1) how developers classify the predominant paradigm in Python code and 2) how the paradigm affects their ability to debug Python code. The goal is to uncover if specific language features are looked at more often while classifying and debugging code with a predominant paradigm. Twenty-nine developers (primarily students) were recruited for the study and were each given four classification and four debugging tasks in Python. Eye movements were recorded during all the tasks. The results indicate confusion in labeling Functional and Procedural paradigms, but not Object-Oriented. The code with predominantly functional paradigms also took the longest to complete. Changing the predominant paradigm did not affect the ability to debug the code, though developers did rate themselves with lower confidence for Functional code. We report significant differences in reading patterns during debugging, especially in the Functional code. During classification, results show that developers do not necessarily read paradigm-relevant token types.

</details>


### [5] [A Self-Improving Architecture for Dynamic Safety in Large Language Models](https://arxiv.org/abs/2511.07645)
*Tyler Slater*

Main category: cs.SE

TL;DR: The paper presents SISF, a new architecture for AI safety that autonomously learns and adapts its policies in real time, reducing vulnerabilities and maintaining user utility, thus providing a scalable solution for robust AI system protection.


<details>
  <summary>Details</summary>
Motivation: Current software architecture patterns are static and cannot adapt to evolving threats once deployed. Existing safety assurance methods for AI models are also non-scalable, which leaves systems vulnerable to new adversarial attacks, especially as integration of LLMs accelerates.

Method: This paper introduces the Self-Improving Safety Framework (SISF), a runtime architecture combining an unprotected base LLM (Mistral-7B) with a dynamic feedback loop. The loop includes an AI Adjudicator (GPT-4o) to detect safety breaches and a Policy Synthesis Module (GPT-4 Turbo) to autonomously generate new safety policies when failures are detected.

Result: In tests with the 520-prompt AdvBench dataset, the SISF learned over time by detecting 237 breaches and synthesizing 234 new safety policies, reducing Attack Success Rate from 100% (unprotected) to 45.58%. On 520 benign prompts, it achieved 0% false positives, balancing safety and utility.

Conclusion: Self-adaptive, runtime AI safety architectures are viable and effective. The SISF framework shows how autonomous adaptation at runtime can create robust and scalable systems, moving safety assurance from a static to an automated, ongoing process.

Abstract: Context: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats.
  Objective: To design, implement, and evaluate a novel software architecture that enables an AI-driven system to autonomously and continuously adapt its own safety protocols at runtime.
  Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures.
  Results: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility.
  Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.

</details>


### [6] [Smart but Costly? Benchmarking LLMs on Functional Accuracy and Energy Efficiency](https://arxiv.org/abs/2511.07698)
*Mohammadjavad Mehditabar,Saurabhsingh Rajput,Antonio Mastropaolo,Tushar Sharma*

Main category: cs.SE

TL;DR: BRACE is a new benchmarking tool for AI code models, rating 22 models on energy efficiency and accuracy with two scoring methods. Results show summarization tasks are easier, and model size isn’t the key for better ratings—efficient use of parameters matters. BRACE helps pick models balancing environmental and performance goals.


<details>
  <summary>Details</summary>
Motivation: AI code language models are increasingly used in software development, raising concerns about their environmental impact and accuracy. Existing evaluations focus on one or the other and lack systematic frameworks, especially for accuracy-energy trade-offs in CLMs.

Method: The authors introduce BRACE, a benchmarking framework that assesses CLMs on both energy efficiency and functional correctness (accuracy). They benchmark 22 state-of-the-art models on code generation and summarization tasks using two rating methods: CIRC, which applies deterministic Euclidean-based rankings, and OTER, which provides trend-aware, dynamic evaluations.

Result: Models generally perform better on summarization tasks, as these don’t require grammatically or syntactically correct code. Model size does not significantly affect ratings—efficient parameter utilization matters more. The dual rating methods (CIRC and OTER) reveal different aspects of model performance, enabling practitioners to choose models based on deployment needs.

Conclusion: The BRACE framework supports evidence-based selection of AI code models, combining sustainability with task performance. It offers practitioners flexibility: CIRC for clear, static comparisons and OTER for nuanced, trend-aware analysis, depending on the deployment context.

Abstract: The rapid advancement of AI technologies and their accelerated adoption in software development necessitates a systematic evaluation of their environmental impact alongside functional correctness. While prior studies have examined sustainability in large language models, existing approaches lack systematic frameworks for evaluating accuracy-energy trade-offs in Code Language Models (CLMs). In this paper, we present a framework, BRACE, to benchmark CLMs on a unified scale of energy efficiency and functional correctness (referred to as accuracy). We benchmark 22 state-of-the-art models on code generation and summarization tasks, proposing two rating methods: Concentric Incremental Rating Circles (CIRC) and Observation to Expectation Rating (OTER). CIRC provides deterministic Euclidean-based rankings with static trade-offs that are robust to outliers, and OTER offers trend-aware evaluation with dynamic trade-offs that capture the complex correlation between energy and accuracy, each offering a distinct perspective and addressing the problem in a unique way. These rating methods enable us to rate LLMs on a 1-5 scale reflecting their combined capabilities in terms of energy efficiency and functional correctness. Our analysis reveals models generally perform better in the code summarization tasks as they are not enforced to generate a grammar-based and syntactically correct output. Also, we find that models' size does not have a significant impact on their ratings, indicating that if models utilize their parameters efficiently, they can be ranked higher on these scales. The proposed BRACE framework empowers practitioners to make evidence-based model selections that balance sustainability with task requirements, guiding rating choice -- CIRC for deterministic comparisons or OTER for trend-aware evaluation -- based on deployment priorities.

</details>


### [7] [Post Processing Graphical User Interface for Heat Flow Visualization](https://arxiv.org/abs/2511.07709)
*Lars Olt,Luis Diego Fonseca Flores,Ian Mckinley*

Main category: cs.SE

TL;DR: A custom GUI using MATLAB and C++ makes extracting and visualizing heat flow metrics in Thermal Desktop much faster by using a new parsing method on compressed files, but some data-reading challenges remain.


<details>
  <summary>Details</summary>
Motivation: Thermal engineers using Thermal Desktop face challenges in efficiently extracting and visualizing heat flow metrics due to limited existing software, hindering quick analysis of thermal models.

Method: A graphical user interface (GUI) developed in MATLAB and C++ leverages the Thermal Desktop API (OpenTD) and a custom data parser to efficiently load and correlate temperature, conductance, and submodel metrics. The method exploits a side effect of TD's Compressed Solution Results (CSR) files for improved performance.

Result: The GUI and method significantly reduce runtime for correlating model nodes and conductors with submodel IDs—by orders of magnitude.

Conclusion: The proposed GUI and methodology address key limitations in thermal metric analysis in TD, though some shortcomings exist. The authors discuss future directions for the GUI and recommend improvements for future OpenTD releases.

Abstract: Thermal Desktop (TD) is an industry-standard thermal analysis tool used to create and analyze thermal models for landers, rovers, spacecraft, and instrument payloads. Currently, limited software exists to extract and visualize metrics relevant to heat flow within TD, impeding thermal engineers from analyzing their results quickly. This paper discusses a graphical user interface (GUI) built in MATLAB and C++ which uses TDs application programming interface (API), OpenTD, and a custom parser to address this void. Specifically, we present a method for efficiently loading temperature, conductance, and submodel metrics using a side effect of TDs Compressed Solution Results (CSR) files. This approach can reduce the runtime for correlating model nodes and conductors with submodel IDs by orders of magnitude. Lastly, we reflect on the shortcomings of this method for reading data, consider the future of the GUI, and provide recommendations for subsequent OpenTD releases.

</details>


### [8] [Event-Driven Inconsistency Detection Between UML Class and Sequence Diagrams](https://arxiv.org/abs/2511.07742)
*Luan Lazzari,Kleinner Farias*

Main category: cs.SE

TL;DR: This paper introduces Harmony Validator, a Papyrus plugin that automatically detects inconsistencies in UML models. Tested with students, the tool increases understanding of model consistency and supports better learning in software engineering education.


<details>
  <summary>Details</summary>
Motivation: Modeling in software engineering is crucial but complex, requiring skills that are difficult to teach. A major challenge is handling inconsistencies in UML models, which educators and students often find hard to detect and manage.

Method: The authors developed Harmony Validator, a plugin for the Papyrus UML modeling environment. It uses an event-driven architecture to continuously monitor and automatically report modeling inconsistencies in real time. The tool's architecture, detection mechanisms, and educational usage scenarios are described. Additionally, a case study with software engineering students evaluates its effectiveness.

Result: Harmony Validator improved students' awareness of model integrity and their understanding of consistency in UML models. It promoted more reflective learning and was perceived as beneficial for teaching and learning UML modeling.

Conclusion: Harmony Validator effectively assists in teaching model consistency by automatically detecting UML inconsistencies and supporting real-time feedback, enhancing the educational experience in software engineering courses.

Abstract: Modeling is a central and demanding activity in software engineering that requires skills such as abstraction, consistency maintenance, and precise communication. These skills are difficult to master and even harder to teach effectively. Educators and students often struggle to understand and manage inconsistencies that arise during the modeling process. To address this challenge, we present \texttt{Harmony Validator}, a tool integrated as a plugin for the Papyrus modeling environment, designed to automatically detect and report inconsistencies in UML models, including class and sequence diagrams. The tool adopts an event-driven architecture that continuously monitors modeling actions and notifies users of emerging inconsistencies in real time. This approach enhances awareness of model integrity and supports the iterative refinement of design artifacts. The paper describes the architecture, detection mechanisms, and usage scenarios of Harmony Validator. It also includes a case study conducted with students in a software engineering course to evaluate the perceived usefulness and benefits of UML modeling in teaching and learning. Our results indicate that Harmony Validator fosters a better understanding of model consistency and promotes reflective learning practices in software modeling education.

</details>


### [9] [Uncovering Scientific Software Sustainability through Community Engagement and Software Quality Metrics](https://arxiv.org/abs/2511.07851)
*Sharif Ahmed,Addi Malviya Thakur,Gregory R. Watson,Nasir U. Eisty*

Main category: cs.SE

TL;DR: This paper studies sustainability in scientific open-source software by analyzing ten major GitHub projects, focusing on community engagement and software quality. The authors introduce a new visualization method that better displays project health over time, and reveal that even similar projects sustain themselves differently. Their findings underscore the importance of project-specific feedback and provide new tools for stakeholders interested in maintaining long-term open-source software.


<details>
  <summary>Details</summary>
Motivation: Sustaining the long-term development and usability of scientific open-source software (Sci-OSS) is a significant challenge, despite its critical role in advancing research. There is a recognized need to better understand factors—such as community engagement and software quality—that influence sustainability, and to improve the tools available for monitoring and analyzing project health.

Method: The study mined data from ten prominent Sci-OSS projects hosted on GitHub and conducted a multimodal analysis using repository metrics identified in prior literature. The researchers also developed a novel visualization technique to robustly display evolving software metrics over time. Statistical and natural language analyses were performed to understand sustainability patterns and the impact of project-specific feedback.

Result: The findings show that even projects within the same scientific domain exhibit substantial differences in how they sustain themselves. The new visualization method effectively consolidates multiple types of traditional sustainability indicators and provides a clearer picture of project health. Natural language analysis confirms that project-specific feedback is crucial for maintaining software quality.

Conclusion: The study offers practical visualization and analysis tools for assessing the sustainability of Sci-OSS, delivering valuable insights for researchers, funders, and developers. Community engagement and project-specific feedback are critical for longevity, and different projects require tailored approaches for successful sustainability.

Abstract: Scientific open-source software (Sci-OSS) projects are critical for advancing research, yet sustaining these projects long-term remains a major challenge. This paper explores the sustainability of Sci-OSS hosted on GitHub, focusing on two factors drawn from stewardship organizations: community engagement and software quality. We map sustainability to repository metrics from the literature and mined data from ten prominent Sci-OSS projects. A multimodal analysis of these projects led us to a novel visualization technique, providing a robust way to display both current and evolving software metrics over time, replacing multiple traditional visualizations with one. Additionally, our statistical analysis shows that even similar-domain projects sustain themselves differently. Natural language analysis supports claims from the literature, highlighting that project-specific feedback plays a key role in maintaining software quality. Our visualization and analysis methods offer researchers, funders, and developers key insights into long-term software sustainability.

</details>


### [10] [LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost](https://arxiv.org/abs/2511.07865)
*Daisuke Kikuta,Hiroki Ikeuchi,Kengo Tajiri*

Main category: cs.SE

TL;DR: The paper introduces ChaosEater, an LLM-powered system that fully automates Chaos Engineering cycles for Kubernetes, successfully reducing labor, time, and cost without sacrificing result quality.


<details>
  <summary>Details</summary>
Motivation: Chaos Engineering is valuable for improving system resilience, but current practice requires manual effort and expertise for experiment planning and remediation, making the process labor-intensive and costly.

Method: The authors propose ChaosEater, a system using Large Language Models to automate the entire Chaos Engineering cycle. ChaosEater assigns subdivided CE tasks within an agentic workflow powered by LLMs, targeting software systems on Kubernetes and handling requirements, coding, testing, and debugging.

Result: Case studies on both small- and large-scale Kubernetes systems show that ChaosEater effectively automates CE cycles, achieving reasonable results with significantly reduced time and cost. The quality of outcomes is validated by both human engineers and LLMs.

Conclusion: ChaosEater enables automated Chaos Engineering for Kubernetes, lowering expertise and cost barriers while maintaining quality and effectiveness, as demonstrated in empirical evaluations.

Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.

</details>


### [11] [Testing Question Answering Software with Context-Driven Question Generation](https://arxiv.org/abs/2511.07924)
*Shuang Liu,Zhirun Zhang,Jinhao Dong,Zan Wang,Qingchao Shen,Junjie Chen,Wei Lu,Xiaoyong Du*

Main category: cs.SE

TL;DR: CQ^2A is a context-driven method that uses LLMs to generate more natural and diverse test questions for QA systems, leading to better bug detection and improved system performance.


<details>
  <summary>Details</summary>
Motivation: Existing testing methods for question-answering (QA) software often generate test questions that are unnatural and lack contextual diversity, limiting their effectiveness for real-world bug detection. There is a need for more contextually relevant and natural question generation to improve QA system testing.

Method: The authors propose CQ^2A, a context-driven question generation approach. It extracts entities and relationships from the context to form ground truth answers and leverages large language models (LLMs) to generate questions based on these answers and the surrounding context. The method also includes consistency verification and constraint checking to improve the reliability of LLM-generated outputs. Experiments were conducted on three datasets to evaluate effectiveness.

Result: CQ^2A surpasses state-of-the-art methods in bug detection capability, the naturalness of generated questions, and the contextual coverage. Additionally, when test cases from CQ^2A are used to fine-tune QA software, the error rate decreases.

Conclusion: CQ^2A offers a more reliable and effective approach for testing QA systems by generating more natural and contextually relevant test questions, enhancing both bug detection and software fine-tuning.

Abstract: Question-answering software is becoming increasingly integrated into our daily lives, with prominent examples including Apple Siri and Amazon Alexa. Ensuring the quality of such systems is critical, as incorrect answers could lead to significant harm. Current state-of-the-art testing approaches apply metamorphic relations to existing test datasets, generating test questions based on these relations. However, these methods have two key limitations. First, they often produce unnatural questions that humans are unlikely to ask, reducing the effectiveness of the generated questions in identifying bugs that might occur in real-world scenarios. Second, these questions are generated from pre-existing test datasets, ignoring the broader context and thus limiting the diversity and relevance of the generated questions.
  In this work, we introduce CQ^2A, a context-driven question generation approach for testing question-answering systems. Specifically, CQ^2A extracts entities and relationships from the context to form ground truth answers, and utilizes large language models to generate questions based on these ground truth answers and the surrounding context. We also propose the consistency verification and constraint checking to increase the reliability of LLM's outputs. Experiments conducted on three datasets demonstrate that CQ^2A outperforms state-of-the-art approaches on the bug detection capability, the naturalness of the generated questions as well as the coverage of the context. Moreover, the test cases generated by CQ^2A reduce error rate when utilized for fine-tuning the QA software under test

</details>


### [12] ["I need to learn better searching tactics for privacy policy laws.'' Investigating Software Developers' Behavior When Using Sources on Privacy Issues](https://arxiv.org/abs/2511.08059)
*Stefan Albert Horstmann,Sandy Hong,Maziar Niazian,Cristiana Santos,Alena Naiakshina*

Main category: cs.SE

TL;DR: Developers face significant challenges in achieving privacy compliance due to inadequate support from personal expertise, online resources, and AI assistants. New, better-designed privacy resources are needed to address these shortcomings.


<details>
  <summary>Details</summary>
Motivation: With growing legal requirements for privacy (GDPR, CCPA), developers must make privacy decisions, but often lack legal expertise and struggle with compliance. The effectiveness of current support tools/resources for developers is unclear.

Method: A qualitative study involving 30 developers, using think-aloud sessions and follow-up interviews. Developers were tasked with a privacy-sensitive scenario, using their own knowledge, online resources, and an AI assistant to identify issues and measures.

Result: Developers struggled with all three approaches: personal expertise was inadequate, online resources were overly complex, and AI assistants gave clear but contextually irrelevant or incomplete advice. None effectively supported privacy-sensitive development tasks.

Conclusion: Current support resources (knowledge, web content, AI assistants) are insufficient for helping developers make privacy-compliant decisions. There is a clear need for privacy resources that are more accessible, understandable, and tailored to developer needs.

Abstract: Since the introduction of the European General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), software developers increasingly have to make privacy-related decisions during system design and implementation. However, past research showed that they often lack legal expertise and struggle with privacy-compliant development. To shed light on how effective current information sources are in supporting them with privacy-sensitive implementation, we conducted a qualitative study with 30 developers. Participants were presented with a privacy-sensitive scenario and asked to identify privacy issues and suggest measures using their knowledge, online resources, and an AI assistant. We observed developers' decision-making in think-aloud sessions and discussed it in follow-up interviews. We found that participants struggled with all three sources: personal knowledge was insufficient, web content was often too complex, and while AI assistants provided clear and user-tailored responses, they lacked contextual relevance and failed to identify scenario-specific issues. Our study highlights major shortcomings in existing support for privacy-related development tasks. Based on our findings, we discuss the need for more accessible, understandable, and actionable privacy resources for developers.

</details>


### [13] [A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models](https://arxiv.org/abs/2511.08127)
*Weiye Li,Wenyi Tang*

Main category: cs.SE

TL;DR: This paper exposes significant, underexplored transferable vulnerabilities in code models (SCMs and LLM4Code) and introduces an effective, classifier-agnostic adversarial attack framework (HABITAT). Their method achieves high attack success rates, highlighting urgent security needs in AI-driven software development.


<details>
  <summary>Details</summary>
Motivation: Despite advances in Source Code Models (SCMs) and Large Language Models for code (LLM4Code), their vulnerabilities—particularly transferable ones that are critical for software security—are underexplored. Existing research either requires impractical access to model classifiers or overlooks LLM4Code's role in modern development workflows.

Method: The authors systematically study the vulnerability transferability in both traditional SCMs and LLM4Code. They propose a victim-agnostic approach using HABITAT, a framework that combines a specialized perturbation-inserting mechanism and a hierarchical Reinforcement Learning algorithm to generate adversarial code samples without needing access to downstream classifiers.

Result: Findings show a correlation between vulnerabilities in traditional SCMs and LLM4Code, revealing key factors affecting the success of transfer attacks. Their HABITAT-generated adversarial examples achieve up to a 64% success rate against LLM4Code, outperforming previous approaches by over 15%.

Conclusion: The prevalent transferable vulnerabilities between SCMs and LLM4Code present serious security concerns. The proposed victim-agnostic adversarial generation approach proves highly effective, emphasizing the urgent need for robust adversarial defenses in the AI-enabled software ecosystem.

Abstract: Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.

</details>


### [14] [OWLAPY: A Pythonic Framework for OWL Ontology Engineering](https://arxiv.org/abs/2511.08232)
*Alkid Baci,Luke Friedrichs,Caglar Demir,Axel-Cyrille Ngonga Ngomo*

Main category: cs.SE

TL;DR: OWLAPY is a powerful Python library for OWL ontology engineering, featuring native and external reasoner integration, multiple format conversions, and custom LLM-powered workflows, widely adopted by the community.


<details>
  <summary>Details</summary>
Motivation: Most existing ontology engineering tools for OWL are Java-based, and Python users lack versatile, native frameworks for creating and managing OWL ontologies. There is also a growing need to integrate modern technologies like large language models into ontology workflows.

Method: The authors present OWLAPY, a Python framework designed for OWL 2 ontology engineering. The framework supports native Python and external Java reasoners, multiple ontology component implementations, format conversions (Description Logics, Manchester Syntax, SPARQL), and custom workflows for using large language models.

Result: OWLAPY provides a flexible and comprehensive environment for ontology engineering in Python, with extensive interoperability features, and strong community adoption (over 50,000 downloads).

Conclusion: OWLAPY is a robust, well-tested Python framework facilitating all aspects of OWL ontology engineering, making it a valuable tool for both current Python users and those transitioning from Java-based solutions.

Abstract: In this paper, we introduce OWLAPY, a comprehensive Python framework for OWL ontology engineering. OWLAPY streamlines the creation, modification, and serialization of OWL 2 ontologies. It uniquely integrates native Python-based reasoners with support for external Java reasoners, offering flexibility for users. OWLAPY facilitates multiple implementations of core ontology components and provides robust conversion capabilities between OWL class expressions and formats such as Description Logics, Manchester Syntax, and SPARQL. It also allows users to define custom workflows to leverage large language models (LLMs) in ontology generation from natural language text. OWLAPY serves as a well-tested software framework for users seeking a flexible Python library for advanced ontology engineering, including those transitioning from Java-based environments. The project is publicly available on GitHub at https://github.com/dice-group/owlapy and on the Python Package Index (PyPI) at https://pypi.org/project/owlapy/ , with over 50,000 downloads at the time of writing.

</details>


### [15] [Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale](https://arxiv.org/abs/2511.08475)
*Yangxiao Cai,Ruiyin Li,Peng Liang,Mojtaba Shahin,Zengyang Li*

Main category: cs.SE

TL;DR: This paper systematically reviews the design of LLM-based Multi-Agent Systems for software engineering tasks. It highlights code generation as the primary application, functional suitability as the top quality concern, role-based cooperation as the prevalent design pattern, and code quality improvement as the main design rationale. The study offers design implications for future systems.


<details>
  <summary>Details</summary>
Motivation: The complexity of software engineering tasks is increasing, and Multi-Agent Systems (MASs) combined with Large Language Models (LLMs) offer promising autonomy and scalability. However, there is a lack of systematic study on how LLM-based MASs are designed for SE, including which quality attributes, design patterns, and rationales are prioritized.

Method: The authors conducted a systematic literature review, collecting and analyzing 94 papers related to LLM-based MASs in software engineering tasks. They identified key SE tasks, quality attributes, design patterns, and the rationale behind design choices.

Result: The study found that code generation is the most common software engineering task addressed by LLM-based MASs. Functional suitability is the most prioritized quality attribute. Role-based cooperation is the most frequent design pattern among 16 observed. The leading rationale for the design of LLM-based MASs is to improve the quality of generated code.

Conclusion: There are established trends in LLM-based MAS design for SE, with emphasis on code generation, functional suitability, and role-based cooperation. The findings provide practical implications for future development and design decisions of LLM-based MASs in SE tasks.

Abstract: As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Dynamic Stability of LLM-Generated Code](https://arxiv.org/abs/2511.07463)
*Prateek Rajput,Abdoul Aziz Bonkoungou,Yewei Song,Abdoul Kader Kabore,Iyiola E. Olatunji,Jacques Klein,Tegewende Bissyande*

Main category: cs.PL

TL;DR: Current evaluations of code-generating LLMs ignore algorithmic complexity differences between correct solutions, risking performance issues. The authors introduce new metrics to measure structural and runtime variance in generated code, finding that increasing diversity improves correctness but harms stability. The results suggest code generation benchmarks should incorporate stability considerations and asymptotic test cases to ensure more robust, real-world evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for LLM-generated code focus solely on functional correctness, neglecting differences in algorithmic complexity and performance among correct solutions. This oversight leads to potential issues in production due to varying performance costs, prompting the need for better evaluation methods.

Method: The paper introduces a framework using opcode distribution metrics to assess dynamic stability in generated code. Specifically, it proposes Static Canonical Trace Divergence (SCTD) to measure structural diversity and Dynamic Canonical Trace Divergence (DCTD) to quantify runtime behavioral differences. The ratio of these metrics, Behavioral Expression Factor (BEF), serves as a diagnostic indicator of stability and redundancy.

Result: Empirical results on BigOBench and CodeContests show that even the best LLMs generate functionally correct solutions with significant algorithmic variance. Increasing sampling temperature boosts correctness but undermines stability, highlighting a trade-off between correct and stable solutions.

Conclusion: Current LLM code evaluations miss critical behavioral and performance variance among solutions. New metrics and stability-aware objectives are needed for robust evaluation, and future benchmarks should include tests of asymptotic behavior to properly assess real-world deployment risks.

Abstract: Current evaluations of LLMs for code generation emphasize functional correctness, overlooking the fact that functionally correct solutions can differ significantly in algorithmic complexity. For instance, an $(O(n^2))$ versus $(O(n \log n))$ sorting algorithm may yield similar output but incur vastly different performance costs in production. This discrepancy reveals a critical limitation in current evaluation methods: they fail to capture the behavioral and performance diversity among correct solutions. To address this, we introduce a principled framework for evaluating the dynamic stability of generated code. We propose two metrics derived from opcode distributions: Static Canonical Trace Divergence (SCTD), which captures algorithmic structure diversity across generated solutions, and Dynamic Canonical Trace Divergence (DCTD), which quantifies runtime behavioral variance. Their ratio, the Behavioral Expression Factor (BEF), serves as a diagnostic signal: it indicates critical runtime instability when BEF $\ll$ 1 and functional redundancy when BEF $\gg$ 1. Empirical results on BigOBench and CodeContests show that state-of-the-art LLMs exhibit significant algorithmic variance even among functionally correct outputs. Notably, increasing sampling temperature improves pass@1 rates but degrades stability, revealing an unrecognized trade-off: searching for correct solutions in diverse output spaces introduces a "penalty of instability" between correctness and behavioral consistency. Our findings call for stability-aware objectives in code generation and new benchmarks with asymptotic test cases for robust, real-world LLM evaluation.

</details>


### [17] [Streaming Tensor Program: A streaming abstraction for dynamic parallelism](https://arxiv.org/abs/2511.07776)
*Gina Sohn,Genghan Zhang,Konstantin Hossfeld,Jungwoo Kim,Nathan Sobotka,Nathan Zhang,Olivia Hsu,Kunle Olukotun*

Main category: cs.PL

TL;DR: STeP is a new programming abstraction that enables efficient support and optimization for dynamic tensor workloads on spatial dataflow accelerators, achieving notable improvements in memory, latency, and compute utilization over previous methods.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the prevalence of dynamic behaviors in modern tensor-based machine learning applications, such as dynamically shaped tensors and data-dependent control flows. Prior programming abstractions for spatial dataflow accelerators are limited and cannot efficiently support these dynamic tensor workloads, which restricts optimization and efficient execution.

Method: The paper introduces Streaming Tensor Program (STeP), a new streaming abstraction designed for spatial dataflow accelerators. STeP includes flexible routing operators, an explicit memory hierarchy, and symbolic shape semantics to expose dynamic data rates and tensor shapes, allowing the system to better adapt to dynamic behaviors.

Result: The proposed STeP abstraction enables new optimizations: dynamic tiling, dynamic parallelization, and configuration time-multiplexing. Experiments using a cycle-approximate simulator on representative large language model (LLM) layers with real-world traces show that STeP's dynamic tiling reduces on-chip memory usage by 2.18x, dynamic parallelization improves latency by 1.5x, and time-multiplexing increases compute utilization by 2.57x compared to prior abstractions.

Conclusion: STeP effectively addresses the limitations of prior spatial dataflow accelerator programming models by providing programmability and visibility for dynamic tensor workloads, enabling significant performance improvements through adaptive optimizations.

Abstract: Dynamic behaviors are becoming prevalent in many tensor applications. In machine learning, for example, the input tensors are dynamically shaped or ragged, and data-dependent control flow is widely used in many models. However, the limited expressiveness of prior programming abstractions for spatial dataflow accelerators forces the dynamic behaviors to be implemented statically or lacks the visibility for performance-critical decisions. To address these challenges, we present the Streaming Tensor Program (STeP), a new streaming abstraction that enables dynamic tensor workloads to run efficiently on spatial dataflow accelerators. STeP introduces flexible routing operators, an explicit memory hierarchy, and symbolic shape semantics that expose dynamic data rates and tensor dimensions. These capabilities unlock new optimizations-dynamic tiling, dynamic parallelization, and configuration time-multiplexing-that adapt to dynamic behaviors while preserving dataflow efficiency. Using a cycle-approximate simulator on representative LLM layers with real-world traces, dynamic tiling reduces on-chip memory requirement by 2.18x, dynamic parallelization improves latency by 1.5x, and configuration time-multiplexing improves compute utilization by 2.57x over implementations available in prior abstractions.

</details>
