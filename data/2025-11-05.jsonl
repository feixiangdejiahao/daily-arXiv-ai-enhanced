{"id": "2511.02491", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.02491", "abs": "https://arxiv.org/abs/2511.02491", "authors": ["Roland Meyer", "Jakob Tepe"], "title": "Oriented Metrics for Bottom-Up Enumerative Synthesis", "comment": "37 pages, 16 figures", "summary": "In syntax-guided synthesis, one of the challenges is to reduce the enormous\nsize of the search space. We observe that most search spaces are not just flat\nsets of programs, but can be endowed with a structure that we call an oriented\nmetric. Oriented metrics measure the distance between programs, like ordinary\nmetrics do, but are designed for settings in which operations have an\norientation. Our focus is on the string and the bitvector domains, where\noperations like concatenation and bitwise conjunction transform an input into\nan output in a way that is not symmetric. We develop several new oriented\nmetrics for these domains. Oriented metrics are designed for search space\nreduction, and we present four techniques: (i) pruning the search space to a\nball around the ground truth, (ii) factorizing the search space by an\nequivalence that is induced by the oriented metric, (iii) abstracting the\noriented metric (and hence the equivalence) and refining it, and (iv) improving\nthe enumeration order by learning from abstract information. We acknowledge\nthat these techniques are inspired by developments in the literature. By\nunderstanding their roots in oriented metrics, we can substantially increase\ntheir applicability and efficiency. We have integrated these techniques into a\nnew synthesis algorithm and implemented the algorithm in a new solver. Notably,\nour solver is generic in the oriented metric over which it computes. We\nconducted experiments in the string and the bitvector domains, and consistently\nimprove the performance over the state-of-the-art by more than an order of\nmagnitude."}
{"id": "2511.01941", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.01941", "abs": "https://arxiv.org/abs/2511.01941", "authors": ["Sogol Masoumzadeh"], "title": "Detecting Vulnerabilities from Issue Reports for Internet-of-Things", "comment": "ACCEPTED/To Appear in the Proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE) 2025.\n  https://conf.researchr.org/details/ase-2025/ase-2025-student-research-competition/5/Detecting-Vulnerabilities-from-Issue-Reports-for-Internet-of-Things", "summary": "Timely identification of issue reports reflecting software vulnerabilities is\ncrucial, particularly for Internet-of-Things (IoT) where analysis is slower\nthan non-IoT systems. While Machine Learning (ML) and Large Language Models\n(LLMs) detect vulnerability-indicating issues in non-IoT systems, their IoT use\nremains unexplored. We are the first to tackle this problem by proposing two\napproaches: (1) combining ML and LLMs with Natural Language Processing (NLP)\ntechniques to detect vulnerability-indicating issues of 21 Eclipse IoT projects\nand (2) fine-tuning a pre-trained BERT Masked Language Model (MLM) on 11,000\nGitHub issues for classifying \\vul. Our best performance belongs to a Support\nVector Machine (SVM) trained on BERT NLP features, achieving an Area Under the\nreceiver operator characteristic Curve (AUC) of 0.65. The fine-tuned BERT\nachieves 0.26 accuracy, emphasizing the importance of exposing all data during\ntraining. Our contributions set the stage for accurately detecting IoT\nvulnerabilities from issue reports, similar to non-IoT systems."}
{"id": "2511.02108", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02108", "abs": "https://arxiv.org/abs/2511.02108", "authors": ["Steven Cho", "Stefano Ruberto", "Valerio Terragni"], "title": "Metamorphic Testing of Large Language Models for Natural Language Processing", "comment": null, "summary": "Using large language models (LLMs) to perform natural language processing\n(NLP) tasks has become increasingly pervasive in recent times. The versatile\nnature of LLMs makes them applicable to a wide range of such tasks. While the\nperformance of recent LLMs is generally outstanding, several studies have shown\nthat they can often produce incorrect results. Automatically identifying these\nfaulty behaviors is extremely useful for improving the effectiveness of LLMs.\nOne obstacle to this is the limited availability of labeled datasets, which\nnecessitates an oracle to determine the correctness of LLM behaviors.\nMetamorphic testing (MT) is a popular testing approach that alleviates this\noracle problem. At the core of MT are metamorphic relations (MRs), which define\nrelationships between the outputs of related inputs. MT can expose faulty\nbehaviors without the need for explicit oracles (e.g., labeled datasets). This\npaper presents the most comprehensive study of MT for LLMs to date. We\nconducted a literature review and collected 191 MRs for NLP tasks. We\nimplemented a representative subset (36 MRs) to conduct a series of experiments\nwith three popular LLMs, running approximately 560,000 metamorphic tests. The\nresults shed light on the capabilities and opportunities of MT for LLMs, as\nwell as its limitations."}
{"id": "2511.02197", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02197", "abs": "https://arxiv.org/abs/2511.02197", "authors": ["Shufan Wang", "Xing Hu", "Junkai Chen", "Zhiyuan Pan", "Xin Xia"], "title": "Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs", "comment": "13 pages, 4 figures", "summary": "With the widespread application of large language models (LLMs) in the field\nof code intelligence, increasing attention has been paid to the reliability and\ncontrollability of their outputs in code reasoning tasks. Confidence estimation\nserves as an effective and convenient approach for evaluating these aspects.\nThis paper proposes a confidence analysis and enhancement framework for LLMs\ntailored to code reasoning tasks. We conduct a comprehensive empirical study on\nthe confidence reliability of mainstream LLMs across different tasks, and\nfurther evaluate the effectiveness of techniques such as prompt strategy\noptimisation and mathematical calibration (e.g., Platt Scaling) in improving\nconfidence reliability. Our results show that DeepSeek-Reasoner achieves the\nbest performance across various tasks, outperforming other models by up to\n$0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance\nScore, respectively. The hybrid strategy combining the reassess prompt strategy\nand Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$\nover the original performance in the aforementioned three metrics. These\nresults indicate that models with reasoning capabilities demonstrate superior\nconfidence reliability, and that the hybrid strategy is the most effective in\nenhancing the confidence reliability of various models. Meanwhile, we elucidate\nthe impact of different task complexities, model scales, and strategies on\nconfidence performance, and highlight that the confidence of current LLMs in\ncomplex reasoning tasks still has considerable room for improvement. This study\nnot only provides a research foundation and technical reference for the\napplication of confidence in LLM-assisted software engineering, but also points\nthe way for future optimisation and engineering deployment of confidence\nmechanisms."}
{"id": "2511.02203", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02203", "abs": "https://arxiv.org/abs/2511.02203", "authors": ["Gerhard Yu", "Mithila Sivakumar", "Alvine B. Belle", "Soude Ghari", "Song Wang", "Timothy C. Lethbridge"], "title": "LLMs as Judges: Toward The Automatic Review of GSN-compliant Assurance Cases", "comment": null, "summary": "Assurance cases allow verifying the correct implementation of certain\nnon-functional requirements of mission-critical systems, including their\nsafety, security, and reliability. They can be used in the specification of\nautonomous driving, avionics, air traffic control, and similar systems. They\naim to reduce risks of harm of all kinds including human mortality,\nenvironmental damage, and financial loss. However, assurance cases often tend\nto be organized as extensive documents spanning hundreds of pages, making their\ncreation, review, and maintenance error-prone, time-consuming, and tedious.\nTherefore, there is a growing need to leverage (semi-)automated techniques,\nsuch as those powered by generative AI and large language models (LLMs), to\nenhance efficiency, consistency, and accuracy across the entire assurance-case\nlifecycle. In this paper, we focus on assurance case review, a critical task\nthat ensures the quality of assurance cases and therefore fosters their\nacceptance by regulatory authorities. We propose a novel approach that\nleverages the \\textit{LLM-as-a-judge} paradigm to automate the review process.\nSpecifically, we propose new predicate-based rules that formalize\nwell-established assurance case review criteria, allowing us to craft LLM\nprompts tailored to the review task. Our experiments on several\nstate-of-the-art LLMs (GPT-4o, GPT-4.1, DeepSeek-R1, and Gemini 2.0 Flash) show\nthat, while most LLMs yield relatively good review capabilities, DeepSeek-R1\nand GPT-4.1 demonstrate superior performance, with DeepSeek-R1 ultimately\noutperforming GPT-4.1. However, our experimental results also suggest that\nhuman reviewers are still needed to refine the reviews LLMs yield."}
{"id": "2511.02352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02352", "abs": "https://arxiv.org/abs/2511.02352", "authors": ["Sanket Mhatre", "Yasharth Bajpai", "Sumit Gulwani", "Emerson Murphy-Hill", "Gustavo Soares"], "title": "SWE-Sharp-Bench: A Reproducible Benchmark for C# Software Engineering Tasks", "comment": null, "summary": "AI coding agents have shown great progress on Python software engineering\nbenchmarks like SWE-Bench, and for other languages like Java and C in\nbenchmarks like Multi-SWE-Bench. However, C# -- a prominent enterprise language\nranking #5 in the TIOBE index -- remains absent from such benchmarks. We\nintroduce SWE-Sharp-Bench, a reproducible software engineering benchmark for\nC\\# featuring 150 instances from 17 repositories. Evaluating identical\nmodel-agent configurations across languages reveals a significant performance\ngap: while 70% of Python tasks in SWE-Bench Verified are solved, $only 40% of\nour C\\# tasks are resolved. We open-source SWE-Sharp-Bench and our entire\ncuration pipeline."}
{"id": "2511.02399", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02399", "abs": "https://arxiv.org/abs/2511.02399", "authors": ["Junwei Liu", "Chen Xu", "Chong Wang", "Tong Bai", "Weitong Chen", "Kaseng Wong", "Yiling Lou", "Xin Peng"], "title": "EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents", "comment": "14 pages, 6 figures", "summary": "Recent advances in large language model agents offer the promise of\nautomating end-to-end software development from natural language requirements.\nHowever, existing approaches largely adopt linear, waterfall-style pipelines,\nwhich oversimplify the iterative nature of real-world development and struggle\nwith complex, large-scale projects. To address these limitations, we propose\nEvoDev, an iterative software development framework inspired by feature-driven\ndevelopment. EvoDev decomposes user requirements into a set of user-valued\nfeatures and constructs a Feature Map, a directed acyclic graph that explicitly\nmodels dependencies between features. Each node in the feature map maintains\nmulti-level information, including business logic, design, and code, which is\npropagated along dependencies to provide context for subsequent development\niterations. We evaluate EvoDev on challenging Android development tasks and\nshow that it outperforms the best-performing baseline, Claude Code, by a\nsubstantial margin of 56.8%, while improving single-agent performance by\n16.0%-76.6% across different base LLMs, highlighting the importance of\ndependency modeling, context propagation, and workflow-aware agent design for\ncomplex software projects. Our work summarizes practical insights for designing\niterative, LLM-driven development frameworks and informs future training of\nbase LLMs to better support iterative software development."}
{"id": "2511.02434", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02434", "abs": "https://arxiv.org/abs/2511.02434", "authors": ["Dominik Fuchß", "Haoyu Liu", "Sophie Corallo", "Tobias Hey", "Jan Keim", "Johannes von Geisau", "Anne Koziolek"], "title": "Who's Who? LLM-assisted Software Traceability with Architecture Entity Recognition", "comment": null, "summary": "Identifying architecturally relevant entities in textual artifacts is crucial\nfor Traceability Link Recovery (TLR) between Software Architecture\nDocumentation (SAD) and source code. While Software Architecture Models (SAMs)\ncan bridge the semantic gap between these artifacts, their manual creation is\ntime-consuming. Large Language Models (LLMs) offer new capabilities for\nextracting architectural entities from SAD and source code to construct SAMs\nautomatically or establish direct trace links. This paper presents two\nLLM-based approaches: ExArch extracts component names as simple SAMs from SAD\nand source code to eliminate the need for manual SAM creation, while ArTEMiS\nidentifies architectural entities in documentation and matches them with\n(manually or automatically generated) SAM entities. Our evaluation compares\nagainst state-of-the-art approaches SWATTR, TransArC and ArDoCode. TransArC\nachieves strong performance (F1: 0.87) but requires manually created SAMs;\nExArch achieves comparable results (F1: 0.86) using only SAD and code. ArTEMiS\nis on par with the traditional heuristic-based SWATTR (F1: 0.81) and can\nsuccessfully replace it when integrated with TransArC. The combination of\nArTEMiS and ExArch outperforms ArDoCode, the best baseline without manual SAMs.\nOur results demonstrate that LLMs can effectively identify architectural\nentities in textual artifacts, enabling automated SAM generation and TLR,\nmaking architecture-code traceability more practical and accessible."}
{"id": "2511.02445", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02445", "abs": "https://arxiv.org/abs/2511.02445", "authors": ["Eriks Klotins", "Magnus Ahlgren", "Nicolas Martin Vivaldi", "Even-Andre Karlsson"], "title": "When Continuous Delivery Is Not an Option: Practical Paths to Continuous Engineering in Complex Organizations", "comment": null, "summary": "Purpose: Continuous Software Engineering (CSE) promises improved efficiency,\nquality, and responsiveness in software-intensive organizations. However, fully\nadopting CSE is often constrained by complex products, legacy systems,\norganizational inertia, and regulatory requirements. In this paper, we examine\nfour industrial cases from the automation, automotive, retail, and chemical\nsectors to explore how such constraints shape CSE adoption in practice.\nMethods: We apply and extend a previously proposed CSE Industry Readiness Model\nto assess the current and potential levels of adoption in each case. Through\nexpert interviews and narrative synthesis, we identify common driving forces\nand adoption barriers, including organizational preparedness,\ncross-organizational dependencies, and limited customer demand for continuous\ndelivery. Results: Based on our findings, we propose an updated readiness model\nthat introduces additional levels of internal and external feedback,\ndistinguishes market- and organization-facing constraints, and better guides\npractitioners in setting realistic CSE adoption goals. Conclusions: Our results\nhighlight that while full end-to-end CSE adoption may not always be feasible,\nmeaningful internal improvements are still possible and beneficial. This study\nprovides empirically grounded guidance for organizations navigating partial or\nconstrained CSE transformations."}
{"id": "2511.02475", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02475", "abs": "https://arxiv.org/abs/2511.02475", "authors": ["Jürgen Cito", "Dominik Bork"], "title": "Lost in Code Generation: Reimagining the Role of Software Models in AI-driven Software Engineering", "comment": null, "summary": "Generative AI enables rapid ``vibe coding,\" where natural language prompts\nyield working software systems. While this lowers barriers to software\ncreation, it also collapses the boundary between prototypes and engineered\nsoftware, leading to fragile systems that lack robustness, security, and\nmaintainability. We argue that this shift motivates a reimagining of software\nmodels. Rather than serving only as upfront blueprints, models can be recovered\npost-hoc from AI-generated code to restore comprehension, expose risks, and\nguide refinement. In this role, models serve as mediators between human intent,\nAI generation, and long-term system evolution, providing a path toward\nsustainable AI-driven software engineering."}
{"id": "2511.02713", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02713", "abs": "https://arxiv.org/abs/2511.02713", "authors": ["Qianru Meng", "Zhaochun Ren", "Joost Visser"], "title": "ReleaseEval: A Benchmark for Evaluating Language Models in Automated Release Note Generation", "comment": null, "summary": "Automated release note generation addresses the challenge of documenting\nfrequent software updates, where manual efforts are time-consuming and prone to\nhuman error. Although recent advances in language models further enhance this\nprocess, progress remains hindered by dataset limitations, including the lack\nof explicit licensing and limited reproducibility, and incomplete task design\nthat relies mainly on commit messages for summarization while overlooking\nfine-grained contexts such as commit hierarchies and code changes. To fill this\ngap, we introduce ReleaseEval, a reproducible and openly licensed benchmark\ndesigned to systematically evaluate language models for automated release note\ngeneration. ReleaseEval comprises 94,987 release notes from 3,369 repositories\nacross 6 programming languages, and supports three task settings with three\nlevels of input granularity: (1) commit2sum, which generates release notes from\ncommit messages; (2) tree2sum, which incorporates commit tree structures; and\n(3) diff2sum, which leverages fine-grained code diffs. Both automated and human\nevaluations show that large language models consistently outperform traditional\nbaselines across all tasks, achieving substantial gains on tree2sum, while\nstill struggling on diff2sum. These findings highlight LLMs' proficiency in\nleveraging structured information while revealing challenges in abstracting\nfrom long code diffs."}
{"id": "2511.02736", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02736", "abs": "https://arxiv.org/abs/2511.02736", "authors": ["Madalena Sasportes", "Grischa Liebel", "Miguel Goulão"], "title": "Investigating the Experience of Autistic Individuals in Software Engineering", "comment": null, "summary": "Context: Autism spectrum disorder (ASD) leads to various issues in the\neveryday life of autistic individuals, often resulting in unemployment and\nmental health problems. To improve the inclusion of autistic adults, existing\nstudies have highlighted the strengths these individuals possess in comparison\nto non-autistic individuals, e.g., high attention to detail or excellent\nlogical reasoning skills. If fostered, these strengths could be valuable in\nsoftware engineering activities, such for identifying specific kinds of bugs in\ncode. However, existing work in SE has primarily studied the challenges of\nautistic individuals and possible accommodations, with little attention their\nstrengths. Objective: Our goal is to analyse the experiences of autistic\nindividuals in software engineering activities, such as code reviews, with a\nparticular emphasis on strengths. Methods: This study combines Social-Technical\nGrounded Theory through semi-structured interviews with 16 autistic software\nengineers and a survey with 49 respondents, including 5 autistic participants.\nWe compare the emerging themes with the theory by Gama et al. on the Effect of\nNeurodivergent Cognitive Dysfunctions in Software Engineering Performance.\nResults: Our results suggest that autistic software engineers are often skilled\nin logical thinking, attention to detail, and hyperfocus in programming; and\nthey enjoy learning new programming languages and programming-related\ntechnologies. Confirming previous work, they tend to prefer written\ncommunication and remote work. Finally, we report a high comfort level in\ninteracting with AI-based systems. Conclusions: Our findings extend existing\nwork by providing further evidence on the strengths of autistic software\nengineers."}
{"id": "2511.02810", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02810", "abs": "https://arxiv.org/abs/2511.02810", "authors": ["Suddhasvatta Das", "Kevin Gary"], "title": "Formalizing Regression Testing for Agile and Continuous Integration Environments", "comment": "This is the first attempt to formalize regression testing in agile\n  context as a continuous/near-continuos activity. This formalization will help\n  practitioners and researchers to answer 'when', 'what' and 'how much'\n  question of regression testing in real world time constrained agile projects.\n  This work is currently under review with Software Quality Journal", "summary": "Software developed using modern agile practices delivers a stream of software\nversions that require continuous regression testing rather than testing once\nclose to the delivery or maintenance phase, as assumed by classical\nregression-testing theory. In this work, we formalize the phenomenon of\ncontinuous or near-continuous regression testing using successive builds as a\ntime-ordered chain, where each build contains the program, requirements, and\nthe accompanying tests. We also formalize the regression test window between\nany two builds, which captures the limited time budget available for regression\ntesting. As the time limit is set to infinity and the chain is closed to two\nbuilds, the model degenerates to retest-all, thereby preserving semantics for\nthe classical two-version case. The formalization is validated by directly\nrepresenting two state-of-the-art agile regression testing algorithms in terms\nof build-tuple operations without requiring auxiliary assumptions, followed by\nproof of the soundness and completeness of our formalization."}
{"id": "2511.02827", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02827", "abs": "https://arxiv.org/abs/2511.02827", "authors": ["Mohamed Almukhtar", "Anwar Ghammam", "Marouane Kessentini", "Hua Ming"], "title": "From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu", "comment": "Accepted for publication in the proceedings of IEEE/ACM 48th\n  International Conference on Software Engineering", "summary": "In an era shaped by Generative Artificial Intelligence for code generation\nand the rising adoption of Python-based Machine Learning systems (MLS),\nsoftware quality has emerged as a major concern. As these systems grow in\ncomplexity and importance, a key obstacle lies in understanding exactly how\nspecific code changes affect overall quality-a shortfall aggravated by the lack\nof quality assessment tools and a clear mapping between ML systems code changes\nand their quality effects. Although prior work has explored code changes in\nMLS, it mostly stops at what the changes are, leaving a gap in our knowledge of\nthe relationship between code changes and the MLS quality. To address this gap,\nwe conducted a large-scale empirical study of 3,340 open-source Python ML\nprojects, encompassing more than 3.7 million commits and 2.7 trillion lines of\ncode. We introduce PyQu, a novel tool that leverages low level software metrics\nto identify quality-enhancing commits with an average accuracy, precision, and\nrecall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic\nanalysis, we identified 61 code changes, each demonstrating a direct impact on\nenhancing software quality, and we classified them into 13 categories based on\ncontextual characteristics. 41% of the changes are newly discovered by our\nstudy and have not been identified by state-of-the-art Python changes detection\ntools. Our work offers a vital foundation for researchers, practitioners,\neducators, and tool developers, advancing the quest for automated quality\nassessment and best practices in Python-based ML software."}
