{"id": "2510.19860", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19860", "abs": "https://arxiv.org/abs/2510.19860", "authors": ["Ketai Qiu", "Luca Di Grazia", "Leonardo Mariani", "Mauro Pezz\u00e8"], "title": "E-Test: E'er-Improving Test Suites", "comment": "Accepted at the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)", "summary": "Test suites are inherently imperfect, and testers can always enrich a suite\nwith new test cases that improve its quality and, consequently, the reliability\nof the target software system. However, finding test cases that explore\nexecution scenarios beyond the scope of an existing suite can be extremely\nchallenging and labor-intensive, particularly when managing large test suites\nover extended periods.\n  In this paper, we propose E-Test, an approach that reduces the gap between\nthe execution space explored with a test suite and the executions experienced\nafter testing by augmenting the test suite with test cases that explore\nexecution scenarios that emerge in production. E-Test (i) identifies executions\nthat have not yet been tested from large sets of scenarios, such as those\nmonitored during intensive production usage, and (ii) generates new test cases\nthat enhance the test suite. E-Test leverages Large Language Models (LLMs) to\npinpoint scenarios that the current test suite does not adequately cover, and\naugments the suite with test cases that execute these scenarios.\n  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred\nopen-source Java projects already in production and Defects4J, demonstrates\nthat E-Test retrieves not-yet-tested execution scenarios significantly better\nthan state-of-the-art approaches. While existing regression testing and field\ntesting approaches for this task achieve a maximum F1-score of 0.34, and\nvanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These\nresults highlight the impact of E-Test in enhancing test suites by effectively\ntargeting not-yet-tested execution scenarios and reducing manual effort\nrequired for maintaining test suites.", "AI": {"tldr": "E-Test uses LLMs to augment test suites with cases reflecting real production scenarios, achieving far better coverage than previous methods and decreasing manual effort in maintaining test suites.", "motivation": "Test suites often fail to cover all possible execution scenarios, especially those occurring in real production environments. Expanding test suites to include such scenarios is challenging and requires significant manual effort, particularly as software and test suites grow larger over time.", "method": "The paper proposes E-Test, a method that uses Large Language Models (LLMs) to analyze execution scenarios gathered from production environments. E-Test identifies scenarios not yet covered by the existing test suite and generates new test cases to improve coverage.", "result": "E-Test is evaluated on a dataset of 1,975 scenarios from open-source Java projects and Defects4J. It significantly outperforms state-of-the-art methods and vanilla LLMs in identifying and covering not-yet-tested scenarios, achieving an F1-score of 0.55 compared to a maximum of 0.39 for other methods.", "conclusion": "E-Test effectively enriches test suites by targeting previously untested execution scenarios, resulting in improved coverage and reduced manual maintenance effort."}}
{"id": "2510.19864", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19864", "abs": "https://arxiv.org/abs/2510.19864", "authors": ["Amila Indika", "Igor Molybog"], "title": "SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations", "comment": "14 pages, 5 figures, 4 tables", "summary": "Numerous knowledge workers utilize spreadsheets in business, accounting, and\nfinance. However, a lack of systematic documentation methods for spreadsheets\nhinders automation, collaboration, and knowledge transfer, which risks the loss\nof crucial institutional knowledge. This paper introduces Spreadsheet\nOperations Documentation (SOD), an AI task that involves generating\nhuman-readable explanations from spreadsheet operations. Many previous studies\nhave utilized Large Language Models (LLMs) for generating spreadsheet\nmanipulation code; however, translating that code into natural language for SOD\nis a less-explored area. To address this, we present a benchmark of 111\nspreadsheet manipulation code snippets, each paired with a corresponding\nnatural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,\nLLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and\nMETEOR metrics. Our findings suggest that LLMs can generate accurate\nspreadsheet documentation, making SOD a feasible prerequisite step toward\nenhancing reproducibility, maintainability, and collaborative workflows in\nspreadsheets, although there are challenges that need to be addressed.", "AI": {"tldr": "This paper presents an AI approach to generate readable explanations for spreadsheet code, showing that leading LLMs can automate documentation tasks, which could enhance teamwork and knowledge retention in professional settings.", "motivation": "Spreadsheet documentation is crucial for automation, collaboration, and knowledge transfer, but systematic approaches are lacking. This deficiency threatens loss of institutional knowledge in domains like business and finance.", "method": "The paper introduces Spreadsheet Operations Documentation (SOD), which uses AI to generate human-readable documentation from spreadsheet code. It presents a benchmark with 111 code-summary pairs and evaluates five Large Language Models (LLMs) using standard metrics (BLEU, GLEU, ROUGE-L, METEOR).", "result": "All tested LLMs, including GPT-4o and LLaMA-3.3-70B, can generate reasonably accurate documentation for spreadsheet operations. However, performance varies and challenges remain.", "conclusion": "LLMs are capable tools for producing natural language documentation of spreadsheet operations, potentially improving reproducibility, maintainability, and collaboration. There is still room for improvement and more research is needed in this area."}}
{"id": "2510.19868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19868", "abs": "https://arxiv.org/abs/2510.19868", "authors": ["Qian Xiong", "Bo Yang", "Weisong Sun", "Yiran Zhang", "Tianlin Li", "Yang Liu", "Zhi Jin"], "title": "Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation", "comment": null, "summary": "Automated code generation driven by Large Lan- guage Models (LLMs) has\nenhanced development efficiency, yet generating complex application-level\nsoftware code remains challenging. Multi-agent frameworks show potential, but\nexisting methods perform inadequately in large-scale application-level software\ncode generation, failing to ensure reasonable orga- nizational structures of\nproject code and making it difficult to maintain the code generation process.\nTo address this, this paper envisions a Knowledge-Guided Application-Level Code\nGeneration framework named KGACG, which aims to trans- form software\nrequirements specification and architectural design document into executable\ncode through a collaborative closed- loop of the Code Organization & Planning\nAgent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a\nfeedback mechanism. We demonstrate the collaborative process of the agents in\nKGACG in a Java Tank Battle game case study while facing challenges. KGACG is\ndedicated to advancing the automation of application-level software\ndevelopment.", "AI": {"tldr": "KGACG is a new multi-agent framework that uses knowledge and feedback to automatically generate organized, maintainable application-level software code from specifications, resolving issues seen with earlier systems.", "motivation": "While LLM-driven code generation has improved development efficiency, generating complex, application-level software remains difficult, especially for code requiring coherent organization and maintainability.", "method": "KGACG introduces a multi-agent framework with three agents: Code Organization & Planning Agent (COPA), Coding Agent (CA), and Testing Agent (TA). These agents work collaboratively in a closed-loop, supported by a feedback mechanism, to transform software requirements and architecture documents into executable code.", "result": "KGACG\u2019s collaborative agent process is illustrated through a Java Tank Battle game case study, demonstrating how these agents can tackle application-level code generation and organization challenges.", "conclusion": "KGACG offers a knowledge-guided, agent-driven approach to enhance automation, organization, and maintainability in application-level software code generation, overcoming shortcomings of previous multi-agent frameworks."}}
{"id": "2510.19898", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19898", "abs": "https://arxiv.org/abs/2510.19898", "authors": ["Atharv Sonwane", "Isadora White", "Hyunji Lee", "Matheus Pereira", "Lucas Caccia", "Minseon Kim", "Zhengyan Shi", "Chinmay Singh", "Alessandro Sordoni", "Marc-Alexandre C\u00f4t\u00e9", "Xingdi Yuan"], "title": "BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills", "comment": null, "summary": "High quality bugs are key to training the next generation of language model\nbased software engineering (SWE) agents. We introduce a novel method for\nsynthetic generation of difficult and diverse bugs. Our method instructs SWE\nAgents to introduce a feature into the codebase whereby they may\nunintentionally break tests, resulting in bugs. Prior approaches often induce\nan out-of-distribution effect by generating bugs intentionally (e.g. by\nintroducing local perturbation to existing code), which does not reflect\nrealistic development processes. We perform qualitative analysis to demonstrate\nthat our approach for generating bugs more closely reflects the patterns found\nin human-authored edits. Through extensive experiments, we demonstrate that our\nbugs provide more efficient training data for supervised fine-tuning,\noutperforming other bug datasets by 2% with half the training data (1.2k vs. 3k\nbugs). We train on our newly generated bugs in addition to existing bug\ndatasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench\nVerified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on\nSWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over\nthree seeds.", "AI": {"tldr": "The paper presents a novel bug generation technique for training SWE agents, creating more realistic bugs by simulating developer workflows. These bugs improve training efficiency and model performance, yielding state-of-the-art results on SWE-bench Verified benchmarks.", "motivation": "Previous methods for generating bug training data for software engineering agents often rely on unnatural and unrealistic bug creation, which fails to capture real-life coding workflows. High quality, human-like bugs are needed to better train large language model-based SWE agents.", "method": "The authors propose an approach where agents are asked to deliberately add new features to codebases, possibly introducing mistakes that break existing tests and cause bugs. This simulates realistic development, unlike previous direct perturbation techniques.", "result": "Qualitative analysis shows that their synthetic bugs mirror human editing patterns more closely. Experiments indicate that their generated bugs result in more effective supervised fine-tuning, outperforming previous bug datasets by 2% even with half the amount of training data. Their models, FrogBoss (32B) and FrogMini (14B), achieve state-of-the-art performance on the SWE-bench Verified benchmark.", "conclusion": "The synthetic bug generation method produces higher quality, more efficient bug datasets for training SWE agents, enabling state-of-the-art results on standard benchmarks with less data."}}
{"id": "2510.19850", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.19850", "abs": "https://arxiv.org/abs/2510.19850", "authors": ["Mostapha Kalami Heris"], "title": "Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs", "comment": null, "summary": "Large Language Models (LLMs) are central to reasoning, writing, and\ndecision-support workflows, yet users lack consistent control over how they\nreason and express outputs. Conventional prompt engineering relies on verbose\nnatural-language instructions, limiting reproducibility, modularity, and\ninterpretability. This paper introduces Prompt Decorators, a declarative,\ncomposable syntax that governs LLM behavior through compact control tokens such\nas +++Reasoning, +++Tone(style=formal), and +++Import(topic=\"Systems\nThinking\"). Each decorator modifies a behavioral dimension, such as reasoning\nstyle, structure, or tone, without changing task content. The framework\nformalizes twenty core decorators organized into two functional families\n(Cognitive & Generative and Expressive & Systemic), each further decomposed\ninto subcategories that govern reasoning, interaction, expression, and\nsession-control. It defines a unified syntax, scoping model, and deterministic\nprocessing pipeline enabling predictable and auditable behavior composition. By\ndecoupling task intent from execution behavior, Prompt Decorators create a\nreusable and interpretable interface for prompt design. Illustrative use cases\ndemonstrate improved reasoning transparency, reduced prompt complexity, and\nstandardized model behavior across domains. The paper concludes with\nimplications for interoperability, behavioral consistency, and the development\nof declarative interfaces for scalable AI systems.", "AI": {"tldr": "This paper presents 'Prompt Decorators,' a set of concise syntax tools for controlling LLM output style, reasoning, and structure. The approach simplifies prompt engineering, making it more modular, reproducible, and interpretable, and demonstrates benefits for consistency and transparency in AI behavior.", "motivation": "Large Language Models (LLMs) are widely used, but users lack fine-grained and consistent control over their reasoning and output style. Existing prompt engineering with natural-language instructions is verbose, hard to reproduce, and not modular or interpretable.", "method": "The authors introduce 'Prompt Decorators,' a declarative, composable syntax using compact control tokens to directly control LLM behavioral facets such as reasoning, tone, and structure. They formalize a set of 20 decorators, present a syntax and processing model, and organize them into functional categories to standardize and systematize their application.", "result": "The proposed framework allows predictable and auditable composition of behaviors, making prompt design reusable and interpretable. Illustrative use cases show increased transparency in reasoning, reduced prompt complexity, and more standardized outputs across domains.", "conclusion": "Prompt Decorators provide a scalable, interpretable, and modular approach to prompt design for LLMs, improving behavioral consistency, reducing complexity, and paving the way for interoperable and declarative interfaces in AI workflows."}}
{"id": "2510.19984", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.19984", "abs": "https://arxiv.org/abs/2510.19984", "authors": ["Konstantinos Kitsios", "Marcel B\u00f6hme", "Alberto Bacchelli"], "title": "On Interaction Effects in Greybox Fuzzing", "comment": "12 pages, 2 figures, Accepted for presentation at the 48th\n  International Conference on Software Engineering (ICSE '26)", "summary": "A greybox fuzzer is an automated software testing tool that generates new\ntest inputs by applying randomly chosen mutators (e.g., flipping a bit or\ndeleting a block of bytes) to a seed input in random order and adds all\ncoverage-increasing inputs to the corpus of seeds. We hypothesize that the\norder in which mutators are applied to a seed input has an impact on the\neffectiveness of greybox fuzzers. In our experiments, we fit a linear model to\na dataset that contains the effectiveness of all possible mutator pairs and\nindeed observe the conjectured interaction effect. This points us to more\nefficient fuzzing by choosing the most promising mutator sequence with a higher\nlikelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the\nmost promising mutator sequences. MuoFuzz learns the conditional probability\nthat the next mutator will yield an interesting input, given the previously\nselected mutator. Then, it samples from the learned probability using a random\nwalk to generate mutator sequences. We compare the performance of MuoFuzz to\nAFL++, which uses a fixed selection probability, and MOPT, which optimizes the\nselection probability of each mutator in isolation. Experimental results on the\nFuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code\ncoverage and finds four bugs missed by AFL++ and one missed by both AFL++ and\nMOPT.", "AI": {"tldr": "By leveraging learned sequences of input mutators, MuoFuzz outperforms state-of-the-art fuzzers in code coverage and bug detection, demonstrating that optimizing mutator order can significantly enhance automated software testing.", "motivation": "The paper is motivated by the observation that the order in which mutators are applied during fuzzing may influence the effectiveness of generating new, interesting test inputs. Traditional fuzzers typically select mutators in isolation or at random, without considering the potential interactions between them.", "method": "The authors design and implement MuoFuzz, a greybox fuzzer. MuoFuzz builds a linear model using effectiveness data from mutator pairs to learn the conditional probability that a sequence of mutators will yield an interesting input. Using this learned model, MuoFuzz samples promising mutator sequences via a random walk. The approach is evaluated against benchmarks FuzzBench and MAGMA, and compared to state-of-the-art fuzzers AFL++ and MOPT.", "result": "MuoFuzz achieves superior code coverage compared to AFL++ and MOPT. It discovers four bugs not found by AFL++, and one bug not found by either AFL++ or MOPT, demonstrating its effectiveness in generating novel, interesting inputs by optimizing mutator sequences.", "conclusion": "The order of mutator application during fuzzing profoundly affects the outcome. By learning and exploiting the effectiveness of mutator pairs, MuoFuzz outperforms existing greybox fuzzers both in coverage and bug-finding ability."}}
{"id": "2510.19853", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19853", "abs": "https://arxiv.org/abs/2510.19853", "authors": ["Assaf Marron", "David Harel"], "title": "A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification", "comment": null, "summary": "An algorithm specification in natural language or pseudocode is expected to\nbe clear and explicit enough to enable mechanical execution. In this position\npaper we contribute an initial characterization of the knowledge that an\nexecuting agent, human or machine, should possess in order to be able to carry\nout the instructions of a given algorithm specification as a stand-alone\nentity, independent of any system implementation. We argue that, for that\nalgorithm specification, such prerequisite knowledge, whether unique or shared\nwith other specifications, can be summarized in a document of practical size.\nWe term this document the realm of the algorithm specification. The generation\nof such a realm is itself a systematic analytical process, significant parts of\nwhich can be automated with the help of large language models and the reuse of\nexisting documents. The algorithm-specification's realm would consist of\nspecification language syntax and semantics, domain knowledge restricted to the\nreferenced entities, inter-entity relationships, relevant underlying\ncause-and-effect rules, and detailed instructions and means for carrying out\ncertain operations. Such characterization of the realm can contribute to\nmethodological implementation of the algorithm specification in diverse systems\nand to its formalization for mechanical verification. The paper also touches\nupon the question of assessing execution faithfulness, which is distinct from\ncorrectness: in the absence of a reference interpretation of natural language\nor pseudocode specification with a given vocabulary, how can we determine if an\nobserved agent's execution indeed complies with the input specification.", "AI": {"tldr": "This paper proposes a systematic way of documenting the essential knowledge required to execute any algorithm specification (called its 'realm') and suggests that parts of creating this documentation can be automated, improving implementation, verification, and faithfulness assessment.", "motivation": "To define what knowledge an agent (human or machine) needs to execute an algorithm specification based only on the specification itself, without relying on existing system implementations.", "method": "The paper proposes an initial framework by characterizing the required 'realm' for algorithm execution. It discusses how this 'realm'\u2014a concise supporting document\u2014can be generated systematically, possibly leveraging large language models and reuse of existing documents. The approach emphasizes language syntax, semantics, domain knowledge, interrelations, and procedural instructions.", "result": "The authors introduce the concept of the 'realm' of an algorithm specification\u2014a document detailing all prerequisite knowledge necessary for mechanical execution. They outline its composition and suggest its automation potential. The paper also discusses the issue of measuring execution faithfulness versus correctness, in lieu of a canonical interpretation.", "conclusion": "Systematically constructing and documenting the prerequisite knowledge (the 'realm') required to execute an algorithm enables more reliable, faithful, and potentially automatable implementation and verification processes across diverse systems."}}
{"id": "2510.19997", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19997", "abs": "https://arxiv.org/abs/2510.19997", "authors": ["Abraham Itzhak Weinberg"], "title": "A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) presents transformative\nopportunities for organizations, yet both midsize organizations and larger\nenterprises face distinctive adoption challenges. Midsize organizations\nencounter resource constraints and limited AI expertise, while enterprises\nstruggle with organizational complexity and coordination challenges. Existing\ntechnology adoption frameworks, including TAM (Technology Acceptance Model),\nTOE (Technology Organization Environment), and DOI (Diffusion of Innovations)\ntheory, lack the specificity required for GenAI implementation across these\ndiverse contexts, creating a critical gap in adoption literature. This paper\nintroduces FAIGMOE (Framework for the Adoption and Integration of Generative AI\nin Midsize Organizations and Enterprises), a conceptual framework addressing\nthe unique needs of both organizational types. FAIGMOE synthesizes technology\nadoption theory, organizational change management, and innovation diffusion\nperspectives into four interconnected phases: Strategic Assessment, Planning\nand Use Case Development, Implementation and Integration, and\nOperationalization and Optimization. Each phase provides scalable guidance on\nreadiness assessment, strategic alignment, risk governance, technical\narchitecture, and change management adaptable to organizational scale and\ncomplexity. The framework incorporates GenAI specific considerations including\nprompt engineering, model orchestration, and hallucination management that\ndistinguish it from generic technology adoption frameworks. As a perspective\ncontribution, FAIGMOE provides the first comprehensive conceptual framework\nexplicitly addressing GenAI adoption across midsize and enterprise\norganizations, offering actionable implementation protocols, assessment\ninstruments, and governance templates requiring empirical validation through\nfuture research.", "AI": {"tldr": "The paper identifies a gap in existing technology adoption frameworks for Generative AI in midsize and enterprise organizations. It proposes FAIGMOE, a new conceptual framework with four phases, combining theory and GenAI-specific guidance. FAIGMOE offers protocols and tools for effective GenAI adoption, but requires empirical testing in real-world settings.", "motivation": "Existing technology adoption frameworks (TAM, TOE, DOI) do not address the unique challenges presented by adopting Generative AI in both midsize and enterprise organizations. Midsize organizations face resource and expertise constraints, while enterprises struggle with complexity and coordination. This creates a gap in the literature for GenAI implementation.", "method": "The authors propose FAIGMOE, a conceptual framework specifically designed for GenAI adoption and integration in both midsize organizations and enterprises. FAIGMOE is structured into four phases (Strategic Assessment, Planning and Use Case Development, Implementation and Integration, Operationalization and Optimization) and synthesizes established technology adoption theories and organizational change management perspectives. It includes specific guidance and protocols, as well as GenAI-specific considerations such as prompt engineering and hallucination management.", "result": "FAIGMOE offers scalable guidance and protocols for GenAI adoption, tailored to organizational size and complexity. It provides actionable implementation protocols, readiness assessment tools, and governance templates. The framework addresses GenAI-specific requirements that existing frameworks lack.", "conclusion": "FAIGMOE is the first comprehensive conceptual framework that explicitly addresses the challenges of GenAI adoption in midsize and enterprise organizations. It fills a gap in the literature and offers actionable, scalable tools and protocols. However, empirical validation of the framework is needed in future research."}}
{"id": "2510.20018", "categories": ["cs.PL", "68N18 (Primary), 03B70 (Secondary)", "F.3.3; D.3.1"], "pdf": "https://arxiv.org/pdf/2510.20018", "abs": "https://arxiv.org/abs/2510.20018", "authors": ["Ryan Kavanagh", "Chuta Sano", "Brigitte Pientka"], "title": "Deconstructed Proto-Quipper: A Rational Reconstruction", "comment": "Submitted to the 35th European Symposium on Programming (ESOP 2026)", "summary": "The Proto-Quipper family of programming languages aims to provide a formal\nfoundation for the Quipper quantum programming language. Unfortunately,\nProto-Quipper languages have complex operational semantics: they are inherently\neffectful, and they rely on set-theoretic operations and fresh name generation\nto manipulate quantum circuits. This makes them difficult to reason about using\nstandard programming language techniques and, ultimately, to mechanize. We\nintroduce Proto-Quipper-A, a rational reconstruction of Proto-Quipper languages\nfor static circuit generation. It uses a linear $\\lambda$-calculus to describe\nquantum circuits with normal forms that closely correspond to box-and-wire\ncircuit diagrams. Adjoint-logical foundations integrate this circuit language\nwith a linear/non-linear functional language and let us reconstruct\nProto-Quipper's circuit programming abstractions using more primitive\nadjoint-logical operations. Proto-Quipper-A enjoys a simple call-by-value\nreduction semantics, and to illustrate its tractability as a foundation for\nProto-Quipper languages, we show that it is normalizing. We show how to use\nstandard logical relations to prove normalization of linear and substructural\nsystems, thereby avoiding the inherent complexity of existing linear logical\nrelations.", "AI": {"tldr": "Proto-Quipper-A offers a simpler, mechanizable quantum circuit language based on linear \u03bb-calculus, enabling formal reasoning and normalization proofs using standard methods, thus overcoming the complexity of previous Proto-Quipper operational semantics.", "motivation": "Proto-Quipper programming languages are crucial for providing a formal basis for the Quipper quantum programming language but suffer from operational complexity, making them hard to reason about and mechanize with conventional programming language frameworks.", "method": "The paper introduces Proto-Quipper-A, which utilizes a linear \u03bb-calculus to describe quantum circuits in normal forms resembling box-and-wire diagrams. It also embeds adjoint-logical foundations, integrating the circuit language with a linear/non-linear functional language and reconstructs circuit programming abstractions using primitive adjoint-logical operations. The semantics follow a simple call-by-value reduction.", "result": "Proto-Quipper-A is shown to be tractable, having a simpler semantics than its predecessors, and proven to be normalizing. The approach also demonstrates how standard logical relations can be used to prove normalization for linear and substructural systems without the complexity of previous methods.", "conclusion": "Proto-Quipper-A provides a foundational, tractable, and mechanizable framework for static circuit generation in the Proto-Quipper family of languages, significantly simplifying its operational semantics and facilitating reasoning through standard logical relations."}}
{"id": "2510.20041", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20041", "abs": "https://arxiv.org/abs/2510.20041", "authors": ["Gareema Ranjan", "Mahmoud Alfadel", "Gengyi Sun", "Shane McIntosh"], "title": "The Cost of Downgrading Build Systems: A Case Study of Kubernetes", "comment": null, "summary": "Since developers invoke the build system frequently, its performance can\nimpact productivity. Modern artifact-based build tools accelerate builds, yet\nprior work shows that teams may abandon them for alternatives that are easier\nto maintain. While prior work shows why downgrades are performed, the\nimplications of downgrades remain largely unexplored. In this paper, we\ndescribe a case study of the Kubernetes project, focusing on its downgrade from\nan artifact-based build tool (Bazel) to a language-specific solution (Go\nBuild). We reproduce and analyze the full and incremental builds of change sets\nduring the downgrade period. On the one hand, we find that Bazel builds are\nfaster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose\na larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel\nbuilds also impose a greater CPU load at parallelism settings above eight for\nfull builds and above one for incremental builds. We estimate that downgrading\nfrom Bazel can increase CI resource costs by up to 76 explore whether our\nobservations generalize by replicating our Kubernetes study on four other\nprojects that also downgraded from Bazel to older build tools. We observe that\nwhile build time penalties decrease, Bazel consistently consumes more memory.\nWe conclude that abandoning artifact-based build tools, despite perceived\nmaintainability benefits, tends to incur considerable performance costs for\nlarge projects. Our observations may help stakeholders to balance trade-offs in\nbuild tool adoption", "AI": {"tldr": "Switching from a fast, artifact-based build tool (Bazel) to an easier-to-maintain alternative (Go Build) in large projects like Kubernetes leads to slower builds and higher CI resource costs, despite simpler maintenance. Teams should weigh performance costs against maintainability benefits when choosing build tools.", "motivation": "The motivation stems from frequent developer reliance on build systems, making their performance critical for productivity. While modern, artifact-based build tools like Bazel offer speed, many teams switch to simpler, easier-to-maintain alternatives, but the broader consequences of these downgrades remain understudied.", "method": "The authors present a case study on the Kubernetes project, analyzing its transition from Bazel (an artifact-based build tool) to Go Build (a language-specific build tool). They reproduce and compare full and incremental builds during the downgrade, measuring build time, memory usage, and CPU load. This study is replicated across four other projects that also moved away from Bazel.", "result": "Bazel offers faster build times compared to Go Build but at the cost of significantly higher memory and CPU usage, especially with higher parallelism. Downgrading from Bazel can result in up to a 76% increase in CI resource costs. Other projects showed similar trends, with Bazel consistently using more memory even as build time differences decreased.", "conclusion": "Despite perceived maintainability advantages, downgrading from artifact-based build tools like Bazel incurs notable performance and resource efficiency penalties for large projects. Stakeholders must carefully weigh these trade-offs when selecting build systems."}}
{"id": "2510.20532", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.20532", "abs": "https://arxiv.org/abs/2510.20532", "authors": ["Patrycja Balik", "Szymon J\u0119dras", "Piotr Polesiuk"], "title": "Deciding not to Decide: Sound and Complete Effect Inference in the Presence of Higher-Rank Polymorphism", "comment": null, "summary": "Type-and-effect systems help the programmer to organize data and\ncomputational effects in a program. While for traditional type systems\nexpressive variants with sophisticated inference algorithms have been developed\nand widely used in programming languages, type-and-effect systems did not yet\ngain widespread adoption. One reason for this is that type-and-effect systems\nare more complex and the existing inference algorithms make compromises between\nexpressiveness, intuitiveness, and decidability. In this work, we present an\neffect inference algorithm for a type-and-effect system with subtyping,\nexpressive higher-rank polymorphism, and intuitive set-like semantics of\neffects. In order to deal with scoping issues of higher-rank polymorphism, we\ndelay solving of effect constraints by transforming them into formulae of\npropositional logic. We prove soundness and completeness of our algorithm with\nrespect to a declarative type-and-effect system. All the presented results have\nbeen formalized in the Rocq proof assistant, and the algorithm has been\nsuccessfully implemented in a realistic programming language.", "AI": {"tldr": "This paper presents a sound, complete, and practical effect inference algorithm for expressive type-and-effect systems, resolving key challenges with higher-rank polymorphism using propositional logic. It is both theoretically validated and implemented in a working language.", "motivation": "Type-and-effect systems are powerful for organizing data and effects, but have not been widely adopted due to complexity and unsatisfactory inference algorithms compared to traditional type systems.", "method": "The paper introduces an effect inference algorithm tailored for a type-and-effect system featuring subtyping, higher-rank polymorphism, and set-like effect semantics. The algorithm addresses scoping problems by delaying constraint solving and rephrasing them as propositional logic formulae. Theoretical properties such as soundness and completeness are proven formally in the Rocq proof assistant, and the algorithm is practically implemented in a realistic programming language.", "result": "The algorithm proves to be sound and complete in comparison to the declarative type-and-effect system. It is both formally validated and practically realized.", "conclusion": "A promising, expressive, and formally proven effect inference algorithm is developed and shown to work in a real programming language, paving the way for broader adoption of type-and-effect systems."}}
{"id": "2510.20121", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20121", "abs": "https://arxiv.org/abs/2510.20121", "authors": ["Carlos J. Fernandez-Candel", "Jesus Garcia-Molina", "Francisco Javier Bermudez Ruiz", "Jose Ramon Hoyos Barcelo", "Diego Sevilla Ruiz", "Benito Jose Cuesta Viera"], "title": "Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience", "comment": "31 pages, 22 figures", "summary": "Model-driven software engineering (MDE) techniques are not only useful in\nforward engineering scenarios, but can also be successfully applied to evolve\nexisting systems. RAD (Rapid Application Development) platforms emerged in the\nnineties, but the success of modern software technologies motivated that a\nlarge number of enterprises tackled the migration of their RAD applications,\nsuch as Oracle Forms. Our research group has collaborated with a software\ncompany in developing a solution to migrate PL/SQL monolithic code on Forms\ntriggers and program units to Java code separated in several tiers.\n  Our research focused on the model-driven reengineering process applied to\ndevelop the migration tool for the conversion of PL/SQL code to Java. Legacy\ncode is represented in form of KDM (Knowledge-Discovery Metamodel) models. In\nthis paper, we propose a software process to implement a model-driven\nre-engineering. This process integrates a TDD-like approach to incrementally\ndevelop model transformations with three kinds of validations for the generated\ncode. The implementation and validation of the re-engineering approach are\nexplained in detail, as well as the evaluation of some issues related with the\napplication of MDE.", "AI": {"tldr": "This paper proposes a validated, model-driven process for migrating PL/SQL legacy code from RAD applications to Java, using KDM models and incremental TDD-inspired model transformation. The approach is effective and practical, with detailed implementation and validation results.", "motivation": "Many enterprises have legacy RAD applications (such as Oracle Forms with PL/SQL code) that require migration to modern software platforms, particularly Java. Existing migration approaches can be inefficient or error-prone, motivating the need for model-driven solutions.", "method": "The authors developed a model-driven reengineering process for migrating PL/SQL monolithic code to Java using a KDM (Knowledge-Discovery Metamodel)-based representation of legacy code. Their approach includes a TDD (Test-Driven Development)-like process for incremental development and validation of model transformations, with three types of validation for the generated code.", "result": "The paper presents an implemented migration tool and details its validation, showing the effectiveness of the model-driven approach. The authors analyze issues encountered during the application of model-driven engineering, showing that their process is robust and practical.", "conclusion": "Model-driven reengineering, integrating a TDD-like incremental and validated transformation development, is a viable and effective solution for migrating legacy PL/SQL-based RAD applications to modern Java platforms."}}
{"id": "2510.20547", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.20547", "abs": "https://arxiv.org/abs/2510.20547", "authors": ["Nikolaus Huber", "Susanne Graf", "Philipp R\u00fcmmer", "Wang Yi"], "title": "Compiling the Mimosa programming language to RTOS tasks", "comment": null, "summary": "This paper introduces a compilation scheme for programs written in the Mimosa\nprogramming language, which builds upon the MIMOS model of computation. Mimosa\ndescribes embedded systems software as a collection of time-triggered processes\nwhich communicate through FIFO queues. We formally describe an adaptation of\nthe Lustre compilation scheme to the semantics of Mimosa and show how the\ncoordination layer can be mapped to real-time operating system primitives.", "AI": {"tldr": "The paper adapts Lustre's compilation for Mimosa, detailing how its time-triggered communication processes can be mapped to real-time OS primitives, improving embedded system software deployment.", "motivation": "To provide a systematic way to compile Mimosa programs\u2014characterized by time-triggered, FIFO-based process communication\u2014so they can be effectively executed on real-time operating systems.", "method": "It utilizes a formal adaptation of the compilation process from Lustre, tailored for the semantic structure of Mimosa, and demonstrates the mapping of its coordination layer to RTOS primitives.", "result": "A compilation scheme is proposed and formally described, enabling programs written in Mimosa to leverage real-time OS features through proper mapping of process coordination.", "conclusion": "The paper presents a formal adaptation of the Lustre compilation scheme for the Mimosa programming language, successfully mapping its coordination layer to real-time operating system primitives."}}
{"id": "2510.20211", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20211", "abs": "https://arxiv.org/abs/2510.20211", "authors": ["Zhenning Yang", "Hui Guan", "Victor Nicolet", "Brandon Paulsen", "Joey Dodds", "Daniel Kroening", "Ang Chen"], "title": "Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents", "comment": null, "summary": "Cloud infrastructure is managed through a mix of interfaces -- traditionally,\ncloud consoles, command-line interfaces (CLI), and SDKs are the tools of\nchoice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have\nquickly gained popularity. Unlike conventional tools, IaC~frameworks encode the\ninfrastructure in a \"source-of-truth\" configuration. They are capable of\nautomatically carrying out modifications to the cloud -- deploying, updating,\nor destroying resources -- to bring the actual infrastructure into alignment\nwith the IaC configuration. However, when IaC is used alongside consoles, CLIs,\nor SDKs, it loses visibility into external changes, causing infrastructure\ndrift, where the configuration becomes outdated, and later IaC operations may\nundo valid updates or trigger errors.\n  We present NSync, an automated system for IaC reconciliation that propagates\nout-of-band changes back into the IaC program. Our key insight is that\ninfrastructure changes eventually all occur via cloud API invocations -- the\nlowest layer for cloud management operations. NSync gleans insights from API\ntraces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update\nthe IaC configuration to capture the changes). It employs an agentic\narchitecture that leverages LLMs to infer high-level intents from noisy API\nsequences, synthesize targeted IaC updates using specialized tools, and\ncontinually improve through a self-evolving knowledge base of past\nreconciliations. We further introduce a novel evaluation pipeline for injecting\nrealistic drifts into cloud infrastructure and assessing reconciliation\nperformance. Experiments across five real-world Terraform projects and 372\ndrift scenarios show that NSync outperforms the baseline both in terms of\naccuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\\times$\nimprovement).", "AI": {"tldr": "Traditional cloud management mixes IaC tools and manual interfaces, causing undetected changes ('drift'). NSync uses cloud API traces and LLMs to automatically detect and reconcile such drifts, keeping IaC configurations up-to-date. It surpasses existing solutions in accuracy and efficiency in real-world tests.", "motivation": "Infrastructure-as-Code (IaC) frameworks, like Terraform, are widely adopted for cloud infrastructure management due to their 'source-of-truth' approach. However, when IaC is used together with traditional tools (consoles, CLIs, SDKs), there can be out-of-band changes leading to infrastructure drift, causing the IaC configuration to become outdated and error-prone.", "method": "The authors present NSync, an automated system that detects and reconciles infrastructure drift by leveraging API traces. NSync uses a multi-component agentic architecture with LLMs to deduce high-level intents from API traces, synthesize targeted updates to the IaC configuration, and iteratively refine its performance using a knowledge base of reconciliations. The system includes a unique evaluation pipeline for simulating infrastructure drift and testing reconciliation effectiveness.", "result": "In experiments across five real-world Terraform projects and 372 drift scenarios, NSync demonstrated improved reconciliation accuracy (0.97 pass@3 versus the baseline's 0.71) and greater token efficiency (1.47\u00d7 improvement) compared to traditional methods.", "conclusion": "NSync effectively addresses the challenge of reconciling infrastructure drift in IaC-managed environments by leveraging cloud API traces and LLM-driven automation. This mitigates the risk of outdated configurations and errors when IaC is used alongside other management tools and demonstrates significant improvements in both accuracy and efficiency."}}
{"id": "2510.20688", "categories": ["cs.PL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20688", "abs": "https://arxiv.org/abs/2510.20688", "authors": ["Oliver Braunsdorf", "Tim Lange", "Konrad Hohentanner", "Julian Horsch", "Johannes Kinder"], "title": "SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe Code in Rust and Mixed-Language Applications", "comment": null, "summary": "Unsafe Rust code is necessary for interoperability with C/C++ libraries and\nimplementing low-level data structures, but it can cause memory safety\nviolations in otherwise memory-safe Rust programs. Sanitizers can catch such\nmemory errors at runtime, but introduce many unnecessary checks even for memory\naccesses guaranteed safe by the Rust type system. We introduce SafeFFI, a\nsystem for optimizing memory safety instrumentation in Rust binaries such that\nchecks occur at the boundary between unsafe and safe code, handing over the\nenforcement of memory safety from the sanitizer to the Rust type system. Unlike\nprevious approaches, our design avoids expensive whole-program analysis and\nadds much less compile-time overhead (2.64x compared to over 8.83x). On a\ncollection of popular Rust crates and known vulnerable Rust code, SafeFFI\nachieves superior performance compared to state-of-the-art systems, reducing\nsanitizer checks by up to 98%, while maintaining correctness and flagging all\nspatial and temporal memory safety violations.", "AI": {"tldr": "SafeFFI optimizes memory safety in Rust by shifting checks to the unsafe/safe boundary, drastically reducing runtime sanitizer overhead and compile-time cost, while fully preserving memory violation detection.", "motivation": "Unsafe Rust code is essential for interoperability with C/C++ and implementing low-level data structures, but poses risks to memory safety. Existing sanitizers add unnecessary runtime checks even when Rust's type system already guarantees safety.", "method": "SafeFFI is a system that instruments Rust binaries with memory safety checks only at the boundary between unsafe and safe code. This shifts responsibility for memory safety enforcement from the sanitizer to the Rust type system, minimizing the need for broad runtime checks. Their approach avoids expensive whole-program analysis and greatly reduces compile-time overhead.", "result": "SafeFFI reduces sanitizer checks by up to 98% compared to state-of-the-art systems, while still detecting all spatial and temporal memory safety violations. Compile-time overhead is significantly lowered (2.64x vs >8.83x with previous approaches), and performance is superior on popular Rust crates and vulnerable code.", "conclusion": "SafeFFI provides an efficient and correct method for ensuring memory safety in Rust programs containing unsafe code, optimizing runtime checks and compile-time costs while maintaining high detection rates for memory safety violations."}}
{"id": "2510.20340", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20340", "abs": "https://arxiv.org/abs/2510.20340", "authors": ["Serena Cofano", "Daniel Williams", "Aman Sharma", "Martin Monperrus"], "title": "Classport: Designing Runtime Dependency Introspection for Java", "comment": null, "summary": "Runtime introspection of dependencies, i.e., the ability to observe which\ndependencies are currently used during program execution, is fundamental for\nSoftware Supply Chain security. Yet, Java has no support for it. We solve this\nproblem with Classport, a system that embeds dependency information into Java\nclass files, enabling the retrieval of dependency information at runtime. We\nevaluate Classport on six real-world projects, demonstrating the feasibility in\nidentifying dependencies at runtime. Runtime dependency introspection with\nClassport opens important avenues for runtime integrity checking.", "AI": {"tldr": "Java programs previously couldn't track which dependencies were active during execution, posing security risks. Classport embeds dependency details into class files and allows programs to access this info at runtime. Tested on six projects, it proved effective and could help improve runtime security.", "motivation": "Java lacks built-in support for observing active dependencies during program execution, which is essential for securing the software supply chain.", "method": "Classport embeds dependency information into Java class files, enabling retrieval of dependency data during program execution. The system is evaluated on six real-world Java projects.", "result": "Classport successfully enables runtime identification of dependencies in tested Java projects, showing feasibility and potential for integrity checks.", "conclusion": "Classport provides a solution for runtime introspection of dependencies in Java, addressing a gap in software supply chain security."}}
{"id": "2510.20389", "categories": ["cs.SE", "cs.DC", "D.m"], "pdf": "https://arxiv.org/pdf/2510.20389", "abs": "https://arxiv.org/abs/2510.20389", "authors": ["Bjorn Remseth"], "title": "Symmetry in Software Platforms as an Architectural Principle", "comment": "Working paper, 11 pages", "summary": "Software platforms often act as structure preserving systems. They provide\nconsistent interfaces and behaviors that remain stable under specific\ntransformations that we denote as symmetries. This paper explores the idea that\narchitectural robustness emerges from enforcing such structural regularities", "AI": {"tldr": "Preserving structural regularities and symmetries in software platforms helps maintain robustness by ensuring stability across transformations.", "motivation": "To understand how enforcing structural regularities and symmetries enhances the robustness of software platforms.", "method": "Theoretical exploration of the concept of symmetries and structure preservation in software platforms, analyzing their impact on architectural robustness.", "result": "Identifies that the consistent interfaces and stable behaviors under transformations (symmetries) in software platforms contribute to robustness.", "conclusion": "Architectural robustness in software platforms can be achieved by preserving structural regularities and ensuring stable behaviors through symmetries."}}
{"id": "2510.20403", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20403", "abs": "https://arxiv.org/abs/2510.20403", "authors": ["Santiago Gil", "Ecem E. Ba\u015f", "Christian D. Jensen", "Sebastian Engelsgaard", "Giuseppe Abbiati", "Cl\u00e1udio Gomes"], "title": "FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards", "comment": "6 pages, Proceedings of the 2025 Annual Modeling and Simulation\n  Conference (ANNSIM)", "summary": "Distributed co-simulation plays a key role in enabling collaborative modeling\nand simulation by different stakeholders while protecting their Intellectual\nProperty (IP). Although IP protection is provided implicitly by co-simulation,\nthere is no consensus in the guidelines to conduct distributed co-simulation of\ncontinuous-time or hybrid systems with no exposure to potential hacking\nattacks. We propose an approach for distributed co-simulation on top of UniFMU\nwith enhanced cybersecurity and IP protection mechanisms, ensuring that the\nconnection is initiated by the client and the models and binaries live on\ntrusted platforms. We showcase the functionality of this approach using two\nco-simulation demos in four different network settings and analyze the\ntrade-off between IP-protected distribution and performance efficiency in these\nsettings.", "AI": {"tldr": "The paper introduces a UniFMU-based method for secure, IP-protected distributed co-simulation. It ensures client-initiated connections and keeps model binaries safe on trusted platforms, demonstrated in multiple network setups. The solution addresses security gaps, allowing safe collaboration with some impact on performance.", "motivation": "Distributed co-simulation enables multiple parties to collaboratively model and simulate complex systems, while keeping proprietary models and details confidential. However, existing practices implicitly protect intellectual property but lack formalized measures and guidelines for preventing hacking attacks or exposure.", "method": "The authors propose an approach built on UniFMU, adding specific cybersecurity and IP protection measures. Their method ensures that only the client initiates the connection and model binaries remain on trusted platforms. The approach is demonstrated through two co-simulation demos run across four network configurations, with a detailed analysis of IP-protection versus performance efficiency trade-offs.", "result": "The approach effectively enhances cybersecurity and IP protection during distributed co-simulation. The demo cases show how the solution works in practice, evaluating performance across different network settings and clarifying the balance between security and efficiency.", "conclusion": "The method strengthens IP protection and security in distributed co-simulation, providing practical guidelines for implementation. This enables stakeholders to safely engage in collaborative modeling without exposing sensitive models to risks, though some performance trade-offs are present."}}
{"id": "2510.20514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20514", "abs": "https://arxiv.org/abs/2510.20514", "authors": ["Lea Salome Brugger", "Xavier Denis", "Peter M\u00fcller"], "title": "Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia", "comment": null, "summary": "Deductive verification is an effective method to ensure that a given system\nexposes the intended behavior. In spite of its proven usefulness and\nfeasibility in selected projects, deductive verification is still not a\nmainstream technique. To pave the way to widespread use, we present a study\ninvestigating the factors enabling successful applications of deductive\nverification and the underlying issues preventing broader adoption. We\nconducted semi-structured interviews with 30 practitioners of verification from\nboth industry and academia and systematically analyzed the collected data\nemploying a thematic analysis approach. Beside empirically confirming familiar\nchallenges, e.g., the high level of expertise needed for conducting formal\nproofs, our data reveal several underexplored obstacles, such as proof\nmaintenance, insufficient control over automation, and usability concerns. We\nfurther use the results from our data analysis to extract enablers and barriers\nfor deductive verification and formulate concrete recommendations for\npractitioners, tool builders, and researchers, including principles for\nusability, automation, and integration with existing workflows.", "AI": {"tldr": "Deductive verification secures system behavior but isn't widely adopted. Through interviews, this study uncovers both familiar and new barriers, such as expertise needs and usability. It offers recommendations for overcoming these challenges to promote broader use.", "motivation": "Deductive verification can ensure the correct behavior of systems, but its adoption outside selected projects remains limited. The motivation is to understand what helps or hinders wider use of deductive verification.", "method": "The authors conducted semi-structured interviews with 30 verification practitioners from both industry and academia. The data was systematically analyzed via a thematic analysis approach.", "result": "The study confirmed familiar challenges (e.g., need for high expertise), and uncovered new obstacles (proof maintenance, lack of control over automation, usability issues). The analysis identified enablers and barriers and led to recommendations for enhancing deductive verification's usability, automation, and integration into workflows.", "conclusion": "Wider adoption of deductive verification is impeded by both well-known and less-explored barriers. Addressing issues of usability, automation, and integration, along with leveraging identified enablers, can foster broader and more effective use of this technique."}}
{"id": "2510.20521", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20521", "abs": "https://arxiv.org/abs/2510.20521", "authors": ["YingJian Xiao", "RongQun Hu", "WeiWei Gong", "HongWei Li", "AnQuan Jie"], "title": "Large Language Models for Fault Localization: An Empirical Study", "comment": "in Chinese language", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode-related tasks, particularly in automated program repair. However, the\neffectiveness of such repairs is highly dependent on the performance of\nupstream fault localization, for which comprehensive evaluations are currently\nlacking. This paper presents a systematic empirical study on LLMs in the\nstatement-level code fault localization task. We evaluate representative\nopen-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source\nmodels (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization\ncapabilities on the HumanEval-Java and Defects4J datasets. The study\ninvestigates the impact of different prompting strategies--including standard\nprompts, few-shot examples, and chain-of-reasoning--on model performance, with\na focus on analysis across accuracy, time efficiency, and economic cost\ndimensions. Our experimental results show that incorporating bug report context\nsignificantly enhances model performance. Few-shot learning shows potential for\nimprovement but exhibits noticeable diminishing marginal returns, while\nchain-of-thought reasoning's effectiveness is highly contingent on the model's\ninherent reasoning capabilities. This study not only highlights the performance\ncharacteristics and trade-offs of different models in fault localization tasks,\nbut also offers valuable insights into the strengths of current LLMs and\nstrategies for improving fault localization effectiveness.", "AI": {"tldr": "This study systematically evaluates multiple LLMs on code fault localization, finding that bug report context boosts results, while benefits from few-shot and reasoning approaches vary across models and reach a limit. It provides practical insights for choosing and improving LLM-driven fault localization.", "motivation": "Large language models have shown promise in automated program repair, but their effectiveness often depends on the quality of fault localization, which lacks thorough evaluation.", "method": "A systematic empirical study is performed, evaluating both open-source (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source models (GPT-4.1 mini, Gemini-2.5-flash) on statement-level fault localization. The study considers different prompting strategies (standard, few-shot, chain-of-reasoning), using HumanEval-Java and Defects4J datasets, and analyzes accuracy, time, and economic cost.", "result": "Incorporating bug report context significantly improves model performance. Few-shot learning provides some improvement but with diminishing returns, and the effectiveness of chain-of-thought strategies depends on the model's reasoning capabilities.", "conclusion": "The paper delineates performance characteristics and trade-offs of various LLMs for fault localization. It offers guidance on leveraging model strengths and optimizing prompting strategies for better fault localization."}}
{"id": "2510.20679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20679", "abs": "https://arxiv.org/abs/2510.20679", "authors": ["Jonas Klauke", "Tom Ohlmer", "Stefan Schott", "Serena Elisa Ponta", "Wolfram Fischer", "Eric Bodden"], "title": "A Soundness and Precision Benchmark for Java Debloating Tools", "comment": "Preprint - accepted at the ACM Workshop on Software Supply Chain\n  Offensive Research and Ecosystem Defenses (SCORED '25)", "summary": "Modern software development reuses code by importing libraries as\ndependencies. Software projects typically include an average of 36\ndependencies, with 80% being transitive, meaning they are dependencies of\ndependencies. Recent research indicates that only 24.9% of these dependencies\nare required at runtime, and even within those, many program constructs remain\nunused, adding unnecessary code to the project. This has led to the development\nof debloating tools that remove unnecessary dependencies and program constructs\nwhile balancing precision by eliminating unused constructs and soundness by\npreserving all required constructs. To systematically evaluate this trade-off,\nwe developed Deblometer, a micro-benchmark consisting of 59 test cases designed\nto assess support for various Java language features in debloating tools. Each\ntest case includes a manually curated ground truth specifying necessary and\nbloated classes, methods, and fields, enabling precise measurement of soundness\nand precision. Using Deblometer, we evaluated three popular Java debloating\ntools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools\nremove required program constructs, which results in changed semantics or\nexecution crashes. In particular, the dynamic class loading feature introduces\nunsoundness in all evaluated tools. Our comparison shows that Deptrim retains\nmore bloated constructs, while ProGuard removes more required constructs.\nJShrink's soundness is significantly affected by limited support for\nannotations, which leads to corrupted debloated artifacts. These soundness\nissues highlight the need to improve debloating tools to ensure stable and\nreliable debloated software.", "AI": {"tldr": "Most code debloating tools for Java struggle to precisely remove unused code without breaking software. Using a new benchmark, the paper shows all evaluated tools sometimes remove needed constructs, causing errors or instability, mainly due to poor support for dynamic loading and annotations. Improvements are needed for safer debloating.", "motivation": "Modern software projects import a large number of dependencies, many of which are not used at runtime, leading to bloated projects. There is a need for tools that can effectively remove such unnecessary code while preserving required functionality.", "method": "The authors developed Deblometer, a micro-benchmark with 59 manually curated test cases specifying ground truth about necessary and bloated classes, methods, and fields. This was used to systematically evaluate three popular Java debloating tools: Deptrim, JShrink, and ProGuard.", "result": "The evaluation showed that all three tools remove required program constructs, causing crashes or changing semantics. Dynamic class loading, in particular, led to unsoundness in all tools. Deptrim retains more bloated constructs, ProGuard removes more required constructs, and JShrink's soundness suffers due to poor annotation support.", "conclusion": "Current debloating tools struggle to balance removing unused code and preserving necessary functionality, resulting in reliability and stability issues for debloated software. There is a need for improving these tools, especially in handling dynamic features and annotations."}}
{"id": "2510.20692", "categories": ["cs.SE", "cs.AI", "cs.FL", "D.4.6; D.2.4; I.2.2; I.2.7; F.3.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2510.20692", "abs": "https://arxiv.org/abs/2510.20692", "authors": ["Adarsh Vatsa", "Bethel Hall", "William Eiers"], "title": "Exploring Large Language Models for Access Control Policy Synthesis and Summarization", "comment": "20 pages, 7 figures", "summary": "Cloud computing is ubiquitous, with a growing number of services being hosted\non the cloud every day. Typical cloud compute systems allow administrators to\nwrite policies implementing access control rules which specify how access to\nprivate data is governed. These policies must be manually written, and due to\ntheir complexity can often be error prone. Moreover, existing policies often\nimplement complex access control specifications and thus can be difficult to\nprecisely analyze in determining their behavior works exactly as intended.\nRecently, Large Language Models (LLMs) have shown great success in automated\ncode synthesis and summarization. Given this success, they could potentially be\nused for automatically generating access control policies or aid in\nunderstanding existing policies. In this paper, we explore the effectiveness of\nLLMs for access control policy synthesis and summarization. Specifically, we\nfirst investigate diverse LLMs for access control policy synthesis, finding\nthat: although LLMs can effectively generate syntactically correct policies,\nthey have permissiveness issues, generating policies equivalent to the given\nspecification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time\nfor reasoning LLMs. We then investigate how LLMs can be used to analyze\npolicies by introducing a novel semantic-based request summarization approach\nwhich leverages LLMs to generate a precise characterization of the requests\nallowed by a policy. Our results show that while there are significant hurdles\nin leveraging LLMs for automated policy generation, LLMs show promising results\nwhen combined with symbolic approaches in analyzing existing policies.", "AI": {"tldr": "LLMs can partially automate access control policy writing and greatly aid their analysis, but need improvements for reliable policy synthesis. Combining LLMs with symbolic approaches enhances policy understanding.", "motivation": "Manual creation of cloud access control policies is complex, error-prone, and hard to analyze. Automating policy synthesis and understanding is desirable.", "method": "The paper investigates the use of large language models (LLMs) for automatic generation and analysis of access control policies. It examines the policy synthesis capability of various LLMs and introduces a semantic-based request summarization approach leveraging LLMs for precise policy characterization.", "result": "LLMs can generate syntactically correct access control policies, but struggle with matching the intended specification (permissiveness issues), with success rates of 45.8% (non-reasoning LLMs) and 93.7% (reasoning LLMs). However, LLMs show promise when used with symbolic methods to analyze existing policies.", "conclusion": "While LLMs face challenges in automated policy generation, their combination with symbolic techniques yields promising results for policy analysis and summarization."}}
