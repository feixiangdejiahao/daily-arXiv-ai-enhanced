{"id": "2512.00106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00106", "abs": "https://arxiv.org/abs/2512.00106", "authors": ["Markus Funke", "Patricia Lago"], "title": "Injecting Sustainability in Software Architecture: A Rapid Review", "comment": "(Accepted/In press) 10th IEEE/ACM International Workshop on Green and Sustainable Software (GREENS '26): GREENS@ICSE 2026", "summary": "Sustainability has evolved from an emerging concern into a fundamental responsibility in software design, development, and operation. Research increasingly explores how sustainability can be systematically integrated into existing software engineering practices. Building on an industry-academia collaboration, we contribute to this discourse by conducting a mixed-method empirical study. We combine a rapid review of secondary studies with a focus group of practitioners. The review identifies challenges and opportunities in embedding sustainability in software architecture, while the focus group enriches and compares these findings. Based on the literature and industry synthesis, we derive five tangible takeaways to inform architects working in the field, and to guide our industry partners in the integration of sustainability concerns in architecture practices.", "AI": {"tldr": "This paper reviews literature and consults practitioners to identify challenges and strategies for embedding sustainability in software architecture and provides five actionable guidelines for professionals.", "motivation": "The motivation is to bridge the gap between sustainable software engineering theory and practice by providing industry-validated, actionable recommendations for integrating sustainability principles into software architecture.", "method": "The methodology involves a rapid literature review of secondary sources on sustainability in software architecture, complemented by a focus group discussion with industry practitioners. The results from both are compared and synthesized to extract practical guidance.", "result": "The paper presents a mixed-method empirical study combining a rapid review of secondary literature with a practitioner focus group. Key challenges and opportunities in implementing sustainability within software architecture are identified. Findings from both methods are synthesized to distill five practical recommendations for software architects and industry partners, aimed at better integrating sustainability into their practices.", "conclusion": "A set of five concrete takeaways are proposed for software architects and industry partners to effectively incorporate sustainability in software architectural practices, informed by a combination of academic and industry perspectives."}}
{"id": "2512.00127", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.00127", "abs": "https://arxiv.org/abs/2512.00127", "authors": ["Shailja Thakur", "Vaibhav Saxena", "Rohan Kulkarni", "Shivdeep Singh", "Parameswaran Selvam", "Hima Patel", "Hiroshi Kanayama"], "title": "Generating Verifiable CoT from Execution-Traces", "comment": null, "summary": "Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.\n  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to 30 points on output prediction and 28 points on input prediction over base models, alongside improved explanation and code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. https://github.ibm.com/IBM-Research-AI/Verified-Code-CoT", "AI": {"tldr": "By grounding model reasoning steps in real code execution traces rather than synthetic explanations, the authors create more reliable training data, enabling significant improvements in code reasoning and generation tasks.", "motivation": "Current CoT training data for code reasoning often contains plausible but logically flawed explanations, leading models to mimic incorrect reasoning patterns. The paper aims to eliminate this issue by tying reasoning directly to what the code actually computes.", "method": "The authors developed a pipeline that instruments code to capture its dynamic execution traces and then narrates these traces into natural language rationales. This creates Chain-of-Thought (CoT) sequences grounded in verifiable code behavior, ensuring logical correctness in training data.", "result": "Models trained using this execution-grounded, bi-directional CoT data showed up to 30 points improvement in forward output prediction and 28 points in backward input prediction on benchmark code reasoning tasks, and also improved code explanation and generation performance.", "conclusion": "Grounding reasoning in verifiable code execution traces removes logical hallucinations and dramatically enhances code understanding in language models, as validated by strong benchmark gains."}}
{"id": "2512.00134", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00134", "abs": "https://arxiv.org/abs/2512.00134", "authors": ["Parisa Hamedi", "Hamed Jelodar", "Samita Bai", "Mohammad Meymani", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "Asm2SrcEval: Evaluating Large Language Models for Assembly-to-Source Code Translation", "comment": null, "summary": "Assembly-to-source code translation is a critical task in reverse engineering, cybersecurity, and software maintenance, yet systematic benchmarks for evaluating large language models on this problem remain scarce. In this work, we present the first comprehensive evaluation of five state-of-the-art large language models on assembly-to-source translation. We assess model performance using a diverse set of metrics capturing lexical similarity (BLEU, ROUGE, and METEOR), semantic alignment (BERTScore), fluency (Perplexity), and efficiency (time prediction). Our results reveal clear trade-offs: while certain models excel in text similarity metrics, others demonstrate lower perplexity or faster inference times. We further provide qualitative analyses of typical model successes and failure cases, highlighting challenges such as control flow recovery and identifier reconstruction. Taken together, our benchmark offers actionable insights into the strengths and limitations of current large language models for program translation, establishing a foundation for future research in combining accuracy with efficiency for real-world applications.", "AI": {"tldr": "This paper benchmarks five large language models for assembly-to-source code translation, showing that models excel differently across accuracy, fluency, and efficiency metrics, and discusses key translation challenges.", "motivation": "Systematic benchmarks for evaluating how well LLMs translate assembly code to source code are lacking, despite the importance of this task in reverse engineering and cybersecurity. This work aims to fill that gap, providing insights for both research and practical applications.", "method": "The study assesses five leading LLMs using diverse quantitative metrics and qualitative analysis. It evaluates lexical and semantic similarity, fluency, and efficiency, providing both statistical results and case studies on model behavior.", "result": "The paper evaluates five state-of-the-art large language models (LLMs) on the task of translating assembly code to source code. Using a comprehensive suite of metrics\u2014including BLEU, ROUGE, METEOR (for lexical similarity), BERTScore (for semantic alignment), perplexity (for fluency), and prediction time (for efficiency)\u2014the study analyzes the models\u2019 strengths and weaknesses. The results show that different models have trade-offs, excelling in some metrics but not others. The paper also provides qualitative insights into common challenges such as reconstructing control flow and identifiers.", "conclusion": "Current large language models exhibit notable strengths and weaknesses in assembly-to-source code translation. No single model is superior across all evaluated aspects, and significant challenges remain, especially in reconstructing code structure and identifiers. The benchmark set by this research offers a basis for developing more balanced and effective models in future."}}
{"id": "2512.00215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00215", "abs": "https://arxiv.org/abs/2512.00215", "authors": ["Mohammad Abdollahi", "Khandaker Rifah Tasnia", "Soumit Kanti Saha", "Jinqiu Yang", "Song Wang", "Hadi Hemmati"], "title": "Demystifying Errors in LLM Reasoning Traces: An Empirical Study of Code Execution Simulation", "comment": null, "summary": "Understanding a program's runtime reasoning behavior, meaning how intermediate states and control flows lead to final execution results, is essential for reliable code generation, debugging, and automated reasoning. Although large language models (LLMs) can accurately predict program outputs, most prior work has focused on output accuracy and performance, treating reasoning as a black box. As a result, little is known about the structure or failure modes of their reasoning traces. To address this gap, we conduct the first empirical study on runtime behavior inference with reasoning LLMs, aiming to uncover and characterize errors in their reasoning traces. We curate a benchmark from HumanEval Plus and LiveCodeBench, containing 427 code snippets. For each snippet, we test three input types: regular, edge, and invalid. Twelve input values are selected per snippet, each paired with its ground-truth execution result. We evaluate four state-of-the-art reasoning LLMs. Our results show that these models reach accuracies between 85 percent and 98 percent across input types. We also analyze the produced reasoning traces and develop a taxonomy with nine categories of inference errors. Finally, we explore tool-augmented reasoning. Using failures in the Computation Errors category as a case study, our experiments show that this approach corrects 58 percent of such errors, demonstrating the potential of tool support for improving LLM reasoning.", "AI": {"tldr": "The paper systematically analyzes how well LLMs trace and reason about code execution, identifies common error types, and shows that using tools can fix many reasoning errors.", "motivation": "Most previous LLM research focuses on output prediction in a black-box setting, with little understanding of how LLMs reason through code execution. This study aims to characterize the reasoning process, error structures, and improvement strategies for LLMs in code-related tasks.", "method": "The authors employ a benchmark drawn from HumanEval Plus and LiveCodeBench, test twelve input values (regular/edge/invalid) per code snippet, and compare reasoning traces from four LLMs. They classify reasoning errors and quantify the impact of external tools in correcting these errors.", "result": "The paper provides an empirical study on how reasoning LLMs handle program runtime behavior. It evaluates four LLMs on a curated benchmark of 427 code snippets, using various input types (regular, edge, invalid). Accuracies range from 85% to 98%. The authors introduce a taxonomy with nine types of inference errors, and show that tool-augmented reasoning can correct 58% of certain error types.", "conclusion": "LLMs can achieve high accuracy in program reasoning tasks, but they exhibit systematic inference errors. Tool augmentation can significantly improve their performance, particularly in computation-related reasoning failures."}}
{"id": "2512.01036", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2512.01036", "abs": "https://arxiv.org/abs/2512.01036", "authors": ["Breandan Considine"], "title": "A Word Sampler for Well-Typed Functions", "comment": "2 pages", "summary": "We describe an exact sampler for a simply-typed, first-order functional programming language. Given an acyclic finite automaton, $\u03b1_{\\varnothing}$, it samples a random function uniformly without replacement from well-typed functions in $\\mathcal{L}(\u03b1_{\\varnothing})$. This is achieved via a fixed-parameter tractable reduction from a syntax-directed type system to a context-free grammar, preserving type soundness and completeness w.r.t. $\\mathcal{L}(\u03b1_{\\varnothing})$, while retaining the robust metatheory of formal languages.", "AI": {"tldr": "The paper introduces a way to uniformly sample well-typed functions (without repeats) from a given set defined by an automaton, by translating type systems into context-free grammars.", "motivation": "Uniform and exact sampling of well-typed functions from a constrained set is important for applications in testing, synthesis, and verification of functional programs. Translating between type systems and formal languages provides both theoretical guarantees and practical sampling tools.", "method": "The method is a fixed-parameter tractable reduction from the language's type system to a context-free grammar. This reduction maintains type soundness and completeness and leverages formal language theory for sampling well-typed programs.", "result": "The paper presents an exact sampler for a simply-typed, first-order functional programming language. This sampler takes an acyclic finite automaton and samples uniformly (without replacement) from all well-typed functions that the automaton accepts. The key technical contribution is a reduction from a syntax-directed type system to a context-free grammar, ensuring that type soundness and completeness are preserved relative to the automaton's language, $\ncal{L}(\u03b1_{\nvarnothing})$.", "conclusion": "The approach allows for exact, uniform, and non-redundant sampling of well-typed functions accepted by a defined automaton, all while preserving important type and language-theoretic properties."}}
{"id": "2512.00231", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00231", "abs": "https://arxiv.org/abs/2512.00231", "authors": ["Monique Louise Monteiro", "George G. Cabral", "Adriano L. I. OLiveira"], "title": "CodeFlowLM: Incremental Just-In-Time Defect Prediction with Pretrained Language Models and Exploratory Insights into Defect Localization", "comment": null, "summary": "This work introduces CodeFlowLM, an incremental learning framework for Just-In-Time Software Defect Prediction (JIT-SDP) that leverages pre-trained language models (PLMs). Unlike traditional online learners, CodeFlowLM employs continual fine-tuning to address concept drift, class imbalance, and verification latency without retraining from scratch. We evaluated encoder-only and encoder-decoder PLMs (notably CodeT5+ and UniXCoder) in JIT-SDP scenarios within and between projects, comparing them with the incremental baseline BORB. The results show that CodeFlowLM achieves up to 68% G-Mean gains, confirming its superior adaptability and robustness in evolving software environments. We further extend the analysis to Just-in-Time Defect Localization (JIT-DL), benchmarking Large Language Models (LLMs) such as GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro against attention-based models. GPT-5 delivers comparable performance for Recall@20% and Effort@20% with higher stability, although attention-based methods retain an advantage in fine-grained ranking metrics (Top-k, IFA). A qualitative error analysis reveals that most false positives arise from (1) human-like conservative bias, (2) insufficient contextual information in diff-based prompts, and (3) potential dataset mislabeling in JIT-Defects4J. These findings highlight both the promise and the current limitations of LLM reasoning in defect localization. False negatives occur in smaller proportions. Overall, CodeFlowLM significantly advances the state of the art in incremental JIT-SDP, demonstrating superior adaptability and robustness in evolving software environments. Furthermore, our exploratory analysis of LLMs in JIT-DL not only benchmarks their performance against established attention-based models but also provides critical insights into the current limitations of prompt-based defect reasoning.", "AI": {"tldr": "CodeFlowLM, using continual fine-tuning of PLMs, significantly improves defect prediction in evolving software environments compared to baseline methods. Large Language Models also show promise for defect localization but remain limited by current prompt and dataset challenges.", "motivation": "Traditional online defect prediction models struggle with concept drift, class imbalance, and latency, requiring frequent retraining. The motivation is to leverage PLMs for incremental learning in defect prediction and localization, improving adaptability and robustness as software evolves and exploring the efficacy of LLMs in fine-grained defect localization tasks.", "method": "The authors used continual fine-tuning of both encoder-only and encoder-decoder PLMs (including CodeT5+ and UniXCoder) for incremental learning in JIT-SDP scenarios. They benchmarked incremental and attention-based models, as well as LLMs like GPT-5, Claude Sonnet, and Gemini, across various defect prediction and localization metrics and performed qualitative error analysis on model outputs.", "result": "CodeFlowLM, an incremental learning framework based on continually fine-tuned pre-trained language models (PLMs), outperforms traditional online learners such as BORB for Just-In-Time Software Defect Prediction (JIT-SDP). It achieves up to 68% G-Mean improvement and demonstrates greater adaptability and robustness within and across projects. Additionally, in Just-In-Time Defect Localization (JIT-DL), Large Language Models (LLMs) like GPT-5 show comparable performance to attention-based models for certain recall and effort metrics, though attention models excel at fine-grained rankings. Qualitative error analysis identifies conservative bias, context limitations, and dataset mislabeling as major sources of false positives.", "conclusion": "The study establishes CodeFlowLM as a new state-of-the-art for incremental JIT-SDP, offering unparalleled adaptability and robustness. The analysis of LLMs in JIT-DL reveals both their strong potential and current weaknesses in reasoning and context handling, marking areas for future improvement."}}
{"id": "2512.00250", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00250", "abs": "https://arxiv.org/abs/2512.00250", "authors": ["Shrinivass Arunachalam Balasubramanian"], "title": "ng-reactive-lint: Smarter Linting for Angular Apps", "comment": null, "summary": "Reactivity is central to Angular applications, yet subtle misuse of Observables, Signals, and change-detection often leads to performance regressions that are difficult to diagnose. Although Angular 17 introduced a unified, signal-first model, most enterprise codebases still rely heavily on legacy RxJS patterns that create unpredictable update flows, memory leaks, and excessive change cycles. To address these issues, we developed ng-reactive-lint, a deterministic static analysis tool that understands Angular's component semantics, lifecycle hooks, template bindings, and reactivity patterns. Unlike generic ESLint or RxJS plugins, ng-reactive-lint performs framework-aware analysis to detect high-impact anti-patterns and provide actionable, context-specific fixes. Evaluation across five large real-world projects showed reductions of up to threefold in unnecessary change detection cycles and up to 75% lower peak memory usage. The tool offers a practical, automated path to adopting modern Angular reactivity at scale.", "AI": {"tldr": "ng-reactive-lint is a static analysis tool that detects and fixes Angular reactivity anti-patterns, delivering significant performance improvements and enabling easier migration to modern practices.", "motivation": "Angular's core reactivity model is often misused, particularly with legacy RxJS patterns, leading to difficult-to-diagnose performance issues such as memory leaks and excessive change detection. The motivation is to provide automated, practical tools to facilitate correct, efficient adoption of modern Angular reactivity patterns across large codebases.", "method": "The authors developed a deterministic static analysis tool that deeply understands Angular-specific constructs like component semantics, lifecycle hooks, template bindings, and reactivity patterns. The tool analyzes code, detects anti-patterns, and proposes context-aware fixes. Its effectiveness was validated empirically across five large, real-world Angular projects.", "result": "The authors developed ng-reactive-lint, a deterministic static analysis tool, and evaluated it on five large real-world Angular projects. The tool achieved up to a threefold reduction in unnecessary change detection cycles and up to 75% lower peak memory usage.", "conclusion": "ng-reactive-lint provides actionable feedback and fixes for Angular reactivity misuse, outperforming generic linting tools by leveraging Angular framework-specific knowledge. It enables practical modernization of Angular applications' reactivity with measurable performance benefits."}}
{"id": "2512.00325", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00325", "abs": "https://arxiv.org/abs/2512.00325", "authors": ["Shaira Sadia Karim", "Abrar Mahmud Rahim", "Lamia Alam", "Ishmam Tashdeed", "Lutfun Nahar Lota", "Md. Abu Raihan M. Kamal", "Md. Azam Hossain"], "title": "Progressive Code Integration for Abstractive Bug Report Summarization", "comment": null, "summary": "Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.", "AI": {"tldr": "A new framework integrates both textual and code information into LLM-based bug report summaries, outperforming previous approaches.", "motivation": "Bug reports often include unstructured and verbose text alongside important code snippets, but existing methods focus mainly on text and disregard code, leading to incomplete or redundant summaries. The work aims to address this shortcoming for better defect comprehension.", "method": "The proposed method incrementally adds long code snippets to textual content during summarization using LLMs, thus bypassing the standard context window limitations. It is evaluated on four benchmark datasets and eight LLMs, showing strong performance.", "result": "The authors introduce a progressive code-integration framework for summarizing bug reports using large language models (LLMs). Unlike traditional methods that only consider text, their approach progressively incorporates long code snippets into the summarization process, overcoming context window limitations in LLMs. The method shows substantial improvements, outperforming extractive baselines by 7.5%-58.2% and matching state-of-the-art abstractive summarization methods across four benchmark datasets and eight LLMs.", "conclusion": "Integrating both textual data and code snippets leads to more informative and accurate bug report summaries, and the proposed method effectively handles long code inputs within LLM constraints."}}
{"id": "2512.00380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00380", "abs": "https://arxiv.org/abs/2512.00380", "authors": ["Mingwei Liu", "Zheng Pei", "Yanlin Wang", "Zihao Wang", "Zikang Li", "Enci Lin", "Xin Peng", "Zibin Zheng"], "title": "Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS", "comment": null, "summary": "In the context of software frameworks with limited resources (such as HarmonyOS), large language models (LLMs) often exhibit poor code generation performance because they lack sufficient exposure to such environments during pre-training. Although LLMs can usually maintain correct logical structures across programming languages, they frequently struggle when dealing with framework-specific APIs or syntax, resulting in errors. This indicates that while pre-training equips LLMs with general algorithmic capabilities, they remain unfamiliar with the distinctive syntax and API usage of underrepresented frameworks. As a result, even advanced commercial models like GPT-4o cannot reliably generate correct code without prior adaptation. To address this issue, we propose APIKG4SYN, a framework designed to exploit API knowledge graphs for the construction of API-oriented question-code pairs, specifically tailored for low-resource frameworks without requiring executable code. APIKG4SYN integrates both single-API and multi-API knowledge, where the latter is derived through uncertainty estimation (UE)-driven Monte Carlo Tree Search (MCTS), enabling the creation of a diverse and informative dataset for fine-tuning LLMs. Using HarmonyOS as a case study, we build the first benchmark for HarmonyOS code generation. Experimental results show that fine-tuning Qwen with APIKG4SYN raises pass@1 accuracy to 25.00%, compared with 17.59% for the baseline GPT model. These results confirm that API-oriented data significantly enhance LLM performance in low-resource software development scenarios.", "AI": {"tldr": "Large language models (LLMs) struggle with code generation for low-resource frameworks like HarmonyOS due to lack of pre-training exposure. The authors propose APIKG4SYN, a framework using API knowledge graphs to generate tailored code-data for fine-tuning LLMs, improving their performance.", "motivation": "Current LLMs, including top commercial models, perform poorly when generating code for underrepresented frameworks due to unfamiliarity with unique APIs and syntax; addressing this gap can unlock their potential in diverse development environments.", "method": "APIKG4SYN constructs API-oriented question-code datasets via single/multi-API knowledge graphs and uncertainty estimation-driven Monte Carlo Tree Search, enabling dataset creation for fine-tuning without executable code.", "result": "Fine-tuning Qwen LLM with APIKG4SYN-generated data increased code generation accuracy (pass@1) to 25.00%, compared to 17.59% for the baseline, demonstrating significant performance improvement.", "conclusion": "API-oriented data generation with APIKG4SYN notably boosts LLM code generation accuracy in low-resource software frameworks, verifying the importance of framework-specific data for effective adaptation."}}
{"id": "2512.00556", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00556", "abs": "https://arxiv.org/abs/2512.00556", "authors": ["Sina Salimian", "Gias Uddin", "Sumon Biswas", "Henry Leung"], "title": "Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations", "comment": null, "summary": "The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.", "AI": {"tldr": "This paper introduces a metamorphic testing framework using six new Metamorphic Relations to systematically evaluate and mitigate hidden social biases in LLMs. Their approach detects 14% more hidden biases than existing tools and boosts safe response rates to nearly 89% after fine-tuning, demonstrating strong improvements in conversational AI fairness.", "motivation": "Large Language Models (LLMs) often exhibit subtle social biases in their outputs. Existing guardrails are insufficient, especially for indirect or contextually complex prompts. There is a need for more systematic evaluation and mitigation methods to improve fairness.", "method": "The paper proposes a unified framework using six novel Metamorphic Relations (MRs) based on metamorphic testing principles. These MRs transform direct bias-inducing inputs into semantically equivalent, adversarial variants, enabling automated detection of hidden biases. The same MRs also facilitate the generation of diverse bias-inducing samples for fine-tuning LLMs.", "result": "The framework was tested on six state-of-the-art LLMs with 385 BiasAsker benchmark questions covering seven protected groups. The MRs revealed up to 14% more hidden biases than previous tools. Fine-tuning with both original and MR-mutated samples increased safe response rates from 54.7% to over 88.9% across models.", "conclusion": "Metamorphic relations are an effective and practical approach for both exposing and mitigating hidden social biases in LLMs, leading to significant improvements in fairness and safe response rates."}}
{"id": "2512.00560", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00560", "abs": "https://arxiv.org/abs/2512.00560", "authors": ["Jinyu Cai", "Jialong Li", "Nianyu Li", "Zhenyu Mao", "Mingyue Zhang", "Kenji Tei"], "title": "SAGE: Semantic-Aware Gray-Box Game Regression Testing with Large Language Models", "comment": "This paper has been submitted to the Automated Software Engineering journal", "summary": "The rapid iteration cycles of modern live-service games make regression testing indispensable for maintaining quality and stability. However, existing regression testing approaches face critical limitations, especially in common gray-box settings where full source code access is unavailable: they heavily rely on manual effort for test case construction, struggle to maintain growing suites plagued by redundancy, and lack efficient mechanisms for prioritizing relevant tests. These challenges result in excessive testing costs, limited automation, and insufficient bug detection. To address these issues, we propose SAGE, a semanticaware regression testing framework for gray-box game environments. SAGE systematically addresses the core challenges of test generation, maintenance, and selection. It employs LLM-guided reinforcement learning for efficient, goal-oriented exploration to automatically generate a diverse foundational test suite. Subsequently, it applies a semantic-based multi-objective optimization to refine this suite into a compact, high-value subset by balancing cost, coverage, and rarity. Finally, it leverages LLM-based semantic analysis of update logs to prioritize test cases most relevant to version changes, enabling efficient adaptation across iterations. We evaluate SAGE on two representative environments, Overcooked Plus and Minecraft, comparing against both automated baselines and human-recorded test cases. Across all environments, SAGE achieves superior bug detection with significantly lower execution cost, while demonstrating strong adaptability to version updates.", "AI": {"tldr": "SAGE uses AI-guided techniques to automate and optimize regression testing in gray-box game settings, leading to improved bug detection and reduced costs.", "motivation": "Current regression testing methods in live-service games are inefficient, costly, and require manual effort, especially without full access to source code. Redundant test suites and inadequate automation hinder effective bug detection.", "method": "SAGE, a semantic-aware regression testing framework for gray-box game environments, utilizes LLM-guided reinforcement learning for automatic test generation, semantic-based multi-objective optimization for maintaining test suites, and LLM-based semantic analysis for test case prioritization.", "result": "SAGE outperformed automated baselines and human-crafted tests in bug detection across Overcooked Plus and Minecraft, with lower execution costs and better adaptability to version updates.", "conclusion": "SAGE systematically solves test generation, maintenance, and selection challenges, making regression testing in live-service games more effective, automated, and adaptable, even without full source access."}}
{"id": "2512.00571", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.00571", "abs": "https://arxiv.org/abs/2512.00571", "authors": ["Tarun Chintada", "Uday Kiran Cheera"], "title": "Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization", "comment": "12 pages, 3 figures, 2 tables. Research conducted in June 2024", "summary": "Analogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble.", "AI": {"tldr": "This paper introduces a hybrid FAABE model that uses the Firefly Algorithm to optimize Analogy-Based Estimation for software projects, resulting in more accurate predictions on benchmark datasets.", "motivation": "ABE is commonly used for estimation but lacks optimal approaches for reliable and accurate project prediction, especially for novel software projects.", "method": "The authors combined Firefly Algorithm for optimization with ABE, applied feature selection for efficiency, and evaluated predictions across several datasets using evaluation metrics like MMRE, MAE, MSE, and RMSE.", "result": "The proposed FAABE model, which combines the Firefly Algorithm with ABE, achieved higher prediction precision on five public datasets compared to traditional models, as evidenced by multiple error metrics.", "conclusion": "Incorporating the Firefly Algorithm into analogy-based estimation significantly improves prediction accuracy, suggesting it is a promising direction for software project effort estimation."}}
{"id": "2512.00651", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00651", "abs": "https://arxiv.org/abs/2512.00651", "authors": ["Mohammed Latif Siddiq", "Arvin Islam-Gomes", "Natalie Sekerak", "Joanna C. S. Santos"], "title": "Large Language Models for Software Engineering: A Reproducibility Crisis", "comment": "Submitted to Empirical Software Engineering (EMSE) journal; 112 pages (81 pages of references)", "summary": "Reproducibility is a cornerstone of scientific progress, yet its state in large language model (LLM)-based software engineering (SE) research remains poorly understood. This paper presents the first large-scale, empirical study of reproducibility practices in LLM-for-SE research. We systematically mined and analyzed 640 papers published between 2017 and 2025 across premier software engineering, machine learning, and natural language processing venues, extracting structured metadata from publications, repositories, and documentation. Guided by four research questions, we examine (i) the prevalence of reproducibility smells, (ii) how reproducibility has evolved over time, (iii) whether artifact evaluation badges reliably reflect reproducibility quality, and (iv) how publication venues influence transparency practices. Using a taxonomy of seven smell categories: Code and Execution, Data, Documentation, Environment and Tooling, Versioning, Model, and Access and Legal, we manually annotated all papers and associated artifacts. Our analysis reveals persistent gaps in artifact availability, environment specification, versioning rigor, and documentation clarity, despite modest improvements in recent years and increased adoption of artifact evaluation processes at top SE venues. Notably, we find that badges often signal artifact presence but do not consistently guarantee execution fidelity or long-term reproducibility. Motivated by these findings, we provide actionable recommendations to mitigate reproducibility smells and introduce a Reproducibility Maturity Model (RMM) to move beyond binary artifact certification toward multi-dimensional, progressive evaluation of reproducibility rigor.", "AI": {"tldr": "A large-scale analysis of 640 LLM-for-SE research papers reveals ongoing reproducibility challenges in key areas, limited effectiveness of artifact badges, and the need for multi-dimensional evaluation\u2014culminating in actionable recommendations and a proposed maturity model to enhance reproducibility.", "motivation": "Despite the critical importance of reproducibility in scientific research, the extent to which it is addressed in LLM-based software engineering (SE) research is unclear. There is a need to systematically analyze current reproducibility practices to identify gaps and areas for improvement.", "method": "The authors conducted a large-scale empirical study by systematically mining and analyzing 640 papers from 2017 to 2025 across top SE, machine learning, and NLP venues. They extracted metadata from publications, repositories, and documentation, and manually annotated each paper and associated artifact using a taxonomy of seven reproducibility smell categories. They analyzed trends, artifact evaluation badges, and venue practices through four key research questions.", "result": "The study uncovered persistent shortcomings in artifact availability, environment specification, versioning rigor, and documentation clarity, despite slight improvements and greater artifact evaluation at top venues. They found that artifact evaluation badges often indicate presence, but do not ensure fidelity or reproducibility. Based on their findings, the authors provided recommendations and proposed a Reproducibility Maturity Model (RMM) for better evaluation.", "conclusion": "Current LLM-for-SE research exhibits significant reproducibility issues, which are only partially addressed by existing certification practices. A multi-dimensional, progressive approach, such as the proposed RMM, is needed to strengthen reproducibility rigor in the field."}}
{"id": "2512.00766", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00766", "abs": "https://arxiv.org/abs/2512.00766", "authors": ["Zenghui Zhou", "Yuechen Li", "Yi Cai", "Jinlong Wen", "Xiaohan Yu", "Zheng Zheng", "Beibei Yin"], "title": "Code Comments for Quantum Software Development Kits: An Empirical Study on Qiskit", "comment": "Zenghui Zhou and Yuechen Li contributed equally to this work. Corresponding author is Zheng Zheng", "summary": "Quantum computing is gaining attention from academia and industry. With the quantum Software Development Kits (SDKs), programmers can develop quantum software to explore the power of quantum computing. However, programmers may face challenges in understanding quantum software due to the non-intuitive quantum mechanics. To facilitate software development and maintenance, code comments offered in quantum SDKs serve as a natural language explanation of program functionalities and logical flows. Despite their importance, scarce research systematically reports their value and provides constructive guidelines for programmers. To address this gap, our paper focuses on Qiskit, one of the most popular quantum SDKs, and presents CC4Q, the first dataset of code comments for quantum computing. CC4Q incorporates 9677 code comment pairs and 21970 sentence-level code comment units, the latter of which involve heavy human annotation. Regarding the annotation, we validate the applicability of the developer-intent taxonomy used in classical programs, and also propose a new taxonomy considering quantum-specific knowledge. We conduct an empirical study comprehensively interpreting code comments from three perspectives: comment structure and coverage, developers' intentions, and associated quantum topics. Our findings uncover key differences in code comments between classical and quantum software, and also outline quantum-specific knowledge relevant to quantum software development.", "AI": {"tldr": "This paper introduces CC4Q, the first dataset of quantum SDK code comments (from Qiskit), featuring extensive annotations and a novel taxonomy for quantum code documentation. Their analysis reveals quantum-specific characteristics in code comments, highlighting the need for dedicated documentation guidelines for quantum software.", "motivation": "Quantum SDKs like Qiskit help bridge programmer understanding of quantum computing, but due to the complexity and non-intuitive nature of quantum mechanics, there are challenges in comprehending quantum software. Code comments are crucial for explaining program logic and facilitating maintenance, yet little research has been done on their value or how best to create them in the quantum context.", "method": "The authors present CC4Q, a new dataset of code comments for quantum computing based on Qiskit, comprising thousands of comment pairs and sentence-level annotations. Annotations are validated using both traditional developer-intent taxonomy and a new taxonomy that accounts for quantum-specific knowledge. An empirical study is conducted to analyze comments by their structure, developer intentions, and relevant quantum topics.", "result": "CC4Q dataset includes 9677 comment pairs and 21970 annotated comment units. The study demonstrates that quantum code comments differ substantially from classical ones, particularly in terms of developer intentions and the necessity for quantum-specific explanations. The research also outlines a new taxonomy for classifying comment intentions in quantum software.", "conclusion": "Quantum software requires distinct commenting practices, with specialized knowledge and structures not found in classical software. The CC4Q dataset and new taxonomy provide foundational resources for further research and guideline development in quantum software documentation."}}
{"id": "2512.00844", "categories": ["cs.SE", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00844", "abs": "https://arxiv.org/abs/2512.00844", "authors": ["Giles Winchester", "George Parisis", "Luc Berthouze"], "title": "FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity", "comment": "13 pages, 6 figures, 2 tables", "summary": "Microservices have transformed software architecture through the creation of modular and independent services. However, they introduce operational complexities in service integration and system management that makes swift and accurate anomaly detection and localisation challenging. Despite the complex, dynamic, and interconnected nature of microservice architectures, prior works that investigate metrics for anomaly detection rarely include explicit information about time-varying interdependencies. And whilst prior works on fault localisation typically do incorporate information about dependencies between microservices, they scale poorly to real world large-scale deployments due to their reliance on computationally expensive causal inference. To address these challenges we propose FC-ADL, an end-to-end scalable approach for detecting and localising anomalous changes from microservice metrics based on the neuroscientific concept of functional connectivity. We show that by efficiently characterising time-varying changes in dependencies between microservice metrics we can both detect anomalies and provide root cause candidates without incurring the significant overheads of causal and multivariate approaches. We demonstrate that our approach can achieve top detection and localisation performance across a wide degree of different fault scenarios when compared to state-of-the-art approaches. Furthermore, we illustrate the scalability of our approach by applying it to Alibaba's extremely large real-world microservice deployment.", "AI": {"tldr": "FC-ADL uses functional connectivity to efficiently detect and localize anomalies in large-scale microservice architectures, outperforming existing methods and demonstrating scalability in real-world settings.", "motivation": "Microservices revolutionized software but made anomaly detection and fault localization complex due to dynamic interdependencies and operational overhead. Existing methods either ignore time-varying relationships or don't scale to large deployments.", "method": "They propose FC-ADL, a scalable end-to-end anomaly detection and localization technique using the concept of functional connectivity from neuroscience. It efficiently models time-varying dependencies from microservice metrics, bypassing computationally expensive causal inference.", "result": "FC-ADL detects anomalies and suggests root causes with top performance across diverse fault scenarios, outperforming state-of-the-art methods. The approach is shown to be highly scalable, including successful application to Alibaba's large-scale deployment.", "conclusion": "FC-ADL offers a practical and scalable solution for anomaly detection and localization in microservices, balancing performance and overhead for real-world system management."}}
{"id": "2512.00855", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00855", "abs": "https://arxiv.org/abs/2512.00855", "authors": ["Miikka Kuutila", "Paul Ralph", "Huilian Sophie Qiu", "Ronnie de Souza Santos", "Morakot Choetkiertikul", "Amin Milani Fard", "Rana Alkadhi", "Xavier Devroey", "Gregorio Robles", "Hideaki Hata", "Sebastian Baltes", "Vladimir Kovalenko", "Shalini Chakraborty", "Eray Tuzun", "Hera Arif", "Gianisa Adisaputri", "Kelly Garc\u00e9s", "Anielle S. L. Andrade", "Eyram Amedzor", "Bimpe Ayoola", "Keisha Gaspard-Chickoree", "Arazoo Hoseyni"], "title": "The Software Infrastructure Attitude Scale (SIAS): A Questionnaire Instrument for Measuring Professionals' Attitudes Toward Technical and Sociotechnical Infrastructure", "comment": "Accepted to ICSE 2026, 11 pages + 2 for references, 1 figure, 7 tables", "summary": "Context: Recent software engineering (SE) research has highlighted the need for sociotechnical research, implying a demand for customized psychometric scales. Objective: We define the concepts of technical and sociotechnical infrastructure in software engineering, and develop and validate a psychometric scale that measures attitudes toward them. Method: Grounded in theories of infrastructure, attitudes, and prior work on psychometric measurement, we defined the target constructs and generated scale items. The scale was administered to 225 software professionals and evaluated using a split sample. We conducted an exploratory factor analysis (EFA) on one half of the sample to uncover the underlying factor structure and performed a confirmatory factor analysis (CFA) on the other half to validate the structure. Further analyses with the whole sample assessed face, criterion-related, and discriminant validity. Results: EFA supported a two-factor structure (technical and sociotechnical infrastructure), accounting for 65% of the total variance with strong loadings. CFA confirmed excellent model fit. Face and content validity were supported by the item content reflecting cognitive, affective, and behavioral components. Both subscales were correlated with job satisfaction, perceived autonomy, and feedback from the job itself, supporting convergent validity. Regression analysis supported criterion-related validity, while the Heterotrait-Monotrait ratio of correlations (HTMT), the Fornell-Larcker criterion, and model comparison all supported discriminant validity. Discussion: The resulting scale is a valid instrument for measuring attitudes toward technical and sociotechnical infrastructure in software engineering research. Our work contributes to ongoing efforts to integrate psychological measurement rigor into empirical and behavioral software engineering research.", "AI": {"tldr": "This paper defines and validates a psychometric scale to measure attitudes toward technical and sociotechnical infrastructure in software engineering, confirming its reliability and validity using data from 225 professionals.", "motivation": "Recent software engineering research has emphasized the importance of sociotechnical approaches, but there is a lack of psychometric tools tailored to measure attitudes toward technical and sociotechnical infrastructure. This creates a gap in empirical and behavioral SE research.", "method": "The authors defined target constructs based on theories and prior psychometrics, generated related scale items, and administered the scale to 225 software professionals. They used exploratory factor analysis (EFA) on half the sample to determine the factor structure and confirmatory factor analysis (CFA) on the other half for structure validation. Additional analyses were conducted to assess face, criterion-related, convergent, and discriminant validity.", "result": "The analysis revealed a robust two-factor structure (technical and sociotechnical infrastructure) accounting for 65% of total variance. CFA confirmed excellent model fit. The scale demonstrated strong validity across multiple dimensions, including face, criterion-related, convergent, and discriminant validity.", "conclusion": "The developed scale is a validated and reliable instrument to measure attitudes toward technical and sociotechnical infrastructure in SE research, supporting further integration of psychometric rigor in the field."}}
{"id": "2512.00867", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.00867", "abs": "https://arxiv.org/abs/2512.00867", "authors": ["Obada Kraishan"], "title": "The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development", "comment": "23 pages, 7 figures, 9 tables", "summary": "AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the \"AI attribution paradox\": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work.", "AI": {"tldr": "Developers use AI coding tools extensively but are selective in disclosing their use, balancing transparency against possible scrutiny. Community reactions are largely neutral, and explicit attribution is becoming more common as norms shift.", "motivation": "AI coding assistants are increasingly used in software development, yet practices around disclosing their use ('attribution') are inconsistent. There is a need to understand how and why developers choose to reveal or conceal AI involvement, and how these choices affect community perceptions and accountability.", "method": "The authors analyzed 14,300 GitHub commits across 7,393 repositories from 2023-2025, examining the use of explicit AI attribution and associated community responses across eight major AI coding tools. Temporal trends and variations by tool were assessed to understand attribution strategies and evolving norms.", "result": "The study found widespread AI usage (95.2% of commits), but only 29.5% of contributions explicitly disclose AI assistance, with significant variation across tools. Explicit attribution leads to modestly increased scrutiny, but the tool chosen has a much greater impact on how contributions are received. Community sentiment is generally neutral, suggesting curiosity rather than negativity. Over time, explicit attribution has rapidly increased, showing evolving community norms.", "conclusion": "Attribution of AI assistance is a strategic act by developers, influenced by platform norms and expected reactions rather than pure commitment to transparency. As the community adapts, explicit disclosure is rising, indicating shifting social norms. The work highlights the importance of understanding how attribution evolves and its implications for accountability, platform design, and research into collaborative AI-augmented development."}}
{"id": "2512.00869", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00869", "abs": "https://arxiv.org/abs/2512.00869", "authors": ["Miikka Kuutila", "Paul Ralph", "Huilian Sophie Qiu", "Ronnie de Souza Santos", "Morakot Choetkiertikul", "Rana Alkadhi", "Xavier Devroey", "Gregorio Robles", "Hideaki Hata", "Sebastian Baltes", "Hera Arif", "Vladimir Kovalenko", "Shalini Chakraborty", "Eray Tuzun", "Gianisa Adisaputri"], "title": "Staying or Leaving? How Job Satisfaction, Embeddedness and Antecedents Predict Turnover Intentions of Software Professionals", "comment": "11 pages, 1 figure, 7 tables. Accepted to ICSE 2026 research track", "summary": "Context: Voluntary turnover is common in the software industry, increasing recruitment and onboarding costs and the risk of losing organizational and tacit knowledge. Objective: This study investigates how job satisfaction, work-life balance, job embeddedness, and their antecedents, including job quality, personality traits, attitudes toward technical and sociotechnical infrastructure, and perceptions of organizational justice, relate to software professionals' turnover intentions. Method: We conducted a geographically diverse cross-sectional survey of software professionals (N = 224) and analyzed the data using partial least squares structural equation modeling (PLS-SEM). Our model includes both reflective and formative constructs and tests 15 hypotheses grounded in occupational psychology and software engineering literature. Results: Job satisfaction and embeddedness were significantly negatively associated with software professionals' turnover intentions, while work-life balance showed no direct effect. The strongest antecedents for job satisfaction were work-life balance and job quality, while organizational justice was the strongest predictor of job embeddedness. Discussion: The resulting PLS-SEM model has considerably higher explanatory power for key outcome variables than previous work conducted in the software development context, highlighting the importance of both psychological (e.g., job satisfaction, job embeddedness) and organizational (e.g., organizational justice, job quality) factors in understanding turnover intentions of software professionals. Our results imply that improving job satisfaction and job embeddedness is the key to retaining software professionals. In turn, enhancing job quality, supporting work-life balance, and ensuring high organizational justice can improve job satisfaction and embeddedness, indirectly reducing turnover intentions.", "AI": {"tldr": "Job satisfaction and embeddedness are crucial to reducing turnover among software professionals. Improving job quality, work-life balance, and organizational justice strengthens these factors, leading to better retention.", "motivation": "The motivation behind this study is the high voluntary turnover rate among software professionals, which leads to increased recruitment costs and loss of organizational knowledge. The study aims to identify the psychological and organizational factors influencing turnover intentions in order to better retain talent in the software industry.", "method": "The authors conducted a geographically diverse cross-sectional survey involving 224 software professionals. The collected data was analyzed using partial least squares structural equation modeling (PLS-SEM), testing 15 hypotheses using both reflective and formative constructs, and drawing on theories from occupational psychology and software engineering literature.", "result": "The analysis found that job satisfaction and job embeddedness are both significantly and negatively associated with turnover intentions among software professionals. Work-life balance, however, did not show a direct effect on turnover intention. The strongest predictors for job satisfaction were work-life balance and job quality, while organizational justice was the primary predictor for job embeddedness. The model demonstrated higher explanatory power for turnover intentions compared to previous studies in this field.", "conclusion": "The study concludes that improving job satisfaction and job embeddedness are key to retaining software professionals. Enhancing job quality, supporting work-life balance, and ensuring high levels of organizational justice are effective strategies for improving satisfaction and embeddedness, which together can reduce intentions to leave. Psychological and organizational factors should be prioritized in retention strategies for software companies."}}
{"id": "2512.01141", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01141", "abs": "https://arxiv.org/abs/2512.01141", "authors": ["Muhammad Yousuf", "Akshat Bagade", "Chhittebbayi Penugonda", "Maanas Baraya"], "title": "Neural Variable Name Repair: Learning to Rename Identifiers for Readability", "comment": null, "summary": "Developers routinely work with source files whose variable names are generic or misleading, and with teams moving quickly, many functions are left undocumented. This slows comprehension, increases the risk of subtle bugs, and makes it harder for both humans and large language models (LLMs) to reason about code. We study variable name repair: given a real C++ function where all occurrences of one local or parameter name have been replaced by a placeholder (e.g. ID 1), the goal is to generate a natural, descriptive replacement name. We automatically construct this task from the C++ portion of BigCode's The Stack by parsing functions with Tree-sitter, masking a single identifier, and treating the original name as supervision. On top of Llama 3.1-8B, we build a pipeline with (i) warmup and dropout schedules for more stable fine-tuning, (ii) LoRA adapters for efficient specialization on identifier repair, and (iii) a dual-encoder reranker over top-k generator candidates. We evaluate using exact match, Top-5 Hit, and an embedding-based partial similarity score (0-100) that gives credit for near synonyms and format variants (e.g., jsonValue vs. json). On a held-out set of 200 C++ functions, a zero-shot Llama 3.1 baseline reaches 6.1 percent exact match. Our best LoRA-tuned model (with warmup and dropout) achieves 43.1 percent exact match, 50.2 percent Top-5 Hit, and an 82.03 partial-match score. A dual encoder reranker further improves selection quality without modifying the underlying generator, suggesting that task-specific fine-tuning plus reranking is a promising approach for practical identifier repair tools.", "AI": {"tldr": "Fine-tuned Llama 3.1 with LoRA and reranking techniques is highly effective at repairing variable names in C++ code, outperforming the baseline and suggesting practical applications.", "motivation": "Poorly chosen variable names and lack of documentation hinder code comprehension and increase bug risk, for both humans and LLMs; automating descriptive naming could greatly enhance code readability and reliability.", "method": "They construct a variable name repair task by masking a local/parameter identifier in C++ functions parsed from BigCode's The Stack, fine-tune Llama 3.1-8B with LoRA adapters and warmup/dropout schedules, and employ a dual-encoder reranker over top-k generated candidates.", "result": "Zero-shot Llama 3.1 baseline gets 6.1% exact match; the fine-tuned LoRA model achieves 43.1% exact match, 50.2% Top-5 Hit, and 82.03 partial-match score. Dual-encoder reranking further improves identifier selection quality.", "conclusion": "Task-specific fine-tuning combined with reranking provides a strong approach for repairing and improving code variable names, making codebases easier to understand and maintain."}}
{"id": "2512.01155", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01155", "abs": "https://arxiv.org/abs/2512.01155", "authors": ["Krishna Kumaar Sharma"], "title": "Beyond Greenfield: AI-Driven Productivity in Documentation and Brownfield Engineering", "comment": "53 pages, 7 figures", "summary": "Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and reduced rework for 83% during the Define phase. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.", "AI": {"tldr": "The D3 Framework employs structured LLM prompting strategies to help engineers in complex legacy system tasks, showing preliminary self-reported gains in productivity and reduced cognitive load, but the findings are not from controlled experiments.", "motivation": "Existing research on LLMs has mostly focused on simpler, greenfield tasks, leaving a gap in strategies for complex brownfield systems where documentation is incomplete and architectural knowledge fragmented. The motivation is to create a structured approach to improve LLM usage in these challenging environments.", "method": "The author conducted an exploratory survey study with 52 engineering practitioners, who applied the D3 workflow on real-world tasks. Results were collected via participant self-reports regarding productivity, cognitive load, and documentation quality.", "result": "The study proposes the D3 (Discover-Define-Deliver) Framework, a structured workflow involving large language models (LLMs) for improving engineering tasks related to brownfield systems. The D3 Framework uses a dual-agent prompting architecture, where a Builder generates responses and a Reviewer critiques them. A survey of 52 practitioners indicated perceived increases in task clarity, documentation quality, reduced cognitive load, and productivity when using D3 for legacy system tasks.", "conclusion": "Structured LLM workflows, like the D3 Framework, can potentially enhance productivity and reduce cognitive load in complex brownfield engineering tasks, though current evidence is preliminary and self-reported."}}
{"id": "2512.01232", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01232", "abs": "https://arxiv.org/abs/2512.01232", "authors": ["Donghao Huang", "Shila Chew", "Anna Dutkiewicz", "Zhaoxia Wang"], "title": "LLM-as-a-Judge for Scalable Test Coverage Evaluation: Accuracy, Operational Reliability, and Cost", "comment": "7 pages, accepted by the AAAI 2026 Workshop on Next Gen Code Development with Collaborative AI Agents", "summary": "Assessing software test coverage at scale remains a bottleneck in QA pipelines. We present LLM-as-a-Judge (LAJ), a production-ready, rubric-driven framework for evaluating Gherkin acceptance tests with structured JSON outputs. Across 20 model configurations (GPT-4, GPT-5 with varying reasoning effort, and open-weight models) on 100 expert-annotated scripts over 5 runs (500 evaluations), we provide the first comprehensive analysis spanning accuracy, operational reliability, and cost. We introduce the Evaluation Completion Rate (ECR@1) to quantify first-attempt success, revealing reliability from 85.4% to 100.0% with material cost implications via retries. Results show that smaller models can outperform larger ones: GPT-4o Mini attains the best accuracy (6.07 MAAE), high reliability (96.6% ECR@1), and low cost ($1.01 per 1K), yielding a 78x cost reduction vs. GPT-5 (high reasoning) while improving accuracy. Reasoning effort is model-family dependent: GPT-5 benefits from increased reasoning (with predictable accuracy-cost tradeoffs), whereas open-weight models degrade across all dimensions as reasoning increases. Overall, cost spans 175x ($0.45-$78.96 per 1K). We release the dataset, framework, and code to support reproducibility and deployment.", "AI": {"tldr": "The paper introduces a scalable, reproducible framework using LLMs to judge software test coverage, showing smaller models can significantly outperform larger ones in accuracy and cost. The provided tools and dataset support adoption in industry QA pipelines.", "motivation": "Assessing software test coverage at scale is an ongoing bottleneck in QA. Existing methods are often costly, unreliable, and hard to reproduce. The need is for a scalable, cost-effective, and accurate solution.", "method": "The paper proposes 'LLM-as-a-Judge' (LAJ), a rubric-driven framework leveraging large language models to evaluate Gherkin acceptance tests, producing structured JSON outputs. Evaluation is conducted across 20 LLM configurations over 100 expert-annotated scripts and five runs.", "result": "Smaller models like GPT-4o Mini can outperform larger models, achieving the best accuracy, high reliability, and drastically reduced cost. Increasing reasoning effort improves performance in GPT-5 but not in open-weight models, which degrade with more effort. The Evaluation Completion Rate (ECR@1) is introduced, and comprehensive operational and cost analyses are presented.", "conclusion": "LLM-based evaluation can scale QA coverage assessment efficiently, with cost and accuracy optimizations available by using smaller, targeted models. The open release promotes reproducibility and industry deployment."}}
{"id": "2512.01356", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01356", "abs": "https://arxiv.org/abs/2512.01356", "authors": ["Yuxin Zhang", "Yuxia Zhang", "Zeyu Sun", "Yanjie Jiang", "Hui Liu"], "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM", "comment": "Accepted by the 2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE). Copyright 2025 IEEE. This is the author's accepted manuscript. The final published version may differ and will be available from IEEE Xplore", "summary": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.", "AI": {"tldr": "LAURA is a new context- and knowledge-augmented framework that significantly improves automated code review comment generation using LLMs, outperforming existing solutions.", "motivation": "Code review is essential but has become a bottleneck due to increased software complexity, shortage of expert reviewers, and the time-intensive nature of reviews. Existing automatic methods overlook vital context and prior review knowledge, limiting effectiveness.", "method": "This paper introduces LAURA, a framework that enhances code review comment generation by integrating LLM-based review knowledge augmentation, context awareness, review exemplar retrieval, context augmentation, and systematic guidance. It works with models such as ChatGPT-4o and DeepSeek v3.", "result": "LAURA, combined with ChatGPT-4o and DeepSeek v3, generates review comments that are completely correct or helpful in 42.2% and 40.4% of cases, significantly exceeding state-of-the-art baselines. Ablation studies confirm that each LAURA component improves comment quality.", "conclusion": "By integrating knowledge augmentation and context awareness, LAURA addresses limitations of previous automated code review systems, resulting in substantially better comment quality and more effective use of large language models for software quality assurance."}}
{"id": "2512.01396", "categories": ["cs.SE", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01396", "abs": "https://arxiv.org/abs/2512.01396", "authors": ["Zhiqing Zhong", "Jiaming Huang", "Pinjia He"], "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches", "comment": "Under review", "summary": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.\n  To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.", "AI": {"tldr": "The paper introduces BackportBench, a multilingual benchmark suite for evaluating automated approaches to backporting security patches to older software releases. It assesses current methods, finding agentic approaches superior, especially for complex changes.", "motivation": "Upgrading dependencies in software projects is difficult and risky, leaving users on vulnerable versions. Manual backporting is tedious and error-prone, while automated solutions lack proper evaluation and generalization.", "method": "BackportBench was designed with 202 patch backporting problems across PyPI, Maven, and npm, supported by Docker environments and tests. The study evaluated traditional and LLM-based techniques for patch porting using this benchmark.", "result": "Agentic, likely LLM-based, methods achieve better results than traditional techniques for backporting, especially when logical and structural modifications are needed. Differences exist by language, leading to several recommendations for future research and practice.", "conclusion": "Agentic methods outperform traditional patch porting approaches for backporting security patches, particularly for logically and structurally complex cases, though performance depends on the programming language."}}
{"id": "2512.01523", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01523", "abs": "https://arxiv.org/abs/2512.01523", "authors": ["Pankaj Jalote", "Y. Raghu Reddy", "Vasudeva Varma"], "title": "Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report", "comment": "7 pages", "summary": "Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled \"AI in Software Engineering\" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own.", "AI": {"tldr": "Online collaborative courses, enabled by Covid-driven digital comfort, successfully connect small institutions, students, and industry experts for advanced topics like 'AI in Software Engineering', providing a scalable model for future applied computer science education.", "motivation": "Covid normalized online education, enabling potential for collaborative multi-institutional courses, especially where individual institutions lack specialized faculty or student numbers.", "method": "Jointly taught a research-level 'AI in Software Engineering' course online between two institutions with active participation from industry experts. Gathered and discussed experiences of both faculty and students.", "result": "Successful collaboration enabled students, faculty, and industry professionals from multiple institutions to participate in advanced coursework and research, overcoming limitations of institutional resources.", "conclusion": "Collaborative online teaching can be effectively used for research-level courses in applied computer science at smaller institutions, making specialized topics more accessible."}}
{"id": "2512.01570", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.01570", "abs": "https://arxiv.org/abs/2512.01570", "authors": ["Stephan Druskat", "Lars Grunske"], "title": "OpenDORS: A dataset of openly referenced open research software", "comment": "5 pages, 3 figures, 1 table", "summary": "In many academic disciplines, software is created during the research process or for a research purpose. The crucial role of software for research is increasingly acknowledged. The application of software engineering to research software has been formalized as research software engineering, to create better software that enables better research. Despite this, large-scale studies of research software and its development are still lacking. To enable such studies, we present a dataset of 134,352 unique open research software projects and 134,154 source code repositories referenced in open access literature. Each dataset record identifies the referencing publication and lists source code repositories of the software project. For 122,425 source code repositories, the dataset provides metadata on latest versions, license information, programming languages and descriptive metadata files. We summarize the distributions of these features in the dataset and describe additional software metadata that extends the dataset in future work. Finally, we suggest examples of research that could use the dataset to develop a better understanding of research software practice in RSE research.", "AI": {"tldr": "This paper introduces a comprehensive dataset of research software projects and source code repositories, providing metadata to support large-scale studies on research software development.", "motivation": "Despite the pivotal role of software in research, there is a lack of large-scale studies on how research software is developed and maintained. The authors aim to fill this gap by providing an extensive dataset to facilitate empirical research in this domain.", "method": "The authors collected data from open access literature, identifying research software projects and linking them to source code repositories. They extracted and summarized metadata\u2014including version information, licenses, languages, and descriptive files\u2014across over 122,000 repositories.", "result": "The paper presents a large dataset of 134,352 unique open research software projects and their corresponding 134,154 source code repositories, as referenced in open access literature. The dataset includes detailed metadata for over 122,000 repositories, such as version info, licenses, programming languages, and descriptive files. The paper also summarizes the distribution of these features and suggests potential applications for the dataset in research software engineering.", "conclusion": "The release of this dataset enables and encourages future large-scale empirical research on research software engineering practices, facilitating a deeper understanding of how research software is developed, shared, and maintained."}}
{"id": "2512.01609", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01609", "abs": "https://arxiv.org/abs/2512.01609", "authors": ["Patrick Herter", "Vincent Ahlrichs", "Ridvan A\u00e7ilan", "Julian Horsch"], "title": "GPTrace: Effective Crash Deduplication Using LLM Embeddings", "comment": "Original publication in 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE '26), April 12-18, 2026, Rio de Janeiro, Brazil. ACM, New York, NY, USA, 12 pages", "summary": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.", "AI": {"tldr": "GPTrace applies language model embeddings and clustering to more efficiently deduplicate fuzzing crash data, improving over previous methods.", "motivation": "Manual effort in analyzing fuzzing crash data is high because many inputs stem from the same bug. Existing deduplication methods (e.g., stack trace matching) are often insufficient. The motivation is to make deduplication more accurate and automated, reducing the burden on analysts.", "method": "The method involves extracting various data sources from crash events, generating embedding vectors using large language models, and applying clustering algorithms for deduplication.", "result": "The paper introduces GPTrace, a new crash deduplication workflow for fuzzing campaigns. GPTrace utilizes large language models to create embedding vectors from crash-related data and uses these embeddings for effective clustering. Evaluated on over 300,000 crash inputs across 14 software targets, it outperformed existing stack trace comparison and other advanced deduplication techniques.", "conclusion": "GPTrace delivers improved and flexible crash deduplication by leveraging language model embeddings and clustering, outperforming both traditional and state-of-the-art alternatives."}}
{"id": "2512.01617", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01617", "abs": "https://arxiv.org/abs/2512.01617", "authors": ["Pierciro Caliandro", "Matteo Ciccaglione", "Alessandro Pellegrini"], "title": "When High-Performance Computing Meets Software Testing: Distributed Fuzzing using MPI", "comment": null, "summary": "This paper explores the integration of MPI-based synchronization techniques into distributed fuzzing frameworks, highlighting possible substantial performance improvements compared to traditional filesystem-based synchronization methods. By employing lightweight MPI primitives, reductions in communication latency are achieved, facilitating more efficient data exchanges across distributed fuzzing nodes. Experimental results obtained over standard benchmarks demonstrate enhanced coverage progression from the early stages of the fuzzing process, which could be beneficial if fuzzing is employed in CI/CD pipelines at any stage of software development. Furthermore, the coordinated exchange of input corpora among clusters of fuzzers effectively addresses coverage stagnation, enabling a sustained exploration of complex and deep execution paths. Overall, the adoption of MPI-based synchronization approaches shows promising potential for significantly enhancing the scalability and efficacy of distributed fuzz testing.", "AI": {"tldr": "Replacing slow, filesystem-based synchronization in distributed fuzzing frameworks with MPI-based techniques greatly boosts communication speed and coverage. Experiments show MPI helps fuzzers work together more efficiently, find deeper bugs sooner, and scale better, making it a strong candidate for modern software testing pipelines.", "motivation": "Traditional distributed fuzzing frameworks use filesystem-based synchronization, which can cause high communication latency and inefficiencies. There is a need for faster, scalable data sharing among fuzzing nodes, especially for use in continuous integration and delivery (CI/CD) pipelines and to overcome coverage stagnation during software testing.", "method": "The paper integrates lightweight Message Passing Interface (MPI) synchronization primitives into distributed fuzzing frameworks and compares their performance against conventional filesystem-based methods using standard benchmarks.", "result": "Experimental results show that MPI-based synchronization reduces communication latency, improves early-stage coverage progression, and enables better sharing of input corpora among fuzzers. This results in sustained exploration of complex software paths and helps address coverage stagnation issues.", "conclusion": "MPI-based synchronization significantly enhances the scalability and effectiveness of distributed fuzz testing, making it a promising approach for adoption in CI/CD pipelines and large-scale software quality assurance."}}
{"id": "2512.01630", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01630", "abs": "https://arxiv.org/abs/2512.01630", "authors": ["Ziheng Liu", "Runzhi He", "Minghui Zhou"], "title": "Package Dashboard: A Cross-Ecosystem Framework for Dual-Perspective Analysis of Software Packages", "comment": null, "summary": "Software supply chain attacks have revealed blind spots in existing SCA tools, which are often limited to a single ecosystem and assess either software artifacts or community activity in isolation. This fragmentation across tools and ecosystems forces developers to manually reconcile scattered data, undermining risk assessments. We present Package Dashboard, a cross-ecosystem framework that provides a unified platform for supply chain analysis, enabling a holistic, dual-perspective risk assessment by integrating package metadata, vulnerability information, and upstream community health metrics. By combining dependency resolution with repository analysis, it reduces cognitive load and improves traceability. Demonstrating the framework's versatility, a large-scale study of 374,000 packages across five Linux distributions shows its ability to uncover not only conventional vulnerabilities and license conflicts but also overlooked risks such as archived or inaccessible repositories. Ultimately, Package Dashboard provides a unified view of risk, equipping developers and DevSecOps engineers with actionable insights to strengthen the transparency, trustworthiness, and traceability of open-source ecosystems. Package Dashboard is publicly available at https://github.com/n19htfall/PackageDashboard, and a demonstration video can be found at https://youtu.be/y9ncftP8KPQ. Besides, the online version is available at https://pkgdash.osslab-pku.org.", "AI": {"tldr": "Package Dashboard overcomes fragmentation in SCA tools by providing a unified platform for supply chain analysis across ecosystems, yielding improved risk assessment and traceability through integrated package and community data.", "motivation": "Existing software composition analysis (SCA) tools are often ecosystem-specific and fragmented, requiring manual and error-prone reconciliation of disparate data, which leads to incomplete and ineffective risk assessments.", "method": "Introduction of Package Dashboard, a cross-ecosystem framework integrating package metadata, vulnerability data, and community health metrics for unified supply chain risk assessment.", "result": "The framework was validated through an extensive study of 374,000 packages from five Linux distributions. It identified conventional risks like vulnerabilities and license conflicts, as well as less obvious issues such as archived or inaccessible repositories.", "conclusion": "Package Dashboard enhances transparency and risk management in open-source ecosystems by offering developers and security engineers a comprehensive and actionable view of package-related risks, supporting better supply chain security practices."}}
{"id": "2512.01649", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01649", "abs": "https://arxiv.org/abs/2512.01649", "authors": ["Daniel Strassler", "Gabe Elkin", "Curran Schiefelbein", "Daniel Herring", "Ian Jessen", "David Johnson", "Santiago A. Paredes", "Tod Shannon", "Jim Flavin"], "title": "MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects", "comment": "Strassler, D., et al. MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects. Zenodo, 2025, https://doi.org/10.5281/zenodo.16878161", "summary": "Software plays an ever increasing role in complex system development and prototyping, and in recent years, MIT Lincoln Laboratory has sought to improve both the effectiveness and culture surrounding software engineering in execution of its mission. The Homeland Protection and Air Traffic Control Division conducted an internal study to examine challenges to effective and efficient research software development, and to identify ways to strengthen both the culture and execution for greater impact on our mission. Key findings of this study fell into three main categories: project attributes that influence how software development activities must be conducted and managed, potential efficiencies from centralization, opportunities to improve staffing and culture with respect to software practitioners. The study delivered actionable recommendations, including centralizing and standardizing software support tooling, developing a common database to help match the right software talent and needs to projects, and creating a software stakeholder panel to assist with continued improvement.", "AI": {"tldr": "MIT Lincoln Laboratory identified and analyzed challenges in research software development and proposed centralized tools, talent matching, and stakeholder panels to strengthen software culture and improve mission impact.", "motivation": "MIT Lincoln Laboratory recognized that effective software engineering is crucial for complex system development and sought to address challenges limiting the impact and efficiency of research software activities.", "method": "An internal study was conducted within the Homeland Protection and Air Traffic Control Division to identify challenges, categorize findings, and develop actionable recommendations to enhance software engineering culture and practices.", "result": "The study found that project management, centralization of resources, and staffing/culture improvements were key areas for impact. It proposed recommendations such as centralizing support tooling, creating a talent-project matching database, and forming a stakeholder panel for ongoing development.", "conclusion": "Actionable steps were outlined to improve software engineering effectiveness, cultural environment, and efficiency, including resource centralization, better talent utilization, and ongoing stakeholder engagement."}}
{"id": "2512.01690", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01690", "abs": "https://arxiv.org/abs/2512.01690", "authors": ["Philip Garrett", "Juan P. Galeotti", "Andrea Arcuri", "Alexander Poth", "Olsi Rrjolli"], "title": "Generating REST API Tests With Descriptive Names", "comment": null, "summary": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.\n  To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.\n  These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.", "AI": {"tldr": "Rule-based deterministic test naming methods provide clear, descriptive REST API test names, rivaling or beating LLM approaches\u2014while also being more practical for industry use.", "motivation": "Non-descriptive names in automated REST API test generation hurt readability and maintainability; thus, improving naming supports better comprehension and usage in real-world development.", "method": "Three novel deterministic naming techniques were created, and eight total methods (both heuristic and LLM-based) were benchmarked using test cases generated for nine open-source APIs and evaluated through surveys and an industrial case study at Volkswagen AG.", "result": "Eight techniques for automatically generating descriptive names for REST API test cases were compared: three novel deterministic techniques and five others (including LLM-based methods like GPT-3.5, GPT-4o, and Gemini). The quality of the test names, especially their clarity, was evaluated via surveys and a developer questionnaire conducted on open source and industrial APIs. Rule-based deterministic approaches achieved the highest clarity, matching cutting-edge LLMs and surpassing GPT-3.5. Practitioners confirmed improved test suite readability using these descriptive names.", "conclusion": "Descriptive test naming via deterministic techniques can enhance API test suite readability, serving as an effective and resource-friendly alternative to LLM-based naming methods."}}
{"id": "2512.01939", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01939", "abs": "https://arxiv.org/abs/2512.01939", "authors": ["Yanlin Wang", "Xinyi Xu", "Jiachi Chen", "Tingting Bi", "Wenchao Gu", "Zibin Zheng"], "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks", "comment": null, "summary": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.", "AI": {"tldr": "This paper empirically compares ten major LLM-based agent frameworks based on real-world developer experiences and discussions, uncovering key differences and common challenges, and providing insights for future framework improvement.", "motivation": "There is rapid growth in agent frameworks due to LLMs, but little is known about their practical application and influence on the development process. Developers face recurring issues and difficulties in choosing suitable frameworks, which emphasizes the need for thorough evaluation and better design.", "method": "The paper conducts an empirical study focusing on the real-world usage and developer experience with large language model (LLM)-based agent frameworks. The authors analyze 11,910 developer discussions covering ten prominent frameworks, comparing them across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability.", "result": "The comparative study reveals significant differences between agent frameworks regarding developer needs across the five selected dimensions. More than 80% of developers have trouble selecting the right framework. Common problems and unmatched requirements were identified, highlighting areas for improvement.", "conclusion": "The study offers deep insights into how current LLM-based agent frameworks serve developers and identifies areas where improvement is needed. The findings can guide future framework design and help developers make better-informed choices, ultimately advancing the agent framework ecosystem."}}
