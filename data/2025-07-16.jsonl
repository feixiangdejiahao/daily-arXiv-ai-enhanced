{"id": "2507.10583", "categories": ["cs.SE", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.10583", "abs": "https://arxiv.org/abs/2507.10583", "authors": ["Daniil Orel", "Indraneil Paul", "Iryna Gurevych", "Preslav Nakov"], "title": "$\\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection", "comment": null, "summary": "In this work, we compile $\\textbf{$\\texttt{DroidCollection}$}$, the most\nextensive open data suite for training and evaluating machine-generated code\ndetectors, comprising over a million code samples, seven programming languages,\noutputs from 43 coding models, and over three real-world coding domains.\nAlongside fully AI-generated samples, our collection includes human-AI\nco-authored code, as well as adversarial samples explicitly crafted to evade\ndetection. Subsequently, we develop $\\textbf{$\\texttt{DroidDetect}$}$, a suite\nof encoder-only detectors trained using a multi-task objective over\n$\\texttt{DroidCollection}$. Our experiments show that existing detectors'\nperformance fails to generalise to diverse coding domains and programming\nlanguages outside of their narrow training data. Additionally, we demonstrate\nthat while most detectors are easily compromised by humanising the output\ndistributions using superficial prompting and alignment approaches, this\nproblem can be easily amended by training on a small amount of adversarial\ndata. Finally, we demonstrate the effectiveness of metric learning and\nuncertainty-based resampling as means to enhance detector training on possibly\nnoisy distributions."}
{"id": "2507.10584", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10584", "abs": "https://arxiv.org/abs/2507.10584", "authors": ["Francesco Romeo", "Luigi Arena", "Francesco Blefari", "Francesco Aurelio Pironti", "Matteo Lupinacci", "Angelo Furfaro"], "title": "ARPaCCino: An Agentic-RAG for Policy as Code Compliance", "comment": null, "summary": "Policy as Code (PaC) is a paradigm that encodes security and compliance\npolicies into machine-readable formats, enabling automated enforcement in\nInfrastructure as Code (IaC) environments. However, its adoption is hindered by\nthe complexity of policy languages and the risk of misconfigurations. In this\nwork, we present ARPaCCino, an agentic system that combines Large Language\nModels (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation\nto automate the generation and verification of PaC rules. Given natural\nlanguage descriptions of the desired policies, ARPaCCino generates formal Rego\nrules, assesses IaC compliance, and iteratively refines the IaC configurations\nto ensure conformance. Thanks to its modular agentic architecture and\nintegration with external tools and knowledge bases, ARPaCCino supports policy\nvalidation across a wide range of technologies, including niche or emerging IaC\nframeworks. Experimental evaluation involving a Terraform-based case study\ndemonstrates ARPaCCino's effectiveness in generating syntactically and\nsemantically correct policies, identifying non-compliant infrastructures, and\napplying corrective modifications, even when using smaller, open-weight LLMs.\nOur results highlight the potential of agentic RAG architectures to enhance the\nautomation, reliability, and accessibility of PaC workflows."}
{"id": "2507.10590", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10590", "abs": "https://arxiv.org/abs/2507.10590", "authors": ["Mojtaba Eshghie"], "title": "Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime", "comment": null, "summary": "Language Model (LM) pipelines can dynamically refine their outputs against\nprogrammatic constraints. However, their effectiveness collapses when faced\nwith competing soft constraints, leading to inefficient backtracking loops\nwhere satisfying one constraint violates another. We introduce Meta\nSelf-Refining, a framework that equips LM pipelines with a meta-corrective\nlayer to repair these competitions at runtime/inference-time. Our approach\nmonitors the pipeline's execution history to detect oscillatory failures. Upon\ndetection, it invokes a meta-repairer LM that analyzes the holistic state of\nthe backtracking attempts and synthesizes a strategic instruction to balance\nthe competing requirements. This self-repair instruction guides the original LM\nout of a failing refining loop towards a successful output. Our results show\nMeta Self-Refining can successfully repair these loops, leading to more\nefficient LM programs."}
{"id": "2507.10593", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10593", "abs": "https://arxiv.org/abs/2507.10593", "authors": ["Peng Ding"], "title": "ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs", "comment": null, "summary": "Large Language Model (LLM) applications are increasingly relying on external\ntools to extend their capabilities beyond text generation. However, current\ntool integration approaches suffer from fragmentation, protocol limitations,\nand implementation complexity, leading to substantial development overhead.\nThis paper presents Toolregistry, a protocol-agnostic tool management library\nthat simplifies tool registration, representation, execution, and lifecycle\nmanagement via a unified interface. Our evaluation demonstrates that\n\\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x\nperformance improvements through concurrent execution, and 100% compatibility\nwith OpenAI function calling standards. Real-world case studies show\nsignificant improvements in development efficiency and code maintainability\nacross diverse integration scenarios. \\toolregistry is open-source and\navailable at https://github.com/Oaklight/ToolRegistry, with comprehensive\ndocumentation at https://toolregistry.readthedocs.io/."}
{"id": "2507.10799", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10799", "abs": "https://arxiv.org/abs/2507.10799", "authors": ["Tyler Hou", "Michael Arntzenius", "Max Willsey"], "title": "Stream programs are monoid homomorphisms with state", "comment": null, "summary": "We define a broad class of deterministic stream functions and show they can\nbe implemented as homomorphisms into a \"state\" monoid. The homomorphism laws\nare simpler than the conditions of previous semantic frameworks for stream\nprogram optimization, yet retain support for rich equational reasoning over\nexpressive dataflow programs, including sequential composition, parallel\ncomposition, and feedback. We demonstrate this using examples of partitioned\ndatabase joins, stratified negation, and a simplified model of TCP."}
{"id": "2507.10640", "categories": ["cs.SE", "cs.LG", "cs.SI", "D.2.2"], "pdf": "https://arxiv.org/pdf/2507.10640", "abs": "https://arxiv.org/abs/2507.10640", "authors": ["Labiba Farah", "Mohammad Ridwan Kabir", "Shohel Ahmed", "MD Mohaymen Ul Anam", "Md. Sakibul Islam"], "title": "SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications", "comment": "26 pages, 9 figures, 5 tables", "summary": "The widespread use of social media applications has raised significant\nprivacy concerns, often highlighted in user reviews. These reviews also provide\ndevelopers with valuable insights into improving apps by addressing issues and\nintroducing better features. However, the sheer volume and nuanced nature of\nreviews make manual identification and prioritization of privacy-related\nconcerns challenging for developers. Previous studies have developed software\nutilities to automatically classify user reviews as privacy-relevant,\nprivacy-irrelevant, bug reports, feature requests, etc., using machine\nlearning. Notably, there is a lack of focus on classifying reviews specifically\nas privacy-related feature requests, privacy-related bug reports, or\nprivacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated\nonline annotation tool designed to help developers annotate and classify user\nreviews into these categories. For automating the annotation of such reviews,\nthis paper introduces the annotation model, GRACE (GRU-based Attention with\nCBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words\n(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven\npopular social media apps on Google Play Store, including Instagram, Facebook,\nWhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were\nanalyzed. Two annotators manually labelled the reviews, achieving a Cohen's\nKappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement\nfor training machine learning models. Among the models tested, GRACE\ndemonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:\n0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates\nsignificant potential to assist developers with extracting and addressing\nprivacy-related feature requests or bug reports from user reviews, enhancing\nuser privacy and trust."}
{"id": "2507.11282", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.11282", "abs": "https://arxiv.org/abs/2507.11282", "authors": ["René Rydhof Hansen", "Andreas Stenbæk Larsen", "Aslan Askarov"], "title": "The downgrading semantics of memory safety", "comment": "56 pages, 27 figures", "summary": "Memory safety is traditionally characterized in terms of bad things that\ncannot happen, an approach that is often criticized as unprincipled. Prior work\nsuggest a connection between memory safety and noninterference, but no\nsatisfactory semantic notion of memory safety is currently known.\n  This work proposes a notion of gradual allocator independence that accurately\ncaptures many allocator-specific aspects of memory safety. We consider a\nlow-level language with access to an allocator that provides malloc and free\nprimitives in a flat memory model. Pointers are just integers, and as such it\nis trivial to write memory-unsafe programs. The basic intuition of gradual\nallocator independence is that of noninterference, namely that allocators must\nnot influence program execution. This intuition is refined in two important\nways to account for the allocators running out-of-memory and for programs to\nhave pointer-to-integer casts. The key insight of the definition is to treat\nthese extensions as forms of downgrading and give them satisfactory technical\ntreatment using the state-of-the-art information flow machinery."}
{"id": "2507.10641", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10641", "abs": "https://arxiv.org/abs/2507.10641", "authors": ["Jayant Havare", "Saurav Chaudhary", "Ganesh Ramakrishnan", "Kaushik Maharajan", "Srikanth Tamilselvam"], "title": "A Code Comprehension Benchmark for Large Language Models for Code", "comment": "10 Pages, 5 Figures", "summary": "Large Language Models have shown impressive capabilities in coding tasks like\ncode generation and code completion, as they have been trained on a large\namount of code data. Also, since one of the core pretraining objectives is Next\nToken Prediction, these models tends to learn surface-level syntactic patterns\nin code. However, this does not guarantee code comprehension ability i.e. the\nability to capture the semantics of the code. In our opinion, this is the\nreason why these models often underperform on tasks that require deeper\nsemantic understanding, such as code debugging and code optimization. To\naddress this, we propose fine-tuning these models specifically for code\ncomprehension tasks using large-scale datasets, enabling them to develop a more\nrobust understanding of code semantics. We evaluate three code models of\nvarying sizes on a suite of code comprehension tasks designed to assess\nsemantic understanding beyond surface-level syntactic pattern matching. In\nparticular, we analyze performance on the Subjectivity Grading Task and observe\nthat model performance improves after fine-tuning on relevant downstream tasks.\nThe most significant improvement is seen in the QWQ-32B model, where accuracy\nincreases from 70% to 83.47%. A similar or explainable trend is observed across\nother models, clearly indicating an enhancement in code comprehension ability.\nAmong the models studied, the DPO-fine-tuned Codestral-22B achieves the highest\nmicro-accuracy of 87.66% on the Subjectivity Grading Task."}
{"id": "2507.10646", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10646", "abs": "https://arxiv.org/abs/2507.10646", "authors": ["Myeongsoo Kim", "Shweta Garg", "Baishakhi Ray", "Varun Kumar", "Anoop Deoras"], "title": "CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance", "comment": null, "summary": "Programming assistants powered by large language models have transformed\nsoftware development, yet most benchmarks focus narrowly on code generation\ntasks. Recent efforts like InfiBench and StackEval attempt to address this gap\nusing Stack Overflow data but remain limited to single-turn interactions in\nisolated contexts, require significant manual curation, and fail to represent\ncomplete project environments. We introduce CodeAssistBench (CAB), the first\nbenchmark framework for evaluating multi-turn programming assistance in\nrealistic settings that address real-world questions about actual codebases.\nUnlike existing programming Q&A benchmarks, CAB automatically generates\nscalable datasets from question-related GitHub issues using configurable\nparameters (e.g., repository creation date, star count, programming languages),\nand includes automatic containerization of codebases for evaluation. It then\nevaluates models through simulated users in these containerized environments\nwith full codebase access. Using this framework, we constructed a test set of\n3,286 real-world programming questions across 231 repositories, spanning seven\nprogramming languages and diverse problem domains. Our evaluation of leading\nLLMs reveals a substantial capability gap: while models perform well on Stack\nOverflow questions with success rates of 70-83%, they resolve only up to 16.49%\nof CAB's recent issues. This discrepancy highlights the challenges of providing\nassistance in complex, project-specific contexts versus answering standalone\nquestions."}
{"id": "2507.10729", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10729", "abs": "https://arxiv.org/abs/2507.10729", "authors": ["Duong Nguyen", "Thanh Le-Cong", "Triet Huynh Minh Le", "M. Ali Babar", "Quyet-Thang Huynh"], "title": "Toward Realistic Evaluations of Just-In-Time Vulnerability Prediction", "comment": null, "summary": "Modern software systems are increasingly complex, presenting significant\nchallenges in quality assurance. Just-in-time vulnerability prediction (JIT-VP)\nis a proactive approach to identifying vulnerable commits and providing early\nwarnings about potential security risks. However, we observe that current\nJIT-VP evaluations rely on an idealized setting, where the evaluation datasets\nare artificially balanced, consisting exclusively of vulnerability-introducing\nand vulnerability-fixing commits.\n  To address this limitation, this study assesses the effectiveness of JIT-VP\ntechniques under a more realistic setting that includes both\nvulnerability-related and vulnerability-neutral commits. To enable a reliable\nevaluation, we introduce a large-scale public dataset comprising over one\nmillion commits from FFmpeg and the Linux kernel. Our empirical analysis of\neight state-of-the-art JIT-VP techniques reveals a significant decline in\npredictive performance when applied to real-world conditions; for example, the\naverage PR-AUC on Linux drops 98\\% from 0.805 to 0.016. This discrepancy is\nmainly attributed to the severe class imbalance in real-world datasets, where\nvulnerability-introducing commits constitute only a small fraction of all\ncommits.\n  To mitigate this issue, we explore the effectiveness of widely adopted\ntechniques for handling dataset imbalance, including customized loss functions,\noversampling, and undersampling. Surprisingly, our experimental results\nindicate that these techniques are ineffective in addressing the imbalance\nproblem in JIT-VP. These findings underscore the importance of realistic\nevaluations of JIT-VP and the need for domain-specific techniques to address\ndata imbalance in such scenarios."}
{"id": "2507.10753", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10753", "abs": "https://arxiv.org/abs/2507.10753", "authors": ["Kasper Lien Oftebro", "Anh Nguyen-Duc", "Kai-Kristian Kemell"], "title": "GenAI-Enabled Backlog Grooming in Agile Software Projects: An Empirical Study", "comment": null, "summary": "Effective backlog management is critical for ensuring that development teams\nremain aligned with evolving requirements and stakeholder expectations.\nHowever, as product backlogs consistently grow in scale and complexity, they\ntend to become cluttered with redundant, outdated, or poorly defined tasks,\ncomplicating prioritization and decision making processes. This study\ninvestigates whether a generative-AI (GenAI) assistant can automate backlog\ngrooming in Agile software projects without sacrificing accuracy or\ntransparency. Through Design Science cycles, we developed a Jira plug-in that\nembeds backlog issues with the vector database, detects duplicates via cosine\nsimilarity, and leverage the GPT-4o model to propose merges, deletions, or new\nissues. We found that AI-assisted backlog grooming achieved 100 percent\nprecision while reducing the time-to-completion by 45 percent. The findings\ndemonstrated the tool's potential to streamline backlog refinement processes\nwhile improving user experiences."}
{"id": "2507.10785", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10785", "abs": "https://arxiv.org/abs/2507.10785", "authors": ["Michael Neumann", "Eva-Maria Schön", "Mali Senapathi", "Maria Rauschenberger", "Tiago Silva da Silva"], "title": "Towards a Closer Collaboration Between Practice and Research in Agile Software Development Workshop: A Summary and Research Agenda", "comment": null, "summary": "Agile software development principles and values have been widely adopted\nacross various industries, influencing products and services globally. Despite\nits increasing popularity, a significant gap remains between research and\npractical implementation. This paper presents the findings of the first\ninternational workshop designed to foster collaboration between research and\npractice in agile software development. We discuss the main themes and factors\nidentified by the workshop participants that contribute to this gap, strategies\nto bridge it, and the challenges that require further research attention."}
{"id": "2507.10818", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10818", "abs": "https://arxiv.org/abs/2507.10818", "authors": ["Jasmine Latendresse", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "How Robust are LLM-Generated Library Imports? An Empirical Study using Stack Overflow", "comment": null, "summary": "Software libraries are central to the functionality, security, and\nmaintainability of modern code. As developers increasingly turn to Large\nLanguage Models (LLMs) to assist with programming tasks, understanding how\nthese models recommend libraries is essential. In this paper, we conduct an\nempirical study of six state-of-the-art LLMs, both proprietary and open-source,\nby prompting them to solve real-world Python problems sourced from Stack\nOverflow. We analyze the types of libraries they import, the characteristics of\nthose libraries, and the extent to which the recommendations are usable out of\nthe box. Our results show that LLMs predominantly favour third-party libraries\nover standard ones, and often recommend mature, popular, and permissively\nlicensed dependencies. However, we also identify gaps in usability: 4.6% of the\nlibraries could not be resolved automatically due to structural mismatches\nbetween import names and installable packages, and only two models (out of six)\nprovided installation guidance. While the generated code is technically valid,\nthe lack of contextual support places the burden of manually resolving\ndependencies on the user. Our findings offer actionable insights for both\ndevelopers and researchers, and highlight opportunities to improve the\nreliability and usability of LLM-generated code in the context of software\ndependencies."}
{"id": "2507.10822", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10822", "abs": "https://arxiv.org/abs/2507.10822", "authors": ["Omar Elsisi", "Glaucia Melo"], "title": "Past, Present and Future: Exploring Adaptive AI in Software Development Bots", "comment": null, "summary": "Conversational agents, such as chatbots and virtual assistants, have become\nessential in software development, boosting productivity, collaboration, and\nautomating various tasks. This paper examines the role of adaptive AI-powered\nconversational agents in software development, highlighting their ability to\noffer dynamic, context-aware assistance to developers. Unlike traditional\nrule-based systems, adaptive AI agents use machine learning and natural\nlanguage processing to learn from interactions and improve over time, providing\nmore personalized and responsive help. We look at how these tools have evolved\nfrom simple query-based systems to advanced AI-driven solutions like GitHub\nCopilot and Microsoft Teams bots. We also explore the challenges of integrating\nadaptive AI into software development processes. The study aims to assess the\nbenefits and limitations of these systems, address concerns like data privacy\nand ethical issues, and offer insights into their future use in the field.\nUltimately, adaptive AI chatbots have great potential to revolutionize software\ndevelopment by delivering real-time, customized support and enhancing the\nefficiency of development cycles."}
{"id": "2507.10906", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10906", "abs": "https://arxiv.org/abs/2507.10906", "authors": ["Qunhong Zeng", "Yuxia Zhang", "Zexiong Ma", "Bo Jiang", "Ningyuan Sun", "Klaas-Jan Stol", "Xingyu Mou", "Hui Liu"], "title": "Evaluating Generated Commit Messages with Large Language Models", "comment": null, "summary": "Commit messages are essential in software development as they serve to\ndocument and explain code changes. Yet, their quality often falls short in\npractice, with studies showing significant proportions of empty or inadequate\nmessages. While automated commit message generation has advanced significantly,\nparticularly with Large Language Models (LLMs), the evaluation of generated\nmessages remains challenging. Traditional reference-based automatic metrics\nlike BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit\nmessage quality, as they assume a one-to-one mapping between code changes and\ncommit messages, leading researchers to rely on resource-intensive human\nevaluation. This study investigates the potential of LLMs as automated\nevaluators for commit message quality. Through systematic experimentation with\nvarious prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs\ncombining Chain-of-Thought reasoning with few-shot demonstrations achieve near\nhuman-level evaluation proficiency. Our LLM-based evaluator significantly\noutperforms traditional metrics while maintaining acceptable reproducibility,\nrobustness, and fairness levels despite some inherent variability. This work\nconducts a comprehensive preliminary study on using LLMs for commit message\nevaluation, offering a scalable alternative to human assessment while\nmaintaining high-quality evaluation."}
{"id": "2507.11059", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11059", "abs": "https://arxiv.org/abs/2507.11059", "authors": ["Pavel Adamenko", "Mikhail Ivanov", "Aidar Valeev", "Rodion Levichev", "Pavel Zadorozhny", "Ivan Lopatin", "Dmitry Babayev", "Alena Fenogenova", "Valentin Malykh"], "title": "SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) in software engineering\nhas revealed critical limitations in existing benchmarks, particularly the\nwidely used SWE-bench dataset. Recent studies have uncovered severe data\ncontamination issues, e.g. SWE-bench reports 32.67% of successful patches\ninvolve direct solution leakage and 31.08\\% pass due to inadequate test cases.\nWe introduce SWE-MERA, a dynamic, continuously updated benchmark designed to\naddress these fundamental challenges through an automated collection of\nreal-world GitHub issues and rigorous quality validation. Our approach\nimplements a reliable pipeline that ensures quality while minimizing\ncontamination risks, resulting in approximately 10,000 potential tasks with 300\nsamples currently available. Evaluation using the Aider coding agent\ndemonstrates strong discriminative power in state-of-the-art models. We report\nperformance across a dozen recent LLMs evaluated on tasks collected between\nSeptember 2024 and June 2025."}
{"id": "2507.11092", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11092", "abs": "https://arxiv.org/abs/2507.11092", "authors": ["Gong Chen", "Wenjie Liu", "Xiaoyuan Xie", "Xunzhu Tang", "Tegawendé F. Bissyandé", "Songqiang Chen"], "title": "MT4DP: Data Poisoning Attack Detection for DL-based Code Search Models via Metamorphic Testing", "comment": "27 pages", "summary": "Recently, several studies have indicated that data poisoning attacks pose a\nsevere security threat to deep learning-based (DL-based) code search models.\nAttackers inject carefully crafted malicious patterns into the training data,\nmisleading the code search model to learn these patterns during training.\nDuring the usage of the poisoned code search model for inference, once the\nmalicious pattern is triggered, the model tends to rank the vulnerability code\nhigher. However, existing detection methods for data poisoning attacks on\nDL-based code search models remain insufficiently effective. To address this\ncritical security issue, we propose MT4DP, a Data Poisoning Attack Detection\nFramework for DL-based Code Search Models via Metamorphic Testing. MT4DP\nintroduces a novel Semantically Equivalent Metamorphic Relation (SE-MR)\ndesigned to detect data poisoning attacks on DL-based code search models.\nSpecifically, MT4DP first identifies the high-frequency words from search\nqueries as potential poisoning targets and takes their corresponding queries as\nthe source queries. For each source query, MT4DP generates two semantically\nequivalent follow-up queries and retrieves its source ranking list. Then, each\nsource ranking list is re-ranked based on the semantic similarities between its\ncode snippets and the follow-up queries. Finally, variances between the source\nand re-ranked lists are calculated to reveal violations of the SE-MR and warn\nthe data poisoning attack. Experimental results demonstrate that MT4DP\nsignificantly enhances the detection of data poisoning attacks on DL-based code\nsearch models, outperforming the best baseline by 191% on average F1 score and\n265% on average precision. Our work aims to promote further research into\neffective techniques for mitigating data poisoning threats on DL-based code\nsearch models."}
{"id": "2507.11146", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11146", "abs": "https://arxiv.org/abs/2507.11146", "authors": ["Tom Yaacov", "Gera Weiss", "Gal Amram", "Avi Hayoun"], "title": "Automata Models for Effective Bug Description", "comment": "Accepted to the ACM/IEEE 28th International Conference on Model\n  Driven Engineering Languages and Systems (MODELS 2025)", "summary": "Debugging complex systems is a crucial yet time-consuming task. This paper\npresents the use of automata learning and testing techniques to obtain concise\nand informative bug descriptions. We introduce the concepts of Failure\nExplanations (FE), Eventual Failure Explanations (EFE), and Early Detection\n(ED) to provide meaningful summaries of failing behavior patterns. By factoring\nout irrelevant information and focusing on essential test patterns, our\napproach aims to enhance bug detection and understanding. We evaluate our\nmethods using various test patterns and real-world benchmarks, demonstrating\ntheir effectiveness in producing compact and informative bug descriptions."}
{"id": "2507.11199", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11199", "abs": "https://arxiv.org/abs/2507.11199", "authors": ["Jinhan Kim", "Nargiz Humbatova", "Gunel Jahangirova", "Shin Yoo", "Paolo Tonella"], "title": "New Formulation of DNN Statistical Mutation Killing for Ensuring Monotonicity: A Technical Report", "comment": null, "summary": "Mutation testing has emerged as a powerful technique for evaluating the\neffectiveness of test suites for Deep Neural Networks. Among existing\napproaches, the statistical mutant killing criterion of DeepCrime has leveraged\nstatistical testing to determine whether a mutant significantly differs from\nthe original model. However, it suffers from a critical limitation: it violates\nthe monotonicity property, meaning that expanding a test set may result in\npreviously killed mutants no longer being classified as killed. In this\ntechnical report, we propose a new formulation of statistical mutant killing\nbased on Fisher exact test that preserves the statistical rigour of it while\nensuring monotonicity."}
{"id": "2507.11272", "categories": ["cs.SE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.11272", "abs": "https://arxiv.org/abs/2507.11272", "authors": ["Anh Nguyen-Duc", "Chien Vu Manh", "Bao Anh Tran", "Viet Phuong Ngo", "Luan Le Chi", "Anh Quang Nguyen"], "title": "An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling", "comment": null, "summary": "This paper presents MARAUS (Multi-Agent and Retrieval-Augmented University\nAdmission System), a real-world deployment of a conversational AI platform for\nhigher education admissions counseling in Vietnam. While large language models\n(LLMs) offer potential for automating advisory tasks, most existing solutions\nremain limited to prototypes or synthetic benchmarks. MARAUS addresses this gap\nby combining hybrid retrieval, multi-agent orchestration, and LLM-based\ngeneration into a system tailored for real-world university admissions. In\ncollaboration with the University of Transport Technology (UTT) in Hanoi, we\nconducted a two-phase study involving technical development and real-world\nevaluation. MARAUS processed over 6,000 actual user interactions, spanning six\ncategories of queries. Results show substantial improvements over LLM-only\nbaselines: on average 92 percent accuracy, hallucination rates reduced from 15\nprecent to 1.45 percent, and average response times below 4 seconds. The system\noperated cost-effectively, with a two-week deployment cost of 11.58 USD using\nGPT-4o mini. This work provides actionable insights for the deployment of\nagentic RAG systems in low-resource educational settings."}
{"id": "2507.11346", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11346", "abs": "https://arxiv.org/abs/2507.11346", "authors": ["Pedro Simões", "Rohit Gheyi", "Rian Melo", "Jonhnanthan Oliveira", "Márcio Ribeiro", "Wesley K. G. Assunção"], "title": "RefModel: Detecting Refactorings using Foundation Models", "comment": "Accepted at Brazilian Symposium on Software Engineering (SBES 2025)", "summary": "Refactoring is a common software engineering practice that improves code\nquality without altering program behavior. Although tools like ReExtractor+,\nRefactoringMiner, and RefDiff have been developed to detect refactorings\nautomatically, they rely on complex rule definitions and static analysis,\nmaking them difficult to extend and generalize to other programming languages.\nIn this paper, we investigate the viability of using foundation models for\nrefactoring detection, implemented in a tool named RefModel. We evaluate\nPhi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation\ntransformations applied to artificially generated Java programs, covering\nwidely-used refactoring types. We also extend our evaluation by including\nGemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world\nrefactorings extracted from four open-source projects. These models are\ncompared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is\ncompetitive with, and in some cases outperform, traditional tools. In\nreal-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified\n97% of all refactorings, surpassing the best-performing static-analysis-based\ntools. The models showed encouraging generalization to Python and Golang. They\nprovide natural language explanations and require only a single sentence to\ndefine each refactoring type."}
{"id": "2507.11362", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11362", "abs": "https://arxiv.org/abs/2507.11362", "authors": ["Chaima Boufaied", "Taher Ghaleb", "Zainab Masood"], "title": "Security Debt in Practice: Nuanced Insights from Practitioners", "comment": null, "summary": "With the increasing reliance on software and automation nowadays, tight\ndeadlines, limited resources, and prioritization of functionality over security\ncan lead to insecure coding practices. When not handled properly, these\nconstraints cause unaddressed security vulnerabilities to accumulate over time,\nforming Security Debts (SDs). Despite their critical importance, there is\nlimited empirical evidence on how software practitioners perceive, manage, and\ncommunicate SDs in real-world settings. In this paper, we present a qualitative\nempirical study based on semi-structured interviews with 22 software\npractitioners across various roles, organizations, and countries. We address\nfour research questions: i) we assess software practitioners' knowledge of SDs\nand awareness of associated security risks, ii) we investigate their behavior\ntowards SDs, iii) we explore common tools and strategies used to mitigate SDs,\nand iv) we analyze how security risks are communicated within teams and to\ndecision makers. We observe variations in how practitioners perceive and manage\nSDs, with some prioritizing delivery speed over security, while others\nconsistently maintain security as a priority. Our findings emphasize the need\nfor stronger integration of security practices across the Software Development\nLife Cycle (SDLC), more consistent use of mitigation strategies, better\nbalancing of deadlines, resources, and security-related tasks, with attention\nto the Confidentiality, Integrity, and Availability (CIA) triad."}
