<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: Peer code review significantly boosts research software quality and reliability, but research software engineers encounter unique adoption challenges. By implementing structured processes, better tools, and RSE-specific training, these challenges can be overcome, leading to broader and more effective code review practices in research computing.


<details>
  <summary>Details</summary>
Motivation: Research software is essential for scientific progress, but its quality and maintainability are often limited by complex requirements and outdated dependencies. While peer code review can help address these issues, its uptake among research software engineers (RSEs) is not well understood.

Method: The researchers conducted a survey among RSEs, designing the questions to allow comparison with previous studies while also including RSE-specific queries, to gather detailed insights about peer code review practices and attitudes.

Result: The study collected 61 valid survey responses. The results both corroborate existing findings and reveal unique challenges and practices specific to RSEs, differentiating them from the broader group of research software developers.

Conclusion: Peer code review is crucial for improving the quality, maintainability, and reliability of research software. Although RSEs face specific challenges, these can be mitigated through structured processes, better tools, and targeted training to promote and enhance the adoption and effectiveness of peer code review.

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [2] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: This paper introduces a scalable, LLM-assisted human-in-the-loop framework to judge patch validity for Automated Program Repair. By using LLM-generated, human-refined rubrics, the system achieves high agreement with human consensus and greatly reduces manual evaluation effort, though there's room for improvement in challenging cases.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the need for more reliable and scalable evaluation methods in Automated Program Repair (APR). Current benchmarks often use unit test pass rates as evaluation, but these do not accurately reflect true patch validity. Manually determining the validity of patches is costly and time-consuming. This paper aims to reduce the human cost while achieving reliable patch validation.

Method: The proposed method is a human-in-the-loop approach leveraging Large Language Models (LLMs). First, an LLM generates a per-bug rubric for patch validation. Humans review and optionally refine this rubric once per bug. Then, using the refined rubric, LLMs judge the validity of program patches. The method is applied to patches related to issues found by Google sanitizer tools.

Result: The approach achieves substantial agreement with human consensus (Cohen's kappa 0.75) as well as high recall (0.94) and high precision (0.80) on patches where human raters unanimously agree. For all patches, even those with non-unanimous human raters, the approach still shows reasonable performance (Cohen's kappa 0.57, recall 0.93, precision 0.65), though improvement is possible.

Conclusion: The proposed LLM-assisted, human-in-the-loop framework shows promise for scalable and reliable evaluation of program patches, significantly reducing human labor without sacrificing alignment with human judgment. Future improvements could further enhance its performance on ambiguous cases.

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [3] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: The paper presents an approach combining LLM-driven source code instrumentation with conformance checking to detect control-flow anomalies in complex systems at run-time. Tested on a railway standard, it achieved high coverage and detection accuracy, indicating the effectiveness of LLMs when enhanced with domain expertise.


<details>
  <summary>Details</summary>
Motivation: Modern computer-based systems have complex behaviors, making it difficult to ensure reliable operation. Traditional design-time validation may miss run-time control-flow anomalies, especially those resulting from unknown issues.

Method: The paper proposes a methodology that utilizes Large Language Models (LLMs) to automate the process of instrumenting source code for software monitoring. This instrumentation generates execution logs which are then analyzed using conformance checking to identify control-flow anomalies.

Result: On a case study involving the ERTMS/ETCS railway standard, the methodology achieved 84.775% control-flow coverage in the reference process model. The anomaly detection using conformance checking reached a 96.610% F1-score and 93.515% AUC.

Conclusion: Incorporating domain-specific knowledge to guide LLMs during source-code instrumentation was essential for generating reliable execution logs, resulting in effective detection of control-flow anomalies via conformance checking.

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [4] [Beyond Accuracy: Behavioral Dynamics of Agentic Multi-Hunk Repair](https://arxiv.org/abs/2511.11012)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: This paper finds that state-of-the-art coding agents have variable but often limited success repairing challenging multi-hunk bugs. Performance drops with bug complexity, repairs often use excessive resources when they fail, but supplying agents with broader code context (via the Maple tool) can drive substantial accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Automated program repair has traditionally focused on single-hunk defects, neglecting the more complex and prevalent multi-hunk bugs found in real-world software systems. Addressing multi-hunk bugs is challenging because it requires coordinated modifications across multiple, often unrelated, sections of code.

Method: The paper conducts a systematic evaluation of various LLM-driven coding agents such as Claude Code, Codex, Gemini-cli, and Qwen Code. It examines their ability to fix multi-hunk bugs using 372 defects from the Hunk4J dataset and analyzes 1,488 repair attempts with fine-grained metrics encompassing localization, repair accuracy, regression patterns, and operational resource usage. Additionally, the tool Maple is introduced to enhance repository-level context and agent performance.

Result: Agents show highly variable performance, with repair accuracy ranging from 25.8% to 93.3%. As bug complexity increases, accuracy drops. High-performing agents maintain semantic consistency and reduce regressions, while others may cause new failures. Failed repair attempts consume significantly more computational resources and time. The Maple tool boosts Gemini-cli’s repair accuracy by 30% owing to better bug localization.

Conclusion: The study provides new insights into how LLM-driven coding agents approach multi-hunk program repair, emphasizing the need for better localization and resource efficiency. It demonstrates that strategic enhancements, like repository-level contextualization, substantially improve agent performance, and underscores the complexity and resource demands of multi-hunk bug repair.

Abstract: Automated program repair has traditionally focused on single-hunk defects, overlooking multi-hunk bugs that are prevalent in real-world systems. Repairing these bugs requires coordinated edits across multiple, disjoint code regions, posing substantially greater challenges. We present the first systematic study of LLM-driven coding agents (Claude Code, Codex, Gemini-cli, and Qwen Code) on this task. We evaluate these agents on 372 multi-hunk bugs from the Hunk4J dataset, analyzing 1,488 repair trajectories using fine-grained metrics that capture localization, repair accuracy, regression behavior, and operational dynamics. Results reveal substantial variation: repair accuracy ranges from 25.8% (Qwen Code) to 93.3% (Claude Code) and consistently declines with increasing bug dispersion and complexity. High-performing agents demonstrate superior semantic consistency, achieving positive regression reduction, whereas lower-performing agents often introduce new test failures. Notably, agents do not fail fast; failed repairs consume substantially more resources (39%-343% more tokens) and require longer execution time (43%-427%). Additionally, we developed Maple to provide agents with repository-level context. Empirical results show that Maple improves the repair accuracy of Gemini-cli by 30% through enhanced localization. By analyzing fine-grained metrics and trajectory-level analysis, this study moves beyond accuracy to explain how coding agents localize, reason, and act during multi-hunk repair.

</details>


### [5] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: This paper finds that companies can use LLMs with few-shot prompts to solve basic tasks in proprietary industrial automation languages without training specialized models or risking data privacy.


<details>
  <summary>Details</summary>
Motivation: Most research on LLMs in software engineering focuses on general-purpose languages, leaving their utility for industrial automation and proprietary domain-specific languages underexplored.

Method: The study explores the effectiveness of few-shot prompting with LLMs on proprietary domain-specific languages used in industrial process automation, without extensive domain-specific training, and on-premise to protect data privacy.

Result: Few-shot prompting enables LLMs to solve simple problems in niche, poorly-supported domain-specific languages, even without additional model training. These solutions can be executed on-premise to safeguard sensitive company data.

Conclusion: Enterprises can leverage LLMs effectively for proprietary automation languages using few-shot prompting approaches, achieving adequate results with minimal investment and maintaining data security.

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [6] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: SQuaD is a newly introduced, publicly available dataset with multidimensional, time-aware software quality metrics from 450 open-source projects. It enables wide-ranging empirical studies and supports further research through its scale and comprehensiveness.


<details>
  <summary>Details</summary>
Motivation: Existing software quality datasets are limited because they only focus on narrow aspects such as code smells or technical debt, which prevents comprehensive, large-scale analysis over time and multiple dimensions of quality.

Method: The authors created the Software Quality Dataset (SQuaD), assembling metrics from nine advanced static analysis tools across 450 mature open-source projects from various ecosystems. The dataset incorporates over 700 unique metrics at different granularities (method, class, file, project), and enriches this with version control histories, vulnerability data, and process metrics.

Result: The SQuaD dataset covers 63,586 project releases and provides extensive multi-dimensional software quality data, enabling large-scale empirical studies on maintainability, technical debt, evolution, and defect prediction.

Conclusion: SQuaD fills an important gap by enabling comprehensive, time-aware software quality research at scale. Its public availability supports new research directions like automated updates and cross-project modeling, advancing software analytics.

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [7] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: Logic-level misuse of smart contract components is dangerous and hard to detect. SCRUTINEER, a novel automated system combining advanced feature extraction and large language models, detects these issues with high accuracy on real datasets.


<details>
  <summary>Details</summary>
Motivation: Smart Contract Reusable Components (SCRs) speed up business contract development through modularity and code reuse. However, using SCRs incorrectly, especially at the logic level, poses significant security risks. Traditional detection methods struggle to uncover these violations as they require deep semantic understanding of specific business logic.

Method: The authors propose SCRUTINEER, an automated system to detect logic-level usage violations in SCRs. It combines composite feature extraction (three complementary representations), a large language model-powered knowledge construction framework, a retrieval-augmented generation inspector, and a logic-level violation analysis engine that uses similarity and snapshot-based inference checking.

Result: SCRUTINEER was tested on three ground-truth datasets, achieving a precision of 80.77%, recall of 82.35%, and F1-score of 81.55% in detecting logic-level SCR usage violations.

Conclusion: SCRUTINEER is a practical and effective solution for uncovering complex logic-level usage violations in smart contract reusable components, greatly improving detection accuracy compared to prior approaches.

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [8] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: This paper introduces CertiA360, an automation tool designed to integrate Agile methods into aerospace software development while maintaining strict regulatory compliance (DO-178C). Validated by industry experts, the tool improves traceability, automates changes, reduces manual effort, and shows that Agile practices can complement safety-critical certification processes.


<details>
  <summary>Details</summary>
Motivation: Integrating Agile methods into the aerospace industry's safety-critical software development is challenging, especially due to strict regulatory requirements such as DO-178C. There is a need to align Agile’s flexibility with certification mandates.

Method: The study developed and validated CertiA360, an automation tool designed in consultation with aerospace experts to improve requirements maturity and automate traceability changes within Agile methodologies, supporting regulatory compliance.

Result: CertiA360 automates and manages change requests, ensures robust traceability, and supports regulatory compliance. Expert feedback indicated reduced manual effort, improved response to changing requirements, and alignment with DO-178C standards. The tool is still pending DO-330 qualification.

Conclusion: Agile methods, when appropriately adapted and supported by automation tools like CertiA360, can coexist with and even enhance efficiency within the regulatory framework of aerospace safety-critical systems.

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [Data Race Detection by Digest-Driven Abstract Interpretation (Extended Version)](https://arxiv.org/abs/2511.11055)
*Michael Schwarz,Julian Erhard*

Main category: cs.PL

TL;DR: By extending digest summaries to include concurrency aspects beyond locksets, the paper presents a formal framework and practical implementation that dramatically boost the performance of static data race detection in multi-threaded programs.


<details>
  <summary>Details</summary>
Motivation: Existing lockset-based static analysis for data race detection is limited, often failing to account for more complex concurrency patterns and thread interactions.

Method: The authors introduce and formalize digest-based summarization concepts to capture and exclude potential race conditions in thread-modular local trace semantics, implementing these in the Goblint static analyzer.

Result: Combining lockset digests with additional reasoning on thread IDs and thread joins yields a fivefold increase in correctly solved data race-free tasks on SV-COMP benchmarks, indicating a major improvement in detection precision.

Conclusion: Digest-driven data race detection significantly improves the accuracy of static race detection over basic lockset reasoning, increasing the number of correctly solved benchmark tasks in Goblint.

Abstract: Sound static analysis can prove the absence of data races by establishing that no two conflicting memory accesses can occur at the same time. We repurpose the concept of digests -- summaries of computational histories originally introduced to bring tunable concurrency-sensitivity to thread-modular value analysis by abstract interpretation, extending this idea to race detection: We use digests to capture the conditions under which conflicting accesses may not happen in parallel. To formalize this, we give a definition of data races in the thread-modular local trace semantics and show how exclusion criteria for potential conflicts can be expressed as digests. We report on our implementation of digest-driven data race detection in the static analyzer Goblint, and evaluate it on the SV-COMP benchmark suite. Combining the lockset digest with digests reasoning on thread ids and thread joins increases the number of correctly solved tasks by more than a factor of five compared to lockset reasoning alone.

</details>


### [10] [Optimising Density Computations in Probabilistic Programs via Automatic Loop Vectorisation](https://arxiv.org/abs/2511.11070)
*Sangho Lim,Hyoungjin Lim,Wonyeol Lee,Xavier Rival,Hongseok Yang*

Main category: cs.PL

TL;DR: A new method for automatically vectorising loops in probabilistic programming languages greatly increases inference performance with broader applicability than previous vectorisation techniques, as shown by speedups up to 6x and reduced memory usage in tested models.


<details>
  <summary>Details</summary>
Motivation: Probabilistic inference in probabilistic programming languages (PPLs) is computationally expensive, especially due to the iteration over large datasets or samples. Manual vectorisation is error-prone, and current automatic techniques are limited, often failing with complex loops or requiring significant user intervention.

Method: The authors propose a formal method to automatically vectorise loops in probabilistic programs using speculative parallel execution and a fixed-point check to maintain original loop semantics. This is realized via a translation from an imperative PPL to a lower-level, vectorisation-friendly language. The method was implemented in Pyro and experimentally evaluated.

Result: Experimental results demonstrate significant performance improvements, with speedups ranging from 1.1x to 6x and reduced GPU memory usage over an existing vectorisation baseline. The method also successfully handled every tested model, whereas the baseline was limited to a subset.

Conclusion: The proposed method provides sound, general, and automated vectorisation for loops in probabilistic programs, leading to substantial efficiency gains and broader applicability compared to existing approaches.

Abstract: Probabilistic programming languages (PPLs) are a popular tool for high-level modelling across many fields. They provide a range of algorithms for probabilistic inference, which analyse models by learning their parameters from a dataset or estimating their posterior distributions. However, probabilistic inference is known to be very costly. One of the bottlenecks of probabilistic inference stems from the iteration over entries of a large dataset or a long series of random samples. Vectorisation can mitigate this cost, but manual vectorisation is error-prone, and existing automatic techniques are often ad-hoc and limited, unable to handle general repetition structures, such as nested loops and loops with data-dependent control flow, without significant user intervention. To address this bottleneck, we propose a sound and effective method for automatically vectorising loops in probabilistic programs. Our method achieves high throughput using speculative parallel execution of loop iterations, while preserving the semantics of the original loop through a fixed-point check. We formalise our method as a translation from an imperative PPL into a lower-level target language with primitives geared towards vectorisation. We implemented our method for the Pyro PPL and evaluated it on a range of probabilistic models. Our experiments show significant performance gains against an existing vectorisation baseline, achieving $1.1$--$6\times$ speedups and reducing GPU memory usage in many cases. Unlike the baseline, which is limited to a subset of models, our method effectively handled all the tested models.

</details>


### [11] [Kleene Algebra](https://arxiv.org/abs/2511.11264)
*Tobias Kappé,Alexandra Silva,Jana Wagemaker*

Main category: cs.PL

TL;DR: The booklet introduces Kleene Algebra as a tool for proving program equivalence, explaining its connection to regular expressions and automata, and supports learning with exercises and a coalgebraic perspective.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide an accessible introduction to Kleene Algebra (KA), explaining its laws and significance in understanding program equivalences. The work aims to help readers develop intuition for using KA to model programs and to connect regular expressions to automata.

Method: The booklet explains the basic laws of KA, demonstrates the modeling of programs through regular expressions, shows their correspondence to automata, and uses these tools to prove when two regular expressions are equivalent within KA. It also provides exercises and an optional chapter relating automata theory to coalgebra.

Result: The booklet demonstrates that an equivalence between two regular expressions holds if and only if it can be logically proven using the established laws of Kleene Algebra. It provides readers with both theoretical background and practical exercises.

Conclusion: Kleene Algebra offers a foundational framework for proving program equivalence via regular expressions and automata, and the booklet supports intuitive learning with theory, exercises, and perspectives from automata theory and coalgebra.

Abstract: This booklet serves as an introduction to Kleene Algebra (KA), a set of laws that can be used to study general equivalences between programs. It discusses how general programs can be modeled using regular expressions, how those expressions correspond to automata, and how this correspondence can be exploited to obtain the central result of KA, namely that an equivalence of regular expressions is true if and only if it can be proved using the laws of KA. Each chapter closes with a set of exercises to further build intuition and understanding, and there is an optional chapter that develops automata theory through the lens of coalgebra.

</details>


### [12] [The Jasmin Compiler Preserves Cryptographic Security](https://arxiv.org/abs/2511.11292)
*Santiago Arranz-Olmos,Gilles Barthe,Lionel Blatter,Benjamin Grégoire,Vincent Laporte,Paolo Torrini*

Main category: cs.PL

TL;DR: This paper enhances the Jasmin compiler by formally proving it preserves cryptographic security (IND-CCA) in its front-end, using a new program logic and semantics, thus extending previous functional correctness guarantees to essential cryptographic contexts.


<details>
  <summary>Details</summary>
Motivation: Jasmin enables efficient, formally verified cryptographic implementations, but prior functional correctness guarantees for its compiler did not encompass nonterminating or probabilistic computations—both crucial for cryptography. The paper addresses this gap.

Method: The authors introduce a Relational Hoare Logic designed for compiler correctness proofs, prove its soundness with respect to a new denotational semantics of Jasmin programs using interaction trees, and use this logic to prove the functional correctness and cryptographic security preservation of the Jasmin compiler's front-end.

Result: They have formally shown that 25 out of 30 passes of the Jasmin compiler's front-end preserve cryptographic security, focusing on IND-CCA security, leveraging their new program logic and semantics.

Conclusion: The Jasmin compiler not only remains functionally correct, but its front-end also preserves important cryptographic security properties, confirmed by formal proofs in the Rocq prover.

Abstract: Jasmin is a programming and verification framework for developing efficient, formally verified, cryptographic implementations. A main component of the framework is the Jasmin compiler, which empowers programmers to write efficient implementations of state-of-the-art cryptographic primitives, including post-quantum cryptographic standards. The Jasmin compiler is proven functionally correct in the Rocq prover. However, this functional correctness statement does not apply to nonterminating or probabilistic computations, which are essential features in cryptography.
  In this paper, we significantly enhance the guarantees of the compiler by showing, in the Rocq prover, that its front-end (25 out of 30 passes) preserves cryptographic security. To this end, we first define a Relational Hoare Logic tailored for compiler correctness proofs. We prove the soundness of our logic w.r.t. a new denotational semantics of Jasmin programs based on interaction trees. Secondly, we use our program logic to prove the functional correctness of the (unmodified) Jasmin compiler w.r.t. said semantics. Lastly, we formalize cryptographic security -- focusing on IND-CCA -- with interaction trees and prove that the Jasmin compiler preserves cryptographic security.

</details>
