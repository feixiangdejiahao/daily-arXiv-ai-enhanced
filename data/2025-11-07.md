<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: Improving the English proficiency of prompts reliably boosts the correctness of code generated by Large Language Models, highlighting the importance of clear and advanced natural language in software engineering AI tools.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to investigate an underexplored aspect of code generation using LLMs: the impact of English language proficiency of prompts on the quality and correctness of the generated code, independent of prompt structure.

Method: The method involves creating prompts for 164 programming tasks in the HumanEval dataset, systematically varying the English proficiency of these prompts from basic to advanced, and measuring the generated code's proficiency and correctness across different LLM models.

Result: Higher English proficiency in prompts consistently led to more correct code across all models, although the effect on code proficiency varied by model. Notably, LLMs generally default to an intermediate (B2) English level.

Conclusion: Natural language proficiency in prompts is a significant factor in controlling and improving code generation in LLM-powered tools, enabling developers to enhance the reliability and accuracy of AI-generated code.

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [2] [Tutorial Debriefing: Applied Statistical Causal Inference in Requirements Engineering](https://arxiv.org/abs/2511.03875)
*Julian Frattini,Hans-Martin Heyn,Robert Feldt,Richard Torkar*

Main category: cs.SE

TL;DR: Software engineering improvements require evidence their tools/processes/guidelines work. When experiments aren't possible, the paper advocates using statistical causal inference to analyze observational data and confirm actual benefits.


<details>
  <summary>Details</summary>
Motivation: Software engineering research aims to positively impact software producers and consumers by transferring research knowledge into practical tools, processes, and guidelines. However, assessing the true value of these contributions requires rigorous evidence of their causal effects.

Method: The abstract discusses two approaches: randomized controlled trials (RCTs) and statistical causal inference (SCI) from observational data. RCTs involve randomly allocating subjects to intervention and control groups, but when infeasible, SCI offers an alternative using statistical methods.

Result: SCI can provide reliable evidence of causality when RCTs are not possible, allowing researchers to understand the effects of tools, processes, or guidelines using observational data.

Conclusion: The paper emphasizes the importance of causal evidence for software engineering research outputs and proposes that SCI is a viable substitute for RCTs in cases where experiments are not feasible.

Abstract: As any scientific discipline, the software engineering (SE) research
community strives to contribute to the betterment of the target population of
our research: software producers and consumers. We will only achieve this
betterment if we manage to transfer the knowledge acquired during research into
practice. This transferal of knowledge may come in the form of tools,
processes, and guidelines for software developers. However, the value of these
contributions hinges on the assumption that applying them causes an improvement
of the development process, user experience, or other performance metrics. Such
a promise requires evidence of causal relationships between an exposure or
intervention (i.e., the contributed tool, process or guideline) and an outcome
(i.e., performance metrics). A straight-forward approach to obtaining this
evidence is via controlled experiments in which a sample of a population is
randomly divided into a group exposed to the new tool, process, or guideline,
and a control group. However, such randomized control trials may not be
legally, ethically, or logistically feasible. In these cases, we need a
reliable process for statistical causal inference (SCI) from observational
data.

</details>


### [3] [Collaborative Agents for Automated Program Repair in Ruby](https://arxiv.org/abs/2511.03925)
*Nikta Akbarpour,Mahdieh Sadat Benis,Fatemeh Hendijani Fard,Ali Ouni,Mohamed Aymen Saied*

Main category: cs.SE

TL;DR: RAMP introduces a lightweight, multi-agent Automated Program Repair approach for Ruby, outperforming previous methods with quick convergence and resource efficiency, highlighting the value of feedback-driven test generation and self-reflection.


<details>
  <summary>Details</summary>
Motivation: Ruby, despite its popularity in web development, has received little attention in Automated Program Repair (APR) research, which has mostly focused on other languages and relies on expensive computational resources.

Method: RAMP is a lightweight framework employing a team of collaborative agents that iteratively generate tests, reflect on errors, and refine fixes for Ruby code using feedback-driven prompting, without large repair databases or costly fine-tuning.

Result: RAMP achieves a pass@1 of 67% on the Ruby XCodeEval benchmark, outperforming previous methods; it converges quickly within five iterations, with ablation studies showing test generation and self-reflection as performance keys, and is effective at fixing wrong answers, compilation errors, and runtime errors.

Conclusion: RAMP provides a fast, resource-efficient multi-agent APR approach for Ruby, demonstrating the feasibility of extending LLM-based debugging tools to under-studied languages and offering new perspectives for multi-agent repair strategies.

Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language
Models (LLMs), but most existing methods remain computationally expensive, and
focused on a small set of languages. Ruby, despite its widespread use in web
development and the persistent challenges faced by its developers, has received
little attention in APR research. In this paper, we introduce RAMP, a novel
lightweight framework that formulates program repair as a feedback-driven,
iterative process for Ruby. RAMP employs a team of collaborative agents that
generate targeted tests, reflect on errors, and refine candidate fixes until a
correct solution is found. Unlike prior approaches, RAMP is designed to avoid
reliance on large multilingual repair databases or costly fine-tuning, instead
operating directly on Ruby through lightweight prompting and test-driven
feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a
pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly
within five iterations, and ablation studies confirm that test generation and
self-reflection are key drivers of its performance. Further analysis shows that
RAMP is particularly effective at repairing wrong answers, compilation errors,
and runtime errors. Our approach provides new insights into multi-agent repair
strategies, and establishes a foundation for extending LLM-based debugging
tools to under-studied languages.

</details>


### [4] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: This paper introduces a multi-agent, self-correcting system combining LLMs and simulations to automate and improve Register Transfer Level (RTL) code generation from natural language. It achieves new state-of-the-art results in both accuracy and efficiency on standard datasets.


<details>
  <summary>Details</summary>
Motivation: Register Transfer Level (RTL) code generation from natural language is a complex task requiring precise translation, correctness, and efficiency. Automation of this process could significantly streamline hardware design, but prior approaches struggle with error correction and leveraging both open- and closed-source LLMs.

Method: The authors propose an agentic flow composed of multiple agents, each integrating specialized large language models (LLMs) and hardware simulation tools to generate RTL code without human intervention. The key innovation is the Progressive Error Feedback System of Agents (PEFA), which introduces a self-correcting, iterative error feedback loop. The flow checks the generated RTL for compilation, functional correctness, and synthesizability.

Result: The proposed method is benchmarked on two open-source natural language-to-RTL datasets. It achieves state-of-the-art pass rates, improves overall efficiency as measured by token usage, and effectively narrows the performance gap between open- and closed-source LLMs.

Conclusion: The agentic multi-LLM flow with progressive error feedback demonstrates substantial improvements in automated RTL generation, achieving higher accuracy and efficiency over previous methods.

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [5] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: PSD2Code introduces a multi-modal pipeline that accurately converts PSD design files into production-ready React+SCSS code through advanced asset alignment and structured prompting with large language models, outperforming existing solutions in fidelity, consistency, and industrial usability.


<details>
  <summary>Details</summary>
Motivation: Current design-to-code systems struggle with structural inconsistencies, asset misalignment, and unreliable code readiness for production. There is a need for automated solutions that can more faithfully translate design prototypes into deployable frontend code.

Method: The paper introduces PSD2Code, a multi-modal pipeline called ParseAlignGenerate. It starts by parsing PSD files to extract hierarchical structures, layer properties, and metadata, then aligns assets using constraint-based methods. Finally, it generates frontend React+SCSS code via structured prompts fed to large language models, enabling controllable and high-quality output.

Result: PSD2Code achieves higher code similarity, better visual fidelity, and greater production readiness compared to existing approaches. It works robustly across different large language models, proving the value of integrating detailed design data for automated code generation.

Conclusion: PSD2Code represents a significant advance in design-driven frontend automation. Leveraging structured design data and multimodal large language models, it produces production-ready code and closes the gap between design and deployment.

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, existing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi-modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGenerate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relationships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates significant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [6] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: VulInstruct helps large language models detect new vulnerabilities by teaching them security specifications from past cases, resulting in significantly better vulnerability detection and discovery than previous methods.


<details>
  <summary>Details</summary>
Motivation: LLMs lack explicit knowledge of security specifications, which hinders their ability to detect vulnerabilities in code and differentiate between unsafe and patched code. This paper aims to fill that gap by guiding LLMs with knowledge of expected safe code behaviors.

Method: Specification-guided approach: VulInstruct extracts security specifications from historical vulnerabilities through high-quality patches and repeated violations, builds a knowledge base, retrieves relevant cases, and enables LLMs to reason about safe code behavior.

Result: On strict evaluation benchmarks, VulInstruct achieves a 45.0% F1-score (32.7% improvement), 37.7% recall (50.8% improvement), detects 24.3% of vulnerabilities (2.4x higher than baselines), and uncovers a new high-severity vulnerability in production code.

Conclusion: VulInstruct significantly improves the vulnerability detection capabilities of large language models by providing specification-guided reasoning, achieving higher accuracy and discovering previously unknown vulnerabilities.

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [7] [LLM-Driven Adaptive Source-Sink Identification and False Positive Mitigation for Static Analysis](https://arxiv.org/abs/2511.04023)
*Shiyin Lin*

Main category: cs.SE

TL;DR: AdaTaint uses LLMs and symbolic reasoning to make static vulnerability analysis more accurate by reducing false positives and improving recall, outperforming existing tools.


<details>
  <summary>Details</summary>
Motivation: Static analysis tools are useful for finding software vulnerabilities but struggle with incomplete source–sink specifications and generate too many false positives.

Method: AdaTaint is a static taint analysis framework that leverages large language models (LLMs) to adaptively infer source/sink specifications and uses neuro-symbolic reasoning to filter out spurious alerts. It combines model suggestions with program facts and constraints for improved accuracy and determinism.

Result: Evaluations on standard benchmarks (Juliet 1.3, SV-COMP-style C) and real-world projects show AdaTaint cuts false positives by 43.7% and increases recall by 11.2% over state-of-the-art (CodeQL, Joern, LLM-only pipelines), while retaining competitive runtime.

Conclusion: Combining LLM inference with symbolic validation using AdaTaint significantly enhances the accuracy and reliability of static vulnerability analysis, providing fewer false positives and higher recall without substantial performance cost.

Abstract: Static analysis is effective for discovering software vulnerabilities but
notoriously suffers from incomplete source--sink specifications and excessive
false positives (FPs). We present \textsc{AdaTaint}, an LLM-driven taint
analysis framework that adaptively infers source/sink specifications and
filters spurious alerts through neuro-symbolic reasoning. Unlike LLM-only
detectors, \textsc{AdaTaint} grounds model suggestions in program facts and
constraint validation, ensuring both adaptability and determinism.
  We evaluate \textsc{AdaTaint} on Juliet 1.3, SV-COMP-style C benchmarks, and
three large real-world projects. Results show that \textsc{AdaTaint} reduces
false positives by \textbf{43.7\%} on average and improves recall by
\textbf{11.2\%} compared to state-of-the-art baselines (CodeQL, Joern, and
LLM-only pipelines), while maintaining competitive runtime overhead. These
findings demonstrate that combining LLM inference with symbolic validation
offers a practical path toward more accurate and reliable static vulnerability
analysis.

</details>


### [8] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: The paper introduces a tough benchmark and a hybrid evaluation framework for LLM-based software development agents, showing current models succeed with only about half the requirements, mainly struggling with requirement coverage and self-verification. It guides future research toward better planning and requirement handling.


<details>
  <summary>Details</summary>
Motivation: Large language model (LLM)-based autonomous agents are transforming software engineering, but there are difficulties in scientifically evaluating them. Current benchmarks are too simplistic, and fair comparison between agent architectures is hindered by confounding variables from different implementations.

Method: The authors create E2EDevBench, a dynamically curated benchmark that simulates realistic software development scenarios. They also introduce a hybrid evaluation framework combining functional assessment via test cases and fine-grained LLM-based requirement verification. A controlled empirical study is performed with three agent architectures implemented on a unified base to focus on the impact of workflow designs.

Result: The agents fulfilled about 50% of requirements in the benchmark. Performance was highly dependent on how tasks were decomposed and how agents collaborated. Major issues included missing requirements and insufficient self-verification.

Conclusion: The paper delivers a more realistic benchmark, a thorough evaluation framework, and fundamental insights, highlighting the need for improvements in requirement comprehension and planning for LLM-based software agents.

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [9] [Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment](https://arxiv.org/abs/2511.04157)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.SE

TL;DR: LLMs' value preferences in software engineering are closer to AI practitioners than the general population, but their practical application of these values is inconsistent, indicating a need for better monitoring when using LLMs in requirements engineering.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are widely used in software engineering tasks, but it's unclear how closely their value judgments align with those of humans, especially regarding responsible AI values. This motivates an investigation to assess LLMs' alignment with human perspectives on key ethical and practical decisions in software development.

Method: The study evaluates 23 LLMs on four tasks: (T1) selecting responsible AI values, (T2) rating their importance in specific contexts, (T3) resolving trade-offs between competing values, and (T4) prioritizing software requirements reflecting those values. The results are compared to preferences from two human groups: a US-representative sample and AI practitioners.

Result: LLMs align more closely with AI practitioners than with the general US population, emphasizing values like fairness, privacy, transparency, safety, and accountability. However, there are notable inconsistencies between the values LLMs claim to uphold and how they apply those values in practical requirement prioritization, exposing faithfulness gaps.

Conclusion: LLMs demonstrate value preferences similar to AI practitioners but face inconsistency between their stated and applied behaviors. This presents practical risks if employed in requirements engineering without human oversight. There is a need for systematic benchmarking and monitoring of value alignment in AI-assisted software development.

Abstract: Large Language Models (LLMs) are increasingly employed in software
engineering tasks such as requirements elicitation, design, and evaluation,
raising critical questions regarding their alignment with human judgments on
responsible AI values. This study investigates how closely LLMs' value
preferences align with those of two human groups: a US-representative sample
and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key
responsible AI values, (T2) rating their importance in specific contexts, (T3)
resolving trade-offs between competing values, and (T4) prioritizing software
requirements that embody those values. The results show that LLMs generally
align more closely with AI practitioners than with the US-representative
sample, emphasizing fairness, privacy, transparency, safety, and
accountability. However, inconsistencies appear between the values that LLMs
claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4),
revealing gaps in faithfulness between stated and applied behavior. These
findings highlight the practical risk of relying on LLMs in requirements
engineering without human oversight and motivate the need for systematic
approaches to benchmark, interpret, and monitor value alignment in AI-assisted
software development.

</details>


### [10] [Explaining Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2511.04179)
*Oshando Johnson,Alexandra Fomina,Ranjith Krishnamurthy,Vaibhav Chaudhari,Rohith Kumar Shanmuganathan,Eric Bodden*

Main category: cs.SE

TL;DR: Traditional SAST tools suffer from poor usability due to vague warnings. This paper introduces SAFE, a plugin leveraging GPT-4o to provide clear vulnerability explanations, demonstrating improved developer understanding and response in a user study.


<details>
  <summary>Details</summary>
Motivation: Security vulnerabilities in software remain widespread, prompting widespread use of SAST tools. However, the effectiveness of these tools is limited by poor usability; developers often fail to grasp the significance of generic warnings, leading to overlooked critical vulnerabilities.

Method: The authors propose a hybrid approach: integrating Large Language Models (LLMs) to improve SAST tool explainability. Specifically, they introduce SAFE, an IDE plugin that uses GPT-4o to generate detailed explanations for vulnerabilities detected by SAST tools, covering causes, impacts, and mitigation strategies. They evaluate the approach through an expert user study.

Result: The study found that explanations generated by SAFE substantially help beginner to intermediate developers understand and address security vulnerabilities. This improvement leads to enhanced usability of SAST tools.

Conclusion: Using LLMs like GPT-4o can bridge the gap in SAST explainability, making vulnerability reports clearer and more actionable for developers. This approach enhances developers' ability to handle security issues and improves the utility of SAST tools.

Abstract: The prevalence of security vulnerabilities has prompted companies to adopt
static application security testing (SAST) tools for vulnerability detection.
Nevertheless, these tools frequently exhibit usability limitations, as their
generic warning messages do not sufficiently communicate important information
to developers, resulting in misunderstandings or oversight of critical
findings. In light of recent developments in Large Language Models (LLMs) and
their text generation capabilities, our work investigates a hybrid approach
that uses LLMs to tackle the SAST explainability challenges. In this paper, we
present SAFE, an Integrated Development Environment (IDE) plugin that leverages
GPT-4o to explain the causes, impacts, and mitigation strategies of
vulnerabilities detected by SAST tools. Our expert user study findings indicate
that the explanations generated by SAFE can significantly assist beginner to
intermediate developers in understanding and addressing security
vulnerabilities, thereby improving the overall usability of SAST tools.

</details>


### [11] [GITER: A Git-Based Declarative Exchange Model Using Kubernetes-Style Custom Resources](https://arxiv.org/abs/2511.04182)
*Christos Tranoris*

Main category: cs.SE

TL;DR: This paper proposes using Git repositories, inspired by Kubernetes concepts, as a flexible and auditable alternative to APIs and message brokers for asynchronous communication among distributed entities. The method boosts transparency, traceability, and autonomy, especially for cross-domain and air-gapped collaborations, but comes with trade-offs versus traditional integrations.


<details>
  <summary>Details</summary>
Motivation: Complex asynchronous communication between distributed entities often relies on message brokers or APIs, which can be opaque, hard to audit, and tightly coupled. There is a need for a transparent, loosely coupled, and auditable information exchange method, especially in multi-domain, inter-organizational, or air-gapped environments.

Method: The approach uses Git as the central coordination and communication medium, inspired by Kubernetes Operators and Custom Resources. Entities, termed Publishers and Consumers, interact via a shared Git repository, with 'spec' fields representing desired states and 'status' fields showing outcomes. Git's built-in features like version control, commit signing, and access control underpin the system. Architectural principles and implementation details are discussed, as are comparisons to traditional REST and broker-based systems.

Result: The Git-based model enables transparent, traceable, and reproducible asynchronous information exchange. It supports loose coupling and autonomy between systems, facilitating collaboration across organizations and domains, including in air-gapped scenarios. Advantages such as audibility and declarativity are highlighted, alongside trade-offs compared to traditional integration methods.

Conclusion: Using Git as a declarative communication substrate provides a lightweight, auditable mechanism for distributed asynchronous exchange. The model expands GitOps principles to broader applications, ensures traceability and autonomy, and offers alternative integration patterns for complex environments.

Abstract: This paper introduces a lightweight and auditable method for asynchronous
information exchange between distributed entities using Git as the coordination
medium. The proposed approach replaces traditional APIs and message brokers
with a Git-based communication model built on the principles of Kubernetes
Operators and Custom Resources (CRs). Each participating entity, designated as
a Publisher or Consumer, interacts through a shared repository that serves as a
single source of truth, where the spec field captures the desired state and the
status field reflects the observed outcome. This pattern extends GitOps beyond
infrastructure management to support cross-domain, inter-organizational, and
air-gapped collaboration scenarios. By leveraging Git native features
(versioning, commit signing, and access control) the model ensures
transparency, traceability, and reproducibility while preserving loose coupling
and autonomy between systems. The paper discusses architectural principles,
implementation considerations, and comparisons with RESTful and broker-based
integrations, highlighting both the advantages and trade-offs of adopting Git
as a declarative communication substrate.

</details>


### [12] [A Tool for Benchmarking Large Language Models' Robustness in Assessing the Realism of Driving Scenarios](https://arxiv.org/abs/2511.04267)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali*

Main category: cs.SE

TL;DR: DriveRLR benchmark assesses how well LLMs judge the realism of driving scenarios, showing different robustness levels between models and offering practical value for autonomous driving simulation testing.


<details>
  <summary>Details</summary>
Motivation: As simulation-based testing becomes more critical for autonomous driving due to the risks and costs of real-world tests, effectively assessing the realism of simulated scenarios remains challenging. The reasoning power of LLMs can address this gap.

Method: DriveRLR creates mutated scenario variants and corresponding textual prompts, which are used to test LLMs' ability to evaluate scenario realism. The tool was validated using the DeepScenario dataset and tested on three advanced LLMs.

Result: DriveRLR demonstrates that it can discriminate the robustness of different LLMs, proving its utility in realism assessment and supporting simulation-based autonomous driving system testing workflows.

Conclusion: DriveRLR is an effective benchmark tool for evaluating the robustness of LLMs in assessing the realism of autonomous driving scenarios. It can reveal differences among LLMs and can be practically applied in driving simulation workflows.

Abstract: In recent years, autonomous driving systems have made significant progress,
yet ensuring their safety remains a key challenge. To this end, scenario-based
testing offers a practical solution, and simulation-based methods have gained
traction due to the high cost and risk of real-world testing. However,
evaluating the realism of simulated scenarios remains difficult, creating
demand for effective assessment methods. Recent advances show that Large
Language Models (LLMs) possess strong reasoning and generalization
capabilities, suggesting their potential in assessing scenario realism through
scenario-related textual prompts. Motivated by this, we propose DriveRLR, a
benchmark tool to assess the robustness of LLMs in evaluating the realism of
driving scenarios. DriveRLR generates mutated scenario variants, constructs
prompts, which are then used to assess a given LLM's ability and robustness in
determining the realism of driving scenarios. We validate DriveRLR on the
DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4
Maverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals
differences in the robustness of various LLMs, demonstrating its effectiveness
and practical value in scenario realism assessment. Beyond LLM robustness
evaluation, DriveRLR can serve as a practical component in applications such as
an objective function to guide scenario generation, supporting simulation-based
ADS testing workflows.

</details>


### [13] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: Benchmarks for code generation LLMs mostly show rankings but conceal common failures. This study inspects where and why LLMs fail, discovering four main weakness patterns and repeated task challenges, providing vital insight for improving LLMs.


<details>
  <summary>Details</summary>
Motivation: While LLMs excel in code generation and are evaluated through benchmarks and leaderboards, these do not sufficiently reveal the specific tasks where models consistently fail, which is vital for understanding limitations.

Method: The authors analyzed code generation tasks across four popular benchmarks, pinpointing tasks that major LLMs tend to fail. They also examined whether static code complexity is a root cause and conducted a systematic review of 114 tasks with consistent failures.

Result: The analysis uncovered four repeated types of weaknesses in LLMs and identified common task complications that are most likely to cause failures.

Conclusion: Standard benchmarks reveal overall performance but hide recurring failure modes. By systematically analyzing failed tasks and their characteristics, the study highlights key weaknesses and recommends focus areas for future LLM development.

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [14] [Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development](https://arxiv.org/abs/2511.04427)
*Hao He,Courtney Miller,Shyam Agarwal,Christian Kästner,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: LLM agent Cursor makes software development faster at first, but increases code complexity and errors, causing long-term slowdown.


<details>
  <summary>Details</summary>
Motivation: There is a growing belief that LLM agent assistants dramatically improve software development productivity, but solid empirical evidence is missing.

Method: The paper uses a difference-in-differences research design comparing GitHub projects that adopted Cursor (an LLM agent assistant) with similar control projects that did not, along with panel generalized method of moments estimation for further analysis.

Result: Cursor adoption yields a large but short-lived increase in development velocity, while causing enduring increases in static analysis warnings and code complexity. These drawbacks eventually slow down long-term development velocity.

Conclusion: While LLM agents like Cursor boost initial productivity, their use leads to more complex and warning-prone code, which ultimately hinders sustained project velocity. This finding informs practitioners, designers, and researchers of the nuanced impacts of LLM agent adoption.

Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize
the field of software engineering. Among other things, LLM agents are rapidly
gaining momentum in their application to software development, with
practitioners claiming a multifold productivity increase after adoption. Yet,
empirical evidence is lacking around these claims. In this paper, we estimate
the causal effect of adopting a widely popular LLM agent assistant, namely
Cursor, on development velocity and software quality. The estimation is enabled
by a state-of-the-art difference-in-differences design comparing
Cursor-adopting GitHub projects with a matched control group of similar GitHub
projects that do not use Cursor. We find that the adoption of Cursor leads to a
significant, large, but transient increase in project-level development
velocity, along with a significant and persistent increase in static analysis
warnings and code complexity. Further panel generalized method of moments
estimation reveals that the increase in static analysis warnings and code
complexity acts as a major factor causing long-term velocity slowdown. Our
study carries implications for software engineering practitioners, LLM agent
assistant designers, and researchers.

</details>


### [15] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: EDIT-Bench is a new real-world benchmark for code editing by LLMs, showing current models struggle with realistic developer tasks, and highlighting the importance of authentic context in evaluation.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLM code editing are primarily based on artificial datasets and do not fully represent real-world usage. There's a need for evaluation grounded in actual developer instructions and contexts.

Method: The authors introduce EDIT-Bench, a new benchmark consisting of 545 problems collected from real-world user instructions and code contexts. The benchmark covers multiple natural and programming languages and includes a variety of use cases. It specifically evaluates context-dependent problems, requiring understanding of code, instructions, highlighted segments, and cursor position.

Result: 40 different LLMs were evaluated on EDIT-Bench. Only 5 models scored over 60%, demonstrating the challenge presented by realistic contexts. Performance varied by task category and was strongly affected by the amount of contextual information provided, with task success rates changing by up to 11%.

Conclusion: EDIT-Bench provides a challenging, realistic benchmark for evaluating LLM code editing capabilities, emphasizing the need to assess models in authentic contexts. Model performance significantly depends on the realism of the provided context.

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>


### [16] [Microservices Is Dying, A New Method for Module Division Based on Universal Interfaces](https://arxiv.org/abs/2511.04548)
*Qing Wang,Yong Zhang*

Main category: cs.SE

TL;DR: Instead of simply separating modules as in microservices, the paper introduces a way to rigorously define and achieve module independence through universal interfaces. This foundation allows flexible, runtime module changes even in monolithic systems, and the new EIGHT platform demonstrates this principle in practice, suggesting a new direction for system architecture design.


<details>
  <summary>Details</summary>
Motivation: Microservices have physically separated modules but often fail to eliminate dependency propagation, causing inter-module coupling and limiting module independence.

Method: The paper introduces a method to calculate module independence, deduces necessary independence conditions, and proposes a design approach involving universal interfaces to isolate modules. This is demonstrated via the implementation of a platform architecture (EIGHT).

Result: The EIGHT architecture shows that guaranteeing module independence allows dynamic modification (loading/unloading/changing parts) at runtime, even in a monolithic application.

Conclusion: The proposed methodology and architecture provide a new pathway for designing complex systems, overcoming limitations of both traditional monolithic and microservice architectures.

Abstract: Although microservices have physically isolated modules, they have failed to
prevent the propagation and diffusion of dependencies. To trace the root cause
of the inter-module coupling, this paper, starting from the impact assessment
approach for module changes, proposes a conceptual method for calculating
module independence and utilizes this method to derive the necessary conditions
for module independence. Then, a new system design philosophy and software
engineering methodology is proposed, aimed at eliminating dependencies between
modules. A specific pattern is employed to design a set of universal
interfaces, serving as a universal boundary between modules. Subsequently, this
method is used to implement a platform architecture named EIGHT, demonstrating
that, as long as module independence is guaranteed, even a monolithic
application within a single process can dynamically load, unload, or modify any
part at runtime. Finally, the paper concludes that this architecture aims to
explore a novel path for increasingly complex systems, beyond microservice and
monolithic architectures.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [Modular abstract syntax trees (MAST): substitution tensors with second-class sorts](https://arxiv.org/abs/2511.03946)
*Marcelo P. Fiore,Ohad Kammar,Georg Moser,Sam Staton*

Main category: cs.PL

TL;DR: This paper generalizes a categorical framework for abstract syntax to better handle programming languages with second-class sorts (like call-by-value). The new theory uses actegories and bicategories, providing tools to prove important lemmas in these languages.


<details>
  <summary>Details</summary>
Motivation: To address the gap in modeling abstract syntax for languages that use second-class sorts, as traditional approaches focus on first-class sorts and do not handle the restrictions required by second-class sorts in variable contexts.

Method: The authors adapt the framework of abstract syntax with binding, substitution, and holes by Fiore, Plotkin, and Turi. They shift the characterization from monoids in monoidal categories to actions in actegories, using bicategorical arguments to generalize the theory. They then apply this approach to prove substitution lemmata for call-by-value calculi.

Result: They provide a new categorical framework for modeling abstract syntax in languages with second-class sorts and successfully use it to prove substitution lemmas for different versions of call-by-value calculi.

Conclusion: The proposed adaptation and generalization via actegories and bicategorical methods successfully capture abstract syntax in languages with second-class sorts, demonstrating the framework’s applicability to major programming calculi and their metatheory.

Abstract: We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with
binding, substitution, and holes to account for languages with second-class
sorts. These situations include programming calculi such as the Call-by-Value
lambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting
second-class sorts from appearing in variable contexts changes the
characterisation of the abstract syntax from monoids in monoidal categories to
actions in actegories. We reproduce much of the development through
bicategorical arguments. We apply the resulting theory by proving substitution
lemmata for varieties of CBV.

</details>
