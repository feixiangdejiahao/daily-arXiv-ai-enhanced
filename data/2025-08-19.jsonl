{"id": "2508.11715", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11715", "abs": "https://arxiv.org/abs/2508.11715", "authors": ["Ananya Singha", "Harshita Sahijwani", "Walt Williams", "Emmanuel Aboah Boateng", "Nick Hausman", "Miguel Di Luca", "Keegan Choudhury", "Chaya Binet", "Vu Le", "Tianwei Chen", "Oryan Rokeah Chen", "Sulaiman Vesal", "Sadid Hasan"], "title": "Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs", "comment": "Accepted at the KDD workshop on Evaluation and Trustworthiness of\n  Agentic and Generative AI Models", "summary": "Excel is a pervasive yet often complex tool, particularly for novice users,\nwhere runtime errors arising from logical mistakes or misinterpretations of\nfunctions pose a significant challenge. While large language models (LLMs)\noffer promising assistance by explaining formula errors, the automated\ncorrection of these semantic runtime errors remains an open problem. A primary\nchallenge to advancing models for such scenarios is the severe lack of\nhigh-quality, comprehensive datasets for training and rigorous evaluation. This\npaper addresses this gap by introducing a novel approach for constructing a\nbenchmark dataset specifically designed for Excel formula repair. We propose a\ndata generation pipeline, which leverages a small set of curated seed samples\nfrom online forums to synthetically expand the dataset. Our pipeline integrates\nfew-shot prompting with LLMs and employs a robust \\textit{LLM-as-a-Judge}\nvalidation framework, combined with execution-based checks to ensure the\ncorrectness and semantic fidelity of the generated data. This process produced\na benchmark dataset of 618 high-quality samples, covering common runtime\nerrors. Furthermore, we propose a context-aware baseline technique for Excel\nformula repair that utilizes LLMs to leverage both the faulty formula, and\nrelevant spreadsheet context. We evaluate the performance of various LLMs\n(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using\nexecution-based metrics. Our analysis demonstrates the dataset's quality\nthrough manual annotation and provides insights into error and function\ndistributions. The proposed generation methodology is highly scalable and can\nbe readily adapted to create evaluation benchmarks for similar code repair\ntasks in other low-resource programming languages."}
{"id": "2508.11717", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11717", "abs": "https://arxiv.org/abs/2508.11717", "authors": ["Dhruv Kolhatkar", "Soubhagya Akkena", "Edward F. Gehringer"], "title": "WIP: Leveraging LLMs for Enforcing Design Principles in Student Code: Analysis of Prompting Strategies and RAG", "comment": "Accepted for presentation at the Frontiers in Education Conference,\n  Nashville, Tennessee, USA, 2-5 November 2025", "summary": "This work-in-progress research-to-practice paper explores the integration of\nLarge Language Models (LLMs) into the code-review process for open-source\nsoftware projects developed in computer science and software engineering\ncourses. The focus is on developing an automated feedback tool that evaluates\nstudent code for adherence to key object-oriented design principles, addressing\nthe need for more effective and scalable methods to teach software design best\npractices. The innovative practice involves leveraging LLMs and\nRetrieval-Augmented Generation (RAG) to create an automated feedback system\nthat assesses student code for principles like SOLID, DRY, and design patterns.\nIt analyzes the effectiveness of various prompting strategies and the RAG\nintegration. Preliminary findings show promising improvements in code quality.\nFuture work will aim to improve model accuracy and expand support for\nadditional design principles."}
{"id": "2508.11824", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.11824", "abs": "https://arxiv.org/abs/2508.11824", "authors": ["Satyam Kumar Navneet", "Joydeep Chandra"], "title": "Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering", "comment": null, "summary": "The integration of Large Language Models (LLMs) into software engineering has\nrevolutionized code generation, enabling unprecedented productivity through\npromptware and autonomous AI agents. However, this transformation introduces\nsignificant risks, including insecure code generation, hallucinated outputs,\nirreversible actions, and a lack of transparency and accountability. Incidents\nlike the Replit database deletion underscore the urgent need for robust safety\nand governance mechanisms. This paper comprehensively analyzes the inherent\nchallenges of LLM-assisted code generation, such as vulnerability inheritance,\novertrust, misinterpretation, and the absence of standardized validation and\nrollback protocols. To address these, we propose the SAFE-AI Framework, a\nholistic approach emphasizing Safety, Auditability, Feedback, and\nExplainability. The framework integrates guardrails, sandboxing, runtime\nverification, risk-aware logging, human-in-the-loop systems, and explainable AI\ntechniques to mitigate risks while fostering trust and compliance. We introduce\na novel taxonomy of AI behaviors categorizing suggestive, generative,\nautonomous, and destructive actions to guide risk assessment and oversight.\nAdditionally, we identify open problems, including the lack of standardized\nbenchmarks for code specific hallucinations and autonomy levels, and propose\nfuture research directions for hybrid verification, semantic guardrails, and\nproactive governance tools. Through detailed comparisons of autonomy control,\nprompt engineering, explainability, and governance frameworks, this paper\nprovides a roadmap for responsible AI integration in software engineering,\naligning with emerging regulations like the EU AI Act and Canada's AIDA to\nensure safe, transparent, and accountable AI-driven development."}
{"id": "2508.11867", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11867", "abs": "https://arxiv.org/abs/2508.11867", "authors": ["Mohammad Baqar", "Saba Naqvi", "Rajat Khanda"], "title": "AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions", "comment": "13 Pages", "summary": "Modern software delivery has accelerated from quarterly releases to multiple\ndeployments per day. While CI/CD tooling has matured, human decision points\ninterpreting flaky tests, choosing rollback strategies, tuning feature flags,\nand deciding when to promote a canary remain major sources of latency and\noperational toil. We propose AI-Augmented CI/CD Pipelines, where large language\nmodels (LLMs) and autonomous agents act as policy-bounded co-pilots and\nprogressively as decision makers. We contribute: (1) a reference architecture\nfor embedding agentic decision points into CI/CD, (2) a decision taxonomy and\npolicy-as-code guardrail pattern, (3) a trust-tier framework for staged\nautonomy, (4) an evaluation methodology using DevOps Research and Assessment (\nDORA) metrics and AI-specific indicators, and (5) a detailed industrial-style\ncase study migrating a React 19 microservice to an AI-augmented pipeline. We\ndiscuss ethics, verification, auditability, and threats to validity, and chart\na roadmap for verifiable autonomy in production delivery systems."}
{"id": "2508.11665", "categories": ["cs.PL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11665", "abs": "https://arxiv.org/abs/2508.11665", "authors": ["Xinkui Zhao", "Yifan Zhang", "Zhengyi Zhou", "Yueshen Xu"], "title": "StackPilot: Autonomous Function Agents for Scalable and Environment-Free Code Execution", "comment": null, "summary": "Recent advances in large language models (LLMs) have substantially enhanced\nautomated code generation across a wide range of programming languages.\nNonetheless, verifying the correctness and executability of LLM-generated code\nremains a significant challenge, as traditional methods rely on\nlanguage-specific compilers and environment-dependent runtimes. To overcome\nthese limitations, we introduce StackPilot, an LLM-native, multi-agent\nframework designed for language-agnostic code verification and execution, which\noperates independently of conventional toolchains. StackPilot offers three\nprincipal innovations: (1) a Function-as-Agents paradigm, in which each\nfunction is modeled as an autonomous agent capable of fine-grained reasoning\nand collaborative verification; (2) an LLM-as-Executor strategy, which enables\nscalable verification via stack-based scheduling; and (3) a novel snapshot\nmechanism that preserves complete execution contexts, facilitating\ndeterministic and lossless context switching during verification. Empirical\nevaluations demonstrate that StackPilot achieves framework reliability rates\nbetween 89% and 97%, substantially outperforming baseline approaches. These\nresults indicate that StackPilot can reliably verify and execute a\nsignificantly larger proportion of LLM-generated code across diverse\nprogramming tasks compared to existing methods."}
{"id": "2508.11958", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11958", "abs": "https://arxiv.org/abs/2508.11958", "authors": ["Zhipeng Xue", "Xiaoting Zhang", "Zhipeng Gao", "Xing Hu", "Shan Gao", "Xin Xia", "Shanping Li"], "title": "Clean Code, Better Models: Enhancing LLM Performance with Smell-Cleaned Dataset", "comment": null, "summary": "The Large Language Models (LLMs) have demonstrated great potential in\ncode-related tasks. However, most research focuses on improving the output\nquality of LLMs (e.g., correctness), and less attention has been paid to the\nLLM input (e.g., the training code quality). Given that code smells are widely\nexisted in practice and can negatively impact software maintainability and\nreadability, this study takes the first systematic research to assess and\nimprove dataset quality in terms of code smells. In this work, we first conduct\na preliminary study to explore the presence of code smells in a popular\nbenchmark dataset (i.e., CodeSearchNet-Python}) and evaluate the output of\nseveral popular LLMs (i.e., DeepSeek-Coder, CodeLlama, and MagiCoder),\nrevealing that code smell issues extensively exist in LLM's input (e.g.,\nbenchmark dataset) and output (e.g., generated code). We then conduct our\nsystematic research by taking three main steps: Firstly, we propose an\nLLM-based code smell cleaning tool, named SmellCC, which automatically\nrefactors and removes code smells. To evaluate the correctness of the code\nrefactoring, we construct a test set of 50 repositories sourced from the\nCodeSearchNet-Python benchmark for functional testing. Then we apply our\ncurated smell-cleaned dataset to fine-tune two LLMs (i.e., DeepSeek-V2 and\nQwen-Coder) to explore their potential for generating high-quality code.\nThirdly, we investigate the impact of code smells on two downstream tasks: code\ncompletion and code search. Lastly, we derive several actionable implications\nfor software engineering researchers and industry practitioners from our\nfindings."}
{"id": "2508.12054", "categories": ["cs.PL", "11A51", "D.3.1"], "pdf": "https://arxiv.org/pdf/2508.12054", "abs": "https://arxiv.org/abs/2508.12054", "authors": ["Guilherme de Oliveira Silva", "Fernando Magno Quintão Pereira"], "title": "Certified Compilation based on Gödel Numbers", "comment": "32 pages, 19 figures", "summary": "In his 1984 Turing Award lecture, Ken Thompson showed that a compiler could\nbe maliciously altered to insert backdoors into programs it compiles and\nperpetuate this behavior by modifying any compiler it subsequently builds.\nThompson's hack has been reproduced in real-world systems for demonstration\npurposes. Several countermeasures have been proposed to defend against\nThompson-style backdoors, including the well-known {\\it Diverse\nDouble-Compiling} (DDC) technique, as well as methods like translation\nvalidation and CompCert-style compilation. However, these approaches ultimately\ncircle back to the fundamental question: \"How can we trust the compiler used to\ncompile the tools we rely on?\" In this paper, we introduce a novel approach to\ngenerating certificates to guarantee that a binary image faithfully represents\nthe source code. These certificates ensure that the binary contains all and\nonly the statements from the source code, preserves their order, and maintains\nequivalent def-use dependencies. The certificate is represented as an integer\nderivable from both the source code and the binary using a concise set of\nderivation rules, each applied in constant time. To demonstrate the\npracticality of our method, we present Charon, a compiler designed to handle a\nsubset of C expressive enough to compile FaCT, the Flexible and Constant Time\ncryptographic programming language."}
{"id": "2508.11993", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11993", "abs": "https://arxiv.org/abs/2508.11993", "authors": ["Kota Someya", "Lei Chen", "Michael J. Decker", "Shinpei Hayashi"], "title": "How Much Can a Behavior-Preserving Changeset Be Decomposed into Refactoring Operations?", "comment": "(C) 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Developers sometimes mix behavior-preserving modifications, such as\nrefactorings, with behavior-altering modifications, such as feature additions.\nSeveral approaches have been proposed to support understanding such\nmodifications by separating them into those two parts. Such refactoring-aware\napproaches are expected to be particularly effective when the\nbehavior-preserving parts can be decomposed into a sequence of more primitive\nbehavior-preserving operations, such as refactorings, but this has not been\nexplored. In this paper, as an initial validation, we quantify how much of the\nbehavior-preserving modifications can be decomposed into refactoring operations\nusing a dataset of functionally-equivalent method pairs. As a result, when\nusing an existing refactoring detector, only 33.9% of the changes could be\nidentified as refactoring operations. In contrast, when including 67 newly\ndefined functionally-equivalent operations, the coverage increased by over\n128%. Further investigation into the remaining unexplained differences was\nconducted, suggesting improvement opportunities."}
{"id": "2508.12427", "categories": ["cs.PL", "D.3.3; F.3.2; F.3.3"], "pdf": "https://arxiv.org/pdf/2508.12427", "abs": "https://arxiv.org/abs/2508.12427", "authors": ["Paul Downen"], "title": "Controlling Copatterns: There and Back Again (Extended Version)", "comment": "To find the detailed step-by-step process, which serves as their\n  proof of correctness, see https://github.com/pdownen/derive-copat", "summary": "Copatterns give functional programs a flexible mechanism for responding to\ntheir context, and composition can greatly enhance their expressiveness.\nHowever, that same expressive power makes it harder to precisely specify the\nbehavior of programs. Using Danvy's functional and syntactic correspondence\nbetween different semantic artifacts, we derive a full suite of semantics for\ncopatterns, twice. First, a calculus of monolithic copatterns is taken on a\njourney from small-step operational semantics to abstract machine to\ncontinuation-passing style. Then within continuation-passing style, we refactor\nthe semantics to derive a more general calculus of compositional copatterns,\nand take the return journey back to derive the other semantic artifacts in\nreverse order."}
{"id": "2508.12232", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12232", "abs": "https://arxiv.org/abs/2508.12232", "authors": ["Arshia Akhavan", "Alireza Hosseinpour", "Abbas Heydarnoori", "Mehdi Keshani"], "title": "LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery", "comment": null, "summary": "Issue-to-commit link recovery plays an important role in software\ntraceability and improves project management. However, it remains a challenging\ntask. A study on GitHub shows that only 42.2% of the issues are correctly\nlinked to their commits. This highlights the potential for further development\nand research in this area. Existing studies have employed various AI/ML-based\napproaches, and with the recent development of large language models,\nresearchers have leveraged LLMs to tackle this problem. These approaches suffer\nfrom two main issues. First, LLMs are constrained by limited context windows\nand cannot ingest all of the available data sources, such as long commit\nhistories, extensive issue comments, and large code repositories. Second, most\nmethods operate on individual issue-commit pairs; that is, given a single\nissue-commit pair, they determine whether the commit resolves the issue. This\nquickly becomes impractical in real-world repositories containing tens of\nthousands of commits. To address these limitations, we present LinkAnchor, the\nfirst autonomous LLM-based agent designed for issue-to-commit link recovery.\nThe lazy-access architecture of LinkAnchor enables the underlying LLM to access\nthe rich context of software, spanning commits, issue comments, and code files,\nwithout exceeding the token limit by dynamically retrieving only the most\nrelevant contextual data. Additionally, LinkAnchor is able to automatically\npinpoint the target commit rather than exhaustively scoring every possible\ncandidate. Our evaluations show that LinkAnchor outperforms state-of-the-art\nissue-to-commit link recovery approaches by 60-262% in Hit@1 score across all\nour case study projects. We also publicly release LinkAnchor as a ready-to-use\ntool, along with our replication package. LinkAnchor is designed and tested for\nGitHub and Jira, and is easily extendable to other platforms."}
{"id": "2508.12475", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.12475", "abs": "https://arxiv.org/abs/2508.12475", "authors": ["Abhijit Paul"], "title": "Type-Driven Prompt Programming: From Typed Interfaces to a Calculus of Constraints", "comment": "Accepted as Extended Abstract in TyDe Workshop 2025,co-located with\n  ICFP", "summary": "Prompt programming treats large language model prompts as software components\nwith typed interfaces. Based on a literature survey of 15 recent works from\n2023 to 2025, we observe a consistent trend: type systems are central to\nemerging prompt programming frameworks. However, there are gaps in constraint\nexpressiveness and in supporting algorithms. To address these issues, we\nintroduce the notion of Lambda Prompt, a dependently typed calculus with\nprobabilistic refinements for syntactic and semantic constraints. While this is\nnot yet a full calculus, the formulation motivates a type-theoretic foundation\nfor prompt programming. Our catalog of 13 constraints highlights underexplored\nareas in constraint expressiveness (constraints 9 through 13). To address the\nalgorithmic gap, we propose a constraint-preserving optimization rule. Finally,\nwe outline research directions on developing a compiler for prompt programs."}
{"id": "2508.12285", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12285", "abs": "https://arxiv.org/abs/2508.12285", "authors": ["Yunbo Lyu", "Zhou Yang", "Jieke Shi", "Jianming Chang", "Yue Liu", "David Lo"], "title": "\"My productivity is boosted, but ...\" Demystifying Users' Perception on AI Coding Assistants", "comment": "13 pages, Camera-Ready Version that will appear in ASE 2025", "summary": "This paper aims to explore fundamental questions in the era when AI coding\nassistants like GitHub Copilot are widely adopted: what do developers truly\nvalue and criticize in AI coding assistants, and what does this reveal about\ntheir needs and expectations in real-world software development? Unlike\nprevious studies that conduct observational research in controlled and\nsimulated environments, we analyze extensive, first-hand user reviews of AI\ncoding assistants, which capture developers' authentic perspectives and\nexperiences drawn directly from their actual day-to-day work contexts. We\nidentify 1,085 AI coding assistants from the Visual Studio Code Marketplace.\nAlthough they only account for 1.64% of all extensions, we observe a surge in\nthese assistants: over 90% of them are released within the past two years. We\nthen manually analyze the user reviews sampled from 32 AI coding assistants\nthat have sufficient installations and reviews to construct a comprehensive\ntaxonomy of user concerns and feedback about these assistants. We manually\nannotate each review's attitude when mentioning certain aspects of coding\nassistants, yielding nuanced insights into user satisfaction and\ndissatisfaction regarding specific features, concerns, and overall tool\nperformance. Built on top of the findings-including how users demand not just\nintelligent suggestions but also context-aware, customizable, and\nresource-efficient interactions-we propose five practical implications and\nsuggestions to guide the enhancement of AI coding assistants that satisfy user\nneeds."}
{"id": "2508.12303", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12303", "abs": "https://arxiv.org/abs/2508.12303", "authors": ["Xu Long", "Yishun Wang", "Xiaoqi Li"], "title": "From Fomo3D to Lottery DAPP: Analysis of Ethereum-Based Gambling Applications", "comment": null, "summary": "As blockchain technology advances, Ethereum based gambling decentralized\napplications (DApps) represent a new paradigm in online gambling. This paper\nexamines the concepts, principles, implementation, and prospects of Ethereum\nbased gambling DApps. First, we outline the concept and operational principles\nof gambling DApps. These DApps are blockchain based online lottery platforms.\nThey utilize smart contracts to manage the entire lottery process, including\nissuance, betting, drawing, and prize distribution. Being decentralized,\nlottery DApps operate without central oversight, unlike traditional lotteries.\nThis ensures fairness and eliminates control by any single entity. Automated\nsmart contract execution further reduces management costs, increases\nprofitability, and enhances game transparency and credibility. Next, we analyze\nan existing Ethereum based gambling DApp, detailing its technical principles,\nimplementation, operational status, vulnerabilities, and potential solutions.\nWe then elaborate on the implementation of lottery DApps. Smart contracts\nautomate the entire lottery process including betting, drawing, and prize\ndistribution. Although developing lottery DApps requires technical expertise,\nthe expanding Ethereum ecosystem provides growing tools and frameworks,\nlowering development barriers. Finally, we discuss current limitations and\nprospects of lottery DApps. As blockchain technology and smart contracts\nevolve, lottery DApps are positioned to significantly transform the online\nlottery industry. Advantages like decentralization, automation, and\ntransparency will likely drive broader future adoption."}
{"id": "2508.12325", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12325", "abs": "https://arxiv.org/abs/2508.12325", "authors": ["Tim Kräuter", "Adrian Rutle", "Yngve Lamo", "Harald König", "Francisco Durán"], "title": "Towards the Coordination and Verification of Heterogeneous Systems with Data and Time", "comment": "This is the authors accepted version of a paper to be published in\n  MODELS-2025, DOI: TBD", "summary": "Modern software systems are often realized by coordinating multiple\nheterogeneous parts, each responsible for specific tasks. These parts must work\ntogether seamlessly to satisfy the overall system requirements. To verify such\ncomplex systems, we have developed a non-intrusive coordination framework\ncapable of performing formal analysis of heterogeneous parts that exchange data\nand include real-time capabilities. The framework utilizes a linguistic\nextension, which is implemented as a central broker and a domain-specific\nlanguage for the integration of heterogeneous languages and coordination of\nparts. Moreover, abstract rule templates are reified as language adapters for\nnon-intrusive communications with the broker. The framework is implemented\nusing rewriting logic (Maude), and its applicability is demonstrated by\nverifying certain correctness properties of a heterogeneous road-rail crossing\nsystem."}
{"id": "2508.12358", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12358", "abs": "https://arxiv.org/abs/2508.12358", "authors": ["Haolin Jin", "Huaming Chen"], "title": "Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications", "comment": "Accepted to the NIER track of the 40th IEEE/ACM International\n  Conference on Automated Software Engineering (ASE 2025)", "summary": "Large language models (LLMs) have become essential tools in software\ndevelopment, widely used for requirements engineering, code generation and\nreview tasks. Software engineers often rely on LLMs to assess whether system\ncode implementation satisfy task requirements, thereby enhancing code\nrobustness and accuracy. However, it remains unclear whether LLMs can reliably\ndetermine whether the code complies fully with the given task descriptions,\nwhich is usually natural language specifications. In this paper, we uncover a\nsystematic failure of LLMs in evaluating whether code aligns with natural\nlanguage requirements. Specifically, with widely used benchmarks, we employ\nunified prompts to judge code correctness. Our results reveal that LLMs\nfrequently misclassify correct code implementations as either ``not satisfying\nrequirements'' or containing potential defects. Surprisingly, more complex\nprompting, especially when leveraging prompt engineering techniques involving\nexplanations and proposed corrections, leads to higher misjudgment rate, which\nhighlights the critical reliability issues in using LLMs as code review\nassistants. We further analyze the root causes of these misjudgments, and\npropose two improved prompting strategies for mitigation. For the first time,\nour findings reveals unrecognized limitations in LLMs to match code with\nrequirements. We also offer novel insights and practical guidance for effective\nuse of LLMs in automated code review and task-oriented agent scenarios."}
{"id": "2508.12436", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12436", "abs": "https://arxiv.org/abs/2508.12436", "authors": ["Feifei Niu", "Chuanyi Li", "Haosheng Zuo", "Jionghan Wu", "Xin Xia"], "title": "Feature Request Analysis and Processing: Tasks, Techniques, and Trends", "comment": null, "summary": "Feature requests are proposed by users to request new features or\nenhancements of existing features of software products, which represent users'\nwishes and demands. Satisfying users' demands can benefit the product from both\ncompetitiveness and user satisfaction. Feature requests have seen a rise in\ninterest in the past few years and the amount of research has been growing.\nHowever, the diversity in the research topics suggests the need for their\ncollective analysis to identify the challenges and opportunities so as to\npromote new advances in the future. In this work, following a defined process\nand a search protocol, we provide a systematic overview of the research area by\nsearching and categorizing relevant studies. We select and analyze 131 primary\nstudies using descriptive statistics and qualitative analysis methods. We\nclassify the studies into different topics and group them from the perspective\nof requirements engineering activities. We investigate open tools as well as\ndatasets for future research. In addition, we identify several key challenges\nand opportunities, such as: (1) ensuring the quality of feature requests, (2)\nimproving their specification and validation, and (3) developing high-quality\nbenchmarks for large language model-driven tasks."}
{"id": "2508.12546", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12546", "abs": "https://arxiv.org/abs/2508.12546", "authors": ["Bin Duan", "Ruican Dong", "Naipeng Dong", "Dan Dongseong Kim", "Guowei Yang"], "title": "XAMT: Cross-Framework API Matching for Testing Deep Learning Libraries", "comment": null, "summary": "Deep learning powers critical applications such as autonomous driving,\nhealthcare, and finance, where the correctness of underlying libraries is\nessential. Bugs in widely used deep learning APIs can propagate to downstream\nsystems, causing serious consequences. While existing fuzzing techniques detect\nbugs through intra-framework testing across hardware backends (CPU vs. GPU),\nthey may miss bugs that manifest identically across backends and thus escape\ndetection under these strategies. To address this problem, we propose XAMT, a\ncross-framework fuzzing method that tests deep learning libraries by matching\nand comparing functionally equivalent APIs across different frameworks. XAMT\nmatches APIs using similarity-based rules based on names, descriptions, and\nparameter structures. It then aligns inputs and applies variance-guided\ndifferential testing to detect bugs. We evaluated XAMT on five popular\nframeworks, including PyTorch, TensorFlow, Keras, Chainer, and JAX. XAMT\nmatched 839 APIs and identified 238 matched API groups, and detected 17 bugs,\n12 of which have been confirmed. Our results show that XAMT uncovers bugs\nundetectable by intra-framework testing, especially those that manifest\nconsistently across backends. XAMT offers a complementary approach to existing\nmethods and offers a new perspective on the testing of deep learning libraries."}
{"id": "2508.12620", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.12620", "abs": "https://arxiv.org/abs/2508.12620", "authors": ["Xiaoning Ren", "Qiang Hu", "Wei Ma", "Yan Li", "Yao Zhang", "Lingxiao Jiang", "Yinxing Xue"], "title": "Strengthening Programming Comprehension in Large Language Models through Code Generation", "comment": "11 pages, 7 figures", "summary": "Large language models (LLMs) have recently shown impressive results on\ndiverse code-related tasks, benefiting from large-scale training and\ninstruction tuning. However, studies reveal that their grasp of fundamental\nprogramming concepts, such as data flow and control flow, remains shallow,\nleading to fragile performance when code requires deeper reasoning. This\nlimitation restricts the practical adoption of LLMs in real-world software\ndevelopment. To address this issue, this work introduces a counterfactual code\naugmentation framework combined with concept-aware tuning, designed to guide\nLLMs toward stronger conceptual understanding. Comprehensive evaluation across\nmultiple models and benchmarks demonstrates the effectiveness of the proposed\napproach."}
{"id": "2508.12649", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12649", "abs": "https://arxiv.org/abs/2508.12649", "authors": ["Lei Chen", "Michele Lanza", "Shinpei Hayashi"], "title": "ChangePrism: Visualizing the Essence of Code Changes", "comment": "5 pages, 5 figures, VISSOFT 2025", "summary": "Understanding the changes made by developers when they submit a pull request\nand/or perform a commit on a repository is a crucial activity in software\nmaintenance and evolution. The common way to review changes relies on examining\ncode diffs, where textual differences between two file versions are highlighted\nin red and green to indicate additions and deletions of lines. This can be\ncumbersome for developers, making it difficult to obtain a comprehensive\noverview of all changes in a commit. Moreover, certain types of code changes\ncan be particularly significant and may warrant differentiation from standard\nmodifications to enhance code comprehension. We present a novel visualization\napproach supported by a tool named ChangePrism, which provides a way to better\nunderstand code changes. The tool comprises two components: extraction, which\nretrieves code changes and relevant information from the git history, and\nvisualization, which offers both general and detailed views of code changes in\ncommits. The general view provides an overview of different types of code\nchanges across commits, while the detailed view displays the exact changes in\nthe source code for each commit."}
{"id": "2508.12922", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12922", "abs": "https://arxiv.org/abs/2508.12922", "authors": ["Yue Wang", "Zhenyu Chen", "Yuan Zhao", "Chunrong Fang", "Ziyuan Wang", "Song Huang"], "title": "RUM: Rule+LLM-Based Comprehensive Assessment on Testing Skills", "comment": null, "summary": "Over the past eight years, the META method has served as a multidimensional\ntesting skill assessment system in the National College Student Contest on\nSoftware Testing, successfully assessing over 100,000 students' testing skills.\nHowever, META is primarily limited to the objective assessment of test scripts,\nlacking the ability to automatically assess subjective aspects such as test\ncase and test report. To address this limitation, this paper proposes RUM, a\ncomprehensive assessment approach that combines rules and large language models\n(LLMs). RUM achieves a comprehensive assessment by rapidly processing objective\nindicators through rules while utilizing LLMs for in-depth subjective analysis\nof test case documents, test scripts, and test reports. The experimental\nresults show that compared to traditional manual testing skill assessment, RUM\nimproves assessment efficiency by 80.77\\% and reduces costs by 97.38\\%, while\nmaintaining high accuracy and consistency of assessment. By applying RUM on the\ncontest on software testing, we find that it not only enhances the efficiency\nand scalability of skill assessment in software testing education, but also\nprovides teachers with more comprehensive and objective evidence for student\nability assessment, facilitating personalized teaching and learning. This study\noffers new insights into the assessment of testing skills, which are expected\nto promote further development in test process optimization and software\nquality assurance."}
{"id": "2508.13051", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.13051", "abs": "https://arxiv.org/abs/2508.13051", "authors": ["Yi Wang", "Chetan Arora", "Xiao Liu", "Thuong Hoang", "ZHengxin Zhang", "Henry Been Lirn Duh", "John Grundy"], "title": "Investigating VR Accessibility Reviews for Users with Disabilities: A Qualitative Analysis", "comment": null, "summary": "Accessibility reviews provide valuable insights into both the limitations and\nbenefits experienced by users with disabilities when using virtual reality (VR)\napplications. However, a comprehensive investigation into VR accessibility for\nusers with disabilities is still lacking. To fill this gap, this study analyzes\nuser reviews from the Meta and Steam stores of VR apps, focusing on the\nreported issues affecting users with disabilities. We applied selection\ncriteria to 1,367,419 reviews from the top 40, the 20 most popular, and the 40\nlowest-rated VR applications on both platforms. In total, 1,076 (0.078%) VR\naccessibility reviews referenced various disabilities across 100 VR\napplications. These applications were categorized into Action, Sports, Social,\nPuzzle, Horror, and Simulation, with Action receiving the highest number of\naccessibility related-reviews. We identified 16 different types of disabilities\nacross six categories. Furthermore, we examined the causes of accessibility\nissues as reported by users with disabilities. Overall, VR accessibility\nreviews were predominantly under-supported."}
{"id": "2508.13134", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13134", "abs": "https://arxiv.org/abs/2508.13134", "authors": ["Glauber da Rocha Balthazar", "Marcia Ito"], "title": "Influencia de fatores organizacionais e sociais na etapa de levantamento de requisitos", "comment": "VI Workshop de P\\'os-Gradua\\c{c}\\~ao e Pesquisa do Centro Paula\n  Souza, in Portuguese language", "summary": "The most critical and fragile stage of a software development project is\nrequirements gathering. Because of this, Requirements Engineering has been\nevolving its techniques to minimize the challenges faced by Requirements\nAnalysts. However, few studies consider the humanistic relationships and\nbehaviors of those involved in this stage. This article presents a survey of\nsome studies conducted at this stage that consider non-technical factors such\nas emotions, organizational environment, and social context."}
