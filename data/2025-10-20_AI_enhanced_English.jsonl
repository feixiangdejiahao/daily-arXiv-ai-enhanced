{"id": "2510.15004", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15004", "abs": "https://arxiv.org/abs/2510.15004", "authors": ["Zhiming Zhang", "Qingfu Zhu", "Xianzhen Luo", "Yixuan Wang", "Bohan Li", "Wanxiang Che"], "title": "Automated Snippet-Alignment Data Augmentation for Code Translation", "comment": null, "summary": "Code translation aims to translate the code from its source language to the\ntarget language and is used in various software development scenarios. Recent\ndevelopments in Large Language Models (LLMs) have showcased their capabilities\nin code translation, and parallel corpora play a crucial role in training\nmodels for code translation. Parallel corpora can be categorized into\nprogram-alignment (PA) and snippet-alignment (SA) data. Although PA data has\ncomplete context and is suitable for semantic alignment learning, it may not\nprovide adequate fine-grained training signals due to its extended length,\nwhile the brevity of SA data enables more fine-grained alignment learning. Due\nto limited parallel corpora, researchers explore several augmentation methods\nfor code translation. Previous studies mainly focus on augmenting PA data. In\nthis paper, we propose a data augmentation method that leverages LLMs to\ngenerate SA data automatically. To fully leverage both PA data and SA data, we\nexplore a simple yet effective two-stage training strategy, which consistently\nenhances model performance compared to fine-tuning solely on PA data.\nExperiments on TransCoder-test demonstrate that our augmented SA data combined\nwith the two-stage training approach yields consistent improvements over the\nbaseline, achieving a maximum gain of 3.78% on pass@k.", "AI": {"tldr": "The paper introduces an LLM-based method for generating snippet-alignment training data for code translation, and demonstrates that a two-stage training method using both program- and snippet-level data significantly improves translation accuracy, achieving up to 3.78% improvement on the TransCoder-test benchmark.", "motivation": "Code translation is essential for various software development purposes, and recent Large Language Models (LLMs) show promising abilities in this area. Effective training for code translation depends heavily on the availability and use of parallel corpora, but there is a limitation in both the quantity and fine-grained alignment of existing data. Conventional research primarily augments program-alignment (PA) data; snippet-alignment (SA) data, which may help with finer-grained alignment, is underexplored.", "method": "The authors propose a data augmentation approach that leverages LLMs to automatically create snippet-alignment (SA) data. They then introduce a two-stage training strategy: models are first trained on program-alignment (PA) data and subsequently fine-tuned on the enriched SA data to maximize knowledge transfer and alignment quality.", "result": "The experiments, conducted on the TransCoder-test dataset, show that augmenting the available data with automatically generated SA data and utilizing the two-stage training process consistently improves code translation model performance. The best result is a 3.78% gain in pass@k compared to the baseline.", "conclusion": "Automatically expanding snippet-alignment data using LLMs, combined with a two-stage training approach that leverages both program-level and snippet-level supervision, provides a substantial and consistent boost to code translation models."}}
{"id": "2510.15079", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15079", "abs": "https://arxiv.org/abs/2510.15079", "authors": ["Changshu Liu", "Yang Chen", "Reyhaneh Jabbarvand"], "title": "Assessing Coherency and Consistency of Code Execution Reasoning by Large Language Models", "comment": null, "summary": "This paper proposes CES, a task to evaluate the abilities of LLMs in\nsimulating program execution and using that reasoning in programming tasks.\nBesides measuring the correctness of variable predictions during execution\nsimulation, CES introduces the notion of coherence to determine whether the\nsimulation complies with commonsense execution logic, even if the predicted\nvalues along the simulations are incorrect. This enables CES to rule out\nsuspiciously correct output predictions due to reasoning shortcuts,\nhallucinations, or potential data leakage. CES also introduces a novel metric\nto measure reasoning consistency across tests with the same or different prime\npath coverage in a spectrum: strong, weak, and random. Evaluating 16 LLMs\n(including three reasoning LLMs) using CES indicates 81.42% coherent execution\nsimulation on HumanEval, 46.92% and 53.08% of which result in correct and\nincorrect output predictions. Frontier LLMs such as GPT-4 and DeepSeek-R1 have\nthe most incoherent execution reasoning, mostly due to natural language\nshortcuts. Despite relatively coherent execution simulation, LLMs' reasoning\nperformance across different tests is inconsistent, mostly random (48.87%) or\nweak (45.37%), potentially explaining their weakness in programming tasks that\nrequire path-sensitive program analysis to succeed. We also compare CES with\nbug prediction/localization/repair, which intuitively requires control- and\ndata-flow awareness. We observe that LLMs barely incorporate execution\nreasoning into their analysis for bug-related tasks, and their success is\nprimarily due to inherent abilities in pattern matching or natural language\nshortcuts, if not data leakage. Without reasoning, there is a threat to the\ngeneralizability of LLMs in dealing with unseen bugs or patterns in different\ncontexts. CES can be used to vet the suspicious success of LLMs in these tasks\nsystematically.", "AI": {"tldr": "The paper introduces CES, a new framework to assess LLMs' reasoning in program execution and programming tasks, emphasizing coherence over mere output correctness. Experimental results reveal that LLMs often rely on shortcuts, displaying incoherent reasoning and inconsistent performance, especially in bug-related tasks. CES can uncover and address these limitations, improving evaluation and development of reasoning-capable LLMs.", "motivation": "Current evaluation methods for LLMs in programming tasks often overestimate their reasoning ability by focusing solely on correct output predictions, which can be achieved through shortcuts or data leakage rather than genuine program understanding. There is a need for a more robust evaluation framework that ensures LLMs truly reason about the execution logic.", "method": "The paper introduces CES, a framework that evaluates LLMs' ability to simulate program execution and apply this reasoning to programming tasks. CES measures both the correctness of simulation and a new notion called coherence, which reflects adherence to commonsense execution logic, irrespective of correct outputs. It also presents a metric for reasoning consistency across test cases using path coverage. CES is used to benchmark 16 LLMs on tasks like HumanEval and bug analysis.", "result": "The evaluation using CES shows that while LLMs deliver coherent execution simulation (81.42% on HumanEval), their output correctness is split (46.92% correct, 53.08% incorrect). Notably, top-performing LLMs like GPT-4 and DeepSeek-R1 sometimes rely on shortcuts, leading to incoherent reasoning. LLMs' reasoning consistency is often weak or random, revealing limitations in handling path-sensitive analyses. For bug-related tasks, LLMs rely on pattern matching or language shortcuts rather than true execution reasoning, threatening their generalizability.", "conclusion": "CES provides a robust framework for diagnosing reasoning shortcuts and inconsistencies in LLMs when simulating program execution. LLMs show reasonable coherence but lack consistent, path-sensitive reasoning, undermining their reliability for unseen programming or bug localization tasks. CES can systematically vet suspiciously successful results and encourage the development of LLMs that truly understand program logic."}}
{"id": "2510.15408", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15408", "abs": "https://arxiv.org/abs/2510.15408", "authors": ["Mohit", "Kuljit Kaur Chahal"], "title": "Community Engagement and the Lifespan of Open-Source Software Projects", "comment": null, "summary": "Open-source software (OSS) projects depend on community engagement (CE) for\nlongevity. However, CE's quantifiable impact on project dynamics and lifespan\nis underexplored. Objectives: This study defines CE in OSS, identifies key\nmetrics, and evaluates their influence on project dynamics (releases, commits,\nbranches) and lifespan. Methods: We analyzed 33,946 GitHub repositories,\ndefining and operationalizing CE with validated per-month metrics (issues,\ncomments, watchers, stargazers). Non-parametric tests and correlations assessed\nrelationships with project dynamics and lifespan across quartiles. Results: CE\nmetrics significantly associate with project dynamics, with stronger\ncorrelations in highly engaged projects. For lifespan, a complex pattern\nemerged: per-month CE rates are highest in younger projects, declining with\nage. Yet, a subset of long-lived projects maintains exceptionally high\nactivity. Initial CE bursts appear crucial for establishment, while sustained\nhigh engagement drives extreme longevity. Active issue engagement's influence\nintensifies with age, but passive attention's declines. Conclusion: CE\ndynamically drives OSS project longevity and development. Our findings\nestablish validated CE metrics and offer deeper insights into how diverse\ncommunity activity patterns contribute to project longevity.", "AI": {"tldr": "Analyzing nearly 34,000 GitHub OSS projects, this paper finds that early and sustained community engagement\u2014especially through active involvement\u2014strongly drives project longevity and development. The research establishes clear CE metrics, revealing nuanced patterns in how community activity affects project lifespans.", "motivation": "The motivation is to quantitatively understand how community engagement (CE) influences the lifespan and development dynamics of open-source software projects, which is a crucial but under-studied aspect of OSS sustainability.", "method": "The study analyzes 33,946 GitHub repositories, defining CE through validated monthly metrics like issues, comments, watchers, and stargazers. Non-parametric statistical tests and correlation analyses are used to examine the relationship between CE metrics and various project dynamics (releases, commits, branches) and lifespan across engagement quartiles.", "result": "CE metrics are significantly correlated with project dynamics, more strongly in highly engaged projects. Younger projects have higher per-month CE rates, which typically decline with age except for a subset of long-lived projects that sustain high engagement. Early bursts of CE are important for establishing projects, while ongoing high engagement tends to predict extreme longevity. Active engagement (issues/comments) grows more influential over time, while passive attention (watchers/stargazers) wanes.", "conclusion": "Community engagement is a dynamic driver of both OSS project longevity and development. The research validates CE measurement approaches and reveals that specific patterns of community activity, especially sustained and active engagement, are vital for ongoing project success and lifespan."}}
{"id": "2510.15480", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15480", "abs": "https://arxiv.org/abs/2510.15480", "authors": ["Muslim Chochlov", "Gul Aftab Ahmed", "James Vincent Patten", "Yuanhua Han", "Guoxian Lu", "David Gregg", "Jim Buckley"], "title": "Selecting and Combining Large Language Models for Scalable Code Clone Detection", "comment": null, "summary": "Source code clones pose risks ranging from intellectual property violations\nto unintended vulnerabilities. Effective and efficient scalable clone\ndetection, especially for diverged clones, remains challenging. Large language\nmodels (LLMs) have recently been applied to clone detection tasks. However, the\nrapid emergence of LLMs raises questions about optimal model selection and\npotential LLM-ensemble efficacy.\n  This paper addresses the first question by identifying 76 LLMs and filtering\nthem down to suitable candidates for large-scale clone detection. The\ncandidates were evaluated on two public industrial datasets, BigCloneBench, and\na commercial large-scale dataset. No uniformly 'best-LLM' emerged, though\nCodeT5+110M, CuBERT and SPTCode were top-performers. Analysis of LLM-candidates\nsuggested that smaller embedding sizes, smaller tokenizer vocabularies and\ntailored datasets are advantageous. On commercial large-scale dataset a\ntop-performing CodeT5+110M achieved 39.71\\% precision: twice the precision of\npreviously used CodeBERT.\n  To address the second question, this paper explores ensembling of the\nselected LLMs: effort-effective approach to improving effectiveness. Results\nsuggest the importance of score normalization and favoring ensembling methods\nlike maximum or sum over averaging. Also, findings indicate that ensembling\napproach can be statistically significant and effective on larger datasets: the\nbest-performing ensemble achieved even higher precision of 46.91\\% over\nindividual LLM on the commercial large-scale code.", "AI": {"tldr": "The paper benchmarks 76 large language models for code clone detection, finding that no single model is always best, but a select few perform strongly. Ensemble methods (especially using maximum or sum) can significantly boost precision above any single model, with clear gains on large commercial datasets.", "motivation": "Source code clones can lead to significant risks such as intellectual property violations and the creation of vulnerabilities. Detecting these code clones, especially those that have diverged, is challenging at scale. The rapid development and diversity of large language models (LLMs) create uncertainty about which models perform best, and whether model ensembling could further improve detection.", "method": "The study surveyed 76 LLMs, filtered them down to suitable candidates, and evaluated them on two large datasets: BigCloneBench (public) and a commercial large-scale code clone dataset. Performance metrics, especially precision, were compared. The study also experimented with different ensemble methods for combining LLM outputs, assessing the impact of score normalization and ensembling strategies (maximum, sum, average).", "result": "No single LLM performed best across all evaluations, but CodeT5+110M, CuBERT, and SPTCode emerged as top candidates. Factors like smaller embedding sizes, smaller token vocabularies, and tailored datasets were found to be advantageous. On the commercial dataset, CodeT5+110M doubled the precision of CodeBERT (39.71%). Ensemble methods, especially those using maximum or sum, outperformed simple averaging and, on the commercial dataset, achieved up to 46.91% precision\u2014statistically significant improvements over individual models.", "conclusion": "Optimal LLM selection for code clone detection depends on context, as no universally best model was found. Model ensemble approaches, when properly normalized and executed, yield further improvements in detection precision, particularly at scale. Consideration of model architecture and dataset tailoring is also critical for best results."}}
{"id": "2510.15178", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.15178", "abs": "https://arxiv.org/abs/2510.15178", "authors": ["Brysen Pfingsten", "Jason Hemann"], "title": "Visualizing miniKanren Search with a Fine-Grained Small-Step Semantics", "comment": "2025 miniKanren Workshop", "summary": "We present a deterministic small-step operational semantics for miniKanren\nthat explicitly represents the evolving search tree during execution. This\nsemantics models interleaving and goal scheduling at fine granularity, allowing\neach evaluation step-goal activation, suspension, resumption, and success -- to\nbe visualized precisely. Building on this model, we implement an interactive\nvisualizer that renders the search tree as it develops and lets users step\nthrough execution. The tool acts as a pedagogical notional machine for\nreasoning about miniKanren's fair search behavior, helping users understand\nsurprising answer orders and operational effects. Our semantics and tool are\nvalidated through property-based testing and illustrated with several examples.", "AI": {"tldr": "The paper introduces a detailed operational semantics and interactive visualizer for miniKanren, making its search process and operational effects more understandable for users through step-by-step visualization and validation.", "motivation": "There is a need for a precise and granular understanding of miniKanren's operational behavior, particularly its interleaving search and goal scheduling, to aid users in reasoning about surprising behaviors and answer orders.", "method": "The authors define a deterministic small-step operational semantics for miniKanren that explicitly models the evolving search tree during program execution, capturing each step like activation, suspension, resumption, and success. They build an interactive visualizer based on this semantics.", "result": "A working visualizer is developed, allowing users to step through and visualize the execution of miniKanren programs. The tool and abstract semantics are validated via property-based testing and demonstrated with various examples.", "conclusion": "Their approach provides an effective pedagogical tool (notional machine) that enhances understanding and reasoning about the fair search behavior and operational characteristics of miniKanren, validated through both testing and illustrative examples."}}
{"id": "2510.15494", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15494", "abs": "https://arxiv.org/abs/2510.15494", "authors": ["Lirong Yi", "Gregory Gay", "Philipp Leitner"], "title": "An Experimental Study of Real-Life LLM-Proposed Performance Improvements", "comment": null, "summary": "Large Language Models (LLMs) can generate code, but can they generate fast\ncode? In this paper, we study this question using a dataset of 65 real-world\ntasks mined from open-source Java programs. We specifically select tasks where\ndevelopers achieved significant speedups, and employ an automated pipeline to\ngenerate patches for these issues using two leading LLMs under four prompt\nvariations. By rigorously benchmarking the results against the baseline and\nhuman-authored solutions, we demonstrate that LLM-generated code indeed\nimproves performance over the baseline in most cases. However, patches proposed\nby human developers outperform LLM fixes by a statistically significant margin,\nindicating that LLMs often fall short of finding truly optimal solutions. We\nfurther find that LLM solutions are semantically identical or similar to the\ndeveloper optimization idea in approximately two-thirds of cases, whereas they\npropose a more original idea in the remaining one-third. However, these\noriginal ideas only occasionally yield substantial performance gains.", "AI": {"tldr": "LLMs can make code faster and often suggest optimizations similar to developers, but their improvements don't usually match human-crafted solutions. Original ideas from LLMs rarely provide major speed boosts, so human expertise still leads in code optimization.", "motivation": "The motivation of this paper is to evaluate whether large language models (LLMs) can not only generate code but also produce code optimizations that improve performance, specifically focusing on real-world Java tasks where developers have already achieved significant speedups.", "method": "The authors construct a dataset of 65 real-world optimization tasks from open-source Java programs where developers have made substantial performance improvements. They use an automated pipeline to generate optimization patches for these tasks using two leading LLMs with four different prompt designs. The resulting code is benchmarked and compared with both the pre-optimization baseline and the human-authored, optimized solutions.", "result": "The study finds that LLM-generated code typically improves performance compared to the baseline but does not match the effectiveness of human-developed optimizations. Human patches consistently outperform LLM-generated fixes by a statistically significant margin. Semantically, LLM solutions are similar or identical to the developer's approach in about two-thirds of cases and are more original in the remaining one-third, though these original solutions seldom result in significant performance gains.", "conclusion": "LLMs can successfully improve code performance over baseline implementations in many cases, but their optimizations are not as effective as those crafted by human developers. While LLMs occasionally offer novel optimization ideas, these do not often lead to superior performance, underscoring a gap between automated and human code optimization capabilities."}}
{"id": "2510.15747", "categories": ["cs.PL", "cs.CR", "cs.DC", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.15747", "abs": "https://arxiv.org/abs/2510.15747", "authors": ["Ehud Shapiro"], "title": "Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language", "comment": null, "summary": "Grassroots platforms are distributed applications run by\\linebreak\ncryptographically-identified people on their networked personal devices, where\nmultiple disjoint platform instances emerge independently and coalesce when\nthey interoperate. Their foundation is the grassroots social graph, upon which\ngrassroots social networks, grassroots cryptocurrencies, and grassroots\ndemocratic federations can be built.\n  Grassroots platforms have yet to be implemented, the key challenge being\nfaulty and malicious participants: without secure programming support, correct\nparticipants cannot reliably identify each other, establish secure\ncommunication, or verify each other's code integrity.\n  We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent,\nlogic programming language for implementing grassroots platforms. GLP extends\nlogic programs with paired single-reader/single-writer (SRSW) logic variables,\nproviding secure communication channels among cryptographically-identified\npeople through encrypted, signed and attested messages, which enable identity\nand code integrity verification. We present GLP progressively: logic programs,\nconcurrent GLP, multiagent GLP, augmenting it with cryptographic security, and\nproviding smartphone implementation-ready specifications. We prove safety\nproperties including that GLP computations are deductions, SRSW preservation,\nacyclicity, and monotonicity. We prove multiagent GLP is grassroots and that\nGLP streams achieve blockchain security properties. We present a grassroots\nsocial graph protocol establishing authenticated peer-to-peer connections and\ndemonstrate secure grassroots social networking applications.", "AI": {"tldr": "The paper introduces Grassroots Logic Programs (GLP), a new secure logic programming language designed for implementing distributed grassroots platforms (like social networks and cryptocurrencies) on personal devices. It solves key security challenges via cryptographic identities, secure messaging, and proofs of safety. They provide protocols and demo applications showing secure, authenticated connections and blockchain-like properties, making GLP a robust foundation for future grassroots platforms.", "motivation": "The motivation of this paper is to enable the creation of secure, distributed grassroots platforms (such as social networks and cryptocurrencies) that are run on individuals' personal devices by cryptographically-identified participants. Existing challenges include malicious or faulty participants and the need for secure programming support to ensure secure communication, reliable identity verification, and code integrity.", "method": "The paper introduces Grassroots Logic Programs (GLP), a secure, multiagent, concurrent logic programming language. GLP uses paired single-reader/single-writer (SRSW) logic variables for secure communication between participants, enabling encrypted, signed, and attested messaging for identity and code integrity verification. The language is presented in stages (concurrent GLP, multiagent GLP, security augmentation), with implementation specifications suitable for smartphones. Formal proofs of important safety properties are provided.", "result": "The paper proves safety properties of GLP including that its computations are deductions, SRSW preservation, acyclicity, and monotonicity. It is shown that multiagent GLP maintains the grassroots property, and GLP streams have blockchain-like security characteristics. The authors also present a protocol to establish authenticated peer-to-peer connections and successfully demonstrate secure grassroots social networking applications.", "conclusion": "GLP is a novel language that enables secure implementation of grassroots platforms with strong guarantees of identity, code integrity, and secure communication. It addresses prior security challenges and provides a foundation for implementing decentralized, secure applications on personal devices."}}
{"id": "2510.15512", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15512", "abs": "https://arxiv.org/abs/2510.15512", "authors": ["Wachiraphan Charoenwet", "Patanamon Thongtanunam", "Van-Thuan Pham", "Christoph Treude"], "title": "Enhancing Code Review through Fuzzing and Likely Invariants", "comment": null, "summary": "Many software projects employ manual code review to gatekeep defects and\nvulnerabilities in the code before integration. However, reviewers often work\nunder time pressure and rely primarily on static inspection, leaving the\ndynamic aspects of the program unexplored. Dynamic analyses could reveal such\nbehaviors, but they are rarely integrated into reviews. Among them, fuzzing is\ntypically applied later to uncover crashing bugs. Yet its ability to exercise\ncode with diverse inputs makes it promising for exposing non-crashing, but\nunexpected, behaviors earlier. Still, without suitable mechanisms to analyze\nprogram behaviors, the rich data produced during fuzzing remains inaccessible\nto reviewers, limiting its practical value in this context.\n  We hypothesize that unexpected variations in program behaviors could signify\npotential bugs. The impact of code changes can be automatically captured at\nruntime. Representing program behavior as likely invariants, dynamic properties\nconsistently observed at specific program points, can provide practical signals\nof behavioral changes. Such signals offer a way to distinguish between intended\nchanges and unexpected behavioral shifts from code changes.\n  We present FuzzSight, a framework that leverages likely invariants from\nnon-crashing fuzzing inputs to highlight behavioral differences across program\nversions. By surfacing such differences, it provides insights into which code\nblocks may need closer attention. In our evaluation, FuzzSight flagged 75% of\nregression bugs and up to 80% of vulnerabilities uncovered by 24-hour fuzzing.\nIt also outperformed SAST in identifying buggy code blocks, achieving ten times\nhigher detection rates with fewer false alarms. In summary, FuzzSight\ndemonstrates the potential and value of leveraging fuzzing and invariant\nanalysis for early-stage code review, bridging static inspection with dynamic\nbehavioral insights.", "AI": {"tldr": "Manual code reviews miss dynamic bugs. FuzzSight uses fuzzing and invariant analysis to find behavioral differences early, flagging more vulnerabilities and bugs than traditional static methods. It helps code reviewers spot issues that static analysis would overlook.", "motivation": "Manual code reviews often miss dynamic bugs and vulnerabilities because they typically rely on static inspection under tight time constraints. Dynamic analysis methods such as fuzzing can expose unexpected behaviors, but the lack of mechanisms to analyze and interpret their data means that their practical value in code review is limited.", "method": "The authors introduce FuzzSight, a framework that uses dynamic analysis to capture likely invariants\u2014dynamically observed properties at various code points\u2014from non-crashing fuzzing inputs. By tracking and highlighting unexpected behavioral differences between program versions, FuzzSight helps reviewers identify which code blocks might warrant closer attention.", "result": "FuzzSight flagged 75% of regression bugs and up to 80% of vulnerabilities in their evaluation using 24-hour fuzzing sessions. It surpassed static application security testing (SAST) tools by achieving ten times higher detection rates and fewer false positives in identifying problematic code blocks.", "conclusion": "Leveraging fuzzing combined with invariant analysis enhances early-stage code review by providing dynamic behavioral insights, bridging the gap between static code inspection and runtime behavior. FuzzSight demonstrates substantial promise in detecting vulnerabilities and behavioral anomalies that static review could miss."}}
{"id": "2510.15585", "categories": ["cs.SE", "cs.CL", "cs.PL", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.15585", "abs": "https://arxiv.org/abs/2510.15585", "authors": ["Dr Simon Thorne", "Dr Advait Sarkar"], "title": "Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework", "comment": "16 pages", "summary": "Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for\ngenerating both traditional software code and spreadsheet logic. Despite their\nimpressive generative capabilities, these models frequently exhibit critical\nissues such as hallucinations, subtle logical inconsistencies, and syntactic\nerrors, risks particularly acute in high stakes domains like financial\nmodelling and scientific computations, where accuracy and reliability are\nparamount. This position paper proposes a structured research framework that\nintegrates the proven software engineering practice of Test-Driven Development\n(TDD) with Large Language Model (LLM) driven generation to enhance the\ncorrectness of, reliability of, and user confidence in generated outputs. We\nhypothesise that a \"test first\" methodology provides both technical constraints\nand cognitive scaffolding, guiding LLM outputs towards more accurate,\nverifiable, and comprehensible solutions. Our framework, applicable across\ndiverse programming contexts, from spreadsheet formula generation to scripting\nlanguages such as Python and strongly typed languages like Rust, includes an\nexplicitly outlined experimental design with clearly defined participant\ngroups, evaluation metrics, and illustrative TDD based prompting examples. By\nemphasising test driven thinking, we aim to improve computational thinking,\nprompt engineering skills, and user engagement, particularly benefiting\nspreadsheet users who often lack formal programming training yet face serious\nconsequences from logical errors. We invite collaboration to refine and\nempirically evaluate this approach, ultimately aiming to establish responsible\nand reliable LLM integration in both educational and professional development\npractices.", "AI": {"tldr": "LLMs are powerful but error-prone in generating code and spreadsheet logic. This paper proposes combining Test-Driven Development (TDD) with LLM usage to address these reliability issues. Their framework guides LLMs to produce more accurate, verifiable outputs and supports users lacking programming expertise, especially in critical domains like finance. The approach is designed for empirical testing and collaborative refinement.", "motivation": "Current LLMs, while powerful in code and spreadsheet generation, frequently suffer from issues like hallucinations and logical/syntactic errors, posing major risks in high-stakes areas requiring high reliability. There's a need for approaches that can systematically improve correctness and user confidence.", "method": "The authors propose integrating Test-Driven Development (TDD) practices with LLM-generated code and logic. This involves using 'test-first' prompts to guide LLMs, structured experimental design for evaluating the approach, and adaptation across different programming languages and contexts.", "result": "The proposed framework aims to improve the accuracy, reliability, and comprehensibility of LLM outputs. It also enhances computational thinking and prompt engineering skills, with particular benefits for non-programmer spreadsheet users. The paper includes an experimental plan and calls for collaborative, empirical validation.", "conclusion": "Integrating TDD with LLM code generation could mitigate key reliability issues and increase user confidence, especially for users lacking formal programming backgrounds. The authors advocate responsible LLM deployment in both education and professional settings via test-driven, empirically validated approaches."}}
{"id": "2510.15565", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15565", "abs": "https://arxiv.org/abs/2510.15565", "authors": ["Vinicius Moraes de Jesus", "Andre Georghton Cardoso Pacheco"], "title": "Colepp: uma ferramenta multiplataforma para coleta de dados de dispositivos vestiveis", "comment": "in Portuguese language", "summary": "The widespread adoption of wearable devices such as smartwatches and fitness\ntrackers has fueled the demand for reliable physiological and movement data\ncollection tools. However, challenges such as limited access to large,\nhigh-quality public datasets and a lack of control over data collection\nconditions hinder the development of robust algorithms. This work presents\nColepp, an open-source, cross-platform tool designed to collect and synchronize\ndata from multiple wearable devices, including heart rate (via ECG and PPG) and\nmotion signals (accelerometer and gyroscope). The system integrates a\nsmartphone as a central hub, receiving data from a Polar H10 chest strap and a\nWear OS smartwatch, and exporting synchronized datasets in CSV format. Through\na custom synchronization protocol and user-friendly interface, Colepp\nfacilitates the generation of customizable, real-world datasets suitable for\napplications such as human activity recognition and heart rate estimation. A\nuse case shows the effectiveness of the tool in producing consistent and\nsynchronized signals.", "AI": {"tldr": "Colepp is a new open-source tool that synchronizes data from multiple wearable devices, solving issues with dataset quality and consistency. It helps researchers gather reliable physiological and movement data for developing improved algorithms.", "motivation": "The paper is motivated by the need for more reliable physiological and movement data collection tools, especially given the challenges with current publicly available datasets and limited control over data collection conditions.", "method": "The authors developed Colepp, an open-source, cross-platform tool that collects and synchronizes data (ECG, PPG, accelerometer, gyroscope) from multiple wearable devices using a smartphone as a central hub. It uses a custom synchronization protocol to ensure data from the devices is consistent, and exports datasets in CSV format.", "result": "Colepp enables the generation of customizable, real-world datasets with synchronized physiological and movement signals. A specific use case demonstrates its effectiveness in producing consistent and synchronized data.", "conclusion": "Colepp addresses major challenges in wearable data collection by providing a flexible, user-friendly tool for obtaining high-quality, synchronized datasets, which can benefit applications like human activity recognition and heart rate estimation."}}
{"id": "2510.15642", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15642", "abs": "https://arxiv.org/abs/2510.15642", "authors": ["Sian Brooke"], "title": "Interact and React: Exploring Gender Patterns in Development and the Impact on Innovation and Robustness of a User Interface Tool", "comment": "Published in AoIR 2025", "summary": "In open-source software design, the inclusion of women is often highlighted\nsimply to remind programmers that women exist. Yet, little attention is given\nto how greater gender diversity, specifically women's participation, could\nfundamentally alter development patterns. To understand the potential impact of\ngender inclusion, this study investigates React, a widely used JavaScript\nlibrary for building user interfaces with an active contributor community. I\nexamine gender differences in metrics of robustness and innovation, as well as\nshifts in contribution patterns leading up to major version releases over 11\nyears of the React project. My results show that the exclusion of women is\ndetrimental to software as women contribute significantly more to feature\nenhancement and dependency management. By exploring how gender influences\ninnovation and robustness in the development of React, the study offers\ncritical insights into how increasing gender diversity could lead to more\ninclusive, innovative, and robust software.", "AI": {"tldr": "More women in open-source projects like React make the software more innovative and robust; their exclusion is detrimental, especially for features and dependencies.", "motivation": "The motivation is to move beyond merely acknowledging women's existence in open-source software and to critically examine how increased gender diversity\u2014specifically, women's participation\u2014might fundamentally change development processes and outcomes.", "method": "The paper investigates gender differences among contributors to React\u2014a major JavaScript UI library\u2014by examining metrics of robustness, innovation, and shifting contribution patterns, especially around major version releases, over an 11-year period.", "result": "The results show that excluding women adversely affects software, as women make considerable contributions to feature enhancements and dependency management compared to their male counterparts.", "conclusion": "Gender inclusion positively affects open-source software innovation and robustness; encouraging greater participation of women leads to more effective, innovative, and resilient software projects like React."}}
{"id": "2510.15690", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.15690", "abs": "https://arxiv.org/abs/2510.15690", "authors": ["Shiwen Ou", "Yuwei Li", "Lu Yu", "Chengkun Wei", "Tingke Wen", "Qiangpu Chen", "Yu Chen", "Haizhi Tang", "Zulie Pan"], "title": "MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework APIs Fuzzing", "comment": "Accepted for publication in IEEE Transactions on Software Engineering\n  (TSE), 2025", "summary": "Deep learning (DL) frameworks serve as the backbone for a wide range of\nartificial intelligence applications. However, bugs within DL frameworks can\ncascade into critical issues in higher-level applications, jeopardizing\nreliability and security. While numerous techniques have been proposed to\ndetect bugs in DL frameworks, research exploring common API patterns across\nframeworks and the potential risks they entail remains limited. Notably, many\nDL frameworks expose similar APIs with overlapping input parameters and\nfunctionalities, rendering them vulnerable to shared bugs, where a flaw in one\nAPI may extend to analogous APIs in other frameworks. To address this\nchallenge, we propose MirrorFuzz, an automated API fuzzing solution to discover\nshared bugs in DL frameworks. MirrorFuzz operates in three stages: First,\nMirrorFuzz collects historical bug data for each API within a DL framework to\nidentify potentially buggy APIs. Second, it matches each buggy API in a\nspecific framework with similar APIs within and across other DL frameworks.\nThird, it employs large language models (LLMs) to synthesize code for the API\nunder test, leveraging the historical bug data of similar APIs to trigger\nanalogous bugs across APIs. We implement MirrorFuzz and evaluate it on four\npopular DL frameworks (TensorFlow, PyTorch, OneFlow, and Jittor). Extensive\nevaluation demonstrates that MirrorFuzz improves code coverage by 39.92\\% and\n98.20\\% compared to state-of-the-art methods on TensorFlow and PyTorch,\nrespectively. Moreover, MirrorFuzz discovers 315 bugs, 262 of which are newly\nfound, and 80 bugs are fixed, with 52 of these bugs assigned CNVD IDs.", "AI": {"tldr": "MirrorFuzz introduces a novel, automated API fuzzing approach to detect shared bugs in deep learning frameworks. Tested on four popular frameworks, it greatly increases code coverage and discovers hundreds of new bugs, demonstrating its utility in improving framework security and reliability.", "motivation": "Deep learning frameworks are foundational for AI applications, but bugs in these frameworks can lead to severe reliability and security problems. Many frameworks share similar APIs, making them susceptible to shared vulnerabilities and bugs. Existing research has not sufficiently explored API commonalities and the risks they pose.", "method": "The authors introduce MirrorFuzz, an automated tool for fuzzing APIs in deep learning frameworks. MirrorFuzz works in three stages: (1) it gathers historical bug data to identify buggy APIs in a framework; (2) it matches these APIs with similar ones across different frameworks; (3) it uses large language models to generate targeted test code aimed at reproducing and identifying analogous bugs across the matched APIs.", "result": "MirrorFuzz was implemented and tested on TensorFlow, PyTorch, OneFlow, and Jittor. It improved code coverage by 39.92% on TensorFlow and 98.20% on PyTorch compared to the state-of-the-art. It discovered 315 bugs, 262 of which were new discoveries. Of the discovered bugs, 80 have been fixed so far and 52 have been assigned official CNVD IDs.", "conclusion": "MirrorFuzz is highly effective in uncovering shared bugs across APIs in multiple deep learning frameworks, significantly outperforms existing methods in both coverage and new bug discovery, and contributes to improving the reliability of DL frameworks."}}
{"id": "2510.15767", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15767", "abs": "https://arxiv.org/abs/2510.15767", "authors": ["Rathi Adarshi Rammohan", "Moritz Meier", "Dennis K\u00fcster", "Tanja Schultz"], "title": "EASELAN: An Open-Source Framework for Multimodal Biosignal Annotation and Data Management", "comment": null, "summary": "Recent advancements in machine learning and adaptive cognitive systems are\ndriving a growing demand for large and richly annotated multimodal data. A\nprominent example of this trend are fusion models, which increasingly\nincorporate multiple biosignals in addition to traditional audiovisual\nchannels. This paper introduces the EASELAN annotation framework to improve\nannotation workflows designed to address the resulting rising complexity of\nmultimodal and biosignals datasets. It builds on the robust ELAN tool by adding\nnew components tailored to support all stages of the annotation pipeline: From\nstreamlining the preparation of annotation files to setting up additional\nchannels, integrated version control with GitHub, and simplified\npost-processing. EASELAN delivers a seamless workflow designed to integrate\nbiosignals and facilitate rich annotations to be readily exported for further\nanalyses and machine learning-supported model training. The EASELAN framework\nis successfully applied to a high-dimensional biosignals collection initiative\non human everyday activities (here, table setting) for cognitive robots within\nthe DFG-funded Collaborative Research Center 1320 Everyday Activity Science and\nEngineering (EASE). In this paper we discuss the opportunities, limitations,\nand lessons learned when using EASELAN for this initiative. To foster research\non biosignal collection, annotation, and processing, the code of EASELAN is\npublicly available(https://github.com/cognitive-systems-lab/easelan), along\nwith the EASELAN-supported fully annotated Table Setting Database.", "AI": {"tldr": "EASELAN is a new annotation framework for complex multimodal biosignal datasets, enhancing ELAN with features like version control and streamlined annotation. It was applied successfully to collect and annotate biosignals from human activities (table setting) for cognitive robotics research, and both the code and annotated database are publicly released.", "motivation": "There is a growing demand for large, richly annotated multimodal data driven by advances in machine learning and adaptive cognitive systems. Fusion models increasingly require handling multiple biosignals alongside audiovisual channels, necessitating sophisticated annotation workflows.", "method": "This paper introduces the EASELAN annotation framework, which extends the existing ELAN tool with components for all stages of the annotation pipeline. EASELAN streamlines annotation file preparation, supports adding new channels, integrates version control, and simplifies post-processing. The framework was applied to a biosignals collection on table setting activities for cognitive robots.", "result": "EASELAN was successfully utilized in annotating high-dimensional biosignal data for the EASE initiative on human everyday activities. The workflow facilitated seamless integration of multiple modalities and export of rich annotations for further analysis and machine learning. Both the framework's code and an annotated dataset are made publicly available.", "conclusion": "EASELAN improves workflows for multimodal and biosignal data annotation, enabling efficient, integrated processing. Its use in an actual biosignal collection project demonstrates its applicability, and the publicly released code and data aim to support broader research."}}
{"id": "2510.15794", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.15794", "abs": "https://arxiv.org/abs/2510.15794", "authors": ["Rachna Raj", "Diego Elias Costa"], "title": "Towards Supporting Open Source Library Maintainers with Community-Based Analytics", "comment": null, "summary": "Open-source software (OSS) is a pillar of modern software development. Its\nsuccess depends on the dedication of maintainers who work constantly to keep\ntheir libraries stable, adapt to changing needs, and support a growing\ncommunity. Yet, they receive little to no continuous feedback on how the\nprojects that rely on their libraries actually use their APIs. We believe that\ngaining these insights can help maintainers make better decisions, such as\nrefining testing strategies, understanding the impact of changes, and guiding\nthe evolution of their libraries more effectively. We propose the use of\ncommunity-based analytics to analyze how an OSS library is used across its\ndependent ecosystem. We conduct an empirical study of 10 popular Java libraries\nand each with their respective dependent ecosystem of 50 projects. Our results\nreveal that while library developers offer a wide range of API methods, only\n16% on average are actively used by their dependent ecosystem. Moreover, only\n74% of the used API methods are partially or fully covered by their library\ntest suite. We propose two metrics to help developers evaluate their test suite\naccording to the APIs used by their community, and we conduct a survey on\nopen-source practitioners to assess the practical value of these insights in\nguiding maintenance decisions.", "AI": {"tldr": "OSS maintainers often lack feedback on how their libraries are used. An empirical study shows that most API methods are unused, and not all used methods are fully tested. The paper introduces metrics and survey results supporting analytics-based decisions for library testing and evolution.", "motivation": "Open-source software libraries are crucial, but maintainers lack feedback on how their APIs are actually used by dependent projects. Understanding this usage can inform better decisions in testing and library evolution.", "method": "The paper uses community-based analytics to examine OSS library usage across dependent ecosystems. An empirical study is performed on 10 popular Java libraries and 50 dependent projects for each library. Additionally, two new metrics are proposed to evaluate test coverage, and a survey of open-source practitioners is conducted.", "result": "The study finds that only 16% of API methods are actively used by dependent projects, and merely 74% of those used methods are (at least partially) covered by library test suites. Open-source practitioners found the insights valuable for maintenance and decision making.", "conclusion": "Providing maintainers with usage analytics helps them optimize testing strategies and guides the evolution of OSS libraries. The proposed metrics allow for evaluating test coverage relevant to actual API usage, and feedback from practitioners supports the value of these insights."}}
