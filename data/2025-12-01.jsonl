{"id": "2511.21769", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21769", "abs": "https://arxiv.org/abs/2511.21769", "authors": ["Royer David Estrada-Esponda", "Gerardo Matturro", "Jose Reinaldo Sabogal-Pinilla"], "title": "Technical knowledge and soft skills in software startups within the Colombian entrepreneurial ecosystem", "comment": "30 pages", "summary": "The technical knowledge and soft skills of entrepreneurial team members significantly impact the early stages of software startups. It is widely recognized that the success or failure of a startup is determined by the quality of the individuals who constitute the founding team. This article presents the findings of a study conducted within the Colombian entrepreneurial ecosystem, focusing on which technical knowledge and soft skills are the most valued by founding teams of software startups, and how the needs for knowledge and skills evolve as the startup grows. A survey of software startup representatives revealed that the most valued knowledge includes requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting. The most valued soft skills are typically communication, leadership, and teamwork. The outcomes of this work are relevant to software entrepreneurs, incubators, and researchers."}
{"id": "2511.21788", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21788", "abs": "https://arxiv.org/abs/2511.21788", "authors": ["Md. Raihan Tapader", "Md. Mostafizer Rahman", "Ariful Islam Shiplu", "Md Faizul Ibne Amin", "Yutaka Watanobe"], "title": "Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings", "comment": null, "summary": "In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring."}
{"id": "2511.21877", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21877", "abs": "https://arxiv.org/abs/2511.21877", "authors": ["Nenad Petrovic", "Norbert Kroth", "Axel Torschmied", "Yinglei Song", "Fengjunjie Pan", "Vahid Zolfaghari", "Nils Purschke", "Sven Kirchner", "Chengdong Wu", "Andre Schamschurko", "Yi Zhang", "Alois Knoll"], "title": "LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems", "comment": null, "summary": "This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining."}
{"id": "2511.21878", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.21878", "abs": "https://arxiv.org/abs/2511.21878", "authors": ["Kaiyao Ke", "Ali Reza Ibrahimzada", "Rangeet Pan", "Saurabh Sinha", "Reyhaneh Jabbarvand"], "title": "Advancing Automated In-Isolation Validation in Repository-Level Code Translation", "comment": null, "summary": "Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation."}
{"id": "2511.22075", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22075", "abs": "https://arxiv.org/abs/2511.22075", "authors": ["Doruk Alp Mutlu"], "title": "Expanding Specification Capabilities of a Gradual Verifier with Pure Functions", "comment": "Submitted to the 53rd ACM SIGPLAN Symposium on Principles of Programming Languages (POPL 2026) Student Research Competition", "summary": "Gradual verification soundly combines static checking and dynamic checking to provide an incremental approach for software verification. With gradual verification, programs can be partially specified first, and then the full specification of a program can be achieved in incremental steps. The first and only practicable gradual verifier based on symbolic execution, Gradual C0, supports recursive heap data structures. Despite recent efforts to improve the expressivity of Gradual C0's specification language, Gradual C0's specification language is still limited in its capabilities for complex expressions. This work explores an extension to Gradual C0's design with a common construct supported by many static verification tools, pure functions, which both extend the specification capabilities of Gradual C0 and increase the ease of encoding observer methods in Gradual C0. Our approach addresses the technical challenges related to the axiomatisation of pure functions with imprecise specifications."}
{"id": "2511.21920", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21920", "abs": "https://arxiv.org/abs/2511.21920", "authors": ["Apu Kumar Chakroborti", "Yi Ding", "Lipeng Wan"], "title": "Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code", "comment": null, "summary": "As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools."}
{"id": "2511.22419", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22419", "abs": "https://arxiv.org/abs/2511.22419", "authors": ["Ken Sakayori", "Andrea Colledan", "Ugo Dal Lago"], "title": "On Circuit Description Languages, Indexed Monads, and Resource Analysis", "comment": "Extended version of a paper to be published at POPL 2026", "summary": "In this paper, a monad-based denotational model is introduced and shown adequate for the Proto-Quipper family of calculi, themselves being idealized versions of the Quipper programming language. The use of a monadic approach allows us to separate the value to which a term reduces from the circuit that the term itself produces as a side effect. In turn, this enables the denotational interpretation and validation of rich type systems in which the size of the produced circuit can be controlled. Notably, the proposed semantic framework, through the novel concept of circuit algebra, suggests forms of effect typing guaranteeing quantitative properties about the resulting circuit, even in presence of optimizations."}
{"id": "2511.21956", "categories": ["cs.SE", "physics.acc-ph"], "pdf": "https://arxiv.org/pdf/2511.21956", "abs": "https://arxiv.org/abs/2511.21956", "authors": ["M. Polzin", "M. Guzman"], "title": "Beyond Like-for-Like: A User-centered Approach to Modernizing Legacy Applications", "comment": "The 20th International Conference on Accelerator and Large Experimental Physics Control Systems", "summary": "When modernizing a legacy application, it is easy to fall back on a like-for-like replica with new tools and updated design stylings, but this is an opportunity to explore making a more intuitive application that supports user tasks and efficiency. Rather than having a blank canvas-unburdened by legacy tech debt-to create a new application, you are working with an existing application that is integral to accelerator operations and one that expert users are already familiar with. Due to this, you might assume people will prefer the like-for-like, but you could be carrying forward the pain points, processes that are inefficient, and ultimately wind up with an application that no one wants to use because it doesn't solve existing problems. Getting users involved can make all the difference in your approach to modernizing a legacy application that caters to both newer and expert users. It also can bridge the gap between like-for-like and introducing new GUI design. Having a legacy application doesn't have to make the modernized one difficult to develop, as the existing application is a tool in how you move forward with the new application. It provides insight into areas that an application with a clean slate doesn't give you."}
{"id": "2511.22692", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22692", "abs": "https://arxiv.org/abs/2511.22692", "authors": ["David Castro-Perez", "Francisco Ferreira", "Sung-Shik Jongmans"], "title": "A Synthetic Reconstruction of Multiparty Session Types (with Appendix)", "comment": null, "summary": "Multiparty session types (MPST) provide a rigorous foundation for verifying the safety and liveness of concurrent systems. However, existing approaches often force a difficult trade-off: classical, projection-based techniques are compositional but limited in expressiveness, while more recent techniques achieve higher expressiveness by relying on non-compositional, whole-system model checking, which scales poorly.\n  This paper introduces a new approach to MPST that delivers both expressiveness and compositionality, called the synthetic approach. Our key innovation is a type system that verifies each process directly against a global protocol specification, represented as a labelled transition system (LTS) in general, with global types as a special case. This approach uniquely avoids the need for intermediate local types and projection.\n  We demonstrate that our approach, while conceptually simpler, supports a benchmark of challenging protocols that were previously beyond the reach of compositional techniques in the MPST literature. We generalise our type system, showing that it can validate processes against any specification that constitutes a \"well-behaved\" LTS, supporting protocols not expressible with the standard global type syntax. The entire framework, including all theorems and many examples, has been formalised and mechanised in Agda, and we have developed a prototype implementation as an extension to VS Code."}
{"id": "2511.21964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21964", "abs": "https://arxiv.org/abs/2511.21964", "authors": ["Ali Sayedsalehi", "Peter C. Rigby", "Audris Mockus"], "title": "DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction", "comment": "8 pages, 4 figures, includes system architecture diagrams, Web UI screenshots, GitHub App examples, and an appendix with API endpoints. Full replication package and demo materials available", "summary": "In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website."}
{"id": "2511.23283", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23283", "abs": "https://arxiv.org/abs/2511.23283", "authors": ["Alexandre Moine", "Sam Westrick", "Joseph Tassarotti"], "title": "All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs", "comment": "32 pages, 26 figures, extended version of the same paper accepted at POPL 2026", "summary": "Nondeterminism makes parallel programs challenging to write and reason about. To avoid these challenges, researchers have developed techniques for internally deterministic parallel programming, in which the steps of a parallel computation proceed in a deterministic way. Internal determinism is useful because it lets a programmer reason about a program as if it executed in a sequential order. However, no verification framework exists to exploit this property and simplify formal reasoning about internally deterministic programs.\n  To capture the essence of why internally deterministic programs should be easier to reason about, this paper defines a property called schedule-independent safety. A program satisfies schedule-independent safety, if, to show that the program is safe across all orderings, it suffices to show that one terminating execution of the program is safe. We then present a separation logic called Musketeer for proving that a program satisfies schedule-independent safety. Once a parallel program has been shown to satisfy schedule-independent safety, we can verify it with a new logic called Angelic, which allows one to dynamically select and verify just one sequential ordering of the program.\n  Using Musketeer, we prove the soundness of MiniDet, an affine type system for enforcing internal determinism. MiniDet supports several core algorithmic primitives for internally deterministic programming that have been identified in the research literature, including a deterministic version of a concurrent hash set. Because any syntactically well-typed MiniDet program satisfies schedule-independent safety, we can apply Angelic to verify such programs.\n  All results in this paper have been verified in Rocq using the Iris separation logic framework."}
{"id": "2511.22118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22118", "abs": "https://arxiv.org/abs/2511.22118", "authors": ["Yihan Dai", "Dimitrios Stamatios Bouras", "Haoxiang Jia", "Sergey Mechtaev"], "title": "Statistical Independence Aware Caching for LLM Workflows", "comment": null, "summary": "Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness."}
{"id": "2511.23358", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23358", "abs": "https://arxiv.org/abs/2511.23358", "authors": ["Alexandre Moine", "Stephanie Balzer", "Alex Xu", "Sam Westrick"], "title": "TypeDis: A Type System for Disentanglement", "comment": "34 pages, 24 figures, extended version of the same paper accepted at POPL 2026", "summary": "Disentanglement is a runtime property of parallel programs guaranteeing that parallel tasks remain oblivious to each other's allocations. As demonstrated in the MaPLe compiler and run-time system, disentanglement can be exploited for fast automatic memory management, especially task-local garbage collection with no synchronization between parallel tasks. However, as a low-level property, disentanglement can be difficult to reason about for programmers. The only means of statically verifying disentanglement so far has been DisLog, an Iris-fueled variant of separation logic, mechanized in the Rocq proof assistant. DisLog is a fully-featured program logic, allowing for proof of functional correctness as well as verification of disentanglement. Yet its employment requires significant expertise and per-program proof effort.\n  This paper explores the route of automatic verification via a type system, ensuring that any well-typed program is disentangled and lifting the burden of carrying out manual proofs from the programmer. It contributes TypeDis, a type system inspired by region types, where each type is annotated with a timestamp, identifying the task that allocated it. TypeDis supports iso-recursive types as well as polymorphism over both types and timestamps. Crucially, timestamps are allowed to change during type-checking, at join points as well as via a form of subtyping, dubbed subtiming. The paper illustrates TypeDis and its features on a range of examples. The soundness of TypeDis and the examples are mechanized in the Rocq proof assistant, using an improved version of DisLog, dubbed DisLog2."}
{"id": "2511.22186", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22186", "abs": "https://arxiv.org/abs/2511.22186", "authors": ["Chayanid Termphaiboon", "Raula Gaikovina Kula", "Youmei Fan", "Morakot Choetkiertikul", "Chaiyong Ragkhitwetsagul", "Thanwadee Sunetnanta", "Kenichi Matsumoto"], "title": "Exploring the SECURITY.md in the Dependency Chain: Preliminary Analysis of the PyPI Ecosystem", "comment": "8 pages, 5 figures, accepted to ISE 2025 (International Workshop on Intelligent Software Engineering)", "summary": "Security policies, such as SECURITY.md files, are now common in open-source projects. They help guide responsible vulnerability reporting and build trust among users and contributors. Despite their growing use, it is still unclear how these policies influence the structure and evolution of software dependencies. Software dependencies are external packages or libraries that a project relies on, and their interconnected nature affects both functionality and security. This study explores the relationship between security policies and dependency management in PyPI projects. We analyzed projects with and without a SECURITY.md file by examining their dependency trees and tracking how dependencies change over time. The analysis shows that projects with a security policy tend to rely on a broader set of direct dependencies, while overall depth and transitive dependencies remain similar. Historically, projects created after the introduction of SECURITY.md, particularly later adopters, show more frequent dependency updates. These results suggest that security policies are linked to more modular and feature-rich projects, and highlight the role of SECURITY.md in promoting proactive dependency management and reducing risks in the software supply chain."}
{"id": "2511.23472", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23472", "abs": "https://arxiv.org/abs/2511.23472", "authors": ["Yusuke Matsushita", "Kengo Hirata", "Ryo Wakizaka", "Emanuele D'Osualdo"], "title": "RapunSL: Untangling Quantum Computing with Separation, Linear Combination and Mixing", "comment": "Full version of the conference paper at POPL 2026. The first two authors contributed equally to this work", "summary": "Quantum Separation Logic (QSL) has been proposed as an effective tool to improve the scalability of deductive reasoning for quantum programs. In QSL, separation is interpreted as disentanglement, and the frame rule brings a notion of entanglement-local specification (one that only talks about the qubits entangled with those acted upon by the program). In this paper, we identify two notions of locality unique to the quantum domain, and we construct a novel quantum separation logic, RapunSL, which is able to soundly reduce reasoning about superposition states to reasoning about pure states (basis-locality), and reasoning about mixed states arising from measurement to reasoning about pure states (outcome-locality). To do so, we introduce two connectives, linear combination and mixing, which together with separation provide a dramatic improvement in the scalability of reasoning, as we demonstrate on a series of challenging case studies."}
{"id": "2511.22359", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.22359", "abs": "https://arxiv.org/abs/2511.22359", "authors": ["Vadim Safronov", "Ionut Bostan", "Nicholas Allott", "Andrew Martin"], "title": "UniBOM -- A Unified SBOM Analysis and Visualisation Tool for IoT Systems and Beyond", "comment": "This paper has been accepted at the ACM 15th International Conference on the Internet of Things (ACM IoT 2025)", "summary": "Modern networked systems rely on complex software stacks, which often conceal vulnerabilities arising from intricate interdependencies. A Software Bill of Materials (SBOM) is effective for identifying dependencies and mitigating security risks. However, existing SBOM solutions lack precision, particularly in binary analysis and non-package-managed languages like C/C++.\n  This paper introduces UniBOM, an advanced tool for SBOM generation, analysis, and visualisation, designed to enhance the security accountability of networked systems. UniBOM integrates binary, filesystem, and source code analysis, enabling fine-grained vulnerability detection and risk management. Key features include historical CPE tracking, AI-based vulnerability classification by severity and memory safety, and support for non-package-managed C/C++ dependencies.\n  UniBOM's effectiveness is demonstrated through a comparative vulnerability analysis of 258 wireless router firmware binaries and the source code of four popular IoT operating systems, highlighting its superior detection capabilities compared to other widely used SBOM generation and analysis tools. Packaged for open-source distribution, UniBOM offers an end-to-end unified analysis and visualisation solution, advancing SBOM-driven security management for dependable networked systems and broader software."}
{"id": "2511.21878", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.21878", "abs": "https://arxiv.org/abs/2511.21878", "authors": ["Kaiyao Ke", "Ali Reza Ibrahimzada", "Rangeet Pan", "Saurabh Sinha", "Reyhaneh Jabbarvand"], "title": "Advancing Automated In-Isolation Validation in Repository-Level Code Translation", "comment": null, "summary": "Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation."}
{"id": "2511.22409", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22409", "abs": "https://arxiv.org/abs/2511.22409", "authors": ["Polydoros Giannouris", "Sophia Ananiadou"], "title": "NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements", "comment": null, "summary": "Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows."}
{"id": "2511.22513", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22513", "abs": "https://arxiv.org/abs/2511.22513", "authors": ["Jérôme Pfeiffer", "Nicolai Maisch", "Sebastian Friedl", "Matthias Milan Strljic", "Armin Lechler", "Oliver Riedel", "Andreas Wortmann"], "title": "Declarative Policy Control for Data Spaces: A DSL-Based Approach for Manufacturing-X", "comment": null, "summary": "The growing adoption of federated data spaces, such as in the GAIA-X and the International Data Spaces (IDS) initiative, promises secure and sovereign data sharing across organizational boundaries in Industry 4.0. In manufacturing ecosystems, this enables use cases, such as cross-factory process optimization, predictive maintenance, and supplier integration. Frameworks and standards, such as the Asset Administration Shell (AAS), Eclipse Dataspace Connector (EDC), ID-Link and Open Platform Communications Unified Architecture (OPC UA) provide a strong foundation to realize this ecosystem. However, a major open challenge is the practical description and enforcement of context-dependent data usage policies using these base technologies - especially by domain experts without software engineering backgrounds. Therefore, this article proposes a method for leveraging domain-specific languages (DSLs) to enable declarative, human-readable, and machine-executable policy definitions for sovereign data sharing via data space connectors. The DSL empowers domain experts to specify fine-grained data governance requirements - such as restricting access to data from specific production batches or enforcing automatic deletion after a defined retention period - without writing imperative code."}
{"id": "2511.22726", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22726", "abs": "https://arxiv.org/abs/2511.22726", "authors": ["Ethan Friesen", "Sasha Morton-Salmon", "Md Nahidul Islam Opu", "Shahidul Islam", "Shaiful Chowdhury"], "title": "The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods", "comment": null, "summary": "Identifying the small subset of source code that repeatedly attracts bugs is critical for reducing long-term maintenance effort. We define ExtremelyBuggy methods as those involved in more than one bug fix and present the first large-scale study of their prevalence, characteristics, and predictability. Using a dataset of over 1.25 million methods from 98 open-source Java projects, we find that ExtremelyBuggy methods constitute only a tiny fraction of all methods, yet frequently account for a disproportionately large share of bugs. At their inception, these methods are significantly larger, more complex, less readable, and less maintainable than both singly-buggy and non-buggy methods. However, despite these measurable differences, a comprehensive evaluation of five machine learning models shows that early prediction of ExtremelyBuggy methods remains highly unreliable due to data imbalance, project heterogeneity, and the fact that many bugs emerge through subsequent evolution rather than initial implementation. To complement these quantitative findings, we conduct a thematic analysis of 265 ExtremelyBuggy methods, revealing recurring visual issues (e.g., confusing control flow, poor readability), contextual roles (e.g., core logic, data transformation, external resource handling), and common defect patterns (e.g., faulty conditionals, fragile error handling, misuse of variables). These results highlight the need for richer, evolution-aware representations of code and provide actionable insights for practitioners seeking to prioritize high-risk methods early in the development lifecycle."}
{"id": "2511.22921", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22921", "abs": "https://arxiv.org/abs/2511.22921", "authors": ["Hengyuan Liu", "Xia Song", "Yong Liu", "Zheng Li"], "title": "MBFL-DKMR: Improving Mutation-based Fault Localization through Denoising-based Kill Matrix Refinement", "comment": null, "summary": "Software debugging is a critical and time-consuming aspect of software development, with fault localization being a fundamental step that significantly impacts debugging efficiency. Mutation-Based Fault Localization (MBFL) has gained prominence due to its robust theoretical foundations and fine-grained analysis capabilities. However, recent studies have identified a critical challenge: noise phenomena, specifically the false kill relationships between mutants and tests, which significantly degrade localization effectiveness. While several approaches have been proposed to rectify the final localization results, they do not directly address the underlying noise. In this paper, we propose a novel approach to refine the kill matrix, a core data structure capturing mutant-test relationships in MBFL, by treating it as a signal that contains both meaningful fault-related patterns and high-frequency noise. Inspired by signal processing theory, we introduce DKMR (Denoising-based Kill Matrix Refinement), which employs two key stages: (1) signal enhancement through hybrid matrix construction to improve the signal-to-noise ratio for better denoising, and (2) signal denoising via frequency domain filtering to suppress noise while preserving fault-related patterns. Building on this foundation, we develop MBFL-DKMR, a fault localization framework that utilizes the refined matrix with fuzzy values for suspiciousness calculation. Our evaluation on Defects4J v2.0.0 demonstrates that MBFL-DKMR effectively mitigates the noise and outperforms the state-of-the-art MBFL techniques. Specifically, MBFL-DKMR achieves 129 faults localized at Top-1 compared to 85 for BLMu and 103 for Delta4Ms, with negligible additional computational overhead (0.11 seconds, 0.001\\% of total time). This work highlights the potential of signal processing techniques to enhance the effectiveness of MBFL by refining the kill matrix."}
{"id": "2511.23007", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23007", "abs": "https://arxiv.org/abs/2511.23007", "authors": ["Yizheng Wang", "Tao Jiang", "Jinyan Bai", "Zhengbin Zou", "Tiancheng Xue", "Nan Zhang", "Jie Luan"], "title": "A transfer learning approach for automatic conflicts detection in software requirement sentence pairs based on dual encoders", "comment": "22 pages, 7 figures, 3 tables", "summary": "Software Requirement Document (RD) typically contain tens of thousands of individual requirements, and ensuring consistency among these requirements is critical for the success of software engineering projects. Automated detection methods can significantly enhance efficiency and reduce costs; however, existing approaches still face several challenges, including low detection accuracy on imbalanced data, limited semantic extraction due to the use of a single encoder, and suboptimal performance in cross-domain transfer learning. To address these issues, this paper proposes a Transferable Software Requirement Conflict Detection Framework based on SBERT and SimCSE, termed TSRCDF-SS. First, the framework employs two independent encoders, Sentence-BERT (SBERT) and Simple Contrastive Sentence Embedding (SimCSE), to generate sentence embeddings for requirement pairs, followed by a six-element concatenation strategy. Furthermore, the classifier is enhanced by a two-layer fully connected feedforward neural network (FFNN) with a hybrid loss optimization strategy that integrates a variant of Focal Loss, domain-specific constraints, and a confidence-based penalty term. Finally, the framework synergistically integrates sequential and cross-domain transfer learning. Experimental results demonstrate that the proposed framework achieves a 10.4% improvement in both macro-F1 and weighted-F1 scores in in-domain settings, and an 11.4% increase in macro-F1 in cross-domain scenarios."}
{"id": "2511.23009", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23009", "abs": "https://arxiv.org/abs/2511.23009", "authors": ["D. Sree Yashaswinee", "Gargie Tambe", "Y. Raghu Reddy", "Karthik Vaidhyanathan"], "title": "APDT: A Digital Twin for Assessing Access Point Characteristics in a Network", "comment": "7 pages, 1 framework, 2 figures", "summary": "Digital twins (DT) have emerged as a transformative technology, enabling real-time monitoring, simulations, and predictive maintenance across various domains, though their Application in the networking domain remains underexplored. This paper focuses on issues such as increasing client density and traffic congestion by proposing a digital twin for computer networks. Our Digital Twin, named Access Point Digital Twin (APDT) is used for tracking user behavior and changing bandwidth demands, directly impacting network performance and Quality of Service (QoS) parameters like latency, jitter, etc. APDT captures the real-time state of networks with data from access points (APs), enabling simulation-based analyses and predictive modelling. APDT facilitates the simulation of various what-if scenarios thereby providing a better understanding of various aspects of the network characteristics. We tested APDT on our University network. APDT uses data collected from three access points via the Ruckus SmartZone API and incorporates NS-3 based simulations. The simulation replicates a real-time snapshot from a Ruckus access point and models metrics such as latency and inter-packet transfer time. Additionally, a forecasting model predicts traffic congestion and suggests proactive client offloading, enhancing network management and performance optimization. Preliminary results indicate that APDT can successfully predict short-term traffic surges, leading to improved QoS and reduced traffic congestion."}
{"id": "2511.23050", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23050", "abs": "https://arxiv.org/abs/2511.23050", "authors": ["Nikita Repnkiov", "Vladimir Faerman"], "title": "Software for Studying CASCADE Error Correction Protocols in Quantum Communications", "comment": "Reported in Omsk State Technical University, November 13", "summary": "This article addresses the development of quantum communication methods in the context of emerging quantum computing threats and emphasizes the importance of key reconciliation in quantum communication systems. The study focuses on the CASCADE protocol and the design of a software prototype intended for research and educational purposes. A parallel error-correction algorithm based on the actor model was implemented, improving the efficiency of key reconciliation and reducing the amount of exchanged data. Evaluation of the prototype revealed limitations, including the computational cost of message passing, complexity of error handling, and code redundancy due to iterative development. Experimental results confirmed the correct implementation of the core CASCADE algorithms and informed the design of future improvements. Proposed enhancements include redesigning the system architecture, developing interfaces for exporting intermediate data, defining the communication channel as a separate component, and expanding tools for systematic verification and comparative analysis of blind key-reconciliation methods."}
{"id": "2511.23157", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23157", "abs": "https://arxiv.org/abs/2511.23157", "authors": ["Hana Kataoka", "Jialong Li", "Yutaka Matsuno"], "title": "Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning", "comment": "Accepted by ICSE-SEET (ACM/IEEE 48th International Conference on Software Engineering: Software Engineering Education and Training)", "summary": "As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as \"equalizers,\" boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as \"amplifiers,\" dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities."}
{"id": "2511.23159", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23159", "abs": "https://arxiv.org/abs/2511.23159", "authors": ["Bertrand Meyer"], "title": "AI for software engineering: from probable to provable", "comment": null, "summary": "Vibe coding, the much-touted use of AI techniques for programming, faces two overwhelming obstacles: the difficulty of specifying goals (\"prompt engineering\" is a form of requirements engineering, one of the toughest disciplines of software engineering); and the hallucination phenomenon. Programs are only useful if they are correct or very close to correct.\n  The solution? Combine the creativity of artificial intelligence with the rigor of formal specification methods and the power of formal program verification, supported by modern proof tools."}
{"id": "2511.23213", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23213", "abs": "https://arxiv.org/abs/2511.23213", "authors": ["Samuele Doria", "Eleonora Losiouk"], "title": "GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis", "comment": null, "summary": "Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.\n  We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.\n  On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\\%, and Guardian, an LLM-based UI automator, reaches 17.12\\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\\% and 9.48\\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.\n  Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\\% of the target methods and dynamically reaches 59.86\\% of them."}
{"id": "2511.23302", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23302", "abs": "https://arxiv.org/abs/2511.23302", "authors": ["Hengyuan Liu", "Zheng Li", "Donghua Wang", "Yankai Wu", "Xiang Chen", "Yong Liu"], "title": "FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation", "comment": null, "summary": "Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components."}
{"id": "2511.23321", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23321", "abs": "https://arxiv.org/abs/2511.23321", "authors": ["Yifei Wang", "Jacky Keung", "Zhenyu Mao", "Jingyu Zhang", "Yuchen Cao"], "title": "Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing", "comment": null, "summary": "Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation."}
