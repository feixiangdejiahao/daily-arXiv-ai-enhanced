<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 27]
- [cs.PL](#cs.PL) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [LLM-based Triplet Extraction for Automated Ontology Generation in Software Engineering Standards](https://arxiv.org/abs/2509.00140)
*Songhui Yue*

Main category: cs.SE

TL;DR: This paper presents an LLM-assisted method for extracting relationships and building ontologies from messy, domain-specific texts in software engineering standards. Its comprehensive workflow outperforms traditional extraction tools and sets a new standard for automated ontology generation in such contexts.


<details>
  <summary>Details</summary>
Motivation: Automated ontology generation (AOG) is essential for efficiently using ontologies, especially in domains with unstructured and noise-heavy text like software engineering standards (SES). However, extracting meaningful relationships from such complex documents is challenging.

Method: The paper proposes an open-source, large language model (LLM)-assisted approach for relation triple extraction (RTE) from SES documents. The workflow includes document segmentation, candidate term mining, LLM-based relation inference, term normalization, and cross-section alignment. Benchmarks at three granularities are constructed to evaluate performance.

Result: The LLM-assisted ontology generation approach provides results that are comparable to, and potentially superior to, the widely-used OpenIE triple extraction method.

Conclusion: Using LLMs as aids instead of relying solely on prompt engineering enhances the AOG process for SES, improving relation extraction and ontology quality. The multi-stage workflow developed in this study proves effective and benchmarks validate its superiority over existing methods.

Abstract: Ontologies have supported knowledge representation and whitebox reasoning for
decades; thus, the automated ontology generation (AOG) plays a crucial role in
scaling their use. Software engineering standards (SES) consist of long,
unstructured text (with high noise) and paragraphs with domain-specific terms.
In this setting, relation triple extraction (RTE), together with term
extraction, constitutes the first stage toward AOG. This work proposes an
open-source large language model (LLM)-assisted approach to RTE for SES.
Instead of solely relying on prompt-engineering-based methods, this study
promotes the use of LLMs as an aid in constructing ontologies and explores an
effective AOG workflow that includes document segmentation, candidate term
mining, LLM-based relation inference, term normalization, and cross-section
alignment. Golden-standard benchmarks at three granularities are constructed
and used to evaluate the ontology generated from the study. The results show
that it is comparable and potentially superior to the OpenIE method of triple
extraction.

</details>


### [2] [LLM-Based Program Generation for Triggering Numerical Inconsistencies Across Compilers](https://arxiv.org/abs/2509.00256)
*Yutong Wang,Cindy Rubio-González*

Main category: cs.SE

TL;DR: LLM4FP uses LLMs to automatically generate programs that reveal floating-point inconsistencies across compilers, outperforming prior tools and improving detection for subtle and broad cases.


<details>
  <summary>Details</summary>
Motivation: Floating-point computations can yield different results on different compilers or optimization levels, which can undermine the reliability of numerical software. Detecting these inconsistencies is crucial for robust scientific and engineering applications.

Method: LLM4FP framework leverages Large Language Models for generating floating-point programs that are likely to expose inconsistencies. It integrates grammar-based generation with feedback-driven mutation to systematically craft diverse and valid test programs.

Result: LLM4FP detects more than twice as many inconsistencies as the previous best tool (Varity). The majority detected are subtle, real-valued differences rather than obvious anomalies such as NaN or infinities. It also identifies mismatches across various optimization levels and between host/device compilers.

Conclusion: Using LLM-guided program generation markedly enhances the discovery of floating-point inconsistencies. The framework offers improved coverage and effectiveness compared to previous methodologies, highlighting the value of LLMs in automated software reliability testing.

Abstract: Floating-point inconsistencies across compilers can undermine the reliability
of numerical software. We present LLM4FP, the first framework that uses Large
Language Models (LLMs) to generate floating-point programs specifically
designed to trigger such inconsistencies. LLM4FP combines Grammar-Based
Generation and Feedback-Based Mutation to produce diverse and valid programs.
We evaluate LLM4FP across multiple compilers and optimization levels, measuring
inconsistency rate, time cost, and program diversity. LLM4FP detects over twice
as many inconsistencies compared to the state-of-the-art tool, Varity. Notably,
most of the inconsistencies involve real-valued differences, rather than
extreme values like NaN or infinities. LLM4FP also uncovers inconsistencies
across a wider range of optimization levels, and finds the most mismatches
between host and device compilers. These results show that LLM-guided program
generation improves the detection of numerical inconsistencies.

</details>


### [3] [JS-TOD: Detecting Order-Dependent Flaky Tests in Jest](https://arxiv.org/abs/2509.00466)
*Negar Hashemi,Amjed Tahir,Shawn Rasheed,August Shi,Rachel Blagojevic*

Main category: cs.SE

TL;DR: JS-TOD is a customizable tool for detecting test order dependency in JavaScript/Jest tests by randomizing and rerunning their execution order. It reveals that shared files and shared mocking state are primary sources of flaky tests.


<details>
  <summary>Details</summary>
Motivation: Test order dependency is a common cause of test flakiness in JavaScript projects using Jest. Ensuring each test is robust and independent is vital, but in reality, tests can affect one another depending on their execution order.

Method: JS-TOD is a tool that systematically randomizes the execution order of Jest tests, test suites, and describe blocks. It extracts, reorders, and reruns tests to detect order-dependent flakiness, allowing customization of the number of reorders and reruns.

Result: Using JS-TOD, the authors identify two main causes of test order dependency: shared files and shared mocking state among tests.

Conclusion: JS-TOD effectively detects order-dependent test flakiness in Jest-based JavaScript projects and helps developers identify underlying issues such as shared files and mocking states.

Abstract: We present JS-TOD (JavaScript Test Order-dependency Detector), a tool that
can extract, reorder, and rerun Jest tests to reveal possible order-dependent
test flakiness. Test order dependency is one of the leading causes of test
flakiness. Ideally, each test should operate in isolation and yield consistent
results no matter the sequence in which tests are run. However, in practice,
test outcomes can vary depending on their execution order. JS-TOD employed a
systematic approach to randomising tests, test suites, and describe blocks. The
tool is highly customisable, as one can set the number of orders and reruns
required (the default setting is 10 reorder and 10 reruns for each test and
test suite). Our evaluation using JS-TOD reveals two main causes of test order
dependency flakiness: shared files and shared mocking state between tests.

</details>


### [4] [Bug Whispering: Towards Audio Bug Reporting](https://arxiv.org/abs/2509.00785)
*Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: Audio bug reporting for mobile apps may boost bug collection and fixing rates, but presents unique challenges for reproduction and analysis; further research is needed.


<details>
  <summary>Details</summary>
Motivation: Traditional bug reporting relies heavily on text-based descriptions, which may limit the information developers receive about faults experienced by end-users. This paper is motivated by the opportunity to enhance bug reporting in mobile applications by leveraging audio messages, making it simpler and possibly more motivating for end-users to report bugs.

Method: The paper explores the idea of using audio recordings as bug reports by allowing end-users to record and submit audio messages describing the problems they encounter. It discusses the design and implementation simplicity of audio reporting and conducts a preliminary experiment to analyze its effectiveness and the challenges.

Result: Audio bug reports increase the potential quantity of bug reports developers can collect, potentially improving bug identification and resolution rates. However, audio bug reports present unique processing and analysis challenges compared to traditional approaches, mainly in bug reproduction.

Conclusion: Audio-based bug reporting has promise to enhance bug data collection and support faster bug resolution in mobile applications, but specific challenges in analyzing and reproducing bugs from audio must be addressed. The paper encourages further research in this direction.

Abstract: Bug reporting is a key feature of mobile applications, as it enables
developers to collect information about faults that escaped testing and thus
affected end-users. This paper explores the idea of allowing end-users to
immediately report the problems that they experience by recording and
submitting audio messages. Audio recording is simple to implement and has the
potential to increase the number of bug reports that development teams can
gather, thus potentially improving the rate at which bugs are identified and
fixed. However, audio bug reports exhibit specific characteristics that
challenge existing techniques for reproducing bugs. This paper discusses these
challenges based on a preliminary experiment, and motivates further research on
the collection and analysis of audio-based bug reports

</details>


### [5] [REConnect: Participatory RE that Matters](https://arxiv.org/abs/2509.01006)
*Daniela Damian,Bachan Ghimire,Ze Shi Li*

Main category: cs.SE

TL;DR: REConnect is a human-centered participatory framework for requirements engineering that focuses on stakeholder relationships, co-design, and empowerment to produce culturally grounded, socially legitimate outcomes. Through case studies, it shows practical ways to ensure sustained impact and ethical use of AI in the RE process.


<details>
  <summary>Details</summary>
Motivation: Current popular RE practices like CrowdRE and AI-assisted elicitation are often disconnected from real users’ values, lived experiences, and social/political contexts. The motivation is to re-center RE on meaningful human connections and participatory processes that matter to communities, leading to socially legitimate and sustainable software systems.

Method: The paper introduces REConnect through three case studies—BloodSync in Nepal, Herluma in Canada, and BridgingRoots in the Canadian Arctic—to illustrate its participatory, relationship-based approach. It also proposes practical actionable guidelines (REActions) for ongoing stakeholder engagement and relationality in requirements engineering tasks.

Result: REConnect demonstrates, through three societal impact projects, that building trust, co-designing with stakeholders, and empowering users result in requirements that are culturally sensitive, socially accepted, and sustainable. It also shows how human-centered participatory approaches can guide the ethical and effective integration of AI in RE processes.

Conclusion: REConnect is an approach that improves requirements engineering (RE) by focusing on human connections, participatory practices, and culturally grounded outcomes. It asserts that requirements work should empower users, ensure community alignment, and sustain impact even as generative AI tools become popular in RE.

Abstract: Software increasingly shapes the infrastructures of daily life, making
requirements engineering (RE) central to ensuring that systems align with human
values and lived experiences. Yet, current popular practices such as CrowdRE
and AI-assisted elicitation strategies risk detaching requirements work from
the cultural, social, and political contexts that shape lived experiences,
human values, and real user needs. In this paper, we introduce REConnect that
re-centers RE on the human connection as central to the understanding of lived
experiences where impact is sought. REConnect advocates for a human-centered
participatory approach "that matters" to the communities and beneficiaries
involved, ensuring alignment with their values and aspirations. Drawing on
three case studies of societal impact: BloodSync in rural Nepal, Herluma
supporting women at risk of homelessness in Canada, and BridgingRoots to
revitalize Indigenous languages in the Canadian Arctic. REConnect argues that
three key principles and enablers: building trusting relationships,
co-designing with and alongside stakeholders, and empowering users as agents of
change, can yield requirements that are culturally grounded, socially
legitimate, and sustainable beyond system delivery. REConnect also proposes a
set of actionable practices (REActions) that embed relationality and ongoing
stakeholder engagement throughout requirements elicitation, analysis, and
validation of solution development. Finally, we situate REConnect in the era of
Generative AI. While AI can accelerate and scale certain RE tasks, its
integration must be guided by participatory practices that not only preserve
human agency but also empower humans' roles to become guardians of values and
ethics, inclusion amplifiers, curators of AI outputs, and co-reflectors in
iterative review cycles.

</details>


### [6] [Generative Goal Modeling](https://arxiv.org/abs/2509.01048)
*Ateeq Sharfuddin,Travis Breaux*

Main category: cs.SE

TL;DR: The paper demonstrates that GPT-4o can automate the extraction and modeling of stakeholder goals from interview transcripts with good accuracy, potentially reducing manual effort for business analysts in software engineering.


<details>
  <summary>Details</summary>
Motivation: Business analysts spend significant effort manually reviewing interview transcripts to extract and document stakeholder requirements. Automating this process can save time and improve consistency. Goal modeling is a key technique in requirements engineering, and leveraging AI could enhance its efficiency.

Method: The paper proposes an approach that utilizes textual entailment via GPT-4o to automatically extract goals from interview transcripts and construct goal models. The approach was evaluated on 15 transcripts across 29 domains. Accuracy metrics were used to assess goal extraction, traceability, and modeling refinement by AI compared to human performance.

Result: GPT-4o was able to extract 62% of the goals identified by humans, trace goals to their transcript origins with 98.7% accuracy, and generate goal model refinement relationships with 72.2% accuracy as judged by human annotators.

Conclusion: GPT-4o is a reliable tool for automating goal extraction and modeling from interview transcripts in requirements engineering, showing strong performance in traceability and reasonable accuracy in goal identification and refinement compared to humans.

Abstract: In software engineering, requirements may be acquired from stakeholders
through elicitation methods, such as interviews, observational studies, and
focus groups. When supporting acquisition from interviews, business analysts
must review transcripts to identify and document requirements. Goal modeling is
a popular technique for representing early stakeholder requirements as it lends
itself to various analyses, including refinement to map high-level goals into
software operations, and conflict and obstacle analysis. In this paper, we
describe an approach to use textual entailment to reliably extract goals from
interview transcripts and to construct goal models. The approach has been
evaluated on 15 interview transcripts across 29 application domains. The
findings show that GPT-4o can reliably extract goals from interview
transcripts, matching 62.0% of goals acquired by humans from the same
transcripts, and that GPT-4o can trace goals to originating text in the
transcript with 98.7% accuracy. In addition, when evaluated by human
annotators, GPT-4o generates goal model refinement relationships among
extracted goals with 72.2% accuracy.

</details>


### [7] [A Survey on the Techniques and Tools for Automated Requirements Elicitation and Analysis of Mobile Apps](https://arxiv.org/abs/2509.01068)
*Chong Wang,Haoning Wu,Peng Liang,Maya Daneva,Marten van Sinderen*

Main category: cs.SE

TL;DR: This paper systematically reviews automated requirements elicitation and analysis methods for mobile apps, finding a trend toward semi-automatic, open-sourced, and third-party tools mainly used for analysis, mining, and classification tasks.


<details>
  <summary>Details</summary>
Motivation: There is a lack of understanding regarding the characteristics of techniques and tools used in automated requirements elicitation and analysis for mobile apps, as well as the specific RE tasks they support.

Method: The authors conducted a systematic mapping study adhering to Kitchenham et al.'s guidelines, reviewing 73 selected papers in the domain.

Result: Semi-automatic techniques are most frequently used. The main tool characteristics are open-source and non-self-developed, primarily catering to requirements analysis and text processing. The top tasks investigated are requirements analysis, mining, and classification.

Conclusion: Automated techniques and tools for requirements elicitation and analysis in mobile apps are increasingly utilized, with semi-automatic methods prevailing. The most supported RE tasks are analysis, mining, and classification, relying mainly on open-sourced, non-self-developed tools for text processing and analysis.

Abstract: [Background:] Research on automated requirements elicitation and analysis of
mobile apps employed lots of techniques and tools proposed by RE researchers
and practitioners. However, little is known about the characteristics of these
techniques and tools as well as the RE tasks in requirements elicitation and
analysis that got supported with the help of respective techniques and tools.
[Aims:] The goal of this paper is to investigate the state-of-the-art of the
techniques and tools used in automated requirements elicitation and analysis of
mobile apps. [Method:] We carried out a systematic mapping study by following
the guidelines of Kitchenham et al. [Results:] Based on 73 selected papers, we
found the most frequently used techniques - semi-automatic techniques, and the
main characteristics of the tools - open-sourced and non-self-developed tools
for requirements analysis and text pre-processing. Plus, the most three
investigated RE tasks are requirements analysis, mining and classification.
[Conclusions:] Our most important conclusions are: (1) there is a growth in the
use of techniques and tools in automated requirements elicitation and analysis
of mobile apps, (2) semi-automatic techniques are mainly used in the
publications on this research topic, (3) requirements analysis, mining and
classification are the top three RE tasks with the support of automatic
techniques and tools, and (4) the most popular tools are open-sourced and
non-self-developed, and they are mainly used in requirements analysis and text
processing.

</details>


### [8] [Compiler Bugs Detection in Logic Synthesis Tools via Linear Upper Confidence Bound](https://arxiv.org/abs/2509.01149)
*Hui Zeng,Zhihao Xu,Hui Li,Siwen Wang,Qian Ma*

Main category: cs.SE

TL;DR: Lin-Hunter is a new testing framework for FPGA logic synthesis tools that generates diverse, bug-triggering test cases via clever transformation rules and adaptive strategy selection. It found more, and previously unknown, bugs than prior methods, confirming its effectiveness and coverage.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the shortcomings of current FPGA logic synthesis tool testing methods, which mainly use random selection strategies. These strategies limit the structural diversity of HDL test cases, making it difficult to thoroughly test and uncover bugs in FPGA logic synthesis tools. Given the critical importance of these tools for hardware reliability, improved methods are needed for systematic validation.

Method: The authors propose Lin-Hunter, a novel testing framework that introduces a set of metamorphic transformation rules to generate functionally equivalent but structurally diverse HDL test cases. The framework also incorporates an adaptive strategy selection mechanism based on the Linear Upper Confidence Bound (LinUCB) algorithm. This enables dynamic prioritization of transformations that are more likely to uncover bugs, using feedback from previous test results.

Result: Lin-Hunter was evaluated through comprehensive experiments over three months. It discovered 18 unique bugs in FPGA synthesis tools, including 10 previously unknown bugs confirmed by official developers. The framework outperformed existing methods in both the diversity of test cases and the efficiency of bug discovery.

Conclusion: Lin-Hunter significantly enhances both the diversity and effectiveness of test cases in FPGA logic synthesis tool validation. Its principled, adaptive approach uncovers more bugs and provides better coverage of design tool features compared to state-of-the-art methods.

Abstract: Field-Programmable Gate Arrays (FPGAs) play an indispensable role in
Electronic Design Automation (EDA), translating Register-Transfer Level (RTL)
designs into gate-level netlists. The correctness and reliability of FPGA logic
synthesis tools are critically important, as unnoticed bugs in these tools may
infect the final hardware implementations. However, recent approaches often
rely heavily on random selection strategies, limiting the structural diversity
of the generated HDL test cases and resulting in inadequate exploration of the
tool's feature space. To address this limitation, we propose Lin-Hunter, a
novel testing framework designed to systematically enhance the diversity of HDL
test cases and the efficiency of FPGA logic synthesis tool validation.
Specifically, Lin-Hunter introduces a principled set of metamorphic
transformation rules to generate functionally equivalent yet structurally
diverse HDL test case variants, effectively addressing the limited diversity of
existing test inputs. To further enhance bug discovery efficiency, Lin-Hunter
integrates an adaptive strategy selection mechanism based on the Linear Upper
Confidence Bound (LinUCB) method. This method leverages feedback from synthesis
logs of previously executed test cases to dynamically prioritize transformation
strategies that have empirically demonstrated a higher likelihood of triggering
synthesis bugs. Comprehensive experiments conducted over a three-month period
demonstrate the practical effectiveness of Lin-Hunter. Our method has
discovered 18 unique bugs, including 10 previously unreported defects, which
have been confirmed by official developers. Moreover, our method outperforms
state-of-the-art testing methods in both test-case diversity and bug-discovery
efficiency.

</details>


### [9] [Policy-driven Software Bill of Materials on GitHub: An Empirical Study](https://arxiv.org/abs/2509.01255)
*Oleksii Novikov,Davide Fucci,Oleksandr Adamov,Daniel Mendez*

Main category: cs.SE

TL;DR: SBOMs meant for security and compliance are rare in top GitHub repos; those that exist reveal many vulnerabilities and missing license details, showing room for improvement in practical SBOM use.


<details>
  <summary>Details</summary>
Motivation: Although SBOMs are mandated by governments to secure the software supply chain, their practical usage and research, particularly for security-oriented SBOMs in open-source projects, is limited. The study seeks to understand the real-world adoption and effectiveness of SBOMs for security and compliance.

Method: A mining software repository study was conducted by collecting and filtering SBOM files hosted on GitHub. The study focused on policy-driven SBOMs and analyzed their reported data, as well as the vulnerabilities associated with their dependencies, using descriptive statistics.

Result: Only 0.56% of popular open-source GitHub repositories contain policy-driven SBOMs. The dependencies listed in these SBOMs collectively have 2,202 unique vulnerabilities, and 22% lack licensing information.

Conclusion: Only a small fraction (0.56%) of popular GitHub repositories utilize policy-driven SBOMs, which are intended to improve security and transparency. The declared dependencies have thousands of unique vulnerabilities, and a significant portion lack licensing information. This highlights gaps in current SBOM adoption and its effectiveness for security and compliance.

Abstract: Background. The Software Bill of Materials (SBOM) is a machine-readable list
of all the software dependencies included in a software. SBOM emerged as way to
assist securing the software supply chain. However, despite mandates from
governments to use SBOM, research on this artifact is still in its early
stages. Aims. We want to understand the current state of SBOM in open-source
projects, focusing specifically on policy-driven SBOMs, i.e., SBOM created to
achieve security goals, such as enhancing project transparency and ensuring
compliance, rather than being used as fixtures for tools or artificially
generated for benchmarking or academic research purposes. Method. We performed
a mining software repository study to collect and carefully select SBOM files
hosted on GitHub. We analyzed the information reported in policy-driven SBOMs
and the vulnerabilities associated with the declared dependencies by means of
descriptive statistics. Results. We show that only 0.56% of popular GitHub
repositories contain policy-driven SBOM. The declared dependencies contain
2,202 unique vulnerabilities, while 22% of them do not report licensing
information. Conclusion. Our findings provide insights for SBOM usage to
support security assessment and licensing.

</details>


### [10] [Metamorphic Testing of Multimodal Human Trajectory Prediction](https://arxiv.org/abs/2509.01294)
*Helge Spieker,Nadjib Lazaar,Arnaud Gotlieb,Nassim Belmecheri*

Main category: cs.SE

TL;DR: Testing human trajectory prediction models is difficult because there's no single correct path for each scenario. This paper introduces a metamorphic testing framework that uses input transformations and probabilistic criteria to systematically evaluate these models’ robustness—without needing ground-truth answers.


<details>
  <summary>Details</summary>
Motivation: Testing human trajectory prediction (HTP) models, which are crucial for autonomous systems, is challenging because there is no single correct answer—multiple future paths can be plausible in any scenario. Current testing is limited by the lack of a clear test oracle for these stochastic, multimodal models, hindering rigorous evaluation.

Method: The paper applies Metamorphic Testing (MT) to HTP models by developing five Metamorphic Relations (MRs). These MRs target both historical trajectory data and environmental maps through geometric transformations, label changes, and obstacle introductions. Outputs are evaluated by measuring probabilistic violations using distance metrics (such as Wasserstein or Hellinger distances) between predicted trajectory distributions.

Result: The proposed MT framework successfully enables systematic, oracle-less testing of multimodal, stochastic HTP models. It assesses model robustness to various input and contextual transformations without needing ground-truth future trajectories.

Conclusion: This research demonstrates that metamorphic testing is a viable methodology for evaluating the robustness of HTP models in autonomous systems, providing a framework that overcomes the test oracle problem and facilitates thorough, reliable testing.

Abstract: Context: Predicting human trajectories is crucial for the safety and
reliability of autonomous systems, such as automated vehicles and mobile
robots. However, rigorously testing the underlying multimodal Human Trajectory
Prediction (HTP) models, which typically use multiple input sources (e.g.,
trajectory history and environment maps) and produce stochastic outputs
(multiple possible future paths), presents significant challenges. The primary
difficulty lies in the absence of a definitive test oracle, as numerous future
trajectories might be plausible for any given scenario. Objectives: This
research presents the application of Metamorphic Testing (MT) as a systematic
methodology for testing multimodal HTP systems. We address the oracle problem
through metamorphic relations (MRs) adapted for the complexities and stochastic
nature of HTP. Methods: We present five MRs, targeting transformations of both
historical trajectory data and semantic segmentation maps used as an
environmental context. These MRs encompass: 1) label-preserving geometric
transformations (mirroring, rotation, rescaling) applied to both trajectory and
map inputs, where outputs are expected to transform correspondingly. 2)
Map-altering transformations (changing semantic class labels, introducing
obstacles) with predictable changes in trajectory distributions. We propose
probabilistic violation criteria based on distance metrics between probability
distributions, such as the Wasserstein or Hellinger distance. Conclusion: This
study introduces tool, a MT framework for the oracle-less testing of
multimodal, stochastic HTP systems. It allows for assessment of model
robustness against input transformations and contextual changes without
reliance on ground-truth trajectories.

</details>


### [11] [Aligning Requirement for Large Language Model's Code Generation](https://arxiv.org/abs/2509.01313)
*Zhao Tian,Junjie Chen*

Main category: cs.SE

TL;DR: Specine is a new technique that improves how LLMs align code generation with programming specifications, solving a key problem overlooked by current methods. Experiments show it boosts baseline results by nearly 30%.


<details>
  <summary>Details</summary>
Motivation: Existing agent-based methods for LLM code generation do not adequately address the misalignment between generated code and programming specifications, which limits their effectiveness. Proper specification perception is essential for accurate code generation.

Method: The authors propose Specine, a specification alignment technique that identifies misaligned input specifications, lifts LLM-perceived specifications, and aligns them to improve code generation. They validate their approach through comprehensive experiments using four state-of-the-art LLMs across five competitive benchmarks and compare performance against ten baselines.

Result: Specine outperforms the strongest baseline by an average of 29.60% (Pass@1) across all tested subjects, demonstrating its efficacy in ensuring specification-code alignment.

Conclusion: Specine significantly improves LLM-based code generation by focusing on specification alignment, achieving a substantial performance increase over existing baselines.

Abstract: Code generation refers to the automatic generation of source code based on a
given programming specification, which has garnered significant attention
particularly with the advancement of large language models (LLMs). However, due
to the inherent complexity of real-world problems, the LLM-generated code often
fails to fully align with the provided specification. While state-of-the-art
agent-based techniques have been proposed to enhance LLM code generation, they
overlook the critical issue of specification perception, resulting in
persistent misalignment issues. Given that accurate perception of programming
specifications serves as the foundation of the LLM-based code generation
paradigm, ensuring specification alignment is particularly crucial. In this
work, we draw on software requirements engineering to propose Specine, a novel
specification alignment technique for LLM code generation. Its key idea is to
identify misaligned input specifications, lift LLM-perceived specifications,
and align them to enhance the code generation performance of LLMs. Our
comprehensive experiments on four state-of-the-art LLMs across five challenging
competitive benchmarks by comparing with ten state-of-the-art baselines,
demonstrate the effectiveness of Specine. For example, Specine outperforms the
most effective baseline, achieving an average improvement of 29.60\% across all
subjects in terms of Pass@1.

</details>


### [12] [Leveraging SystemC-TLM-based Virtual Prototypes for Embedded Software Fuzzing](https://arxiv.org/abs/2509.01318)
*Chiara Ghinami,Jonas Winzer,Nils Bosbach,Lennart M. Reimann,Lukas Jünger,Simon Wörner,Rainer Leupers*

Main category: cs.SE

TL;DR: This paper introduces a novel framework that connects AFL-based fuzzers to SystemC simulators, enabling better peripheral support and flexible, thorough fuzz-testing of embedded software. The solution allows easy integration with various simulators and improves testing capabilities for embedded systems.


<details>
  <summary>Details</summary>
Motivation: Fuzzing has proven highly effective in automated software testing, but its application in embedded software is limited due to simulators lacking hardware peripheral support and flexibility. Existing tools tightly couple fuzzers and simulators, restricting the scope and effectiveness of tests for embedded systems.

Method: The authors present a framework that integrates American-Fuzzy-Lop-based fuzzers with SystemC-based simulators via a decoupled harness. The framework intercepts peripheral accesses during simulation and queries the fuzzer for input values, enabling dynamic and flexible simulation of hardware peripheral behavior.

Result: The framework allows for flexible interchangeability of peripherals within simulation environments and supports interfacing of various SystemC-based virtual prototypes. The authors demonstrate the solution's flexibility by integrating the harness with different simulators and testing various software.

Conclusion: The proposed framework successfully addresses limitations in existing solutions, offering improved flexibility and peripheral support for fuzzing embedded software in SystemC-based simulation environments.

Abstract: SystemC-based virtual prototypes have emerged as widely adopted tools to test
software ahead of hardware availability, reducing the time-to-market and
improving software reliability. Recently, fuzzing has become a popular method
for automated software testing due to its ability to quickly identify
corner-case errors. However, its application to embedded software is still
limited. Simulator tools can help bridge this gap by providing a more powerful
and controlled execution environment for testing. Existing solutions, however,
often tightly couple fuzzers with built-in simulators that lack support for
hardware peripherals and of- fer limited flexibility, restricting their ability
to test embedded software. To address these limitations, we present a framework
that allows the integration of American-Fuzzy-Lop-based fuzzers and
SystemC-based simulators. The framework provides a harness to decouple the
adopted fuzzer and simulator. In addition, it intercepts peripheral accesses
and queries the fuzzer for values, effectively linking peripheral behavior to
the fuzzer. This solution enables flexible interchangeability of peripher- als
within the simulation environment and supports the interfacing of different
SystemC-based virtual prototypes. The flexibility of the pro- posed solution is
demonstrated by integrating the harness with different simulators and by
testing various softwares.

</details>


### [13] [Towards Multi-Platform Mutation Testing of Task-based Chatbots](https://arxiv.org/abs/2509.01389)
*Diego Clerissi,Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: MUTABOT is a mutation testing framework for injecting conversation faults in chatbots, helping to reveal weaknesses in current automated test suites on platforms like Dialogflow and Rasa.


<details>
  <summary>Details</summary>
Motivation: Thoroughly testing all possible conversational scenarios in task-based chatbots is difficult, often resulting in undetected incorrect behaviors. The increasing prevalence of chatbots demands improved testing approaches to ensure higher reliability.

Method: The authors propose MUTABOT, a mutation testing approach that injects faults into chatbot conversations, creating defective chatbots to emulate potential conversational defects. MUTABOT is extended to popular chatbot platforms (Dialogflow and Rasa), and experiments are conducted to evaluate its effectiveness against test suites generated by the Botium test generator.

Result: Experiments demonstrate that MUTABOT's mutation testing can expose weaknesses and gaps in the Botium-generated test suites, revealing scenarios where conversational defects might go unnoticed.

Conclusion: Mutation testing via MUTABOT, across multiple platforms, is an effective strategy to uncover inadequacies in automated test suites for task-based chatbots by simulating realistic conversation faults.

Abstract: Chatbots, also known as conversational agents, have become ubiquitous,
offering services for a multitude of domains. Unlike general-purpose chatbots,
task-based chatbots are software designed to prioritize the completion of tasks
of the domain they handle (e.g., flight booking). Given the growing popularity
of chatbots, testing techniques that can generate full conversations as test
cases have emerged. Still, thoroughly testing all the possible conversational
scenarios implemented by a task-based chatbot is challenging, resulting in
incorrect behaviors that may remain unnoticed. To address this challenge, we
proposed MUTABOT, a mutation testing approach for injecting faults in
conversations and producing faulty chatbots that emulate defects that may
affect the conversational aspects. In this paper, we present our extension of
MUTABOT to multiple platforms (Dialogflow and Rasa), and present experiments
that show how mutation testing can be used to reveal weaknesses in test suites
generated by the Botium state-of-the-art test generator.

</details>


### [14] [Non Technical Debt in Agile Software Development](https://arxiv.org/abs/2509.01445)
*Muhammad Ovais Ahmad,Tomas Gustavsson*

Main category: cs.SE

TL;DR: This paper analyzes the causes and impacts of NonTechnical Debt in agile software development and offers actionable strategies to improve team cohesion, leadership, processes, and HR practices, helping organizations reduce NTD and enhance agility.


<details>
  <summary>Details</summary>
Motivation: NonTechnical Debt (NTD), including process, social, people, and organizational debt, presents major challenges in agile software development. There is a lack of practical strategies and evidence-based understanding to address these issues, especially at scale.

Method: The study combined extensive surveys, in-depth interviews, and statistical analyses involving a diverse set of software professionals across multiple organizations. This collaboration allowed for comprehensive data collection and analysis of the impacts and drivers of NTD.

Result: The research identified major NTD drivers and their effects. Key findings include the importance of structured teams, psychological safety, efficient processes, social cohesion, and meeting human resource needs. Neglecting these areas reduces satisfaction, productivity, innovation, and increases costs and delays, especially in remote/hybrid settings.

Conclusion: The paper provides practical, evidence-based strategies to reduce NTD: refine team composition, clarify roles, prioritize psychological safety, streamline workflows, and treat failure as a learning opportunity. Implementing these can help organizations reclaim agility and maximize team potential.

Abstract: NonTechnical Debt (NTD) is a common challenge in agile software development,
manifesting in four critical forms, Process Debt, Social Debt, People Debt,
Organizational debt. NODLA project is a collaboration between Karlstad
University and four leading Swedish industrial partners, reveals how various
debt types disrupt large scale Agile Software Development (ASD) environments.
Through extensive surveys, indepth interviews, and statistical analyses
involving a diverse group of software professionals, we identified key drivers
of NTD and their impacts. Our findings emphasize (1) Well structured, highly
cohesive teams learn faster, adapt more effectively, and innovate consistently.
(2) Psychological safety, fostered by proactive leadership, is essential for
innovation, experimentation, and keeping employees. (3) Inefficient processes
and unclear roles contribute significantly to drops in job satisfaction,
productivity and team morale. (4) Social fragmentation, particularly in remote
and hybrid settings, breeds rework, delays, and increased costs. (5) Neglected
human resource needs, such as delayed hiring or insufficient training, limit an
organization ability to meet growing demands. This white paper distils these
insights into practical, evidence based strategies, such as refining team
composition, clarifying roles, fostering psychological safety, streamlining
workflows, and embracing failure as a learning tool. By implementing these
strategies, organizations can reduce NTD, reclaim agility, and unlock their
teams full potential.

</details>


### [15] [Benchmarking and Studying the LLM-based Code Review](https://arxiv.org/abs/2509.01494)
*Zhengran Zeng,Ruikai Shi,Keke Han,Yixin Li,Kaicheng Sun,Yidong Wang,Zhuohao Yu,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: This paper introduces SWRBench, a more realistic code review benchmark for evaluating ACR tools and LLMs. Existing systems don't perform well on it, but a multi-review aggregation approach boosts scores dramatically. The work highlights gaps in current methods and offers effective solutions to improve automated code review.


<details>
  <summary>Details</summary>
Motivation: Existing Automated Code Review (ACR) benchmarks fail to reflect the complexity of real-world scenarios, focusing on small code units without project context and using inadequate evaluation metrics. This limits the ability to properly assess modern Large Language Models (LLMs) for code review tasks.

Method: The paper introduces SWRBench, a new benchmark with 1000 manually verified GitHub Pull Requests, capturing full project context. SWRBench uses an LLM-based evaluation method closely aligned with human judgment. The study systematically evaluates current ACR tools and LLMs, and proposes a multi-review aggregation strategy to improve review performance.

Result: Current ACR systems underperform on the new benchmark; ACR tools are more effective at detecting functional errors than LLMs. Applying a multi-review aggregation strategy substantially increases performance, with F1 score improvements up to 43.67%.

Conclusion: SWRBench is a valuable new benchmark with a robust, objective evaluation methodology. It reveals gaps in current ACR and LLM capabilities and demonstrates a simple method to significantly enhance performance. This work provides new directions and insights for advancing automated code review research.

Abstract: Automated Code Review (ACR) is crucial for software quality, yet existing
benchmarks often fail to reflect real-world complexities, hindering the
evaluation of modern Large Language Models (LLMs). Current benchmarks
frequently focus on fine-grained code units, lack complete project context, and
use inadequate evaluation metrics. To address these limitations, we introduce
SWRBench , a new benchmark comprising 1000 manually verified Pull Requests
(PRs) from GitHub, offering PR-centric review with full project context.
SWRBench employs an objective LLM-based evaluation method that aligns strongly
with human judgment (~90 agreement) by verifying if issues from a structured
ground truth are covered in generated reviews. Our systematic evaluation of
mainstream ACR tools and LLMs on SWRBench reveals that current systems
underperform, and ACR tools are more adept at detecting functional errors.
Subsequently, we propose and validate a simple multi-review aggregation
strategy that significantly boosts ACR performance, increasing F1 scores by up
to 43.67%. Our contributions include the SWRBench benchmark, its objective
evaluation method, a comprehensive study of current ACR capabilities, and an
effective enhancement approach, offering valuable insights for advancing ACR
research.

</details>


### [16] [A Privacy-Preserving Recommender for Filling Web Forms Using a Local Large Language Model](https://arxiv.org/abs/2509.01527)
*Amirreza Nayyeri,Abbas Rasoolzadegan*

Main category: cs.SE

TL;DR: This paper introduces a local, privacy-preserving LLM-powered tool that aids web form testing by intelligently recommending field values, thereby enhancing testing efficiency and protecting confidential data.


<details>
  <summary>Details</summary>
Motivation: Web applications are critical in domains such as education, finance, and e-commerce, necessitating reliable and failure-free performance. Traditional web form testing involves time-consuming and error-prone manual generation of form field values. While LLM-based tools can automate this, they often depend on cloud infrastructure, creating concerns about data confidentiality and privacy when testing sensitive web forms.

Method: This paper presents a privacy-preserving recommender system powered by a Large Language Model (LLM) that operates locally rather than on the cloud. The tool analyzes web form HTML structure, detects various input types, extracts field constraints based on type and context, and then suggests effective field values for testers.

Result: The proposed system enables testers to efficiently generate suitable test case values for web forms without compromising confidential data, offering an intelligent, context-aware, and local solution to the problem of web form testing.

Conclusion: A locally operated LLM-based recommender can assist web form testing by suggesting accurate and context-relevant field values while preserving user privacy and preventing sensitive data leakage.

Abstract: Web applications are increasingly used in critical domains such as education,
finance, and e-commerce. This highlights the need to ensure their failure-free
performance. One effective method for evaluating failure-free performance is
web form testing, where defining effective test scenarios is key to a complete
and accurate evaluation. A core aspect of this process involves filling form
fields with suitable values to create effective test cases. However, manually
generating these values is time-consuming and prone to errors. To address this,
various tools have been developed to assist testers. With the appearance of
large language models (LLMs), a new generation of tools seeks to handle this
task more intelligently. Although many LLM-based tools have been introduced, as
these models typically rely on cloud infrastructure, their use in testing
confidential web forms raises concerns about unintended data leakage and
breaches of confidentiality. This paper introduces a privacy-preserving
recommender that operates locally using a large language model. The tool
assists testers in web form testing by suggesting effective field values. This
tool analyzes the HTML structure of forms, detects input types, and extracts
constraints based on each field's type and contextual content, guiding proper
field filling.

</details>


### [17] [WFC/WFD: Web Fuzzing Commons, Dataset and Guidelines to Support Experimentation in REST API Fuzzing](https://arxiv.org/abs/2509.01612)
*Omur Sahin,Man Zhang,Andrea Arcuri*

Main category: cs.SE

TL;DR: This paper introduces WFC (libraries/schemas for authentication and fault cataloging) and WFD (a REST API benchmarking dataset) to advance API fuzzing research, validated with EvoMaster and compared to major fuzzers, providing tools and guidelines to establish fair, standardized evaluations.


<details>
  <summary>Details</summary>
Motivation: Fuzzing REST APIs has critical industry relevance, but current research faces hurdles in handling authentication, fault cataloging, and conducting fair tool comparisons, hindering further progress.

Method: Development of open-source libraries (WFC) and dataset (WFD), experimental validation using EvoMaster, and comparative analysis against other state-of-the-art fuzzers.

Result: WFC and WFD facilitate more effective, standardized fuzzing experiments. Experiments with EvoMaster and comparisons to existing fuzzers highlight benefits and demonstrate guidelines to improve tool benchmarking.

Conclusion: The introduction of Web Fuzzing Commons (WFC) and Web Fuzzing Dataset (WFD) addresses key challenges in REST API fuzzing, providing tools for better authentication management, fault cataloging, and fair comparison benchmarks.

Abstract: Fuzzing REST APIs is an important research problem, with practical
applications and impact in industry. As such, a lot of research work has been
carried out on this topic in the last few years. However, there are three major
issues that hinder further progress: how to deal with API authentication; how
to catalog and compare different fault types found by different fuzzers; and
what to use as case study to facilitate fair comparisons among fuzzers. To
address these important challenges, we present Web Fuzzing Commons (WFC) and
Web Fuzzing Dataset (WFD). WFC is a set of open-source libraries and schema
definitions to declaratively specify authentication info and catalog different
types of faults that fuzzers can automatically detect. WFD is a collection of
36 open-source APIs with all necessary scaffolding to easily run experiments
with fuzzers, supported by WFC. To show the usefulness of WFC/WFD, a set of
experiments is carried out with EvoMaster, a state-of-the-art fuzzer for Web
APIs. However, any fuzzer can benefit from WFC and WFD. We compare EvoMaster
with other state-of-the-art tools such as ARAT-RL, EmRest, LLamaRestTest,
RESTler, and Schemathesis. We discuss common pitfalls in tool comparisons, as
well as providing guidelines with support of WFC/WFD to avoid them.

</details>


### [18] [Automated Generation of Issue-Reproducing Tests by Combining LLMs and Search-Based Testing](https://arxiv.org/abs/2509.01616)
*Konstantinos Kitsios,Marco Castelluccio,Alberto Bacchelli*

Main category: cs.SE

TL;DR: BLAST effectively combines LLMs and SBST to automatically generate tests that reproduce reported issues from patches, substantially outperforming previous approaches and showing promise in live open-source projects.


<details>
  <summary>Details</summary>
Motivation: Many developers commit patches without accompanying tests that reproduce the issue, risking unresolved or recurring bugs. Automating the generation of such tests improves reliability and confidence in bug resolution.

Method: Introduces BLAST, which uses large language models (LLMs) and search-based software testing (SBST) to automatically generate issue-reproducing tests from issue-patch pairs. LLMs extract relevant context and seed SBST generation, with SBST optimized using this intermediate seed.

Result: BLAST generated issue-reproducing tests for 35.4% of issues in a Python benchmark, outperforming the previous state-of-the-art (23.5%). Real-world deployment in three open-source repos saw BLAST propose tests for 11 out of 32 PRs-issue pairs, leading to developer interaction and feedback.

Conclusion: BLAST advances automated issue-reproducing test generation, outperforming existing tools and gaining traction in real development scenarios. Developer feedback highlights both opportunities for tool improvement and broader adoption.

Abstract: Issue-reproducing tests fail on buggy code and pass once a patch is applied,
thus increasing developers' confidence that the issue has been resolved and
will not be re-introduced. However, past research has shown that developers
often commit patches without such tests, making the automated generation of
issue-reproducing tests an area of interest. We propose BLAST, a tool for
automatically generating issue-reproducing tests from issue-patch pairs by
combining LLMs and search-based software testing (SBST). For the LLM part, we
complement the issue description and the patch by extracting relevant context
through git history analysis, static analysis, and SBST-generated tests. For
the SBST part, we adapt SBST for generating issue-reproducing tests; the issue
description and the patch are fed into the SBST optimization through an
intermediate LLM-generated seed, which we deserialize into SBST-compatible
form. BLAST successfully generates issue-reproducing tests for 151/426 (35.4%)
of the issues from a curated Python benchmark, outperforming the
state-of-the-art (23.5%). Additionally, to measure the real-world impact of
BLAST, we built a GitHub bot that runs BLAST whenever a new pull request (PR)
linked to an issue is opened, and if BLAST generates an issue-reproducing test,
the bot proposes it as a comment in the PR. We deployed the bot in three
open-source repositories for three months, gathering data from 32 PRs-issue
pairs. BLAST generated an issue-reproducing test in 11 of these cases, which we
proposed to the developers. By analyzing the developers' feedback, we discuss
challenges and opportunities for researchers and tool builders. Data and
material: https://doi.org/10.5281/zenodo.16949042

</details>


### [19] [Tether: A Personalized Support Assistant for Software Engineers with ADHD](https://arxiv.org/abs/2509.01946)
*Aarsh Shah,Cleyton Magalhaes,Kiev Gama,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: The paper introduces Tether, an LLM-powered desktop app to help developers with ADHD focus and manage workflow challenges. The tool combines activity monitoring, RAG, and gamification, and is more aligned to developers' needs than generic solutions. Initial self-evaluation showed improvement, but user studies remain outstanding.


<details>
  <summary>Details</summary>
Motivation: Existing software engineering equity, diversity, and inclusion efforts often neglect neurodiversity, especially ADHD, and few tools are tailored to address the specific cognitive challenges faced by this community.

Method: The authors used engineering research methodology, including iterative prompt refinement, retrieval-augmented generation (RAG), and self-use validation. The system incorporates local activity monitoring, OS-level tracking, and ADHD-specific resources.

Result: Preliminary self-use validation showed improved contextual accuracy after refining prompts and RAG, helping support focus and dialogue needs. Tether proved more adaptable and relevant than generic tools, but further evaluation with target users is still needed.

Conclusion: Tether provides a foundation for developing neurodiversity-aware tools in software engineering, demonstrating the potential for LLMs to deliver personalized support for cognitive challenges like those experienced by developers with ADHD.

Abstract: Equity, diversity, and inclusion in software engineering often overlook
neurodiversity, particularly the experiences of developers with Attention
Deficit Hyperactivity Disorder (ADHD). Despite the growing awareness about that
population in SE, few tools are designed to support their cognitive challenges
(e.g., sustained attention, task initiation, self-regulation) within
development workflows. We present Tether, an LLM-powered desktop application
designed to support software engineers with ADHD by delivering adaptive,
context-aware assistance. Drawing from engineering research methodology, Tether
combines local activity monitoring, retrieval-augmented generation (RAG), and
gamification to offer real-time focus support and personalized dialogue. The
system integrates operating system level system tracking to prompt engagement
and its chatbot leverages ADHD-specific resources to offer relevant responses.
Preliminary validation through self-use revealed improved contextual accuracy
following iterative prompt refinements and RAG enhancements. Tether
differentiates itself from generic tools by being adaptable and aligned with
software-specific workflows and ADHD-related challenges. While not yet
evaluated by target users, this work lays the foundation for future
neurodiversity-aware tools in SE and highlights the potential of LLMs as
personalized support systems for underrepresented cognitive needs.

</details>


### [20] [Automated Repair of C Programs Using Large Language Models](https://arxiv.org/abs/2509.01947)
*Mahdi Farzandway,Fatemeh Ghassemi*

Main category: cs.SE

TL;DR: This paper introduces an improved autonomous repair framework for C programs, integrating statistical analysis and LLM reasoning. Using an iterative CoT-structured repair loop, it achieves better bug-fixing accuracy than leading baselines, demonstrating the value of combining program analysis with generative AI.


<details>
  <summary>Details</summary>
Motivation: Automating the repair of C programs is challenging, and current approaches using Large Language Models (LLMs) have limitations. The authors aim to improve automated program repair (APR) by explicitly integrating statistical program analysis with LLM reasoning.

Method: The paper presents a framework that combines spectrum-based fault localization (SBFL), runtime feedback, and structured Chain-of-Thought (CoT) prompting in an autonomous repair loop. The LLM reasons over failing tests, suspicious code regions, and previous patch outcomes, generating new candidate patches iteratively and learning from previous attempts.

Result: On the Codeflaws benchmark of 3,902 bugs, the proposed method achieves 44.93% repair accuracy, which is a 3.61% absolute improvement over strong state-of-the-art APR baselines (such as GPT-4 with CoT).

Conclusion: Integrating statistical program analysis with LLM-based reasoning significantly improves automated repair of C programs, pointing to a practical direction for AI-driven debugging.

Abstract: This study explores the potential of Large Language Models (LLMs) in
automating the repair of C programs. We present a framework that integrates
spectrum-based fault localization (SBFL), runtime feedback, and
Chain-of-Thought-structured prompting into an autonomous repair loop. Unlike
prior approaches, our method explicitly combines statistical program analysis
with LLM reasoning. The iterative repair cycle leverages a structured
Chain-of-Thought (CoT) prompting approach, where the model reasons over failing
tests, suspicious code regions, and prior patch outcomes, before generating new
candidate patches. The model iteratively changes the code, evaluates the
results, and incorporates reasoning from previous attempts into subsequent
modifications, reducing repeated errors and clarifying why some bugs remain
unresolved. Our evaluation spans 3,902 bugs from the Codeflaws benchmark, where
our approach achieves 44.93% repair accuracy, representing a 3.61% absolute
improvement over strong state-of-the-art APR baselines such as GPT-4 with CoT.
This outcome highlights a practical pathway toward integrating statistical
program analysis with generative AI in automated debugging.

</details>


### [21] [ProbTest: Unit Testing for Probabilistic Programs (Extended Version)](https://arxiv.org/abs/2509.02012)
*Katrine Christensen,Mahsa Varshosaz,Raúl Pardo*

Main category: cs.SE

TL;DR: ProbTest is a PyTest plug-in that automatically determines and manages the number of test runs needed for probabilistic program testing, ensuring statistically sound results without extra developer work.


<details>
  <summary>Details</summary>
Motivation: Testing probabilistic programs is challenging because their outcomes can vary due to random choices, unlike deterministic programs. Developers struggle to determine how many test executions are needed to effectively verify expected behaviors, making traditional single-execution testing insufficient.

Method: The paper introduces ProbTest, a black-box unit testing method based on the coupon collector's problem, to statistically guarantee the coverage of possible outcomes. ProbTest automatically determines how many times a probabilistic program needs to be executed for effective testing, minimizing required manual intervention. ProbTest is implemented as a plug-in for PyTest, allowing seamless integration with Python unit testing workflows.

Result: ProbTest automates the execution of probabilistic tests and provides statistical guarantees for outcome coverage. Case studies using the Gymnasium reinforcement learning library and a randomized data structure demonstrate the method's effectiveness. The approach enables developers to write unit tests as usual while transparently handling the required number of executions.

Conclusion: ProbTest successfully enables robust, automated testing of probabilistic programs by leveraging the coupon collector's problem to determine the number of necessary test executions. This reduces developer effort and ensures statistical reliability in test outcomes.

Abstract: Testing probabilistic programs is non-trivial due to their stochastic nature.
Given an input, the program may produce different outcomes depending on the
underlying stochastic choices in the program. This means testing the expected
outcomes of probabilistic programs requires repeated test executions unlike
deterministic programs where a single execution may suffice for each test
input. This raises the following question: how many times should we run a
probabilistic program to effectively test it? This work proposes a novel
black-box unit testing method, ProbTest, for testing the outcomes of
probabilistic programs. Our method is founded on the theory surrounding a
well-known combinatorial problem, the coupon collector's problem. Using this
method, developers can write unit tests as usual without extra effort while the
number of required test executions is determined automatically with statistical
guarantees for the results. We implement ProbTest as a plug-in for PyTest, a
well-known unit testing tool for python programs. Using this plug-in,
developers can write unit tests similar to any other Python program and the
necessary test executions are handled automatically. We evaluate the method on
case studies from the Gymnasium reinforcement learning library and a randomized
data structure.

</details>


### [22] [Scalable Thread-Safety Analysis of Java Classes with CodeQL](https://arxiv.org/abs/2509.02022)
*Bjørnar Haugstad Jåtten,Simon Boye Jørgensen,Rasmus Petersen,Raúl Pardo*

Main category: cs.SE

TL;DR: The paper introduces a scalable CodeQL-based analyzer for thread-safety in Java classes, accurately identifying thousands of concurrency issues across major open-source projects, with fast runtimes and adoption by developers.


<details>
  <summary>Details</summary>
Motivation: Determining thread-safety in Java classes is challenging for software developers, yet increasingly necessary for building robust concurrent applications. Existing methods or manual analysis do not scale well to large codebases, creating a need for an automated, highly scalable approach.

Method: The authors define thread-safety for Java classes based on the correctness principle of the Java memory model, specifically data race freedom. They devise a set of properties proven to ensure thread-safety and encode these properties in CodeQL, a static analysis tool, for automated source code analysis.

Result: Applied to the top 1000 GitHub repositories, analyzing over 3.6 million classes (including nearly 2,000 already annotated as @ThreadSafe), the method detected thousands of thread-safety errors. The queries ran efficiently (under 2 minutes for large repos) and several detected errors were validated and accepted by project maintainers. The CodeQL queries are becoming part of mainstream tooling.

Conclusion: The proposed method and CodeQL-based tool can effectively and efficiently analyze thread-safety in large-scale, real-world Java codebases, demonstrated by the successful identification of thousands of concurrency issues and positive developer feedback.

Abstract: In object-oriented languages software developers rely on thread-safe classes
to implement concurrent applications. However, determining whether a class is
thread-safe is a challenging task. This paper presents a highly scalable method
to analyze thread-safety in Java classes. We provide a definition of
thread-safety for Java classes founded on the correctness principle of the Java
memory model, data race freedom. We devise a set of properties for Java classes
that are proven to ensure thread-safety. We encode these properties in the
static analysis tool CodeQL to automatically analyze Java source code. We
perform an evaluation on the top 1000 GitHub repositories. The evaluation
comprises 3632865 Java classes; with 1992 classes annotated as @ThreadSafe from
71 repositories. These repositories include highly popular software such as
Apache Flink (24.6k stars), Facebook Fresco (17.1k stars), PrestoDB (16.2k
starts), and gRPC (11.6k starts). Our queries detected thousands of
thread-safety errors. The running time of our queries is below 2 minutes for
repositories up to 200k lines of code, 20k methods, 6000 fields, and 1200
classes. We have submitted a selection of detected concurrency errors as PRs,
and developers positively reacted to these PRs. We have submitted our CodeQL
queries to the main CodeQL repository, and they are currently in the process of
becoming available as part of GitHub actions. The results demonstrate the
applicability and scalability of our method to analyze thread-safety in
real-world code bases.

</details>


### [23] [Curiosity-Driven Testing for Sequential Decision-Making Process](https://arxiv.org/abs/2509.02025)
*Junda He,Zhou Yang,Jieke Shi,Chengran Yang,Kisub Kim,Bowen Xu,Xin Zhou,David Lo*

Main category: cs.SE

TL;DR: CureFuzz is a curiosity-driven fuzz testing approach for sequential decision-making models that significantly improves the detection of diverse and novel crash-triggering scenarios, outperforming existing tools and enabling system repairs for safer applications.


<details>
  <summary>Details</summary>
Motivation: Sequential decision-making models are increasingly applied in safety-critical domains, such as autonomous driving and robotics, where unsafe behavior can have severe consequences. Despite advancements in deep learning, current models are still prone to unsafe decisions, and effective testing frameworks to uncover diverse failure modes are lacking.

Method: The authors introduce CureFuzz, a curiosity-driven black-box fuzz testing framework specifically designed for Sequential Decision-Making Models (SDMs). CureFuzz incorporates a curiosity mechanism to guide the exploration toward novel scenarios and employs a multi-objective seed selection strategy to balance finding new scenarios and triggering crashes efficiently.

Result: Experimental results show that CureFuzz surpasses state-of-the-art fuzzing methods both in the total number of faults found and the diversity of crash-triggering scenarios identified. Additionally, the scenarios discovered by CureFuzz allow for the repair and improvement of SDMs, demonstrating practical utility.

Conclusion: CureFuzz is an effective and innovative testing tool for SDMs, capable of uncovering more and varied crash scenarios than previous methods. Its findings can directly contribute to the safety and reliability improvements of SDMs in critical applications.

Abstract: Sequential decision-making processes (SDPs) are fundamental for complex
real-world challenges, such as autonomous driving, robotic control, and traffic
management. While recent advances in Deep Learning (DL) have led to mature
solutions for solving these complex problems, SDMs remain vulnerable to
learning unsafe behaviors, posing significant risks in safety-critical
applications. However, developing a testing framework for SDMs that can
identify a diverse set of crash-triggering scenarios remains an open challenge.
To address this, we propose CureFuzz, a novel curiosity-driven black-box fuzz
testing approach for SDMs. CureFuzz proposes a curiosity mechanism that allows
a fuzzer to effectively explore novel and diverse scenarios, leading to
improved detection of crashtriggering scenarios. Additionally, we introduce a
multi-objective seed selection technique to balance the exploration of novel
scenarios and the generation of crash-triggering scenarios, thereby optimizing
the fuzzing process. We evaluate CureFuzz on various SDMs and experimental
results demonstrate that CureFuzz outperforms the state-of-the-art method by a
substantial margin in the total number of faults and distinct types of
crash-triggering scenarios. We also demonstrate that the crash-triggering
scenarios found by CureFuzz can repair SDMs, highlighting CureFuzz as a
valuable tool for testing SDMs and optimizing their performance.

</details>


### [24] [Txt2Sce: Scenario Generation for Autonomous Driving System Testing Based on Textual Reports](https://arxiv.org/abs/2509.02150)
*Pin Ji,Yang Feng,Zongtai Li,Xiangchi Zhou,Jia Liu,Jun Sun,Zhihong Zhao*

Main category: cs.SE

TL;DR: Txt2Sce uses a LLM to automatically turn textual accident reports into standardized test scenarios in OpenSCENARIO format, making scenario-based ADS testing more scalable and effective by increasing diversity and uncovering system defects.


<details>
  <summary>Details</summary>
Motivation: Autonomous Driving Systems (ADSs) face challenges in complex scenarios, as highlighted by real-world accident reports. Existing scenario-based testing methods are limited by reliance on visual data (which requires heavy annotation and storage) and lack of standardized scenario formats, making the results platform-specific and not easily portable.

Method: The authors propose a method called Txt2Sce, which employs a Large Language Model (LLM) to automatically convert textual accident reports into test scenarios in the OpenSCENARIO format. The method includes scenario disassembly, mutation, and re-assembly to generate a derivation-based scenario file tree, enhancing diversity and traceability of scenario variants.

Result: Txt2Sce generated 33 scenario tree files, totaling 4,373 scenario files, which were used to test the open-source ADS, Autoware. The tool effectively created valid OpenSCENARIO files, increased scenario diversity, and successfully revealed unexpected behaviors in Autoware regarding safety, smartness, and smoothness.

Conclusion: Txt2Sce offers an efficient and scalable approach to generate diverse, standardized test scenarios for ADSs from textual accident reports, improving testing effectiveness and portability across different platforms.

Abstract: With the rapid advancement of deep learning and related technologies,
Autonomous Driving Systems (ADSs) have made significant progress and are
gradually being widely applied in safety-critical fields. However, numerous
accident reports show that ADSs still encounter challenges in complex
scenarios. As a result, scenario-based testing has become essential for
identifying defects and ensuring reliable performance. In particular,
real-world accident reports offer valuable high-risk scenarios for more
targeted ADS testing. Despite their potential, existing methods often rely on
visual data, which demands large memory and manual annotation. Additionally,
since existing methods do not adopt standardized scenario formats (e.g.,
OpenSCENARIO), the generated scenarios are often tied to specific platforms and
ADS implementations, limiting their scalability and portability. To address
these challenges, we propose Txt2Sce, a method for generating test scenarios in
OpenSCENARIO format based on textual accident reports. Txt2Sce first uses a LLM
to convert textual accident reports into corresponding OpenSCENARIO scenario
files. It then generates a derivation-based scenario file tree through scenario
disassembly, scenario block mutation, and scenario assembly. By utilizing the
derivation relationships between nodes in the scenario tree, Txt2Sce helps
developers identify the scenario conditions that trigger unexpected behaviors
of ADSs. In the experiments, we employ Txt2Sce to generate 33 scenario file
trees, resulting in a total of 4,373 scenario files for testing the open-source
ADS, Autoware. The experimental results show that Txt2Sce successfully converts
textual reports into valid OpenSCENARIO files, enhances scenario diversity
through mutation, and effectively detects unexpected behaviors of Autoware in
terms of safety, smartness, and smoothness.

</details>


### [25] [Formalizing Operational Design Domains with the Pkl Language](https://arxiv.org/abs/2509.02221)
*Martin Skoglund,Fredrik Warg,Anders Thorsén,Sasikumar Punnekkat,Hans Hansson*

Main category: cs.SE

TL;DR: This paper proposes a novel way to formalize operational design domains for automated functions using the Pkl language, addressing usability and traceability challenges, and demonstrates its effectiveness in an automotive scenario.


<details>
  <summary>Details</summary>
Motivation: With increasing automation and reduced human oversight, traditional safety evaluation methods are inadequate. There is a need for a robust, flexible, and traceable framework to specify and assess automated functions' contextual operation to mitigate risks.

Method: The paper introduces a formalization framework for ODDs using the Pkl configuration language, addressing integration, specification, and validation challenges. An automotive scenario demonstrates its practical application.

Result: The method provides a consistent yet flexible way to specify ODDs, simplifies configuration, and improves the support for safety justification and assessment. Its broad application potential is highlighted via an automotive use case.

Conclusion: The proposed approach effectively formalizes Operational Design Domain (ODD) specifications in the Pkl language, improving consistency, traceability, and usability while supporting diverse formats.

Abstract: The deployment of automated functions that can operate without direct human
supervision has changed safety evaluation in domains seeking higher levels of
automation. Unlike conventional systems that rely on human operators, these
functions require new assessment frameworks to demonstrate that they do not
introduce unacceptable risks under real-world conditions. To make a convincing
safety claim, the developer must present a thorough justification argument,
supported by evidence, that a function is free from unreasonable risk when
operated in its intended context. The key concept relevant to the presented
work is the intended context, often captured by an Operational Design Domain
specification (ODD). ODD formalization is challenging due to the need to
maintain flexibility in adopting diverse specification formats while preserving
consistency and traceability and integrating seamlessly into the development,
validation, and assessment. This paper presents a way to formalize an ODD in
the Pkl language, addressing central challenges in specifying ODDs while
improving usability through specialized configuration language features. The
approach is illustrated with an automotive example but can be broadly applied
to ensure rigorous assessments of operational contexts.

</details>


### [26] [Methodology for Test Case Allocation based on a Formalized ODD](https://arxiv.org/abs/2509.02311)
*Martin Skoglund,Fredrik Warg,Anders Thoren,Sasikumar Punnekkat,Hans Hansson*

Main category: cs.SE

TL;DR: The paper presents a framework for automatically aligning test cases with appropriate test environments for CCAM systems by extending ODDs with testing attributes, demonstrated via a reversing truck case study.


<details>
  <summary>Details</summary>
Motivation: The introduction of Connected, Cooperative, and Automated Mobility (CCAM) systems changes how safety is assessed, as these involve automated functions beyond human driving. Traditional evaluation methods are inadequate for such complex systems. There is a need to properly align and allocate test cases across varied testing environments considering their specific capabilities.

Method: The paper proposes a method that extends an existing Operational Design Domain (ODD) formalization with specific testing attributes. This construct integrates ODD parameters and additional test attributes to represent the relevant capabilities of different test environments. The approach supports automated suitability evaluation and is showcased using a case study on an automated reversing truck function.

Result: The proposed approach enables automatic evaluation of how suitable specific test environments are for particular test case allocations. It demonstrates, via a case study, that test case allocation can be automated based on each environment’s capability (e.g., object-detection sensor assessment).

Conclusion: Extending ODDs with key test attributes creates a robust method for test case allocation in the safety assessment of CCAM systems, facilitating automatic suitability evaluation based on the strengths of available test environments.

Abstract: The emergence of Connected, Cooperative, and Automated Mobility (CCAM)
systems has significantly transformed the safety assessment landscape. Because
they integrate automated vehicle functions beyond those managed by a human
driver, new methods are required to evaluate their safety. Approaches that
compile evidence from multiple test environments have been proposed for
type-approval and similar evaluations, emphasizing scenario coverage within the
systems Operational Design Domain (ODD). However, aligning diverse test
environment requirements with distinct testing capabilities remains
challenging. This paper presents a method for evaluating the suitability of
test case allocation to various test environments by drawing on and extending
an existing ODD formalization with key testing attributes. The resulting
construct integrates ODD parameters and additional test attributes to capture a
given test environments relevant capabilities. This approach supports automatic
suitability evaluation and is demonstrated through a case study on an automated
reversing truck function. The system's implementation fidelity is tied to ODD
parameters, facilitating automated test case allocation based on each
environments capacity for object-detection sensor assessment.

</details>


### [27] [ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation](https://arxiv.org/abs/2509.02330)
*Yicong Zhao,Shisong Chen,Jiacheng Zhang,Zhixu Li*

Main category: cs.SE

TL;DR: This paper presents ReCode—a retrieval-augmented code repair framework addressing inefficiencies in current LLM approaches by leveraging algorithm-aware and modular retrieval strategies. With a new, realistic code repair benchmark (RACodeBench), ReCode shows superior accuracy and efficiency, making it well-suited for practical code repair tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based code repair methods require high training and computational resources, and retrieval strategies often fail to capture code's structural details, limiting repair accuracy.

Method: The paper introduces ReCode, a framework that uses fine-grained retrieval-augmented in-context learning for code repair. It employs an algorithm-aware retrieval strategy and a modular dual-encoder architecture for separate semantic processing of code and text. Additionally, the study proposes RACodeBench, a new benchmark sourced from real-world buggy code.

Result: ReCode achieves higher code repair accuracy and reduces inference cost compared to existing methods, as demonstrated by experiments on RACodeBench and competitive programming datasets.

Conclusion: ReCode presents an effective and efficient solution for code repair applying fine-grained retrieval mechanisms, and its success on realistic benchmarks underscores its practical utility.

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in code-related tasks, such as code generation and automated
program repair. Despite their promising performance, most existing approaches
for code repair suffer from high training costs or computationally expensive
inference. Retrieval-augmented generation (RAG), with its efficient in-context
learning paradigm, offers a more scalable alternative. However, conventional
retrieval strategies, which are often based on holistic code-text embeddings,
fail to capture the structural intricacies of code, resulting in suboptimal
retrieval quality. To address the above limitations, we propose ReCode, a
fine-grained retrieval-augmented in-context learning framework designed for
accurate and efficient code repair. Specifically, ReCode introduces two key
innovations: (1) an algorithm-aware retrieval strategy that narrows the search
space using preliminary algorithm type predictions; and (2) a modular
dual-encoder architecture that separately processes code and textual inputs,
enabling fine-grained semantic matching between input and retrieved contexts.
Furthermore, we propose RACodeBench, a new benchmark constructed from
real-world user-submitted buggy code, which addresses the limitations of
synthetic benchmarks and supports realistic evaluation. Experimental results on
RACodeBench and competitive programming datasets demonstrate that ReCode
achieves higher repair accuracy with significantly reduced inference cost,
highlighting its practical value for real-world code repair scenarios.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [28] [ChopChop: a Programmable Framework for Semantically Constraining the Output of Language Models](https://arxiv.org/abs/2509.00360)
*Shaan Nagy,Timothy Zhou,Nadia Polikarpova,Loris D'Antoni*

Main category: cs.PL

TL;DR: ChopChop is a framework that lets language models generate code guaranteed to meet deep semantic requirements (like type safety), bridging formal methods with modern code generation without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Language models can generate code but often produce code violating crucial properties like type safety and invariants. Existing solutions for controlling generation are limited and not robust for semantic constraints, motivating a principled and programmable approach.

Method: The paper introduces ChopChop, a programmable framework for semantic constrained decoding in language models. It connects token-level generation to program structure reasoning using a coinduction-based formalism, and reduces semantic constraint enforcement to a realizability problem over regular codata.

Result: ChopChop enables language models to provably satisfy rich semantic properties such as type safety and program equivalence during code generation. It works systematically across different models and tasks and maintains practical decoding latency.

Conclusion: ChopChop systematically extends language models to generate code with provable semantic properties, transforming semantic constrained decoding into a practical tool that improves the reliability and practical applicability of code generation.

Abstract: Language models (LMs) can generate code, but cannot guarantee its
correctness--producing outputs that often violate type safety, program
invariants, or semantic equivalence. Constrained decoding offers a solution by
restricting generation to programs that satisfy desired properties. Yet,
existing methods are limited to shallow syntactic constraints or rely on
brittle, ad hoc encodings of semantics over token sequences.
  We present ChopChop, the first programmable framework for semantic
constrained decoding, enabling LMs to generate code that provably satisfies
rich semantic properties. ChopChop connects token-level generation with
reasoning over abstract program structures using a coinduction-based formalism
and reduces constraint enforcement to a realizability problem over regular
codata. We demonstrate ChopChop's generality through generation constrained by
type safety and program equivalence, showing how formal methods can be
seamlessly integrated into LM-driven code generation. ChopChop transforms
semantic constrained decoding from a niche technique into a systematic,
principled extension of LMs--improving success rates across models and tasks
while maintaining practical decoding latency.

</details>


### [29] [A Hoare Logic for Symmetry Properties](https://arxiv.org/abs/2509.00587)
*Vaibhav Mehta,Justin Hsu*

Main category: cs.PL

TL;DR: This paper introduces a logic and tool for verifying program symmetries via group actions, showing it can find real errors missed by previous approaches.


<details>
  <summary>Details</summary>
Motivation: Many program correctness properties are expressed as symmetries, but formal methods currently lack effective support for reasoning about symmetries.

Method: The authors design a syntax for specifying symmetry properties using group actions, create a Hoare-style logic replacing assertions with group actions, and implement a prototype tool called SymVerif to verify these properties.

Result: The SymVerif tool successfully verifies symmetry properties on various benchmarks and discovers an error in a known model from prior literature.

Conclusion: It is feasible to formally verify symmetry properties of programs using group action-based logic, and the proposed tool demonstrates practical utility.

Abstract: Many natural program correctness properties can be stated in terms of
  symmetries, but existing formal methods have little support for reasoning
  about such properties. We consider how to formally verify a broad class of
  symmetry properties expressed in terms of group actions. To specify these
  properties, we design a syntax for group actions, supporting standard
  constructions and a natural notion of entailment. Then, we develop a
  Hoare-style logic for verifying symmetry properties of imperative programs,
  where group actions take the place of the typical pre- and post-condition
  assertions. Finally, we develop a prototype tool $\mathsf{SymVerif}$, and use
  it to verify symmetry properties on a series of handcrafted benchmarks. Our
  tool uncovered an error in a model of a dynamical system described by
\citet{McLachlan_Quispel_2002}.

</details>


### [30] [Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools](https://arxiv.org/abs/2509.00699)
*Yumeng He,Chandrakana Nandi,Sreepathi Pai*

Main category: cs.PL

TL;DR: This paper introduces an algorithm and tool (GlitchFinder) that adapts techniques from compiler validation to 3D printing pipelines, allowing detection of slicing and mesh repair issues, and comparison of slicer outputs.


<details>
  <summary>Details</summary>
Motivation: Despite the similarities between 3D printing fabrication pipelines and programming language compilers, the former lacks established techniques like invariant checking and differential testing, which can improve reliability. The unique nature of 3D models and their representation as both geometric objects and machine code brings challenges to directly applying traditional compiler verification techniques.

Method: The authors propose a novel algorithm that lifts G-code (the machine code for 3D printing) into a set of cuboids and further into an approximate point cloud representation. This facilitates efficient operations and analysis, enabling various validation and comparison tasks in the fabrication pipeline. They implement this approach in a tool called GlitchFinder and evaluate its performance on 58 real-world CAD models.

Result: GlitchFinder effectively identifies slicing issues, particularly those caused by small features. It also highlights the impact of different slicers (Cura and PrusaSlicer) on the same model and detects errors introduced by mesh repair tools (MeshLab and Meshmixer).

Conclusion: The proposed algorithm and GlitchFinder tool successfully bring advanced invariant checking and validation techniques from traditional compiling into the 3D printing domain, enhancing error localization and pipeline reliability.

Abstract: The computational fabrication pipeline for 3D printing is much like a
compiler - users design models in Computer Aided Design (CAD) tools that are
lowered to polygon meshes to be ultimately compiled to machine code by 3D
slicers. For traditional compilers and programming languages, techniques for
checking program invariants are well-established. Similarly, methods like
differential testing are often used to uncover bugs in compilers themselves,
which makes them more reliable. The fabrication pipeline would benefit from
similar techniques but traditional approaches do not directly apply to the
representations used in this domain. Unlike traditional programs, 3D models
exist both as geometric objects as well as machine code that ultimately runs on
the hardware. The machine code, like in traditional compiling, is affected by
many factors like the model, the slicer being used, and numerous
user-configurable parameters that control the slicing process. In this work, we
propose a new algorithm for lifting G-code (a common language used in
fabrication pipelines) by denoting a G-code program to a set of cuboids, and
then defining an approximate point cloud representation for efficiently
operating on these cuboids. Our algorithm opens up new opportunities: we show
three use cases that demonstrate how it enables error localization in CAD
models through invariant checking, quantitative comparisons between slicers,
and evaluating the efficacy of mesh repair tools. We present a prototype
implementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58
real-world CAD models. Our results show that GlitchFinder is particularly
effective in identifying slicing issues due to small features, can highlight
differences in how popular slicers (Cura and PrusaSlicer) slice the same model,
and can identify cases where mesh repair tools (MeshLab and Meshmixer)
introduce new errors during repair.

</details>


### [31] [Decision Procedure for A Theory of String Sequences](https://arxiv.org/abs/2509.00948)
*Denghang Hu,Taolue Chen,Philipp Rümmer,Fu Song,Zhilin Wu*

Main category: cs.PL

TL;DR: A new theory for string sequences enables SMT solvers to support direct string operations (e.g., regex match, split, join) in program analysis. While full satisfiability is undecidable, the straight-line fragment is decidable when encoded into the existing OSTRICH framework. The proposed tool is effective on real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: Many SMT solvers support sequences to model bounded arrays and lists, but lack direct support for critical string-related operations (like regex matching, splitting, joining) that are widely used in practical string-manipulating programs.

Method: The paper proposes a theory of string sequences and analyzes its satisfiability. It presents an encoding scheme that transforms string sequence operations into equivalent string operations. Decidability is regained for the straight-line fragment through this encoding. Furthermore, pre-image computation with respect to automata enables leveraging the OSTRICH string constraint solving framework. The authors implement their decision procedure as a tool, $	ext{ostrichseq}$, and evaluate it on constraints generated from JS programs and benchmarks.

Result: The proposed theory is undecidable in the general case, but decidable for the straight-line fragment. The encoding and integration into the OSTRICH framework work effectively. Experimental results show that the new tool is effective on various benchmarks from real-world applications.

Conclusion: Direct string sequence operations—previously unavailable in SMT solvers—can be supported via the new decision procedure. This expands the power of solvers for string-manipulating program analysis, especially for straight-line code fragments. The provided tool demonstrates practical efficacy.

Abstract: The theory of sequences, supported by many SMT solvers, can model program
data types including bounded arrays and lists. Sequences are parameterized by
the element data type and provide operations such as accessing elements,
concatenation, forming sub-sequences and updating elements. Strings and
sequences are intimately related; many operations, e.g., matching a string
according to a regular expression, splitting strings, or joining strings in a
sequence, are frequently used in string-manipulating programs. Nevertheless,
these operations are typically not directly supported by existing SMT solvers,
which instead only consider the generic theory of sequences. In this paper, we
propose a theory of string sequences and study its satisfiability. We show
that, while it is undecidable in general, the decidability can be recovered by
restricting to the straight-line fragment. This is shown by encoding each
string sequence as a string, and each string sequence operation as a
corresponding string operation. We provide pre-image computation for the
resulting string operations with respect to automata, effectively casting it
into the generic OSTRICH string constraint solving framework. We implement the
new decision procedure as a tool $\ostrichseq$, and carry out experiments on
benchmark constraints generated from real-world JavaScript programs,
hand-crafted templates and unit tests. The experiments confirm the efficacy of
our approach.

</details>


### [32] [Type-Based Incorrectness Reasoning](https://arxiv.org/abs/2509.01511)
*Zhe Zhou,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: This paper explores how coverage types extend refinement types with must-style underapproximate reasoning, highlighting their connection to Incorrectness Logic. It proposes systematic integration of these frameworks, boosting verification and analysis in functional languages, especially for property-based testing.


<details>
  <summary>Details</summary>
Motivation: To enable must-style underapproximate reasoning in functional languages, which is particularly valuable for property-based testing frameworks because it verifies both safety and completeness of test generators.

Method: Exploring coverage types as a generalization of refinement types, and examining their connection with Incorrectness Logic frameworks. The approach is to systematically integrate incorrectness reasoning within expressive refinement type systems.

Result: Identified mechanisms for integrating incorrectness reasoning into refinement type systems, offering new opportunities for functional programmers and tool developers to improve program verification and analysis.

Conclusion: Integrating incorrectness reasoning into refinement type systems can enhance the expressiveness and verification power of functional programming languages and related tools, benefiting property-based testing and program analysis tasks.

Abstract: A coverage type generalizes refinement types found in many functional
languages with support for must-style underapproximate reasoning.
Property-based testing frameworks are one particularly useful domain where such
capabilities are useful as they allow us to verify the completeness, as well as
safety, of test generators. There is a surprising connection between the kind
of underapproximate reasoning coverage types offer and the style of reasoning
enabled by recently proposed Incorrectness Logic frameworks. In our
presentation, we propose to explore this connection more deeply, identifying
mechanisms that more systematically integrate incorrectness reasoning within an
expressive refinement type system and the opportunities that such integration
offers to functional programmers, program verifiers, and program analyzers and
related tools.

</details>


### [33] [From Traces to Program Incorrectness: A Type-Theoretic Approach](https://arxiv.org/abs/2509.02428)
*Yongwei Yuan,Zhe Zhou,Julia Belyakova,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: The paper proposes a trace-based type system using symbolic regular expressions and automata to systematically reason about the possible incorrect behaviors in functional programs interacting with effectful, opaque libraries.


<details>
  <summary>Details</summary>
Motivation: To enable formal and compositional reasoning about incorrect behaviors in functional programs using effectful, opaque library APIs, which is important because current methods lack systematic approaches for such underapproximate analyses.

Method: They use symbolic regular expressions (SREs) and symbolic finite automata (SFAs) to represent and reason about traces of API invocations, together with a novel type inference algorithm that operates modulo specified incorrectness properties.

Result: The approach can infer types that witness that an ADT implementation can exhibit specified incorrect behaviors, using compositional reasoning across function boundaries.

Conclusion: The paper introduces the first systematic framework for underapproximating reasoning about trace-based incorrectness specifications in functional programs, enabling trace-guided compositional analysis.

Abstract: We present a type-theoretic framework for reasoning about incorrectness in
functional programs that interact with effectful, opaque library APIs. Our
approach centers on traces -- temporally-ordered sequences of library API
invocations -- which naturally characterize both the preconditions of
individual APIs and their composite behavior. We represent these traces using
symbolic regular expressions (SREs), enabling formal specification of incorrect
abstract data type (ADT) behaviors across function boundaries. The core
contribution is a novel type inference algorithm that operates modulo specified
incorrectness properties and leverages the symbolic finite automata (SFAs)
representations of regexes for compositional reasoning of traces. When the
algorithm succeeds, the inferred types witness that an ADT implementation can
exhibit some subset of the specified incorrect behaviors. This represents the
first systematic approach to underapproximate reasoning against trace-based
incorrectness specifications, enabling a new form of trace-guided compositional
analysis.

</details>
