<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Tutorial Debriefing: Applied Statistical Causal Inference in Requirements Engineering](https://arxiv.org/abs/2511.03875)
*Julian Frattini,Hans-Martin Heyn,Robert Feldt,Richard Torkar*

Main category: cs.SE

TL;DR: Software engineering research aims to improve development and user experience through new tools and processes. Proving their value requires causal evidence, but experiments aren’t always practical. The paper advocates for reliable statistical causal inference methods using observational data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure that software engineering research translates into practical improvements for software developers and users, by reliably transferring research findings into tools, processes, and guidelines that actually improve performance metrics.

Method: The method discussed involves statistical causal inference (SCI) from observational data as an alternative to randomized controlled trials, which may often be infeasible due to legal, ethical, or practical reasons.

Result: The paper highlights the necessity and role of SCI as a reliable approach to determine causal effects in software engineering research when randomized experiments are not possible.

Conclusion: SCI is essential for validating the impact of research contributions in software engineering when direct experimentation is not feasible, ensuring that interventions have a demonstrable positive effect.

Abstract: As any scientific discipline, the software engineering (SE) research
community strives to contribute to the betterment of the target population of
our research: software producers and consumers. We will only achieve this
betterment if we manage to transfer the knowledge acquired during research into
practice. This transferal of knowledge may come in the form of tools,
processes, and guidelines for software developers. However, the value of these
contributions hinges on the assumption that applying them causes an improvement
of the development process, user experience, or other performance metrics. Such
a promise requires evidence of causal relationships between an exposure or
intervention (i.e., the contributed tool, process or guideline) and an outcome
(i.e., performance metrics). A straight-forward approach to obtaining this
evidence is via controlled experiments in which a sample of a population is
randomly divided into a group exposed to the new tool, process, or guideline,
and a control group. However, such randomized control trials may not be
legally, ethically, or logistically feasible. In these cases, we need a
reliable process for statistical causal inference (SCI) from observational
data.

</details>


### [2] [Collaborative Agents for Automated Program Repair in Ruby](https://arxiv.org/abs/2511.03925)
*Nikta Akbarpour,Mahdieh Sadat Benis,Fatemeh Hendijani Fard,Ali Ouni,Mohamed Aymen Saied*

Main category: cs.SE

TL;DR: RAMP is a lightweight, test-driven, collaborative agent framework for automated Ruby program repair that surpasses previous methods in accuracy and efficiency, does not require heavy resources or databases, and showcases potential for expanding LLM debugging solutions to lesser-studied languages.


<details>
  <summary>Details</summary>
Motivation: Existing Automated Program Repair (APR) approaches mainly focus on a few popular languages and are computationally expensive. Ruby, a widely utilized language in web development, lacks effective APR solutions, despite common debugging challenges.

Method: The paper presents RAMP, a lightweight framework utilizing collaborative multi-agent strategies for feedback-driven, iterative program repair in Ruby. RAMP uses targeted test generation, error reflection, and refinement of candidate fixes through lightweight prompting and test-driven feedback, without dependence on large repair databases or costly fine-tuning.

Result: RAMP achieves a pass@1 of 67% on the XCodeEval benchmark for Ruby, outperforming previous APR methods. It converges within five iterations, with ablation studies highlighting the importance of test generation and self-reflection. RAMP is notably effective at fixing wrong answers, compilation, and runtime errors.

Conclusion: RAMP offers an efficient, LLM-powered, multi-agent approach to APR for Ruby, overcoming prior scalability and language coverage challenges, and lays a foundation for expanding LLM-based debugging tools to under-researched languages.

Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language
Models (LLMs), but most existing methods remain computationally expensive, and
focused on a small set of languages. Ruby, despite its widespread use in web
development and the persistent challenges faced by its developers, has received
little attention in APR research. In this paper, we introduce RAMP, a novel
lightweight framework that formulates program repair as a feedback-driven,
iterative process for Ruby. RAMP employs a team of collaborative agents that
generate targeted tests, reflect on errors, and refine candidate fixes until a
correct solution is found. Unlike prior approaches, RAMP is designed to avoid
reliance on large multilingual repair databases or costly fine-tuning, instead
operating directly on Ruby through lightweight prompting and test-driven
feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a
pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly
within five iterations, and ablation studies confirm that test generation and
self-reflection are key drivers of its performance. Further analysis shows that
RAMP is particularly effective at repairing wrong answers, compilation errors,
and runtime errors. Our approach provides new insights into multi-agent repair
strategies, and establishes a foundation for extending LLM-based debugging
tools to under-studied languages.

</details>


### [3] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: They propose a multi-agent system with self-correcting feedback to generate RTL code from natural language, achieving the best results to date and effectively using both open- and closed-source LLMs.


<details>
  <summary>Details</summary>
Motivation: Automate the complex and error-prone process of RTL generation from natural language, reducing human intervention and leveraging both open- and closed-source LLMs to maximize efficiency and correctness.

Method: A collaborative system of multiple agents integrating specialized LLMs and hardware simulation tools, utilizing a self-correcting progressive error feedback mechanism. The workflow checks generated RTL for compilation, functional correctness, and synthesizability. Benchmarking is performed on two open-source datasets.

Result: State-of-the-art pass rates in RTL generation, efficiency in token usage, demonstrated improved performance using both open- and closed-source LLMs, validated through open-source benchmarks.

Conclusion: The proposed agentic flow with a progressive error feedback system achieves state-of-the-art pass rates in RTL generation tasks, bridging the performance gap between open- and closed-source LLMs efficiently.

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [4] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: PSD2Code is a new method that turns PSD designs into high-quality, production-ready React+SCSS code using advanced file parsing and alignment techniques. It outperforms previous approaches in accuracy, visual match, and reliability, paving the way for better automated frontend development.


<details>
  <summary>Details</summary>
Motivation: Existing design-to-code generation methods have issues like structural inconsistencies, misalignment between assets, and inadequate production-readiness for frontend deployment. There is a need for more robust solutions that produce reliable and deployable code directly from design prototypes.

Method: The paper introduces PSD2Code, using a multi-modal pipeline called ParseAlignGenerate. This involves parsing PSD files to extract layer hierarchies and properties, using a constraint-based alignment strategy, and constructing structured prompts for large language models. This approach enhances both controllability and quality of generated React+SCSS code from design files.

Result: PSD2Code achieves better performance than current methods in key areas: code similarity, visual fidelity, and readiness for production. The pipeline is also effective across various large language models, proving its robustness and generalizability.

Conclusion: Integrating parsed design structure and multimodal information with large language models significantly advances automated, production-ready frontend code generation. PSD2Code represents a meaningful step forward for design-driven, industry-grade code generation systems.

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, existing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi-modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGenerate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relationships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates significant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [5] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: VulInstruct teaches LLMs about security specifications so they understand what 'safe code' should look like. Adding this knowledge greatly boosts their ability to find vulnerabilities and reason about code safety, outperforming existing models and even discovering new real-world security flaws.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are good at code understanding but struggle to detect vulnerabilities and differentiate between vulnerable and patched code. This limitation is attributed to LLMs lacking explicit knowledge of security specifications, which define safe code behavior and are rarely present in training data.

Method: The paper introduces VulInstruct, a specification-guided method that systematically extracts and uses security specifications from historical vulnerabilities to inform LLMs. VulInstruct builds a knowledge base from general specifications extracted from high-quality patches and domain-specific specifications from repeated code violations. This knowledge helps LLMs to reason about safe behaviors, going beyond surface-level code patterns.

Result: VulInstruct outperforms baseline models in vulnerability detection. On the PrimeVul dataset, VulInstruct achieved a 45.0% F1-score (a 32.7% improvement) and 37.7% recall (a 50.8% improvement) over baselines, uniquely detecting 24.3% of vulnerabilities (2.4x more than any baseline). It also discovered a new, high-severity vulnerability (CVE-2025-56538) in production code.

Conclusion: Incorporating explicit security specifications into LLM reasoning significantly improves their effectiveness for vulnerability detection. VulInstruct demonstrates that systematic extraction and integration of both general and domain-specific security expectations enables LLMs to identify vulnerabilities with better accuracy and reasoning, with proven practical value for real-world vulnerability discovery.

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [6] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: Improving the English language proficiency of prompts, regardless of the prompting technique, consistently results in more correct code generated by LLMs, making it a key factor for developers to consider.


<details>
  <summary>Details</summary>
Motivation: With the increasing use of Foundation Model-powered tools in software engineering, the paper seeks to explore how developers' natural language proficiency, rather than just prompt structure, may affect the quality of code generated by LLMs.

Method: The researchers used the HumanEval dataset and systematically varied the English proficiency levels of prompts, ranging from basic to advanced, across 164 programming tasks. The resulting code proficiency and correctness from LLMs were then measured and analyzed.

Result: The study found that LLMs typically respond at an intermediate (B2) natural language level. Although effects on code proficiency depended on the model, higher-proficiency prompts consistently led to the generation of more correct code across all tested models.

Conclusion: Natural language proficiency of prompts is an important factor in controlling and improving the correctness of code generated by LLMs, suggesting that developers can utilize higher language proficiency to enhance AI-generated solutions.

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [7] [LLM-Driven Adaptive Source-Sink Identification and False Positive Mitigation for Static Analysis](https://arxiv.org/abs/2511.04023)
*Shiyin Lin*

Main category: cs.SE

TL;DR: AdaTaint uses LLMs and symbolic analysis to infer sources/sinks and filter spurious alerts in static vulnerability analysis, reducing false positives by 43.7% and improving recall by 11.2% over leading tools, with competitive speed.


<details>
  <summary>Details</summary>
Motivation: Static analysis tools are useful for finding software vulnerabilities, but they struggle with incomplete source/sink specifications and produce too many false positives, limiting their practical utility.

Method: The authors introduce AdaTaint, a taint analysis framework that utilizes large language models (LLMs) for adaptive inference of source/sink specifications and filters false alerts using neuro-symbolic reasoning. AdaTaint combines LLM suggestions with symbolic program analysis to enhance accuracy.

Result: AdaTaint was evaluated on multiple benchmark sets (Juliet 1.3, SV-COMP C benchmarks, three real-world projects) and shown to reduce false positives by 43.7% and increase recall by 11.2% compared to CodeQL, Joern, and LLM-only approaches, without major runtime overhead.

Conclusion: Blending LLM-driven inference with symbolic program validation can substantially improve static taint analysis accuracy and reliability, suggesting a promising direction for vulnerability detection tools.

Abstract: Static analysis is effective for discovering software vulnerabilities but
notoriously suffers from incomplete source--sink specifications and excessive
false positives (FPs). We present \textsc{AdaTaint}, an LLM-driven taint
analysis framework that adaptively infers source/sink specifications and
filters spurious alerts through neuro-symbolic reasoning. Unlike LLM-only
detectors, \textsc{AdaTaint} grounds model suggestions in program facts and
constraint validation, ensuring both adaptability and determinism.
  We evaluate \textsc{AdaTaint} on Juliet 1.3, SV-COMP-style C benchmarks, and
three large real-world projects. Results show that \textsc{AdaTaint} reduces
false positives by \textbf{43.7\%} on average and improves recall by
\textbf{11.2\%} compared to state-of-the-art baselines (CodeQL, Joern, and
LLM-only pipelines), while maintaining competitive runtime overhead. These
findings demonstrate that combining LLM inference with symbolic validation
offers a practical path toward more accurate and reliable static vulnerability
analysis.

</details>


### [8] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: This paper introduces a new benchmark and evaluation framework for LLM-based software development agents, reveals that current systems only meet about half of requirements mainly due to weak planning and verification, and points the way toward better agent design.


<details>
  <summary>Details</summary>
Motivation: Scientific evaluation of LLM-based autonomous software development agents is currently limited by simplistic benchmarks and unstandardized agent comparisons.

Method: 1. Constructed a challenging, dynamically curated benchmark (E2EDevBench) for realistic development scenarios. 2. Proposed a hybrid evaluation framework combining functional assessment and LLM-based requirement verification. 3. Used these to empirically study three agent architectures with a unified implementation foundation.

Result: Results show state-of-the-art agents meet about 50% of requirements. Architectural strategies for task breakdown and collaboration are critical to success. Main bottlenecks are requirement omission and poor self-verification.

Conclusion: The work provides a realistic benchmark, a comprehensive evaluation framework, and key insights into agent capabilities and challenges. It identifies requirement comprehension and planning as major avenues for future improvement.

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [9] [Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment](https://arxiv.org/abs/2511.04157)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.SE

TL;DR: LLMs generally share value preferences with AI practitioners more than the broader public, but their real-world decisions can be inconsistent with their stated values. This reveals potential risks in using LLMs for AI-assisted software development without careful oversight and thorough evaluation of value alignment.


<details>
  <summary>Details</summary>
Motivation: With the increasing use of Large Language Models (LLMs) in software engineering, especially in tasks related to responsible AI, it's crucial to understand whether LLMs' value preferences align with those of humans. This alignment is urgent for ensuring responsible, trustworthy AI-supported development.

Method: The study evaluated 23 LLMs using four tasks: selecting key responsible AI values, rating their importance, resolving trade-offs between values, and prioritizing requirements based on these values. The LLMs' responses were compared with judgments from a US-representative human sample and AI practitioners.

Result: LLMs tend to align more closely with AI practitioners than with the general US-representative sample, consistently highlighting fairness, privacy, transparency, safety, and accountability. However, inconsistencies were observed between LLMs' stated value commitments and their actual prioritization decisions, indicating gaps between their declared and practical behaviors.

Conclusion: There is a practical risk in using LLMs for requirements engineering without human supervision due to inconsistencies between stated and implemented values. The paper calls for systematic tools to benchmark and monitor AI value alignment in software development processes.

Abstract: Large Language Models (LLMs) are increasingly employed in software
engineering tasks such as requirements elicitation, design, and evaluation,
raising critical questions regarding their alignment with human judgments on
responsible AI values. This study investigates how closely LLMs' value
preferences align with those of two human groups: a US-representative sample
and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key
responsible AI values, (T2) rating their importance in specific contexts, (T3)
resolving trade-offs between competing values, and (T4) prioritizing software
requirements that embody those values. The results show that LLMs generally
align more closely with AI practitioners than with the US-representative
sample, emphasizing fairness, privacy, transparency, safety, and
accountability. However, inconsistencies appear between the values that LLMs
claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4),
revealing gaps in faithfulness between stated and applied behavior. These
findings highlight the practical risk of relying on LLMs in requirements
engineering without human oversight and motivate the need for systematic
approaches to benchmark, interpret, and monitor value alignment in AI-assisted
software development.

</details>


### [10] [Explaining Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2511.04179)
*Oshando Johnson,Alexandra Fomina,Ranjith Krishnamurthy,Vaibhav Chaudhari,Rohith Kumar Shanmuganathan,Eric Bodden*

Main category: cs.SE

TL;DR: SAFE leverages GPT-4o to enhance SAST tools with clear, actionable vulnerability explanations, making it easier for developers (especially novices) to understand and fix security issues.


<details>
  <summary>Details</summary>
Motivation: Many SAST tools have poor usability due to generic warning messages, which can lead developers to misunderstand or miss important security vulnerabilities.

Method: The paper introduces SAFE, an IDE plugin that uses GPT-4o (an LLM) to provide detailed explanations of vulnerabilities detected by SAST tools, focusing on causes, impacts, and mitigation strategies.

Result: Expert user studies show that SAFE's explanations help beginner and intermediate developers better understand and address security vulnerabilities.

Conclusion: Integrating LLM-powered explanations into SAST tools improves their usability and effectiveness, particularly for less experienced developers.

Abstract: The prevalence of security vulnerabilities has prompted companies to adopt
static application security testing (SAST) tools for vulnerability detection.
Nevertheless, these tools frequently exhibit usability limitations, as their
generic warning messages do not sufficiently communicate important information
to developers, resulting in misunderstandings or oversight of critical
findings. In light of recent developments in Large Language Models (LLMs) and
their text generation capabilities, our work investigates a hybrid approach
that uses LLMs to tackle the SAST explainability challenges. In this paper, we
present SAFE, an Integrated Development Environment (IDE) plugin that leverages
GPT-4o to explain the causes, impacts, and mitigation strategies of
vulnerabilities detected by SAST tools. Our expert user study findings indicate
that the explanations generated by SAFE can significantly assist beginner to
intermediate developers in understanding and addressing security
vulnerabilities, thereby improving the overall usability of SAST tools.

</details>


### [11] [GITER: A Git-Based Declarative Exchange Model Using Kubernetes-Style Custom Resources](https://arxiv.org/abs/2511.04182)
*Christos Tranoris*

Main category: cs.SE

TL;DR: This paper presents a Git-based method for asynchronous information sharing between distributed entities, moving beyond APIs and brokers to improve transparency and autonomy, while discussing relevant architectural choices and comparison to existing integration models.


<details>
  <summary>Details</summary>
Motivation: Existing asynchronous information exchanges between distributed systems typically rely on APIs and message brokers, which can introduce tight coupling and lack transparency. There's a need for a more auditable and loosely coupled coordination method, especially to facilitate inter-organizational and cross-domain collaborations.

Method: The paper proposes using Git, inspired by Kubernetes Operators and Custom Resources, as the backbone for inter-entity communication. Each party interacts through a shared Git repository, where the 'spec' field describes the desired state and the 'status' field tracks outcomes. This model leverages Git's built-in features for traceability, access control, and reproducibility.

Result: The solution offers lightweight, auditable, and autonomous information exchange extending GitOps principles to broader scenarios. Architectural and implementation details are provided along with a comparison to existing RESTful and message broker paradigms, illustrating both benefits (transparency, autonomy, ease of audit) and potential trade-offs (performance, complexity).

Conclusion: Git can effectively replace traditional APIs and brokers for certain asynchronous communication scenarios, enhancing traceability, autonomy, and transparency while supporting a wider range of use cases, including cross-domain and air-gapped collaborations.

Abstract: This paper introduces a lightweight and auditable method for asynchronous
information exchange between distributed entities using Git as the coordination
medium. The proposed approach replaces traditional APIs and message brokers
with a Git-based communication model built on the principles of Kubernetes
Operators and Custom Resources (CRs). Each participating entity, designated as
a Publisher or Consumer, interacts through a shared repository that serves as a
single source of truth, where the spec field captures the desired state and the
status field reflects the observed outcome. This pattern extends GitOps beyond
infrastructure management to support cross-domain, inter-organizational, and
air-gapped collaboration scenarios. By leveraging Git native features
(versioning, commit signing, and access control) the model ensures
transparency, traceability, and reproducibility while preserving loose coupling
and autonomy between systems. The paper discusses architectural principles,
implementation considerations, and comparisons with RESTful and broker-based
integrations, highlighting both the advantages and trade-offs of adopting Git
as a declarative communication substrate.

</details>


### [12] [A Tool for Benchmarking Large Language Models' Robustness in Assessing the Realism of Driving Scenarios](https://arxiv.org/abs/2511.04267)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali*

Main category: cs.SE

TL;DR: The paper introduces DriveRLR, a benchmark tool for evaluating large language models' robustness in assessing the realism of autonomous driving scenarios. By generating mutated scenarios and using them as prompts, the tool effectively differentiates LLM performance and offers practical applications in ADS simulation-based testing.


<details>
  <summary>Details</summary>
Motivation: Ensuring the safety of autonomous driving systems is challenging, partly due to the difficulty in assessing the realism of simulated testing scenarios. Simulation-based scenario testing is attractive due to the risks and costs of real-world testing, but verifying that these scenarios are realistic remains a problem.

Method: The paper introduces DriveRLR, a benchmark tool that generates mutated scenario variants and constructs textual prompts to assess large language models (LLMs) on their ability and robustness in determining the realism of driving scenarios. The tool is validated on the DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4 Maverick, and Mistral Small 3.2.

Result: DriveRLR successfully highlights differences in the robustness of various LLMs when tasked with evaluating scenario realism, showing that the tool is effective in benchmarking and assessing LLM performance for this task.

Conclusion: DriveRLR is a valuable benchmark for testing and comparing the robustness of LLMs in evaluating the realism of simulation-based driving scenarios. It also offers practical value for guiding scenario generation and supporting autonomous driving system (ADS) testing workflows.

Abstract: In recent years, autonomous driving systems have made significant progress,
yet ensuring their safety remains a key challenge. To this end, scenario-based
testing offers a practical solution, and simulation-based methods have gained
traction due to the high cost and risk of real-world testing. However,
evaluating the realism of simulated scenarios remains difficult, creating
demand for effective assessment methods. Recent advances show that Large
Language Models (LLMs) possess strong reasoning and generalization
capabilities, suggesting their potential in assessing scenario realism through
scenario-related textual prompts. Motivated by this, we propose DriveRLR, a
benchmark tool to assess the robustness of LLMs in evaluating the realism of
driving scenarios. DriveRLR generates mutated scenario variants, constructs
prompts, which are then used to assess a given LLM's ability and robustness in
determining the realism of driving scenarios. We validate DriveRLR on the
DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4
Maverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals
differences in the robustness of various LLMs, demonstrating its effectiveness
and practical value in scenario realism assessment. Beyond LLM robustness
evaluation, DriveRLR can serve as a practical component in applications such as
an objective function to guide scenario generation, supporting simulation-based
ADS testing workflows.

</details>


### [13] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: Rather than just ranking LLMs in code generation, this paper identifies and analyzes the types of tasks LLMs consistently fail, uncovering four common weakness patterns and frequent task complications to better guide future research and development.


<details>
  <summary>Details</summary>
Motivation: Benchmarks for large language models (LLMs) in code generation currently focus on quantitative measures, but lack insight into specific failure points of these models. Understanding where and why LLMs fail is vital for advancing their capabilities.

Method: The analysis involved examining code generation tasks from four popular benchmarks, identifying tasks that major LLMs consistently failed. The study then assessed whether the static complexity of the solution code contributed to these failures and systematically inspected 114 such challenging tasks to find recurring failure patterns and complicating factors.

Result: The research identified four recurring patterns of weaknesses in LLMs and highlighted common complications within tasks that tend to cause failure.

Conclusion: Current benchmarks don't adequately capture where LLMs fail in code generation. This paper's systematic analysis reveals recurring weaknesses and task complications, providing a more nuanced understanding of LLM performance limits and guiding future model improvement.

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [14] [Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development](https://arxiv.org/abs/2511.04427)
*Hao He,Courtney Miller,Shyam Agarwal,Christian Kästner,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: Adopting the LLM agent Cursor causes a short-term productivity spike in software projects, but increases code warnings and complexity that slow progress over time, raising concerns for sustainable software engineering.


<details>
  <summary>Details</summary>
Motivation: Despite ambitious claims about productivity gains from large language model (LLM) agent assistants in software development, there is a lack of empirical evidence validating these assertions.

Method: The paper uses a difference-in-differences research design, comparing GitHub projects that adopted the LLM assistant Cursor with a matched control group that did not. Additional panel generalized method of moments estimation was performed to examine causal mechanisms related to development velocity and code quality.

Result: Cursor adoption leads to a significant but temporary increase in development velocity. However, it also results in a lasting rise in static analysis warnings and code complexity, which eventually contribute to a slow-down in long-term velocity.

Conclusion: While LLM agent assistants like Cursor can temporarily boost productivity in software development projects, their use is associated with enduring negative impacts on code quality and increased complexity, ultimately resulting in long-term development slowdowns.

Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize
the field of software engineering. Among other things, LLM agents are rapidly
gaining momentum in their application to software development, with
practitioners claiming a multifold productivity increase after adoption. Yet,
empirical evidence is lacking around these claims. In this paper, we estimate
the causal effect of adopting a widely popular LLM agent assistant, namely
Cursor, on development velocity and software quality. The estimation is enabled
by a state-of-the-art difference-in-differences design comparing
Cursor-adopting GitHub projects with a matched control group of similar GitHub
projects that do not use Cursor. We find that the adoption of Cursor leads to a
significant, large, but transient increase in project-level development
velocity, along with a significant and persistent increase in static analysis
warnings and code complexity. Further panel generalized method of moments
estimation reveals that the increase in static analysis warnings and code
complexity acts as a major factor causing long-term velocity slowdown. Our
study carries implications for software engineering practitioners, LLM agent
assistant designers, and researchers.

</details>


### [15] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: The paper introduces EDIT-Bench, a new benchmark for evaluating LLM code-editing capabilities using real-world data. Results show most models struggle with these realistic, context-dependent tasks, highlighting the need for better evaluation methods and more context-aware models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for instructed code editing rely on artificial data and do not reflect real-world usage, making them less effective at evaluating LLM capabilities in practice.

Method: The authors created EDIT-Bench by collecting real-world user instructions and code contexts, compiling 545 problems across various natural and programming languages, and then evaluated 40 LLMs using this benchmark.

Result: Only 5 out of 40 evaluated models scored over 60%. Model performance varied by category of user instruction and the amount of contextual information provided, with up to 11% difference due to context realism.

Conclusion: EDIT-Bench is a challenging and realistic benchmark for LLM code-editing performance, with only a few models performing above 60%.

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>


### [16] [Microservices Is Dying, A New Method for Module Division Based on Universal Interfaces](https://arxiv.org/abs/2511.04548)
*Qing Wang,Yong Zhang*

Main category: cs.SE

TL;DR: The paper proposes a new system design approach that achieves true module independence using universal interfaces, demonstrated in the EIGHT platform. This enables dynamic modification of even monolithic applications and suggests a path beyond current microservice and monolithic architectures.


<details>
  <summary>Details</summary>
Motivation: Microservices physically isolate modules but cannot effectively prevent dependencies from propagating between modules, leading to unwanted inter-module coupling.

Method: The paper first proposes a conceptual approach for calculating module independence based on impact assessment of module changes. Using this, it derives necessary conditions for achieving module independence. Then, it introduces a new design philosophy and methodology with a universal interface pattern as the boundary between modules. The method is applied through the development of the 'EIGHT' platform architecture.

Result: The implementation of the EIGHT platform demonstrates that strict module independence enables a system—even if monolithic—to support dynamic loading, unloading, and modification of parts at runtime. This is possible without the pervasive dependencies found in traditional microservices.

Conclusion: This architecture offers a new approach to building complex systems, providing an alternative to traditional microservice and monolithic system designs by prioritizing module independence and eliminating dependencies through universal interfaces.

Abstract: Although microservices have physically isolated modules, they have failed to
prevent the propagation and diffusion of dependencies. To trace the root cause
of the inter-module coupling, this paper, starting from the impact assessment
approach for module changes, proposes a conceptual method for calculating
module independence and utilizes this method to derive the necessary conditions
for module independence. Then, a new system design philosophy and software
engineering methodology is proposed, aimed at eliminating dependencies between
modules. A specific pattern is employed to design a set of universal
interfaces, serving as a universal boundary between modules. Subsequently, this
method is used to implement a platform architecture named EIGHT, demonstrating
that, as long as module independence is guaranteed, even a monolithic
application within a single process can dynamically load, unload, or modify any
part at runtime. Finally, the paper concludes that this architecture aims to
explore a novel path for increasingly complex systems, beyond microservice and
monolithic architectures.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [Modular abstract syntax trees (MAST): substitution tensors with second-class sorts](https://arxiv.org/abs/2511.03946)
*Marcelo P. Fiore,Ohad Kammar,Georg Moser,Sam Staton*

Main category: cs.PL

TL;DR: This paper generalizes categorical frameworks for abstract syntax to better accommodate languages with second-class sorts, such as CBV and CBPV, by reformulating the theory using actegories and bicategories. It proves substitution lemmas for CBV, demonstrating the validity and utility of the new approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to generalize abstract syntax frameworks to handle languages that feature second-class sorts, addressing the limitations of existing treatments when applied to calculi like CBV and CBPV.

Method: The paper adapts the foundational framework of Fiore, Plotkin, and Turi for abstract syntax, binding, substitution, and holes, modifying it to account for second-class sorts. It leverages monoidal category theory and extends to actegory and bicategorical arguments.

Result: The adapted theory successfully characterizes abstract syntax in languages with second-class sorts by using actions in actegories instead of monoids in monoidal categories. It applies the theory to prove substitution lemmata for CBV variants.

Conclusion: The approach extends the categorical treatment of syntax to handle second-class sorts effectively, offering sufficient generalization for languages like CBV and CBPV. The bicategorical framework yields provable substitution properties.

Abstract: We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with
binding, substitution, and holes to account for languages with second-class
sorts. These situations include programming calculi such as the Call-by-Value
lambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting
second-class sorts from appearing in variable contexts changes the
characterisation of the abstract syntax from monoids in monoidal categories to
actions in actegories. We reproduce much of the development through
bicategorical arguments. We apply the resulting theory by proving substitution
lemmata for varieties of CBV.

</details>
