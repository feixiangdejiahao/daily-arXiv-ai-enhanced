{"id": "2508.18587", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18587", "abs": "https://arxiv.org/abs/2508.18587", "authors": ["Barış Bayazıt", "Yao Li", "Xujie Si"], "title": "A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants", "comment": "Accepted by LMPL 2025", "summary": "Large language models (LLMs) can potentially help with verification using\nproof assistants by automating proofs. However, it is unclear how effective\nLLMs are in this task. In this paper, we perform a case study based on two\nmature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the\neffectiveness of LLMs in generating proofs by both quantitative and qualitative\nanalysis. Our study finds that: (1) external dependencies and context in the\nsame source file can significantly help proof generation; (2) LLMs perform\ngreat on small proofs but can also generate large proofs; (3) LLMs perform\ndifferently on different verification projects; and (4) LLMs can generate\nconcise and smart proofs, apply classical techniques to new definitions, but\ncan also make odd mistakes."}
{"id": "2508.18370", "categories": ["cs.SE", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18370", "abs": "https://arxiv.org/abs/2508.18370", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional capabilities when\ntrained within executable runtime environments, notably excelling at software\nengineering tasks through verified feedback loops. Yet, scalable and\ngeneralizable execution-grounded environments remain scarce, limiting progress\nin training more capable ML agents. We introduce CTF-Dojo, the first\nlarge-scale executable runtime tailored for training LLMs with verifiable\nfeedback, featuring 658 fully functional Capture-The-Flag (CTF)-style\nchallenges containerized in Docker with guaranteed reproducibility. To enable\nrapid scaling without manual intervention, we develop CTF-Forge, an automated\npipeline that transforms publicly available artifacts into ready-to-use\nexecution environments in minutes, eliminating weeks of expert configuration\ntraditionally required. We trained LLM-based agents on just 486 high-quality,\nexecution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute\ngains over strong baselines across three competitive benchmarks: InterCode-CTF,\nNYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,\nestablishing a new open-weight state-of-the-art that rivals frontier models\nlike DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a\nbenchmark for executable-agent learning, CTF-Dojo demonstrates that\nexecution-grounded training signals are not only effective but pivotal in\nadvancing high-performance ML agents without dependence on costly proprietary\nsystems."}
{"id": "2508.18431", "categories": ["cs.SE", "cs.ET", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18431", "abs": "https://arxiv.org/abs/2508.18431", "authors": ["Kérian Fiter", "Louis Malassigné-Onfroy", "Bentley Oakes"], "title": "DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting", "comment": null, "summary": "With Digital Twin (DT) construction and evolution occurring over time,\nstakeholders require tools to understand the current characteristics and\nconceptual architecture of the system at any time. We introduce DTInsight, a\nsystematic and automated tool and methodology for producing continuous\nreporting for DTs. DTInsight offers three key features: (a) an interactive\nconceptual architecture visualization of DTs; (b) generation of summaries of DT\ncharacteristics based on ontological data; and (c) integration of these outputs\ninto a reporting page within a continuous integration and continuous deployment\n(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT\nDescription Framework (DTDF), DTInsight enables up-to-date and detailed reports\nfor enhanced stakeholder understanding."}
{"id": "2508.18452", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18452", "abs": "https://arxiv.org/abs/2508.18452", "authors": ["Pierre-Emmanuel Goffi", "Raphaël Tremblay", "Bentley Oakes"], "title": "Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling", "comment": "Accepted for EDTconf 2025", "summary": "Successfully engineering interactive industrial DTs is a complex task,\nespecially when implementing services beyond passive monitoring. We present\nhere an experience report on engineering a safety-critical digital twin (DT)\nfor beer fermentation monitoring, which provides continual sampling and reduces\nmanual sampling time by 91%. We document our systematic methodology and\npractical solutions for implementing bidirectional DTs in industrial\nenvironments. This includes our three-phase engineering approach that\ntransforms a passive monitoring system into an interactive Type 2 DT with\nreal-time control capabilities for pressurized systems operating at seven bar.\nWe contribute details of multi-layered safety protocols, hardware-software\nintegration strategies across Arduino controllers and Unity visualization, and\nreal-time synchronization solutions. We document specific engineering\nchallenges and solutions spanning interdisciplinary integration, demonstrating\nhow our use of the constellation reporting framework facilitates cross-domain\ncollaboration. Key findings include the critical importance of safety-first\ndesign, simulation-driven development, and progressive implementation\nstrategies. Our work thus provides actionable guidance for practitioners\ndeveloping DTs requiring bidirectional control in safety-critical applications."}
{"id": "2508.18547", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18547", "abs": "https://arxiv.org/abs/2508.18547", "authors": ["Youssef Abdelsalam", "Norman Peitek", "Anna-Maria Maurer", "Mariya Toneva", "Sven Apel"], "title": "How do Humans and LLMs Process Confusing Code?", "comment": null, "summary": "Already today, humans and programming assistants based on large language\nmodels (LLMs) collaborate in everyday programming tasks. Clearly, a\nmisalignment between how LLMs and programmers comprehend code can lead to\nmisunderstandings, inefficiencies, low code quality, and bugs.\n  A key question in this space is whether humans and LLMs are confused by the\nsame kind of code. This would not only guide our choices of integrating LLMs in\nsoftware engineering workflows, but also inform about possible improvements of\nLLMs.\n  To this end, we conducted an empirical study comparing an LLM to human\nprogrammers comprehending clean and confusing code. We operationalized\ncomprehension for the LLM by using LLM perplexity, and for human programmers\nusing neurophysiological responses (in particular, EEG-based fixation-related\npotentials).\n  We found that LLM perplexity spikes correlate both in terms of location and\namplitude with human neurophysiological responses that indicate confusion. This\nresult suggests that LLMs and humans are similarly confused about the code.\nBased on these findings, we devised a data-driven, LLM-based approach to\nidentify regions of confusion in code that elicit confusion in human\nprogrammers."}
{"id": "2508.18636", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18636", "abs": "https://arxiv.org/abs/2508.18636", "authors": ["Yan Wang", "Xinyi Hou", "Yanjie Zhao", "Weiguo Lin", "Haoyu Wang", "Junjun Si"], "title": "LaQual: A Novel Framework for Automated Evaluation of LLM App Quality", "comment": null, "summary": "LLM app stores are quickly emerging as platforms that gather a wide range of\nintelligent applications based on LLMs, giving users many choices for content\ncreation, coding support, education, and more. However, the current methods for\nranking and recommending apps in these stores mostly rely on static metrics\nlike user activity and favorites, which makes it hard for users to efficiently\nfind high-quality apps. To address these challenges, we propose LaQual, an\nautomated framework for evaluating the quality of LLM apps. LaQual consists of\nthree main stages: first, it labels and classifies LLM apps in a hierarchical\nway to accurately match them to different scenarios; second, it uses static\nindicators, such as time-weighted user engagement and functional capability\nmetrics, to filter out low-quality apps; and third, it conducts a dynamic,\nscenario-adaptive evaluation, where the LLM itself generates scenario-specific\nevaluation metrics, scoring rules, and tasks for a thorough quality assessment.\nExperiments on a popular LLM app store show that LaQual is effective. Its\nautomated scores are highly consistent with human judgments (with Spearman's\nrho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in\ntravel planning). By effectively screening, LaQual can reduce the pool of\ncandidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual\nsignificantly outperforms baseline systems in decision confidence, comparison\nefficiency (with average scores of 5.45 compared to 3.30), and the perceived\nvalue of its evaluation reports (4.75 versus 2.25). Overall, these results\ndemonstrate that LaQual offers a scalable, objective, and user-centered\nsolution for finding and recommending high-quality LLM apps in real-world use\ncases."}
{"id": "2508.18675", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18675", "abs": "https://arxiv.org/abs/2508.18675", "authors": ["Xu Lu", "Weisong Sun", "Yiran Zhang", "Ming Hu", "Cong Tian", "Zhi Jin", "Yang Liu"], "title": "Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision", "comment": null, "summary": "Automated code generation has long been considered the holy grail of software\nengineering. The emergence of Large Language Models (LLMs) has catalyzed a\nrevolutionary breakthrough in this area. However, existing methods that only\nrely on LLMs remain inadequate in the quality of generated code, offering no\nguarantees of satisfying practical requirements. They lack a systematic\nstrategy for requirements development and modeling. Recently, LLM-based agents\ntypically possess powerful abilities and play an essential role in facilitating\nthe alignment of LLM outputs with user requirements. In this paper, we envision\nthe first multi-agent framework for reliable code generation based on\n\\textsc{re}quirements \\textsc{de}velopment and \\textsc{fo}rmalization, named\n\\textsc{ReDeFo}. This framework incorporates three agents, highlighting their\naugmentation with knowledge and techniques of formal methods, into the\nrequirements-to-code generation pipeline to strengthen quality assurance. The\ncore of \\textsc{ReDeFo} is the use of formal specifications to bridge the gap\nbetween potentially ambiguous natural language requirements and precise\nexecutable code. \\textsc{ReDeFo} enables rigorous reasoning about correctness,\nuncovering hidden bugs, and enforcing critical properties throughout the\ndevelopment process. In general, our framework aims to take a promising step\ntoward realizing the long-standing vision of reliable, auto-generated software."}
{"id": "2508.18721", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18721", "abs": "https://arxiv.org/abs/2508.18721", "authors": ["Yunrui Pei", "Hongshu Wang", "Wenjie Zhang", "Yun Lin", "Weiyu Kong", "Jin song Dong"], "title": "LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging", "comment": null, "summary": "Dynamic data dependency, answering \"why a variable has this value?\", is\ncritical for debugging. Given a program step `s` reading a variable `v`,\nfinding the dynamic definition of `v` is challenging. Traditional methods\nrequire either (1) exhaustive instrumentation of all possible definitions of\n`v` in one run or (2) replicating the run to re-examine reads/writes - both\ncostly. If `v` is defined in a library, instrumentation becomes expensive; for\nnon-deterministic programs, replication is infeasible.\n  We propose RecovSlicing, which computes dynamic data dependency in a single\nrun with partial instrumentation. We leverage LLMs to infer program behavior\nfrom a partially recorded trace and code context. Given a trace and a slicing\ncriterion (step `s` and variable `v`), RecovSlicing estimates the runtime\ndefinition of `v` by recovering the missing execution.It also supports implicit\nvariables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:\n(1) recovering runtime values and structures, and (2) aligning recovered\nvariables with recorded memory to analyze definitions.\n  We evaluate RecovSlicing on 8,300 data dependencies across three slicing\nbenchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution\nSlicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,\noutperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall\n(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug\nlocalizer, it enables finding 16% more regressions."}
{"id": "2508.18771", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18771", "abs": "https://arxiv.org/abs/2508.18771", "authors": ["Kexin Sun", "Hongyu Kuang", "Sebastian Baltes", "Xin Zhou", "He Zhang", "Xiaoxing Ma", "Guoping Rong", "Dong Shao", "Christoph Treude"], "title": "Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions", "comment": null, "summary": "AI-based code review tools automatically review and comment on pull requests\nto improve code quality. Despite their growing presence, little is known about\ntheir actual impact. We present a large-scale empirical study of 16 popular\nAI-based code review actions for GitHub workflows, analyzing more than 22,000\nreview comments in 178 repositories. We investigate (1) how these tools are\nadopted and configured, (2) whether their comments lead to code changes, and\n(3) which factors influence their effectiveness. We develop a two-stage\nLLM-assisted framework to determine whether review comments are addressed, and\nuse interpretable machine learning to identify influencing factors. Our\nfindings show that, while adoption is growing, effectiveness varies widely.\nComments that are concise, contain code snippets, and are manually triggered,\nparticularly those from hunk-level review tools, are more likely to result in\ncode changes. These results highlight the importance of careful tool design and\nsuggest directions for improving AI-based code review systems."}
{"id": "2508.18816", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18816", "abs": "https://arxiv.org/abs/2508.18816", "authors": ["Sabato Nocera", "Davide Fucci", "Giuseppe Scanniello"], "title": "Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study", "comment": "Accepted for ESEM25 NIER track", "summary": "Background: Static Code Analysis (SCA) tools are widely adopted to enforce\ncode quality standards. However, little is known about how open-source projects\nuse and customize these tools. Aims: This paper investigates how GitHub\nprojects use and customize a popular SCA tool, namely SonarQube Cloud. Method:\nWe conducted a mining study of GitHub projects that are linked through GitHub\nActions to SonarQube Cloud projects. Results: Among 321 GitHub projects using\nSonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud\nprojects, while others exhibit misconfigurations or restricted access. Among\n265 accessible SonarQube Cloud projects, 75% use the organization's default\nquality gate, i.e., a set of conditions that deployed source code must meet to\npass automated checks. While 55% of the projects use the built-in quality gate\nprovided by SonarQube Cloud, 45% of them customize their quality gate with\ndifferent conditions. Overall, the most common quality conditions align with\nSonarQube Cloud's \"Clean as You Code\" principle and enforce security,\nmaintainability, reliability, coverage, and a few duplicates on newly added or\nmodified source code. Conclusions: Many projects rely on predefined\nconfigurations, yet a significant portion customize their configurations to\nmeet specific quality goals. Building on our initial results, we envision a\nfuture research agenda linking quality gate configurations to actual software\noutcomes (e.g., improvement of software security). This would enable\nevidence-based recommendations for configuring SCA tools like SonarQube Cloud\nin various contexts."}
{"id": "2508.18955", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.18955", "abs": "https://arxiv.org/abs/2508.18955", "authors": ["Yunbo Ni", "Shaohua Li"], "title": "Interleaving Large Language Models for Compiler Testing", "comment": null, "summary": "Testing compilers with AI models, especially large language models (LLMs),\nhas shown great promise. However, current approaches struggle with two key\nproblems: The generated programs for testing compilers are often too simple,\nand extensive testing with the LLMs is computationally expensive. In this\npaper, we propose a novel compiler testing framework that decouples the testing\nprocess into two distinct phases: an offline phase and an online phase. In the\noffline phase, we use LLMs to generate a collection of small but feature-rich\ncode pieces. In the online phase, we reuse these code pieces by strategically\ncombining them to build high-quality and valid test programs, which are then\nused to test compilers.\n  We implement this idea in a tool, LegoFuzz, for testing C compilers. The\nresults are striking: we found 66 bugs in GCC and LLVM, the most widely used C\ncompilers. Almost half of the bugs are miscompilation bugs, which are serious\nand hard-to-find bugs that none of the existing LLM-based tools could find. We\nbelieve this efficient design opens up new possibilities for using AI models in\nsoftware testing beyond just C compilers."}
{"id": "2508.18993", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18993", "abs": "https://arxiv.org/abs/2508.18993", "authors": ["Ziyi Ni", "Huacan Wang", "Shuo Zhang", "Shuo Lu", "Ziyang He", "Wang You", "Zhenheng Tang", "Yuntao Du", "Bill Sun", "Hongzhang Liu", "Sen Hu", "Ronghao Chen", "Bo Li", "Xin Li", "Chen Hu", "Binxing Jiao", "Daxin Jiang", "Pin Lyu"], "title": "GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging", "comment": "Highly practical, Well-motivated, Actionable", "summary": "Beyond scratch coding, exploiting large-scale code repositories (e.g.,\nGitHub) for practical tasks is vital in real-world software development, yet\ncurrent benchmarks rarely evaluate code agents in such authentic,\nworkflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a\nbenchmark designed to systematically assess this capability via 54 realistic\ntasks across 7 modalities and 7 domains. Each task pairs a relevant repository\nwith an automated, human-curated evaluation harness specifying practical\nsuccess criteria. Beyond measuring execution and task success, we also propose\nthe alpha-value metric to quantify the economic benefit of agent performance,\nwhich integrates task success rates, token cost, and average developer\nsalaries. Experiments across three state-of-the-art agent frameworks with\nmultiple advanced LLMs show that leveraging code repositories for complex task\nsolving remains challenging: even the best-performing system, OpenHands+Claude\n3.7, solves only 48.15% of tasks. Error analysis attributes over half of\nfailures to seemingly mundane yet critical steps like environment setup and\ndependency resolution, highlighting the need for more robust workflow\nmanagement and increased timeout preparedness. By releasing GitTaskBench, we\naim to drive progress and attention toward repository-aware code reasoning,\nexecution, and deployment -- moving agents closer to solving complex,\nend-to-end real-world tasks. The benchmark and code are open-sourced at\nhttps://github.com/QuantaAlpha/GitTaskBench."}
{"id": "2508.19056", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.19056", "abs": "https://arxiv.org/abs/2508.19056", "authors": ["S. Panda", "D. Munjal", "D. P. Mohapatra"], "title": "A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs", "comment": null, "summary": "Test case prioritization focuses on finding a suitable order of execution of\nthe test cases in a test suite to meet some performance goals like detecting\nfaults early. It is likely that some test cases execute the program parts that\nare more prone to errors and will detect more errors if executed early during\nthe testing process. Finding an optimal order of execution for the selected\nregression test cases saves time and cost of retesting. This paper presents a\nstatic approach to prioritizing the test cases by computing the affected\ncomponent coupling (ACC) of the affected parts of object-oriented programs. We\nconstruct a graph named affected slice graph (ASG) to represent these affected\nprogram parts.We determine the fault-proneness of the nodes of ASG by computing\ntheir respective ACC values. We assign higher priority to those test cases that\ncover the nodes with higher ACC values. Our analysis with mutation faults shows\nthat the test cases executing the fault-prone program parts have a higher\nchance to reveal faults earlier than other test cases in the test suite. The\nresult obtained from seven case studies justifies that our approach is feasible\nand gives acceptable performance in comparison to some existing techniques."}
