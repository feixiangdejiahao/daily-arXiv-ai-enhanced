<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.PL](#cs.PL) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: SIADAFIX uses a combination of fast and slow thinking, adaptively choosing repair modes for program repair. It sets a new benchmark in automated program repair, balancing efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents struggle with complex program repair tasks, often unable to efficiently balance speed (efficiency) and accuracy. Enhancing these agents could improve automated program repair.

Method: SIADAFIX combines two components: a slow thinking bug fix agent for complex repairs and fast workflow decision modules that optimize and classify issue descriptions. It adaptively switches between three repair modes (easy, middle, hard) using generalization and test-time scaling as needed.

Result: SIADAFIX achieves 60.67% pass@1 performance using the Claude-4 Sonnet model, outperforming other open-source methods on SWE-bench Lite.

Conclusion: The proposed SIADAFIX method improves automated program repair by adaptively managing repair complexity and workflow. It balances speed and accuracy, providing new perspectives for future research.

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [2] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: MLCPD is a large, universal code dataset providing normalized ASTs for ten languages, supporting cross-language code analysis, with open resources to aid software research.


<details>
  <summary>Details</summary>
Motivation: There is a lack of large-scale, language-agnostic datasets that unify code structure across multiple programming languages, limiting research in cross-language software analysis and representation learning.

Method: The authors constructed MLCPD by parsing over seven million source files from ten major programming languages, applying a universal AST schema to normalize and unify their syntactic structure and metadata.

Result: MLCPD reveals strong cross-language structural regularities, showing that code from varied languages can be aligned under a unified schema. The dataset and associated tools are made publicly available for reproducibility and further research.

Conclusion: MLCPD establishes a new, open-standard resource for cross-language program analysis and representation learning, enabling lossless and consistent structural reasoning between diverse programming languages.

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [3] [Code Contribution and Credit in Science](https://arxiv.org/abs/2510.16242)
*Eva Maxfield Brown,Isaac Slaughter,Nicholas Weber*

Main category: cs.SE

TL;DR: Software contributors in scientific research often go unrecognized in traditional author credits, and their coding activities do not translate into higher scholarly impact. This highlights a misalignment in credit allocation and calls for science policy reforms.


<details>
  <summary>Details</summary>
Motivation: Software has become vital in scientific research, but it's unclear how software contributions are recognized compared to traditional authorship in scholarly metrics.

Method: The researchers compiled a large dataset (~140,000 pairs) linking research articles and software repositories. They developed a model to match article authors to developer accounts and analyzed patterns of credit allocation and impact metrics for software contributors versus non-contributors.

Result: Nearly 30% of articles had code contributors who did not receive formal authorship credit. Code-contributing authors gained only a small citation boost, which was not statistically significant after controlling for factors. First authors were much more likely to contribute code. Frequent code contributors had lower h-indices than their non-coding peers, even when accounting for other variables.

Conclusion: There is a substantial gap between software development contributions and formal scholarly recognition, with frequent code contributors receiving less institutional credit through traditional metrics.

Abstract: Software development has become essential to scientific research, but its
relationship to traditional metrics of scholarly credit remains poorly
understood. We develop a dataset of approximately 140,000 paired research
articles and code repositories, as well as a predictive model that matches
research article authors with software repository developer accounts. We use
this data to investigate how software development activities influence credit
allocation in collaborative scientific settings. Our findings reveal
significant patterns distinguishing software contributions from traditional
authorship credit. We find that nearly 30% of articles include non-author code
contributors- individuals who participated in software development but received
no formal authorship recognition. While code-contributing authors show a modest
$\sim$4.2% increase in article citations, this effect becomes non-significant
when controlling for domain, article type, and open access status. First
authors are significantly more likely to be code contributors than other author
positions. Notably, we identify a negative relationship between coding
frequency and scholarly impact metrics. Authors who contribute code more
frequently exhibit progressively lower h-indices than non-coding colleagues,
even when controlling for publication count, author position, domain, and
article type. These results suggest a disconnect between software contributions
and credit, highlighting important implications for institutional reward
structures and science policy.

</details>


### [4] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: For code translation tasks using LLMs, giving many examples in the prompt does not improve and may worsen functional accuracy. A handful of well-chosen examples works best, challenging the assumption that bigger prompts are always better.


<details>
  <summary>Details</summary>
Motivation: It is commonly assumed that providing more examples in in-context learning, especially with models having large context windows, will enhance task performance. The paper tests this assumption in the context of code translation, a complex and practical task.

Method: Large-scale empirical study evaluating over 90,000 code translations. The study systematically scales in-context examples from zero-shot up to many-shot (over 625 examples) and analyzes functional correctness versus static similarity metrics.

Result: Static similarity metrics improve modestly with more examples, but functional correctness peaks with only a few examples (5-25). Excessive examples degrade functional performance. The optimal number of examples is task-dependent, with 'more is better' not holding universally for code translation.

Conclusion: The quality of a few well-chosen in-context examples yields better functional correctness in code translation tasks compared to simply providing many examples. More examples do not necessarily improve performance and may degrade results.

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


### [5] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: SemOpt boosts automated code optimization by using LLMs and static analysis for semantic understanding, outperforming traditional approaches on benchmarks and real-world projects.


<details>
  <summary>Details</summary>
Motivation: Existing automated code optimization systems often rely on information retrieval techniques like BM25 to mine optimization examples from large codebases. However, these approaches struggle because semantically similar optimizations may have different syntactic forms, making it hard to retrieve pertinent examples and resulting in suboptimal optimized code.

Method: The authors introduce SemOpt, a framework that applies static program analysis and LLMs to automatically identify specific code segments that can be optimized. SemOpt has three components: (1) it builds a library by clustering optimization strategies from real-world code modifications, (2) it generates Semgrep rules to detect applicable code segments, and (3) it leverages the strategy library to synthesize optimized code, all powered by LLMs.

Result: SemOpt achieves far better optimization rates compared to baseline approaches, improving the number of successful code optimizations from 1.38 up to 28 times on a 151-task benchmark. On real-world C/C++ projects, it yields performance gains ranging from 5.04% up to 218.07% on individual metrics, proving its effectiveness and practical value.

Conclusion: SemOpt addresses the limitations of current LLM-driven code optimization systems by focusing on semantic understanding and static analysis, resulting in significantly improved optimization success rates and real-world performance gains.

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [6] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: Current LLMs can help in coding, but enterprise software development faces unique challenges due to its complex, evolving nature and reliance on tacit knowledge. The proposed Code Digital Twin is a structured, evolving framework that makes hidden knowledge explicit and usable by both AI and humans, helping enterprises utilize AI for smarter, sustainable software engineering.


<details>
  <summary>Details</summary>
Motivation: Despite rapid advances in LLM capabilities, applying them to enterprise software development is challenging due to its incremental nature and reliance on tacit knowledge (such as design decisions and historical trade-offs) that go beyond routine coding.

Method: The authors systematically identify challenges from both the software engineering and LLM perspectives, and propose the Code Digital Twin—a framework that models physical and conceptual layers of software, incorporates hybrid knowledge representations and multi-stage extraction pipelines, supports incremental updates and human-in-the-loop feedback, and uses LLM-powered applications.

Result: The Code Digital Twin framework enables the preservation and explicit representation of tacit knowledge, enhances decision-making tasks (like issue localization and impact analysis), and transforms fragmented knowledge into actionable forms. It can co-evolve with the codebase and support sustainable, resilient enterprise software development.

Conclusion: Integrating AI capabilities with structured knowledge frameworks (like Code Digital Twin) can align LLM strengths with the realities of enterprise software engineering. This approach delivers a roadmap for intelligent and sustainable development and evolution of complex software systems, bridging the gap between cutting-edge AI and real-world software practices.

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


### [7] [Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions](https://arxiv.org/abs/2510.16433)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Yasutaka Kamei,Hajimu Iida*

Main category: cs.SE

TL;DR: Continuous fuzzing in software development quickly detects many existing vulnerabilities early, improves code coverage over time, and new code coverage increases help uncover additional bugs, making continuous fuzzing a valuable strategy for improving software security.


<details>
  <summary>Details</summary>
Motivation: Despite the widespread adoption of continuous fuzzing in thousands of software projects, it remains unclear how much continuous fuzzing improves vulnerability detection and what role it plays in discovering bugs during software development.

Method: The authors conducted an empirical study using data collected from OSS-Fuzz, including issue reports, coverage reports, and fuzzing logs from around 1.12 million fuzzing sessions across 878 projects.

Result: The study found: (i) many fuzzing bugs were present before continuous fuzzing started, leading to high bug detection rates in the early phase; (ii) code coverage consistently increases as continuous fuzzing is performed; and (iii) increases in code coverage help find new fuzzing bugs.

Conclusion: Continuous fuzzing is effective for vulnerability detection, particularly due to increased code coverage and the discovery of pre-existing bugs early in the process. These findings inform future strategies and tool development for software security.

Abstract: Software vulnerabilities are constantly being reported and exploited in
software products, causing significant impacts on society. In recent years, the
main approach to vulnerability detection, fuzzing, has been integrated into the
continuous integration process to run in short and frequent cycles. This
continuous fuzzing allows for fast identification and remediation of
vulnerabilities during the development process. Despite adoption by thousands
of projects, however, it is unclear how continuous fuzzing contributes to
vulnerability detection. This study aims to elucidate the role of continuous
fuzzing in vulnerability detection. Specifically, we investigate the coverage
and the total number of fuzzing sessions when fuzzing bugs are discovered. We
collect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an
online service provided by Google that performs fuzzing during continuous
integration. Through an empirical study of a total of approximately 1.12
million fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal
that (i) a substantial number of fuzzing bugs exist prior to the integration of
continuous fuzzing, leading to a high detection rate in the early stages; (ii)
code coverage continues to increase as continuous fuzzing progresses; and (iii)
changes in coverage contribute to the detection of fuzzing bugs. This study
provides empirical insights into how continuous fuzzing contributes to fuzzing
bug detection, offering practical implications for future strategies and tool
development in continuous fuzzing.

</details>


### [8] [On the Use of Large Language Models for Qualitative Synthesis](https://arxiv.org/abs/2510.16502)
*Sebastián Pizard,Ramiro Moreira,Federico Galiano,Ignacio Sastre,Lorena Etcheverry*

Main category: cs.SE

TL;DR: Using LLMs in systematic reviews—especially qualitative synthesis—is promising but risky due to variability and reporting inconsistencies. Collaborative trials highlight the need for caution and rigorous evaluation to avoid amplifying weaknesses and undermining review reliability.


<details>
  <summary>Details</summary>
Motivation: Large language models have the potential to assist with systematic reviews, especially in the complex stage of qualitative synthesis, but application risk remains due to variability and reporting inconsistencies.

Method: A collaborative autoethnography approach was used, involving two trials to assess LLMs' effectiveness in qualitative synthesis. Each trial was evaluated for rigor and practical utility, and results were interpreted technically based on LLM design and current limitations.

Result: The study identified key challenges associated with using LLMs in qualitative synthesis, focusing on the risks of misuse, impact on methodological rigor, and concerns about eroding confidence in review findings.

Conclusion: LLMs offer significant opportunities but also present critical risks when applied to unevenly conducted or reported SR stages. Careful evaluation and understanding of these risks are necessary to prevent undermining the credibility of systematic reviews.

Abstract: Large language models (LLMs) show promise for supporting systematic reviews
(SR), even complex tasks such as qualitative synthesis (QS). However, applying
them to a stage that is unevenly reported and variably conducted carries
important risks: misuse can amplify existing weaknesses and erode confidence in
the SR findings. To examine the challenges of using LLMs for QS, we conducted a
collaborative autoethnography involving two trials. We evaluated each trial for
methodological rigor and practical usefulness, and interpreted the results
through a technical lens informed by how LLMs are built and their current
limitations.

</details>


### [9] [Human-Aligned Code Readability Assessment with Large Language Models](https://arxiv.org/abs/2510.16579)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Pawel Borsukiewicz,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: The paper presents CoReEval, a large benchmark evaluating LLMs for code readability. It finds developer-guided prompts improve result alignment and explanations but increase score variability. CoReEval supports model alignment, prompt engineering, and personalized code review by LLMs.


<details>
  <summary>Details</summary>
Motivation: Code readability is key for software comprehension and maintenance, but traditional metrics are limited and subjective human assessments are hard to scale. LLMs offer a scalable alternative, but have not been systematically evaluated for this task.

Method: The authors introduce CoReEval, a large benchmark with over 1.4 million evaluations across 10 LLMs. They assess code readability in Java, Python, and CUDA using multiple code types, prompts, decoding settings, and developer-guided persona framing. Outputs are compared to human annotations and a static model using alignment and qualitative justification analyses.

Result: Developer-guided prompts improve alignment and explanation quality in structured contexts, allowing personalization for different user personas. However, results show greater score variability, indicating trade-offs between alignment, stability, and interpretability.

Conclusion: CoReEval establishes a scalable framework for evaluating and aligning LLM-based code readability assessment. The findings support future studies in prompt engineering and human-in-the-loop loops, enhancing education, onboarding, and CI/CD uses.

Abstract: Code readability is crucial for software comprehension and maintenance, yet
difficult to assess at scale. Traditional static metrics often fail to capture
the subjective, context-sensitive nature of human judgments. Large Language
Models (LLMs) offer a scalable alternative, but their behavior as readability
evaluators remains underexplored. We introduce CoReEval, the first large-scale
benchmark for evaluating LLM-based code readability assessment, comprising over
1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.
The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types
(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),
9 decoding settings, and developer-guided prompts tailored to junior and senior
personas. We compare LLM outputs against human annotations and a validated
static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and
justification quality (sentiment, aspect coverage, semantic clustering). Our
findings show that developer-guided prompting grounded in human-defined
readability dimensions improves alignment in structured contexts, enhances
explanation quality, and enables lightweight personalization through persona
framing. However, increased score variability highlights trade-offs between
alignment, stability, and interpretability. CoReEval provides a robust
foundation for prompt engineering, model alignment studies, and human in the
loop evaluation, with applications in education, onboarding, and CI/CD
pipelines where LLMs can serve as explainable, adaptable reviewers.

</details>


### [10] [Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios](https://arxiv.org/abs/2510.16665)
*Mohamed Sami Rakha,Andriy Miranskyy,Daniel Alencar da Costa*

Main category: cs.SE

TL;DR: Performance improvements from hyperparameter tuning in software defect prediction vary significantly between different scenarios. IVDP benefits more than CVDP, and in small datasets the difference is greater, meaning scenario selection affects practical gains from tuning.


<details>
  <summary>Details</summary>
Motivation: There is a need to understand the generalized impact of hyperparameter tuning on software defect prediction (SDP) models across different scenarios to enhance model robustness, generalizability, and practical applicability. Existing work lacks comparative insights on how scenario choice affects performance gains from tuning.

Method: The study contrasts hyperparameter tuning effects across two major SDP scenarios: Inner Version Defect Prediction (IVDP) and Cross Version Defect Prediction (CVDP). It uses 28 machine learning algorithms, 53 datasets, two tuning algorithms, five metrics, and statistical methods to analyze overall, per-algorithm, and dataset size impacts.

Result: Findings show that hyperparameter tuning yields significantly greater performance improvements in IVDP than in CVDP. Performance gains that hold for most ML algorithms in IVDP do not necessarily translate to CVDP; small datasets experience larger performance impact differences.

Conclusion: The effect of hyperparameter tuning on SDP is highly dependent on the prediction scenario. Researchers and practitioners should account for scenario choice when anticipating the benefits of tuning.

Abstract: Software defect prediction (SDP) is crucial for delivering high-quality
software products. Recent research has indicated that prediction performance
improvements in SDP are achievable by applying hyperparameter tuning to a
particular SDP scenario. However, the positive impact resulting from the
hyperparameter tuning step may differ based on the targeted SDP scenario.
Comparing the impact of hyperparameter tuning across SDP scenarios is necessary
to provide comprehensive insights and enhance the robustness, generalizability,
and, eventually, the practicality of SDP modeling for quality assurance.
  Therefore, in this study, we contrast the impact of hyperparameter tuning
across two pivotal and consecutive SDP scenarios: (1) Inner Version Defect
Prediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main
distinctions between the two scenarios lie in the scope of defect prediction
and the selected evaluation setups. This study's experiments use common
evaluation setups, 28 machine learning (ML) algorithms, 53 post-release
software datasets, two tuning algorithms, and five optimization metrics. We
apply statistical analytics to compare the SDP performance impact differences
by investigating the overall impact, the single ML algorithm impact, and
variations across different software dataset sizes.
  The results indicate that the SDP gains within the IVDP scenario are
significantly larger than those within the CVDP scenario. The results reveal
that asserting performance gains for up to 24 out of 28 ML algorithms may not
hold across multiple SDP scenarios. Furthermore, we found that small software
datasets are more susceptible to larger differences in performance impacts.
Overall, the study findings recommend software engineering researchers and
practitioners to consider the effect of the selected SDP scenario when
expecting performance gains from hyperparameter tuning.

</details>


### [11] [QuanBench: Benchmarking Quantum Code Generation with Large Language Models](https://arxiv.org/abs/2510.16779)
*Xiaoyu Guo,Minggu Wang,Jianjun Zhao*

Main category: cs.SE

TL;DR: QuanBench, a benchmark for quantum code generation, demonstrates that recent large language models struggle with accuracy (below 40%) and semantics. The work highlights common errors and serves as a resource for future research to enhance LLM performance in quantum programming.


<details>
  <summary>Details</summary>
Motivation: Large language models have excelled in general code generation, but their abilities in quantum code generation are not well-understood. There is a need for systematic benchmarks to evaluate and improve their performance in this specialized field.

Method: The authors introduce QuanBench, a benchmark suite comprising 44 quantum programming tasks that span algorithms, state preparation, gate decomposition, and quantum machine learning. Each task includes an executable canonical solution, and models are evaluated using Pass@K for correctness and Process Fidelity for quantum semantic equivalence. Several recent LLMs, both general-purpose and code-specialized, are assessed.

Result: Current large language models show limited accuracy in quantum code generation, with overall correctness below 40%. Frequent mistakes include outdated API usage, incorrect circuit constructions, and flawed algorithm logic. Semantic errors are common across evaluated models.

Conclusion: QuanBench reveals significant deficiencies in LLM-based quantum code generation, providing insights into error patterns and laying the foundation for targeted improvements in future models.

Abstract: Large language models (LLMs) have demonstrated good performance in general
code generation; however, their capabilities in quantum code generation remain
insufficiently studied. This paper presents QuanBench, a benchmark for
evaluating LLMs on quantum code generation. QuanBench includes 44 programming
tasks that cover quantum algorithms, state preparation, gate decomposition, and
quantum machine learning. Each task has an executable canonical solution and is
evaluated by functional correctness (Pass@K) and quantum semantic equivalence
(Process Fidelity). We evaluate several recent LLMs, including general-purpose
and code-specialized models. The results show that current LLMs have limited
capability in generating the correct quantum code, with overall accuracy below
40% and frequent semantic errors. We also analyze common failure cases, such as
outdated API usage, circuit construction errors, and incorrect algorithm logic.
QuanBench provides a basis for future work on improving quantum code generation
with LLMs.

</details>


### [12] [More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents](https://arxiv.org/abs/2510.16786)
*Pengfei Gao,Chao Peng*

Main category: cs.SE

TL;DR: This study systematically analyzes how to control turns in LLM coding agents for cost-effective deployment. Fixed and dynamic strategies both reduce costs substantially, with dynamic turn-control offering even better results without hurting task performance, making it the recommended approach for practical coding agent deployment.


<details>
  <summary>Details</summary>
Motivation: LLM-powered coding agents are powerful but expensive to deploy due to factors like many iterative turns, growing token counts, costly models, and agent inefficiencies. Existing research has not sufficiently addressed controlling the total number of turns as a way to manage performance and cost.

Method: A comprehensive empirical study is conducted using the SWE-bench benchmark and three state-of-the-art LLM models. The study evaluates the effect of three turn-control strategies: 1) unrestricted turn baseline, 2) fixed-turn limits with reminders, and 3) a novel dynamic-turn strategy with on-demand extensions.

Result: Results show that unrestricted turns create cost-performance trade-offs and no model dominates across all metrics. Imposing a fixed-turn limit at the 75th percentile greatly reduces costs (by 24%-68%) with little loss in task completion rates. The dynamic-turn strategy further cuts costs (by 12%-24%) and maintains or improves solve rates by granting extra turns only when necessary.

Conclusion: Strategic control over the number of agent turns, especially with dynamic resource allocation, enables the deployment of coding agents that are both effective and cost-efficient. Dynamic-turn strategies are simple, powerful, and easy to implement for developers.

Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve
software engineering tasks, are becoming increasingly powerful. However, their
practical deployment is hindered by significant and unpredictable costs. This
challenge arises from a combination of factors: quadratically growing token
counts with each turn, the high price of models, the large number of turns
required for real-world tasks, and the tendency of agents to take inefficient
or unnecessary actions. While existing research focuses on optimizing
individual turns, the strategic control of the total number of turns remains an
underexplored area for managing agent performance and cost. To address this
gap, we conduct a comprehensive empirical study on SWE-bench using three
state-of-the-art models and evaluate the impact of three distinct turn-control
strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a
novel dynamic-turn strategy that grants extensions on-demand. Our findings
first reveal a fundamental trade-off in the unrestricted setting, where no
single model excels across performance, cost, and turn efficiency. We then show
that a fixed-turn limit, specifically at the 75th percentile of the baseline,
serves as a "sweet spot", substantially reducing costs (by 24%-68%) with
minimal impact on solve rates. Most significantly, the dynamic-turn strategy
consistently outperforms fixed-limit approaches, achieving comparable or better
solve rates while further reducing costs by an additional 12%-24% by
intelligently allocating resources only to tasks that need them. This work
provides the first systematic analysis of turn-control strategies, offering
simple yet effective guidelines for developers to balance cost and efficacy. We
demonstrate that dynamic resource allocation is a superior, easy-to-implement
approach for deploying powerful yet economically viable coding agents.

</details>


### [13] [When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation](https://arxiv.org/abs/2510.16823)
*Yue Liu,Zhenchang Xing,Shidong Pan,Chakkrit Tantithamthavorn*

Main category: cs.SE

TL;DR: The study shows that leading LLMs often generate insecure Chrome extensions, with high vulnerability rates especially in sensitive scenarios. The supposed sophistication of newer LLMs does not guarantee better security, highlighting a pressing gap between code generation ability and secure coding in LLMs.


<details>
  <summary>Details</summary>
Motivation: As LLMs increasingly automate software development, there is concern that developers may overlook hidden security vulnerabilities in generated code, especially when frameworks impose specific constraints. This study aims to address the gap in understanding LLMs' security performance in this setting.

Method: Developed ChromeSecBench (140 prompts from known vulnerable extensions); used these to instruct nine LLMs to generate Chrome extensions; analyzed the resulting extensions across scenarios, models, and vulnerability categories for security flaws.

Result: LLMs generated vulnerable Chrome extensions at rates ranging from 18% to 50%; vulnerabilities were particularly severe in authentication/identity (up to 83%) and cookie management (up to 78%) scenarios. Most flaws exposed sensitive user data to untrusted code. Advanced models were surprisingly more prone to generating vulnerabilities than simpler models.

Conclusion: LLMs generate framework-constrained software (Chrome extensions) with high rates of security vulnerabilities, particularly in key scenarios such as authentication and cookie management, and advanced models may perform worse than simpler ones in this context.

Abstract: In recent years, the AI wave has grown rapidly in software development. Even
novice developers can now design and generate complex framework-constrained
software systems based on their high-level requirements with the help of Large
Language Models (LLMs). However, when LLMs gradually "take the wheel" of
software development, developers may only check whether the program works. They
often miss security problems hidden in how the generated programs are
implemented.
  In this work, we investigate the security properties of framework-constrained
programs generated by state-of-the-art LLMs. We focus specifically on Chrome
extensions due to their complex security model involving multiple privilege
boundaries and isolated components. To achieve this, we built ChromeSecBench, a
dataset with 140 prompts based on known vulnerable extensions. We used these
prompts to instruct nine state-of-the-art LLMs to generate complete Chrome
extensions, and then analyzed them for vulnerabilities across three dimensions:
scenario types, model differences, and vulnerability categories. Our results
show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),
particularly in Authentication & Identity and Cookie Management scenarios (up
to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser
data like cookies, history, or bookmarks to untrusted code. Interestingly, we
found that advanced reasoning models performed worse, generating more
vulnerabilities than simpler models. These findings highlight a critical gap
between LLMs' coding skills and their ability to write secure
framework-constrained programs.

</details>


### [14] [Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection](https://arxiv.org/abs/2510.17056)
*Luis F. G. Campos,Leonardo C. Marques,Walter T. Nakamura*

Main category: cs.SE

TL;DR: Generative AI models can't fully replace human experts in usability inspections but can augment the process, discovering new issues and improving coverage. Teaming humans and AI gives the most effective results, pointing to a future of complementary use in software quality assurance.


<details>
  <summary>Details</summary>
Motivation: Usability inspection improves software quality by identifying interface issues, but is resource-intensive and requires specialized human expertise. Recent advances in artificial intelligence present opportunities to ease this process through generative models that may automate or enhance inspections.

Method: The study compares human and AI performance in usability inspections. A software prototype is evaluated by four experts and two generative AI models (GPT-4o and Gemini 2.5 Flash), using precision, recall, and F1-score as metrics. The inspectors' and AIs' reports are analyzed for effectiveness and overlap.

Result: Human inspectors showed highest precision and coverage, while AIs uncovered many new defects but produced more false positives and redundant reports. Combining human and AI inspections yielded the most comprehensive results, showing that they complement each other.

Conclusion: AI cannot currently replace human inspectors in usability inspections, but serves as an effective augmentation tool to improve efficiency and broaden defect coverage. Quantitative evidence supports complementary use of AI and humans in software quality assessment.

Abstract: Usability inspection is a well-established technique for identifying
interaction issues in software interfaces, thereby contributing to improved
product quality. However, it is a costly process that requires time and
specialized knowledge from inspectors. With advances in Artificial Intelligence
(AI), new opportunities have emerged to support this task, particularly through
generative models capable of interpreting interfaces and performing inspections
more efficiently. This study examines the performance of generative AIs in
identifying usability problems, comparing them to those of experienced human
inspectors. A software prototype was evaluated by four specialists and two AI
models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,
and F1-score. While inspectors achieved the highest levels of precision and
overall coverage, the AIs demonstrated high individual performance and
discovered many novel defects, but with a higher rate of false positives and
redundant reports. The combination of AIs and human inspectors produced the
best results, revealing their complementarity. These findings suggest that AI,
in its current stage, cannot replace human inspectors but can serve as a
valuable augmentation tool to improve efficiency and expand defect coverage.
The results provide evidence based on quantitative analysis to inform the
discussion on the role of AI in usability inspections, pointing to viable paths
for its complementary use in software quality assessment contexts.

</details>


### [15] [M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs](https://arxiv.org/abs/2510.17110)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: The paper introduces an MDD approach to automatically generate quantum code for multiple languages, improving efficiency and reliability in quantum system development, as shown by case studies.


<details>
  <summary>Details</summary>
Motivation: Quantum system engineering lacks structured design approaches, especially using Model-Driven Development (MDD), while quantum programming languages are proliferating.

Method: This paper proposes an MDD-based framework that allows structured design and automatic code generation for various quantum programming languages.

Result: The framework successfully generated quantum code for multiple QPLs and was validated with several case studies, showing increased efficiency and consistency.

Conclusion: Applying MDD to quantum systems enables more efficient and reliable development across different platforms, filling a gap in quantum software engineering.

Abstract: With the growing interest in quantum computing, the emergence of quantum
supremacy has marked a pivotal milestone in the field. As a result, numerous
quantum programming languages (QPLs) have been introduced to support the
development of quantum algorithms. However, the application of Model-Driven
Development (MDD) in quantum system engineering remains largely underexplored.
This paper presents an MDD-based approach to support the structured design and
implementation of quantum systems. Our framework enables the automatic
generation of quantum code for multiple QPLs, thereby enhancing development
efficiency and consistency across heterogeneous quantum platforms. The
effectiveness and practicality of our approach have been demonstrated through
multiple case studies.

</details>


### [16] [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](https://arxiv.org/abs/2510.17130)
*Shuzheng Gao,Chaozheng Wang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: SEER is a new framework for code generation with LLMs that improves reasoning diversity, quality assessment, and adaptability, overcoming the main drawbacks of previous Chain-of-Thought reasoning approaches.


<details>
  <summary>Details</summary>
Motivation: Despite improvements in code generation using Chain-of-Thought (CoT) reasoning with Large Language Models (LLMs), existing approaches still face notable challenges such as limited exploration of diverse reasoning paths, poor assessment of intermediate reasoning quality, and an increased risk of overcomplicating solutions. These limitations restrict generalization and reliability across programming tasks.

Method: The paper introduces SEER, a Self-Exploring deep Reasoning framework that frames CoT code generation as a decision-making problem. SEER includes three main components: diverse reasoning path exploration without expert or proprietary intervention, reasoning quality-aware model training involving policy and value models, and adaptive CoT reasoning that flexibly alternates between direct generation and step-by-step approaches.

Result: SEER enables more accurate and adaptive code generation reasoning, by systematically exploring diverse reasoning paths, quantitatively evaluating the quality of intermediate steps, and adapting the reasoning strategy to each problem. This results in improved code generation performance and reliability compared to previous CoT approaches.

Conclusion: SEER effectively addresses key limitations in CoT code generation by providing diverse reasoning exploration, higher reliability through quality assessments, and adaptive reasoning strategies, thereby enhancing the overall effectiveness and generalization of LLM-based code generation.

Abstract: Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

</details>


### [17] [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](https://arxiv.org/abs/2510.17142)
*Xiaoxue Ren,Jun Wan,Yun Peng,Zhongxin Liu,Ming Liang,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: Peace is a new framework for optimizing Python projects' code efficiency using automated code editing. Unlike previous methods that only work at the function level, Peace optimizes entire projects while keeping them correct and intact. Benchmark tests show Peace works significantly better than existing approaches, especially on tougher, multi-function optimizations.


<details>
  <summary>Details</summary>
Motivation: Previous LLM-based approaches to code efficiency optimization are limited to function-level optimization, ignoring interactions between functions, and are not robust in real-world project scenarios. Additionally, project-level code editing faces issues like invalid edits and suboptimal internal functions. There is a need for a framework that can perform project-level optimization while maintaining code correctness and integrity.

Method: The paper proposes Peace, a hybrid framework for project-level code efficiency optimization via automatic code editing. Peace consists of three phases: (1) constructing dependency-aware optimizing function sequences, (2) identifying valid associated code edits, and (3) iteratively performing efficiency optimization editing. The effectiveness of Peace is evaluated using PeacExec, a new benchmark with 146 real-world tasks from 47 popular GitHub Python projects, featuring thorough test cases and executable setups.

Result: Peace achieves a 69.2% correctness rate (pass@1), a 46.9% increase in optimization rate, and a 0.840 speedup in execution efficiency, outperforming state-of-the-art baselines, especially in more complex, multi-function optimization tasks. The experiments also verify the contributions and rationale of each component within Peace.

Conclusion: Peace is a robust solution for project-level code optimization: it not only surpasses previous methods in accuracy and performance but also maintains project correctness and integrity. The framework's effectiveness and design have been thoroughly validated through extensive empirical evaluation.

Abstract: Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

</details>


### [18] [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](https://arxiv.org/abs/2510.17163)
*Shuzheng Gao,Eric John Li,Man Ho Lam,Jingyu Xiao,Yuxuan Wan,Chaozheng Wang,Ng Man Tik,Michael R. Lyu*

Main category: cs.SE

TL;DR: The paper presents TREAT, a comprehensive framework for evaluating the trustworthiness and reliability of code LLMs across various languages, modalities, and tasks. Assessing 26 models, it reveals strengths, weaknesses, and specific challenges in multi-modal scenarios, offering valuable insights for model development and use.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is the lack of comprehensive evaluation methods for large code foundation models. Current benchmarks are limited in scope and do not sufficiently address robustness, reliability, or real-world coding scenarios, which are essential for trusting these models in practical software engineering tasks.

Method: The paper introduces a new evaluation framework named TREAT (Code LLMs Trustworthiness / Reliability Evaluation And Testing). The method includes multi-task, multi-language, and multi-modality evaluations, robustness checks under code transformations, and a rigorous, adaptive methodology for assessing trustworthiness and reliability.

Result: Using the TREAT framework, the authors assessed 26 state-of-the-art models and found notable performance variations across different programming tasks. They also identified limitations in multi-modal models, particularly in UI code generation and editing tasks.

Conclusion: The TREAT framework addresses the deficiencies of existing benchmarks and enables a holistic, multi-dimensional evaluation of code-focused large language models. This comprehensive approach uncovers important strengths and weaknesses, guiding both model development and deployment in software engineering.

Abstract: Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

</details>


### [19] [Software Testing with Large Language Models: An Interview Study with Practitioners](https://arxiv.org/abs/2510.17164)
*Maria Deolinda Santana,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: Software testers are increasingly using LLMs through iterative and cautious practices, emphasizing the need for human validation. The study offers early guidelines for LLM integration and highlights the importance of structured approaches and future research.


<details>
  <summary>Details</summary>
Motivation: The growing use of LLMs in software testing lacks structured guidance, with practitioners relying on informal experimentation. The study aims to understand real-world usage and develop preliminary guidelines for integration.

Method: Qualitative study with 15 software testers from diverse roles and domains; data collected via semi-structured interviews and analyzed using grounded theory and thematic analysis.

Result: Testers employ an iterative process involving prompt engineering, refinement, output evaluation, and learning. Human oversight and validation are necessary due to LLM limitations such as hallucinations and inconsistent reasoning.

Conclusion: LLM adoption in software testing is increasing but depends on cautious, evolving practices, with human oversight being crucial due to model limitations. The study provides an initial, practitioner-informed guideline for integrating LLMs into testing workflows and calls for further research to refine these practices.

Abstract: \textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

</details>


### [20] [OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development](https://arxiv.org/abs/2510.17184)
*Nicolas Robert,Fabien Gandon,Maxime Lefrançois*

Main category: cs.SE

TL;DR: OLIVAW is a tool that integrates ontology development with GitHub workflows, supporting continuous validation and agile ontology evolution. Its effectiveness and generality were confirmed across multiple projects.


<details>
  <summary>Details</summary>
Motivation: Continuous validation and user-driven evolution of ontologies are necessary to keep them aligned with developers' changing requirements during software development.

Method: OLIVAW, a tool designed around the ACIMOV methodology, uses W3C Standards and integrates with GitHub via Composite Actions, pre-commit hooks, or CLI, to support modular ontology development.

Result: OLIVAW was tested on several ontology projects and demonstrated usefulness, genericity, and reusability. Additionally, a template repository was provided for quick setup.

Conclusion: OLIVAW enables agile, collaborative, and validated ontology development workflows, ensuring ontologies stay relevant and effective throughout project lifecycles.

Abstract: Agile and collaborative approaches to ontologies design are crucial because
they contribute to making them userdriven, up-to-date, and able to evolve
alongside the systems they support, hence proper continuous validation tooling
is required to ensure ontologies match developers' requirements all along their
development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV
Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C
Standards to assist the development of modular ontologies through GitHub
Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was
tested on several ontology projects to ensure its usefulness, genericity and
reusability. A template repository is available for a quick start. OLIVAW is

</details>


### [21] [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](https://arxiv.org/abs/2510.17376)
*Yongmin Li,Jia Li,Ge Li,Zhi Jin*

Main category: cs.SE

TL;DR: Current constrained decoding for language model-based code generation can cause outputs to meet constraints but lose intended meaning. AdapTrack adds backtracking, leading to code that both respects constraints and stays true to the model's intent, achieving significant improvements on several benchmarks and aligning better with the model’s natural distribution.


<details>
  <summary>Details</summary>
Motivation: Modern code generation tools using language models often fail to meet required constraints like syntax correctness or valid API usage. Traditional constrained decoding fixes this by eliminating rule-breaking options during generation, but this often distorts the model's intended output, leading to incorrect code. The paper aims to solve this dilemma: producing correct code without sacrificing the original intent generated by the model.

Method: The paper introduces AdapTrack, a technique that incorporates backtracking into code generation. Instead of purely greedy constraint application, AdapTrack allows the generation process to revisit previous steps, helping maintain the original semantics and output intent of the model, while still meeting constraints. The authors present both theoretical proof and empirical evaluation of its alignment with the true model distribution.

Result: AdapTrack significantly outperforms traditional constrained decoding on multiple benchmarks. On synthetic API completion, AdapTrack achieves up to 360.87% improvement; on real-world datasets, up to 38.93% improvement; and on standard code generation benchmarks like HumanEval and MBPP, up to 7.84% and 6.42% improvement, respectively. Experimental results further show AdapTrack produces model-aligned outputs, validated by theoretical analysis.

Conclusion: AdapTrack successfully bridges the gap between constraint satisfaction and preservation of the model's output intent. It offers marked performance improvements over existing constrained decoding techniques, both in accuracy and intent alignment, and its theoretical grounding shows promise for broader application in language model-based code generation.

Abstract: Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

</details>


### [22] [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](https://arxiv.org/abs/2510.17430)
*Kuniaki Kudo,Sherine Devi*

Main category: cs.SE

TL;DR: The paper proposes a scalable CI/CD pipeline using modern tools (GitHub, Jenkins, AWS, Docker) to tackle Japan's legacy IT system challenges, aiming to minimize maintenance costs and support digital transformation, with demonstrated benefits for developers and organizations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the '2025 Japan Cliff' issue, where legacy IT systems in Japan face mass end-of-service, resulting in rising maintenance costs and difficulties in updating or replacing outdated systems. This threatens both business continuity and digital transformation efforts.

Method: The method involves developing and implementing a scalable CI/CD pipeline that utilizes GitHub for source control, Jenkins for automation, AWS for dynamic and scalable environments, and Docker for containerization. Isolated environments are created and removed as needed, supporting safe and efficient development and maintenance.

Result: The result is a flexible and automated development pipeline enabling developers to safely test, update, and experiment with legacy systems. This approach is expected to reduce maintenance costs, facilitate modernization of legacy IT, and support ongoing digital transformation.

Conclusion: Implementing a Scalable CI/CD Pipeline significantly alleviates challenges posed by legacy core IT systems. It improves maintainability, reduces costs, and advances digital transformation by providing developers with modern, isolated, and scalable environments.

Abstract: We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [23] [Latency Based Tiling](https://arxiv.org/abs/2510.15912)
*Jack Cashman*

Main category: cs.PL

TL;DR: Latency Based Tiling quickly estimates cache sizes using latency spikes, then tiles loops accordingly for efficient memory usage without slow auto-tuning. Rust implementation ensures portability and safety.


<details>
  <summary>Details</summary>
Motivation: Traditional auto-tuning methods for loop tiling in compilers, while effective at optimizing locality, are often very slow at compile time. Additionally, existing approaches may not efficiently capture memory hierarchy and working set behaviors, especially in systems with shared caches. There is a need for a faster, portable, and memory-safe tiling system that adapts to real hardware constraints.

Method: The proposed Latency Based Tiling uses triangular loops to analyze and capture miss ratio scaling—a measure of how memory access latency increases with working set size. This approach identifies sharp increases in latency that correspond to cache size limits (L1, L2, L3), providing estimates for memory hierarchy levels. These estimates are used for tiling loop nests in the polyhedral model. The system is implemented in Rust, which enables hardware agnosticism and memory safety.

Result: Latency Based Tiling achieves fast compile times with negligible overhead compared to traditional auto-tuning, maintains locality, and provides effective tiling based on dynamically observed cache effects. The generated tile sizes are portable under-approximations matching practical system constraints, and the Rust implementation ensures robust portability and safety.

Conclusion: Latency Based Tiling offers a practical, efficient, and portable solution to loop tiling by analyzing latency and miss ratio scaling to uncover hardware cache limits. It eliminates the need for slow auto-tuning, makes informed tiling decisions, and can run on any system supporting Rust.

Abstract: Latency Based Tiling provides a systems based approach to deriving
approximate tiling solution that maximizes locality while maintaining a fast
compile time. The method uses triangular loops to characterize miss ratio
scaling of a machine avoiding prefetcher distortion. Miss ratio scaling
captures the relationship between data access latency and working set size with
sharp increases in latency indicating the data footprint exceeds capacity from
a cache level. Through these noticeable increases in latency we can determine
an approximate location for L1, L2, and L3 memory sizes. These sizes are
expected to be under approximations of a systems true memory sizes which is in
line with our expectations given the shared nature of cache in a multi process
system as described in defensive loop tiling. Unlike auto tuning, which can be
effective but prohibitively slow, Latency Based Tiling achieves negligible
compile time overhead. The implementation in Rust enables a hardware agnostic
approach which combined with a cache timing based techniques, yields a
portable, memory safe system running wherever Rust is supported. The tiling
strategy is applied to a subset of the polyhedral model, where loop nestings
are tiled based on both the derived memory hierarchy and the observed data
footprint per iteration.

</details>


### [24] [Typing Strictness (Extended Version)](https://arxiv.org/abs/2510.16133)
*Daniel Sainati,Joseph W. Cutler,Benjamin C. Pierce,Stephanie Weirich*

Main category: cs.PL

TL;DR: This paper refines strictness analysis in lazy languages by introducing a more precise type-theoretic definition, mechanizing proofs that connect static analysis to runtime behavior and supporting translation between evaluation strategies.


<details>
  <summary>Details</summary>
Motivation: Strictness analysis is essential for optimizing non-strict (lazy) programming languages, but traditional approaches are difficult and imprecise for source-level reasoning. The work aims to improve precision and usability of strictness analysis.

Method: The authors develop type-theoretic foundations in call-by-name and call-by-push-value paradigms, prove results via logical relations, and mechanize proofs in Rocq.

Result: The new definition enables precise tracking of variable usage, proves accurate reflection of runtime behavior through logical relations, and provides a translation retaining strictness annotations across evaluation strategies.

Conclusion: The paper establishes a refined definition of strictness at the type-theoretic level, supported by logical relations and mechanized proofs, and demonstrates accurate correspondence between static and dynamic variable usage.

Abstract: Strictness analysis is critical to efficient implementation of languages with
non-strict evaluation, mitigating much of the performance overhead of laziness.
However, reasoning about strictness at the source level can be challenging and
unintuitive. We propose a new definition of strictness that refines the
traditional one by describing variable usage more precisely. We lay
type-theoretic foundations for this definition in both call-by-name and
call-by-push-value settings, drawing inspiration from the literature on type
systems tracking effects and coeffects. We prove via a logical relation that
the strictness attributes computed by our type systems accurately describe the
use of variables at runtime, and we offer a strictness-annotation-preserving
translation from the call-by-name system to the call-by-push-value one. All our
results are mechanized in Rocq.

</details>


### [25] [SimpliPy: A Source-Tracking Notional Machine for Simplified Python](https://arxiv.org/abs/2510.16594)
*Moida Praneeth Jain,Venkatesh Choppella*

Main category: cs.PL

TL;DR: SimpliPy is an educational tool based on a simplified Python semantics. It combines dynamic and static analyses to make control flow and scoping clearer for beginners, with a visual debugger and formal underpinnings that foster better code comprehension.


<details>
  <summary>Details</summary>
Motivation: Novice programmers struggle with misconceptions about program execution, particularly regarding control flow and scoping. Existing tools and teaching methods often fail to make the connection between code and its behavior sufficiently clear.

Method: The authors developed SimpliPy, a notional machine based on a simplified Python subset with a precise operational semantics that tracks source code line numbers at each execution step. The system uses static analysis to generate Control Flow Graphs (CFGs) and identify lexical scopes, aiding structural understanding prior to dynamic tracing. An interactive web debugger visualizes operational state and animates control flow directly on the CFG during step-by-step execution.

Result: SimpliPy integrates formal semantics, program analysis, and visualization, allowing students to clearly observe the relationship between code and behavior. The tool’s debugger visualizes environments and control flow, helping learners understand execution and scoping in Python.

Conclusion: SimpliPy provides a practical pedagogical approach and demonstration of formal methods for program understanding, bridging the gap between theory and practice for novice programmers.

Abstract: Misconceptions about program execution hinder many novice programmers. We
introduce SimpliPy, a notional machine designed around a carefully chosen
Python subset to clarify core control flow and scoping concepts. Its foundation
is a precise operational semantics that explicitly tracks source code line
numbers for each execution step, making the link between code and behavior
unambiguous. Complementing the dynamic semantics, SimpliPy uses static analysis
to generate Control Flow Graphs (CFGs) and identify lexical scopes, helping
students build a structural understanding before tracing. We also present an
interactive web-based debugger built on these principles. This tool embodies
the formal techniques, visualizing the operational state (environments, stack)
and using the static CFG to animate control flow directly on the graph during
step-by-step execution. SimpliPy thus integrates formal semantics, program
analysis, and visualization to offer both a pedagogical approach and a
practical demonstration of applying formal methods to program understanding.

</details>


### [26] [JAX Autodiff from a Linear Logic Perspective (Extended Version)](https://arxiv.org/abs/2510.16883)
*Giulia Giusti,Michele Pagani*

Main category: cs.PL

TL;DR: The paper shows Autodiff can be encoded in linear lambda calculus, grounding it in linear logic and proving this encoding is both correct and preserves computational cost; it also reveals an aspect of current implementations (unzipping) is optional.


<details>
  <summary>Details</summary>
Motivation: Current formalizations of Autodiff, as in JAX and Dex, use a linear typed calculus that is narrowly focused and its alignment with broader substructural logics is unclear. The paper aims to place Autodiff in a more general and theoretically meaningful linear logic context.

Method: The authors encode Autodiff into a linear lambda calculus that corresponds to Girard's linear logic via Curry-Howard correspondence, then prove soundness both qualitatively (semantic equivalence) and quantitatively (work cost preservation).

Result: The encoding into linear lambda calculus is shown to be both qualitatively correct (terms are extensionally equivalent) and quantitatively faithful (preserves original cost), and demonstrates that the 'unzipping' transformation used in backpropagation is optional.

Conclusion: Autodiff can be fully and soundly encoded in a general linear lambda calculus, providing a broader foundational basis for automatic differentiation and showing that some implementation steps, like unzipping, are not necessary.

Abstract: Autodiff refers to the core of the automatic differentiation systems
developed in projects like JAX and Dex. Autodiff has recently been formalised
in a linear typed calculus by Radul et al in arXiv:2204.10923. Although this
formalisation suffices to express the main program transformations of Autodiff,
the calculus is very specific to this task, and it is not clear whether the
type system yields a substructural logic that has interest on its own.
  We propose an encoding of Autodiff into a linear $\lambda$-calculus that
enjoys a Curry-Howard correspondence with Girard's linear logic. We prove that
the encoding is sound both qualitatively (the encoded terms are extensionally
equivalent to the original ones) and quantitatively (the encoding preserves the
original work cost as described in arXiv:2204.10923). As a byproduct, we show
that unzipping, one of the transformations used to implement backpropagation in
Autodiff, is, in fact, optional.

</details>


### [27] [Introducing Linear Implication Types to $λ_{GT}$ for Computing With Incomplete Graphs](https://arxiv.org/abs/2510.17429)
*Jin Sano,Naoki Yamamoto,Kazunori Ueda*

Main category: cs.PL

TL;DR: This paper improves the λ_GT functional programming language's type system to safely support manipulation of graphs—including incomplete ones—without dynamic checks, by using linear implication and soundness constraints.


<details>
  <summary>Details</summary>
Motivation: Conventional destructive memory operations with pointers are error-prone and complex. Existing type systems offer some safety but still lack high-level declarative languages for manipulating complex pointer-based data structures easily and safely.

Method: The paper proposes enhancements to the type system of the λ_GT functional programming language by incorporating linear implication and introducing new constraints for soundness, aiming to support incomplete graphs and eliminate reliance on dynamic type checking.

Result: The improved type system supports incomplete graphs and removes the need for dynamic type checking by ensuring soundness with new constraints.

Conclusion: Incorporating linear implication into the λ_GT type system with new constraints resolves key shortcomings, enabling safe and static manipulation of graph-based data structures in a declarative manner.

Abstract: Designing programming languages that enable intuitive and safe manipulation
of data structures is a critical research challenge. Conventional destructive
memory operations using pointers are complex and prone to errors. Existing type
systems, such as affine types and shape types, address this problem towards
safe manipulation of heaps and pointers, but design of high-level declarative
languages that allow us to manipulate complex pointer data structures at a
higher level of abstraction is largely an open problem. The $\lambda_{GT}$
language, a purely functional programming language that treats hypergraphs
(hereafter referred to as graphs) as primary data structures, addresses some of
these challenges. By abstracting data with shared references and cycles as
graphs, it enables declarative operations through pattern matching and
leverages its type system to guarantee safety of these operations.
Nevertheless, the previously proposed type system of $\lambda_{GT}$ leaves two
significant open challenges. First, the type system does not support
\emph{incomplete graphs}, that is, graphs in which some elements are missing
from the graphs of user-defined types. Second, the type system relies on
dynamic type checking during pattern matching. This study addresses these two
challenges by incorporating linear implication into the $\lambda_{GT}$ type
system, while introducing new constraints to ensure its soundness.

</details>


### [28] [Insum: Sparse GPU Kernels Simplified and Optimized with Indirect Einsums](https://arxiv.org/abs/2510.17505)
*Jaeyeon Won,Willow Ahrens,Joel S. Emer,Saman Amarasinghe*

Main category: cs.PL

TL;DR: This paper presents a new method and compiler for more efficient and easier GPU programming of sparse computations, outclassing existing methods in speed and simplicity.


<details>
  <summary>Details</summary>
Motivation: Programming high-performance sparse GPU kernels is complex and demands significant expertise. Existing sparse compilers are CPU-focused and inefficient for GPU code, particularly failing to optimize mixed sparse-dense computations.

Method: The paper introduces a new way to express sparse computations using format-agnostic Einsums for sparse tensors, which are rewritten into indirect Einsums that incorporate format information through indirect indexing. A compiler called Insum is proposed to generate optimized GPU code for these operations by extending the capabilities of the PyTorch compiler, and two new sparse formats, GroupCOO and BlockGroupCOO, are designed for compatibility.

Result: Their approach produces notable improvements: achieving 1.14x to 3.81x speedups in sparse GPU applications and reducing code size by 202x to 4491x compared to manual implementations.

Conclusion: The proposed format-conscious indirect Einsum approach and the Insum compiler significantly enhance the performance and reduce the complexity of programming sparse GPU kernels, addressing previous limitations of sparse compilers.

Abstract: Programming high-performance sparse GPU kernels is notoriously difficult,
requiring both substantial effort and deep expertise. Sparse compilers aim to
simplify this process, but existing systems fall short in two key ways. First,
they are primarily designed for CPUs and rarely produce high-performance GPU
code. Second, when computations involve both sparse and dense regions, these
compilers often fail to optimize the dense portions effectively. In this paper,
we propose a new approach for expressing sparse computations. We start from
format-agnostic Einsums over sparse tensors and rewrite them into
format-conscious indirect Einsums, which explicitly encode format information
by mapping sparse data and metadata onto dense tensor operations through
indirect indexing. To execute indirect Einsums, we introduce the Insum
compiler, which generates efficient GPU code for these Einsums by lowering to
the PyTorch compiler, extended to better support Tensor Core-enabled indirect
Einsums. We also present two fixed-length sparse formats, GroupCOO and
BlockGroupCOO, designed to fit naturally with indirect Einsums. Our approach
achieves 1.14x to 3.81x speedups across a range of sparse GPU applications
while reducing lines of code by 202x to 4491x compared to hand-written
implementations.

</details>
