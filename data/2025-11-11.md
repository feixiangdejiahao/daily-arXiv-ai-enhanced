<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 27]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [High-Performance Generation of Constrained Input](https://arxiv.org/abs/2511.05987)
*Addison Crump,Alexi Turcotte,José Antonio Zamudio Amaya,Andreas Zeller*

Main category: cs.SE

TL;DR: This paper presents FANDANGO-RS, a new language-based testing tool that uses Rust and advanced evolutionary algorithms to efficiently generate complex, constraint-satisfying test inputs. It is dramatically faster than previous solutions and can handle problems that older methods could not, making it highly suitable for challenging domains like compiler testing.


<details>
  <summary>Details</summary>
Motivation: Language-based testing, which pairs context-free grammars with semantic constraints, provides a powerful framework for generating complex test inputs. However, the challenge is efficiently generating inputs that satisfy both the grammar and constraints, especially for demanding applications such as compiler testing. Previous solutions using SMT solvers are slow, while evolutionary algorithms, though more general, struggle with complex constraints.

Method: The paper introduces an improved evolutionary language-based testing approach. It transforms grammar definitions into Rust types and implements Rust traits, enabling highly optimized operations and leveraging the Rust compiler’s performance. The method also utilizes advanced evolutionary algorithms that better handle complex semantic constraints.

Result: The prototype system, FANDANGO-RS, achieves a dramatic 3-4 orders of magnitude speedup over prior approaches, reducing test input generation from hours to seconds. It successfully generates 401 diverse and valid test inputs per minute for a C compiler, outperforming existing methods, especially for complex constraints.

Conclusion: Novel transformations of grammars to Rust code and better evolutionary algorithms significantly improve the efficiency and capability of language-based testing, enabling it to solve complex constraints previously infeasible for such approaches. The proposed system, FANDANGO-RS, demonstrates these advances in a realistic compiler-testing scenario.

Abstract: Language-based testing combines context-free grammar definitions with semantic constraints over grammar elements to generate test inputs. By pairing context-free grammars with constraints, users have the expressiveness of unrestricted grammars while retaining simple structure. However, producing inputs in the presence of such constraints can be challenging. In past approaches, SMT solvers have been found to be very slow at finding string solutions; evolutionary algorithms are faster and more general, but current implementations still struggle with complex constraints that would be required for domains such as compiler testing. In this paper, we present a novel approach for evolutionary language-based testing that improves performance by 3-4 orders of magnitude over the current state of the art, reducing hours of generation and constraint solving time to seconds. We accomplish this by (1) carefully transforming grammar definitions into Rust types and trait implementations, ensuring that the compiler may near-maximally optimize arbitrary operations on arbitrary grammars; and (2) using better evolutionary algorithms that improve the ability of language-based testing to solve complex constraint systems. These performance and algorithmic improvements allow our prototype, FANDANGO-RS, to solve constraints that previous strategies simply cannot handle. We demonstrate this by a case study for a C subset, in which FANDANGO-RS is able to generate 401 diverse, complex, and valid test inputs for a C compiler per minute.

</details>


### [2] [LLMs as Packagers of HPC Software](https://arxiv.org/abs/2511.05626)
*Caetano Melone,Daniel Nichols,Konstantinos Parasyris,Todd Gamblin,Harshitha Menon*

Main category: cs.SE

TL;DR: SpackIt leverages LLMs, context augmentation, and feedback to automate Spack recipe generation for HPC software, boosting installation success rates and alleviating maintenance burdens.


<details>
  <summary>Details</summary>
Motivation: Maintaining and creating build recipes for HPC software is labor-intensive due to the complexity of heterogeneous ecosystems and manual dependency management.

Method: The authors introduce SpackIt, an end-to-end framework combining repository analysis, retrieval of relevant examples, and iterative refinement through diagnostic feedback. They systematically analyze how large language models (LLMs) and context augmentation assist in generating Spack recipes.

Result: Applying SpackIt to 308 open-source HPC packages, installation success improved from 20% in zero-shot settings to over 80% with the best configuration, showing the framework's effectiveness.

Conclusion: Retrieval of examples and structured diagnostic feedback empower LLMs to reliably generate and maintain Spack build recipes, significantly reducing manual labor.

Abstract: High performance computing (HPC) software ecosystems are inherently heterogeneous, comprising scientific applications that depend on hundreds of external packages, each with distinct build systems, options, and dependency constraints. Tools such as Spack automate dependency resolution and environment management, but their effectiveness relies on manually written build recipes. As these ecosystems grow, maintaining existing specifications and creating new ones becomes increasingly labor-intensive. While large language models (LLMs) have shown promise in code generation, automatically producing correct and maintainable Spack recipes remains a significant challenge. We present a systematic analysis of how LLMs and context-augmentation methods can assist in the generation of Spack recipes. To this end, we introduce SpackIt, an end-to-end framework that combines repository analysis, retrieval of relevant examples, and iterative refinement through diagnostic feedback. We apply SpackIt to a representative subset of 308 open-source HPC packages to assess its effectiveness and limitations. Our results show that SpackIt increases installation success from 20% in a zero-shot setting to over 80% in its best configuration, demonstrating the value of retrieval and structured feedback for reliable package synthesis.

</details>


### [3] [Accelerating Control Systems with GitOps: A Path to Automation and Reliability](https://arxiv.org/abs/2511.05663)
*M. Gonzalez,M. Acosta*

Main category: cs.SE

TL;DR: Modern software paradigms like GitOps and containerization are being adopted to automate and enhance control systems at major research facilities, illustrated by the ACORN project at Fermilab.


<details>
  <summary>Details</summary>
Motivation: To modernize control system infrastructure and software at accelerator and science facilities, improving automation, auditing, and version control, as well as enabling AI/ML integration.

Method: The abstract surveys adoption of GitOps, containerization, infrastructure as code, and data pipelines, referencing their implementation in the ACORN project at Fermilab.

Result: Early adoption by leading institutes like CERN, Diamond Light Source, and Fermilab demonstrates the benefits of transitioning to cloud-native, automated infrastructure paradigms.

Conclusion: GitOps and related modern software practices significantly enhance the manageability, reliability, and scalability of control system infrastructures at research facilities.

Abstract: GitOps is a foundational approach for modernizing infrastructure by leveraging Git as the single source of truth for declarative configurations. The poster explores how GitOps transforms traditional control system infrastructure, services and applications by enabling fully automated, auditable, and version-controlled infrastructure management. Cloud-native and containerized environments are shifting the ecosystem not only in the IT industry but also within the computational science field, as is the case of CERN [1] and Diamond Light Source [2] among other Accelerator/Science facilities which are slowly shifting towards modern software and infrastructure paradigms. The ACORN project, which aims to modernize Fermilab's control system infrastructure and software is implementing proven best-practices and cutting-edge technology standards including GitOps, containerization, infrastructure as code and modern data pipelines for control system data acquisition and the inclusion of AI/ML in our accelerator complex.

</details>


### [4] [An Empirical Study of Java Code Improvements Based on Stack Overflow Answer Edits](https://arxiv.org/abs/2511.05813)
*In-on Wiratsin,Chaiyong Ragkhitwetsagul,Matheus Paixao,Denis De Sousa,Pongpop Lapvikai,Peter Haddawy*

Main category: cs.SE

TL;DR: The paper finds that a notable fraction of edited Stack Overflow Java answers provide improvements that can be successfully applied to open-source software, with maintainers accepting a substantial number. Continuous mining of SO code edits can help address technical debt and enhance code quality in real projects.


<details>
  <summary>Details</summary>
Motivation: Suboptimal code is common in software systems, often due to knowledge gaps or external pressures, leading to maintenance issues and technical debt. Developers frequently seek help from external sources like Stack Overflow for solutions.

Method: Conducted an empirical study analyzing edits in Stack Overflow Java answers, using a modified code clone search tool to link SO code revisions with open-source Java project code. Manually categorized edits and proposed code improvements via GitHub pull requests.

Result: 6.91% of Java answers on SO had multiple revisions; 49.24% of edited SO code snippets could be applied to open-source projects. Of 36 bug fix suggestions submitted to projects, 11 were accepted by maintainers.

Conclusion: Stack Overflow answer edits can improve real-world code: a significant proportion of updated SO code is applicable to open-source projects, and maintainers accept these improvements. Mining evolving SO answers is a practical way to enhance software quality.

Abstract: Suboptimal code is prevalent in software systems. Developers often write low-quality code due to factors like technical knowledge gaps, insufficient experience, time pressure, management decisions, or personal factors. Once integrated, the accumulation of this suboptimal code leads to significant maintenance costs and technical debt.
  Developers frequently consult external knowledge bases, such as API documentation and Q&A websites like Stack Overflow (SO), to aid their programming tasks. SO's crowdsourced, collaborative nature has created a vast repository of programming knowledge. Its community-curated content is constantly evolving, with new answers posted or existing ones edited.
  In this paper, we present an empirical study of SO Java answer edits and their application to improving code in open-source projects. We use a modified code clone search tool to analyze SO code snippets with version history and apply it to open-source Java projects. This identifies outdated or unoptimized code and suggests improved alternatives. Analyzing 140,840 Java accepted answers from SOTorrent and 10,668 GitHub Java projects, we manually categorized SO answer edits and created pull requests to open-source projects with the suggested code improvements. Our results show that 6.91% of SO Java accepted answers have more than one revision (average of 2.82). Moreover, 49.24% of the code snippets in the answer edits are applicable to open-source projects, and 11 out of 36 proposed bug fixes based on these edits were accepted by the GitHub project maintainers.

</details>


### [5] [WAR-Re: Web API Recommendation with Semantic Reasoning](https://arxiv.org/abs/2511.05820)
*Zishuo Xu,Dezhong Yao,Yao Wan*

Main category: cs.SE

TL;DR: WAR-Re is a novel LLM-based Web API recommendation model that provides variable-sized outputs and well-justified explanations, achieving significantly higher accuracy than previous solutions.


<details>
  <summary>Details</summary>
Motivation: The paper identifies two main challenges in Web API recommendation: (1) fixed top-N recommendations that cannot adapt to different mashup requirements, and (2) lack of reasoning or explanations behind recommended APIs, limiting user understanding.

Method: The proposed solution, WAR-Re, is an LLM-based model that introduces special start and stop tokens to generate recommendations of varying sizes. It employs a two-stage training process: supervised fine-tuning and reinforcement learning using Group Relative Policy Optimization (GRPO). It also generates semantic justifications for its recommendations.

Result: WAR-Re outperforms the state-of-the-art baseline model with an accuracy improvement of up to 21.59% and consistently provides high-quality, meaningful explanations for its recommendations.

Conclusion: WAR-Re successfully addresses key limitations in Web API recommendation by offering flexible recommendation sizes and actionable semantic reasoning, leading to enhanced accuracy and user understanding.

Abstract: With the development of cloud computing, the number of Web APIs has increased dramatically, further intensifying the demand for efficient Web API recommendation. Despite the demonstrated success of previous Web API recommendation solutions, two critical challenges persist: 1) a fixed top-N recommendation that cannot accommodate the varying API cardinality requirements of different mashups, and 2) these methods output only ranked API lists without accompanying reasons, depriving users of understanding the recommendation. To address these challenges, we propose WAR-Re, an LLM-based model for Web API recommendation with semantic reasoning for justification. WAR-Re leverages special start and stop tokens to handle the first challenge and uses two-stage training: supervised fine-tuning and reinforcement learning via Group Relative Policy Optimization (GRPO) to enhance the model's ability in both tasks. Comprehensive experimental evaluations on the ProgrammableWeb dataset demonstrate that WAR-Re achieves a gain of up to 21.59\% over the state-of-the-art baseline model in recommendation accuracy, while consistently producing high-quality semantic reasons for recommendations.

</details>


### [6] [ZeroLog: Zero-Label Generalizable Cross-System Log-based Anomaly Detection](https://arxiv.org/abs/2511.05862)
*Xinlong Zhao,Tong Jia,Minghua He,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: This paper introduces ZeroLog, which enables accurate log-based anomaly detection in new systems with no labeled data by using meta-learning and domain adaptation. It achieves results on par with methods requiring labeled logs, outperforming all rivals in zero-label scenarios.


<details>
  <summary>Details</summary>
Motivation: Log-based anomaly detection is vital for system stability and reliability, but there is usually a lack of labeled logs in new or target systems. Existing transfer learning methods still require some labeled logs from the target system, limiting their applicability.

Method: The paper proposes ZeroLog, a system-agnostic meta-learning method for zero-label cross-system log-based anomaly detection. It combines unsupervised domain adaptation with adversarial training to learn generalizable feature representations, and employs meta-learning to ensure zero-label adaptation to the target system.

Result: ZeroLog achieves over 80% F1-score on three public log datasets without using any labels from the target system. Its performance is comparable to state-of-the-art methods that rely on labeled logs, and it outperforms other methods in zero-label settings.

Conclusion: ZeroLog is effective for log-based anomaly detection in new systems where no labeled logs are available, successfully generalizing from mature systems and outperforming existing solutions under zero-label conditions.

Abstract: Log-based anomaly detection is an important task in ensuring the stability and reliability of software systems. One of the key problems in this task is the lack of labeled logs. Existing works usually leverage large-scale labeled logs from mature systems to train an anomaly detection model of a target system based on the idea of transfer learning. However, these works still require a certain number of labeled logs from the target system. In this paper, we take a step forward and study a valuable yet underexplored setting: zero-label cross-system log-based anomaly detection, that is, no labeled logs are available in the target system. Specifically, we propose ZeroLog, a system-agnostic representation meta-learning method that enables cross-system log-based anomaly detection under zero-label conditions. To achieve this, we leverage unsupervised domain adaptation to perform adversarial training between the source and target domains, aiming to learn system-agnostic general feature representations. By employing meta-learning, the learned representations are further generalized to the target system without any target labels. Experimental results on three public log datasets from different systems show that ZeroLog reaches over 80% F1-score without labels, comparable to state-of-the-art cross-system methods trained with labeled logs, and outperforms existing methods under zero-label conditions.

</details>


### [7] [Generality Is Not Enough: Zero-Label Cross-System Log-Based Anomaly Detection via Knowledge-Level Collaboration](https://arxiv.org/abs/2511.05882)
*Xinlong Zhao,Tong Jia,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: GeneralLog is a new system that combines LLMs and small models to detect log anomalies across different software systems without any labeled data, achieving high accuracy and outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Log-based anomaly detection is important for software system stability, but the scarcity of labeled logs in new systems makes rapid deployment and adaptation challenging. Existing methods have trade-offs in effectiveness, adaptability, and computational cost, particularly in zero-label (no labeled data) cross-system scenarios.

Method: The paper proposes GeneralLog, a collaborative framework combining large language models (LLMs) and small models. GeneralLog dynamically routes logs: LLMs process 'proprietary logs' that are unique to the target system, while small models handle 'general logs.' This routing is designed for zero-label environments using knowledge separation rather than uncertainty or supervised routing.

Result: Experiments on three public log datasets show that GeneralLog achieves over 90% F1-score under fully zero-label conditions, significantly surpassing existing methods.

Conclusion: GeneralLog effectively enables accurate log anomaly detection across systems without needing labeled target logs. Its knowledge-separating routing approach allows for robust performance and better generalization compared to previous methods.

Abstract: Log-based anomaly detection is crucial for ensuring software system stability. However, the scarcity of labeled logs limits rapid deployment to new systems. Cross-system transfer has become an important research direction. State-of-the-art approaches perform well with a few labeled target logs, but limitations remain: small-model methods transfer general knowledge but overlook mismatches with the target system's proprietary knowledge; LLM-based methods can capture proprietary patterns but rely on a few positive examples and incur high inference cost. Existing LLM-small model collaborations route 'simple logs' to the small model and 'complex logs' to the LLM based on output uncertainty. In zero-label cross-system settings, supervised sample complexity is unavailable, and such routing does not consider knowledge separation. To address this, we propose GeneralLog, a novel LLM-small model collaborative method for zero-label cross-system log anomaly detection. GeneralLog dynamically routes unlabeled logs, letting the LLM handle 'proprietary logs' and the small model 'general logs,' enabling cross-system generalization without labeled target logs. Experiments on three public log datasets show that GeneralLog achieves over 90% F1-score under a fully zero-label setting, significantly outperforming existing methods.

</details>


### [8] [SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?](https://arxiv.org/abs/2511.06090)
*Jeffrey Jian Ma,Milad Hashemi,Amir Yazdanbakhsh,Kevin Swersky,Ofir Press,Enhui Li,Vijay Janapa Reddi,Parthasarathy Ranganathan*

Main category: cs.SE

TL;DR: The paper introduces SWE-fficiency, a benchmark for repository-level performance optimization using realistic tasks from major data-science and HPC projects. Current automated agents struggle on this benchmark, highlighting the difficulty of automating performance engineering at scale.


<details>
  <summary>Details</summary>
Motivation: Current performance optimization benchmarks focus more on what needs to be fixed rather than the step-by-step process of actually fixing code. There is a gap in evaluating and benchmarking 'how to fix' code to optimize large-scale software repositories, particularly in ways that mimic real-world development workflows and expert interventions.

Method: The authors introduce SWE-fficiency, a benchmark with 498 tasks sourced from nine popular data-science and HPC repositories. The benchmark involves presenting a full codebase and a slow workload, requiring an agent to localize bottlenecks, understand code semantics, and generate patches that achieve performance improvements while maintaining correctness (verified by passing unit tests). The benchmark is constructed via an automated pipeline that scrapes GitHub pull requests for performance edits and uses filtering, static analysis, and execution validation.

Result: Empirical evaluation of state-of-the-art agents shows that they perform poorly on the SWE-fficiency benchmark, achieving less than 0.15x of the expert-obtained speedups. Agents particularly struggle with identifying optimization opportunities, reasoning about execution flow, and ensuring the correctness of their changes.

Conclusion: There is a significant gap between current automated agents and human experts in repository-level performance optimization. SWE-fficiency provides a challenging and realistic testbed for the community, encouraging new research on automated software performance engineering and advanced code reasoning.

Abstract: Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce \textsc{SWE-fficiency}, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.

</details>


### [9] [Quality in model-driven engineering: a tertiary study](https://arxiv.org/abs/2511.06103)
*Miguel Goulão,Vasco Amaral,Marjan Mernik*

Main category: cs.SE

TL;DR: This paper aggregates and analyzes literature reviews on software quality in model-driven engineering, finding that maintainability is most studied while empirical validation and 'quality in use' are lacking. More comparative and in-depth research is needed.


<details>
  <summary>Details</summary>
Motivation: There is scattered information about the impact of model-driven engineering (MDE) on software quality, making it difficult for researchers and practitioners to access consolidated evidence and findings.

Method: The authors conducted a tertiary study (a review of reviews), aggregating and analyzing systematic literature reviews and mapping studies focusing on quality attributes in MDE.

Result: They identified 22 relevant reviews and found maintainability to be the most frequently studied quality attribute in MDE. Most existing secondary research maps the current state rather than offering concrete comparative insights. The studies broadly cover software product quality but highlight the need for more empirical validation. The impact of MDE on 'quality in use' is underexplored.

Conclusion: MDE's impact on software quality is mainly studied through mapping existing work, with maintainability as a central focus and a lack of empirical validation as a main challenge. There are significant gaps in the study of certain quality attributes, notably 'quality in use.'

Abstract: Model-driven engineering (MDE) is believed to have a significant impact in software quality. However, researchers and practitioners may have a hard time locating consolidated evidence on this impact, as the available information is scattered in several different publications. Our goal is to aggregate consolidated findings on quality in MDE, facilitating the work of researchers and practitioners in learning about the coverage and main findings of existing work as well as identifying relatively unexplored niches of research that need further attention. We performed a tertiary study on quality in MDE, in order to gain a better understanding of its most prominent findings and existing challenges, as reported in the literature. We identified 22 systematic literature reviews and mapping studies and the most relevant quality attributes addressed by each of those studies, in the context of MDE. Maintainability is clearly the most often studied and reported quality attribute impacted by MDE. Eighty out of 83 research questions in the selected secondary studies have a structure that is more often associated with mapping existing research than with answering more concrete research questions (e.g., comparing two alternative MDE approaches with respect to their impact on a specific quality attribute). We briefly outline the main contributions of each of the selected literature reviews. In the collected studies, we observed a broad coverage of software product quality, although frequently accompanied by notes on how much more empirical research is needed to further validate existing claims. Relatively, little attention seems to be devoted to the impact of MDE on the quality in use of products developed using MDE.

</details>


### [10] [On the impact of semantic transparency on understanding and reviewing social goal models](https://arxiv.org/abs/2511.06110)
*Mafalda Santos,Catarina Gralha,Miguel Goulão,João Araújo,Ana Moreira*

Main category: cs.SE

TL;DR: Evaluating a more transparent i* syntax did not improve accuracy or speed in model interpretation, though it reduced the visual effort required. The context provided by the language key may help mitigate comprehension difficulties.


<details>
  <summary>Details</summary>
Motivation: i* is a widely used requirements engineering language but is complex and not widely adopted in industry. Previous research indicates stakeholders have difficulty interpreting i* models, prompting efforts to improve its syntax for better understanding.

Method: A quasi-experiment was conducted with 57 novice participants, comparing the standard i* syntax to an alternative with increased semantic transparency. Participants completed understanding and reviewing tasks, with their accuracy, completion speed, and ease monitored through task metrics, eye-tracking, and feedback.

Result: No significant improvement in accuracy or speed was observed with the more transparent syntax. Both syntaxes were perceived as equally easy, but participants spent less visual effort—i.e., less time looking at the model and language key—with the alternative syntax.

Conclusion: Context from the model and language key may compensate for symbol recognition challenges in i*. While the new syntax did not improve performance or perception of ease, it reduced the visual effort required by users.

Abstract: Context: i* is one of the most influential languages in the Requirements Engineering research community. Perhaps due to its complexity and low adoption in industry, it became a natural candidate for studies aiming at improving its concrete syntax and the stakeholders' ability to correctly interpret i* models.
  Objectives: We evaluate the impact of semantic transparency on understanding and reviewing i* models, in the presence of a language key. Methods: We performed a quasi-experiment comparing the standard i* concrete syntax with an alternative that has an increased semantic transparency. We asked 57 novice participants to perform understanding and reviewing tasks on i* models, and measured their accuracy, speed and ease, using metrics of task success, time and effort, collected with eye-tracking and participants' feedback.
  Results: We found no evidence of improved accuracy or speed attributable to the alternative concrete syntax. Although participants' perceived ease was similar, they devoted significantly less visual effort to the model and the provided language key, when using the alternative concrete syntax.
  Conclusions: The context provided by the model and language key may mitigate the i* symbol recognition deficit reported in previous works. However, the alternative concrete syntax required a significantly lower visual effort.

</details>


### [11] [The Lifecycle Workbench - A Configurable Framework for Digitized Product Maintenance Services](https://arxiv.org/abs/2511.06149)
*Dominique Briechle,Mohammed Fahad Ali,Marit Briechle-Mathiszig,Tobias Geger,Robert Werner,Andreas Rausch*

Main category: cs.SE

TL;DR: The paper identifies major challenges in adopting sustainable product lifecycle services: pricing uncertainty for users and condition assessment risk for service providers. It proposes the Lifecycle Workbench (LCW) ecosystem, a digital solution to improve service reliability and sustainability in the Circular Economy by providing robust pricing and product condition insights.


<details>
  <summary>Details</summary>
Motivation: Global production of electric goods is causing severe environmental, health impacts, and depletion of natural resources. There is a need to transition industrial processes to address climate change and resource misuse. Existing Circular Economy approaches help prolong product lifecycles, but both users and service providers face uncertainties—users in pricing, providers in product condition—which reduce adoption and economic sustainability.

Method: The authors propose the "Lifecycle Workbench (LCW)" ecosystem. LCW uses digital representations to improve the reliability of service pricing and the assessment of item, assembly, and part conditions within Circular Economy operations.

Result: The LCW ecosystem is designed to resolve uncertainties for both clients (in service pricing) and providers (in product condition assessment), bolstering economic reliability and encouraging broader adoption of sustainable services in the Circular Economy.

Conclusion: Digital tools such as the LCW ecosystem can close critical gaps in existing Circular Economy services by providing reliable cost and condition information, thus supporting the transition to more sustainable industrial practices.

Abstract: The global production of electric goods is at an all-time high, causing negative environmental and health impacts as well as a continuing depletion of natural resources. Considering the worsening global climate change, a transition of current industrial processes is necessary to tackle the above-mentioned factors. To address this urgent issue, socio-economic systems like the Circular Economy (CE) provide options to reallocate the use of resources and products on a global scale. Especially in terms of product lifecycle-prolonging, this system provides suitable approaches to alter the current modes of product handling by society and industry alike, based on the condition of the products. Although the importance and benefits of sustainable services enabling these options are widely known, users tend to shy away from using them. One of the reasons is the missing reliability in terms of the knowledge of the costs associated with a particular service. This uncertainty in expected pricing can, therefore, lower the willingness of potential clients. However, not only clients struggle with the boundary conditions of such services. On the part of the potential providers of services, the monetary risk is often caused by the incapability to detect the condition of a product in advance. This can result on the provider side in a severe economic loss if this possibility is not covered by the service price or through the mass of items, which could allow equalization of serval service operations. To address these weak points in current service execution, the authors propose the \textit{Lifecycle Workbench (LCW)}-ecosystem, which features digital representations to enhance the reliability of service pricing as well as the assessment of the condition of items, assemblies, and parts in the Circular Economy domain.

</details>


### [12] [Diagnosing and Resolving Android Applications Building Issues: An Empirical Study](https://arxiv.org/abs/2511.06186)
*Lakshmi Priya Bodepudi,Yutong Zhao,Ming Quan Fu,Yuanyuan Wu,Sen He,Yu Zhao*

Main category: cs.SE

TL;DR: This paper empirically analyzes 200 open-source Android projects to discover why builds fail and how to fix them. By systematically diagnosing errors and using repair strategies—supplemented by large language model (LLM) suggestions—developers resolved most build failures. LLMs like GPT-5 can help, but are not yet fully reliable. Results reveal important factors for build success, offering practical tips for building Android apps more reliably.


<details>
  <summary>Details</summary>
Motivation: Building Android applications is error-prone due to complex dependencies, diverse project setups, and the fast-changing Android ecosystem. Developers struggle with frequent and hard-to-diagnose build failures, motivating an empirical study to understand and address these issues.

Method: The study investigated 200 open-source Android projects (Java/Kotlin) using a five-phase methodology: 1) collecting project data, 2) systematically executing builds, 3) classifying build failures, 4) devising and applying targeted repair strategies, and 5) evaluating the potential of Large Language Models (like GPT-5) to assist in diagnosing and fixing build errors.

Result: Of the 135 projects that initially failed to build, 102 (75.56%) were fixed through the proposed diagnostic and repair approach. LLMs (e.g., GPT-5) successfully suggested fixes for 53.3% of error cases. Factors like programming language, project age, and app size were found to correlate with build success rates.

Conclusion: The research demonstrates that systematic error diagnosis and tailored repair strategies can significantly reduce Android build failures. LLMs show promise but have limitations. Insights into the attributes influencing build success can guide better practices and AI-assisted maintenance.

Abstract: Building Android applications reliably remains a persistent challenge due to complex dependencies, diverse configurations, and the rapid evolution of the Android ecosystem. This study conducts an empirical analysis of 200 open-source Android projects written in Java and Kotlin to diagnose and resolve build failures. Through a five-phase process encompassing data collection, build execution, failure classification, repair strategy design, and LLM-assisted evaluation, we identified four primary types of build errors: environment issues, dependency and Gradle task errors, configuration problems, and syntax/API incompatibilities. Among the 135 projects that initially failed to build, our diagnostic and repair strategy enabled developers to resolve 102 cases (75.56%), significantly reducing troubleshooting effort. We further examined the potential of Large Language Models, such as GPT-5, to assist in error diagnosis, achieving a 53.3% success rate in suggesting viable fixes. An analysis of project attributes revealed that build success is influenced by programming language, project age, and app size. These findings provide practical insights into improving Android build reliability and advancing AI-assisted software maintenance.

</details>


### [13] [Assertion-Aware Test Code Summarization with Large Language Models](https://arxiv.org/abs/2511.06227)
*Anamul Haque Mollah,Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: This paper shows that prompting LLMs with assertion semantics leads to better and more concise summaries of unit tests than using the full test context. Codex and Qwen-Coder are best aligned with developer-written summaries. The provided benchmark allows for reproducibility and further study.


<details>
  <summary>Details</summary>
Motivation: Unit tests frequently lack clear, concise summaries that clarify their intent, particularly in auto-generated or poorly documented code. Developing consistent and informative test summaries is difficult, and standard code summarization techniques do not effectively address the unique nature of test code, which focuses on asserting expected behavior rather than implementing functionality.

Method: The authors created a benchmark dataset consisting of 91 real-world Java test cases, paired with developer-written summaries. They performed a controlled ablation study to assess the effect of various test code components (method under test, assertion messages, assertion semantics) on LLM-generated test summaries. They evaluated four code LLMs (Codex, Codestral, DeepSeek, Qwen-Coder) under seven prompt configurations using traditional n-gram metrics (BLEU, ROUGE-L, METEOR), semantic measures (BERTScore), and LLM-based evaluations.

Result: Prompting language models with assertion semantics resulted in better summary quality compared to providing full method context, with an average improvement of 0.10 points (2.3%), while also requiring fewer input tokens. Among the evaluated models, Codex and Qwen-Coder achieved the closest alignment to human-written summaries. DeepSeek, despite high lexical overlap, lagged behind in summarization quality.

Conclusion: Leveraging assertion semantics in prompts enhances the effectiveness of LLM-generated unit test summaries. The study demonstrates that providing focused information rather than full context yields more accurate and concise summaries, thereby improving the utility of automated summaries for test code. The benchmark and findings are made publicly available for reproducibility.

Abstract: Unit tests often lack concise summaries that convey test intent, especially in auto-generated or poorly documented codebases. Large Language Models (LLMs) offer a promising solution, but their effectiveness depends heavily on how they are prompted. Unlike generic code summarization, test-code summarization poses distinct challenges because test methods validate expected behavior through assertions rather than im- plementing functionality. This paper presents a new benchmark of 91 real-world Java test cases paired with developer-written summaries and conducts a controlled ablation study to investigate how test code-related components-such as the method under test (MUT), assertion messages, and assertion semantics-affect the performance of LLM-generated test summaries. We evaluate four code LLMs (Codex, Codestral, DeepSeek, and Qwen-Coder) across seven prompt configurations using n-gram metrics (BLEU, ROUGE-L, METEOR), semantic similarity (BERTScore), and LLM-based evaluation. Results show that prompting with as- sertion semantics improves summary quality by an average of 0.10 points (2.3%) over full MUT context (4.45 vs. 4.35) while requiring fewer input tokens. Codex and Qwen-Coder achieve the highest alignment with human-written summaries, while DeepSeek underperforms despite high lexical overlap. The replication package is publicly available at https://doi.org/10. 5281/zenodo.17067550

</details>


### [14] [WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation](https://arxiv.org/abs/2511.06251)
*Mingde Xu,Zhen Yang,Wenyi Hong,Lihang Pan,Xinyue Fan,Yan Wang,Xiaotao Gu,Bin Xu,Jie Tang*

Main category: cs.SE

TL;DR: WebVIA automates generating interactive UI code from mockups by combining exploration, code generation, and validation modules. It produces more functional and interactive UIs than previous methods, outperforming general-purpose agents and base models.


<details>
  <summary>Details</summary>
Motivation: Translating UI design mockups into usable code is a repetitive and tedious task. Current Vision-Language Models automate code generation but fail to produce interactive and fully functional user interfaces, restricting outputs to static representations.

Method: The paper introduces WebVIA, an agentic framework consisting of three parts: (1) an exploration agent for multi-state UI screenshot capture, (2) a UI2Code model that generates executable, interactive code, and (3) a validation module to check interactivity. The system uses both fine-tuned models and empirical comparisons to general-purpose agents.

Result: WebVIA-Agent is shown to provide more stable and accurate UI exploration compared to general-purpose agents. The WebVIA-UI2Code models generate more executable and interactive HTML/CSS/JavaScript and consistently outperform base models on both interactive and static UI-to-Code tasks.

Conclusion: WebVIA significantly advances interactive UI-to-Code generation. Its modular design and specialized models yield better performance and interactivity, addressing the limitations of prior approaches. Code and models have been open-sourced for public use.

Abstract: User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at \href{https://zheny2751-dotcom.github.io/webvia.github.io/}{\texttt{https://webvia.github.io}}.

</details>


### [15] [State of the Art on Self-adaptive Systems: An Essay](https://arxiv.org/abs/2511.06352)
*Sara Mahdavi Hezavehi,Danny Weyns,Paris Avgeriou*

Main category: cs.SE

TL;DR: This essay provides a conceptual introduction and literature review for research on adaptive systems that consider uncertainty and risk.


<details>
  <summary>Details</summary>
Motivation: To establish the foundational concepts and context for a PhD research project focusing on how systems can adapt to uncertainty and risk.

Method: Literature review and conceptual analysis.

Result: The paper introduces key ideas and reviews prior related work in the field of uncertainty and risk-aware adaptation.

Conclusion: The groundwork has been laid for advancing research on systems that incorporate uncertainty and risk into their adaptation strategies.

Abstract: In this essay, we introduce the basic concepts necessary to lay out the foundation for our PhD research on uncertainty and risk-aware adaptation, and discuss relevant related research.

</details>


### [16] [Understanding Student Interaction with AI-Powered Next-Step Hints: Strategies and Challenges](https://arxiv.org/abs/2511.06362)
*Anastasiia Birillo,Aleksei Rostovskii,Yaroslav Golubev,Hieke Keuning*

Main category: cs.SE

TL;DR: The paper analyzes how students use AI-generated next-step hints in real coding tasks, discovers common patterns and student strategies, and provides data and insights to improve personalized learning support systems.


<details>
  <summary>Details</summary>
Motivation: Automated feedback, especially next-step hints, can boost personalized learning in computer science education. However, how students interact with such feedback in real coding environments is less understood, motivating this study.

Method: The study collected interactions from 34 students using an AI-driven next-step hint system while solving Kotlin programming tasks in an IDE. Detailed interaction logs were analyzed using process mining to identify common usage scenarios. Additionally, semi-structured interviews with 6 students were conducted to understand student strategies when dealing with unhelpful hints.

Result: The analysis identified 16 typical interaction scenarios. Student interviews revealed strategies to cope with unhelpful hints, such as adapting partial hints or editing code to get different suggestions.

Conclusion: This research provides a public dataset and valuable insights into how students utilize and adapt to AI-generated hint feedback. The findings can inform future feedback system design and research, leading to improved learning support.

Abstract: Automated feedback generation plays a crucial role in enhancing personalized learning experiences in computer science education. Among different types of feedback, next-step hint feedback is particularly important, as it provides students with actionable steps to progress towards solving programming tasks. This study investigates how students interact with an AI-driven next-step hint system in an in-IDE learning environment. We gathered and analyzed a dataset from 34 students solving Kotlin tasks, containing detailed hint interaction logs. We applied process mining techniques and identified 16 common interaction scenarios. Semi-structured interviews with 6 students revealed strategies for managing unhelpful hints, such as adapting partial hints or modifying code to generate variations of the same hint. These findings, combined with our publicly available dataset, offer valuable opportunities for future research and provide key insights into student behavior, helping improve hint design for enhanced learning support.

</details>


### [17] [Methodological Considerations for Self-adaptive Systems: An Essay](https://arxiv.org/abs/2511.06367)
*Sara Mahdavi Hezavehi,Danny Weyns,Paris Avgeriou*

Main category: cs.SE

TL;DR: This essay discusses essential methodological points to prepare for PhD research in uncertainty and risk-aware adaptation.


<details>
  <summary>Details</summary>
Motivation: To establish a methodological basis for PhD research focused on uncertainty and risk-aware adaptation.

Method: The paper provides an overview of methodological considerations relevant to the proposed research area.

Result: The essay outlines key methodological foundations required for research on managing uncertainty and risk in adaptation processes.

Conclusion: This work lays the groundwork for future PhD research by systematically discussing necessary methodological approaches regarding uncertainty and risk-aware adaptation.

Abstract: In this essay, we provide an overview of methodological considerations necessary to lay out the foundation for our PhD research on uncertainty and risk-aware adaptation.

</details>


### [18] [Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective](https://arxiv.org/abs/2511.06428)
*Samuel Ferino,Rashina Hoda,John Grundy,Christoph Treude*

Main category: cs.SE

TL;DR: The paper qualitatively examines the impacts of LLMs on software development, revealing both benefits and drawbacks across multiple levels. It provides practical advice for leaders considering LLM integration, highlighting important trade-offs and recommended best practices.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are expected to revolutionize software development, but there is limited empirical insight into their real-world impacts on developers and organizations. There is a knowledge gap in understanding both the positive and negative consequences of LLM adoption, and how to best manage them from a practitioner's viewpoint.

Method: The paper conducted 22 interviews with software practitioners across 3 rounds, spanning from October 2024 to September 2025. Socio-technical grounded theory (STGT) was used for rigorous qualitative analysis of the interview data.

Result: The study discovered both benefits—like supporting software development flow, enhancing developers' mental models, and encouraging entrepreneurship—and drawbacks, such as negative impacts on developer personalities and reputational risks. These effects were analyzed at individual, team, organization, and societal levels, alongside outlining best practices for adoption.

Conclusion: This work systematically presents the trade-offs software professionals face when integrating LLMs, offering actionable insights and guidance for teams and IT leaders evaluating LLM adoption.

Abstract: Background: Large Language Models emerged with the potential of provoking a revolution in software development (e.g., automating processes, workforce transformation). Although studies have started to investigate the perceived impact of LLMs for software development, there is a need for empirical studies to comprehend how to balance forward and backward effects of using LLMs. Objective: We investigated how LLMs impact software development and how to manage the impact from a software developer's perspective. Method: We conducted 22 interviews with software practitioners across 3 rounds of data collection and analysis, between October (2024) and September (2025). We employed socio-technical grounded theory (STGT) for data analysis to rigorously analyse interview participants' responses. Results: We identified the benefits (e.g., maintain software development flow, improve developers' mental model, and foster entrepreneurship) and disadvantages (e.g., negative impact on developers' personality and damage to developers' reputation) of using LLMs at individual, team, organisation, and society levels; as well as best practices on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that software practitioners, teams, and organisations face in working with LLMs. Our findings are particularly useful for software team leaders and IT managers to assess the viability of LLMs within their specific context.

</details>


### [19] [Automatically Identifying Solution-Related Content in Issue Report Discussions with Language Models](https://arxiv.org/abs/2511.06501)
*Antu Saha,Mehedi Sun,Oscar Chaparro*

Main category: cs.SE

TL;DR: The paper shows that fine-tuned language models can accurately identify solution-related content in software issue discussions, outperforming traditional methods and enabling better maintenance and solution reuse across projects.


<details>
  <summary>Details</summary>
Motivation: Manually identifying solution-related content in lengthy software issue discussions is time-consuming and difficult, yet crucial for resolving reopened issues, addressing regressions, and understanding the rationale behind code changes.

Method: The paper automates solution identification using supervised classifiers (MLMs, PLMs, LLMs) based on language models, examining embeddings, prompting, and fine-tuning techniques. A dataset of 356 Mozilla Firefox issues was used to train and evaluate six MLMs, four PLMs, and two LLMs in 68 configurations.

Result: MLMs with LLM embeddings outperform TF-IDF baselines. Prompting methods underperform. Fine-tuned LLMs, especially LLAMAft, achieve the highest F1 score (0.716), and ensemble models further improve performance to 0.737 F1. Models can be transferred to other projects with moderate adaptation.

Conclusion: Automated solution identification in issue discussions using fine-tuned language models is effective and transferable across projects, though context-aware classifiers are needed to address misclassification stemming from missing or misleading discussion clues.

Abstract: During issue resolution, software developers rely on issue reports to discuss solutions for defects, feature requests, and other changes. These discussions contain proposed solutions-from design changes to code implementations-as well as their evaluations. Locating solution-related content is essential for investigating reopened issues, addressing regressions, reusing solutions, and understanding code change rationale. Manually understanding long discussions to identify such content can be difficult and time-consuming.
  This paper automates solution identification using language models as supervised classifiers. We investigate three applications-embeddings, prompting, and fine-tuning-across three classifier types: traditional ML models (MLMs), pre-trained language models (PLMs), and large language models (LLMs). Using 356 Mozilla Firefox issues, we created a dataset to train and evaluate six MLMs, four PLMs, and two LLMs across 68 configurations.
  Results show that MLMs with LLM embeddings outperform TF-IDF features, prompting underperforms, and fine-tuned LLMs achieve the highest performance, with LLAMAft reaching 0.716 F1 score. Ensembles of the best models further improve results (0.737 F1). Misclassifications often arise from misleading clues or missing context, highlighting the need for context-aware classifiers. Models trained on Mozilla transfer to other projects, with a small amount of project-specific data, further enhancing results. This work supports software maintenance, issue understanding, and solution reuse.

</details>


### [20] [LLM For Loop Invariant Generation and Fixing: How Far Are We?](https://arxiv.org/abs/2511.06552)
*Mostafijur Rahman Akhond,Saikat Chakraborty,Gias Uddin*

Main category: cs.SE

TL;DR: LLMs can help infer loop invariants for program verification, especially when provided with extra information, but they struggle with repairing incorrect invariants, achieving only moderate success rates.


<details>
  <summary>Details</summary>
Motivation: Loop invariants are fundamental for automated program safety assessment, but inferring them is challenging. Recent advances in Large Language Models (LLMs) show promise in software engineering tasks, yet their capacity for inferring loop invariants is not well understood.

Method: The authors conduct an empirical study, evaluating both open-source and closed-source LLMs of varying sizes on their ability to infer inductive loop invariants and repair incorrect invariants. They also explore the impact of providing auxiliary information, such as domain knowledge and examples.

Result: LLMs display some ability in inferring and repairing loop invariants. Their performance significantly improves when given additional domain-specific information and examples. The best performance reported is a 78% success rate in generating correct invariants and a 16% success rate in repairing incorrect ones.

Conclusion: LLMs can assist in loop invariant inference, especially when supplemented with domain knowledge and examples. However, their ability to repair incorrect loop invariants remains limited.

Abstract: A loop invariant is a property of a loop that remains true before and after each execution of the loop. The identification of loop invariants is a critical step to support automated program safety assessment. Recent advancements in Large Language Models (LLMs) have demonstrated potential in diverse software engineering (SE) and formal verification tasks. However, we are not aware of the performance of LLMs to infer loop invariants. We report an empirical study of both open-source and closed-source LLMs of varying sizes to assess their proficiency in inferring inductive loop invariants for programs and in fixing incorrect invariants. Our findings reveal that while LLMs exhibit some utility in inferring and repairing loop invariants, their performance is substantially enhanced when supplemented with auxiliary information such as domain knowledge and illustrative examples. LLMs achieve a maximum success rate of 78\% in generating, but are limited to 16\% in repairing the invariant.

</details>


### [21] [PhaseSeed: Precise Call Graph Construction for Split-Phase Applications using Dynamic Seeding](https://arxiv.org/abs/2511.06661)
*Tapti Palit,Seyedhamed Ghavamnia,Michalis Polychronakis*

Main category: cs.SE

TL;DR: PhaseSeed combines dynamic and static analysis to greatly improve the precision of call graph construction for split-phase applications, boosting the effectiveness of CFI, debloating, and syscall filtering in security contexts.


<details>
  <summary>Details</summary>
Motivation: Traditional static pointer analysis for call graph construction is imprecise, as it is unaware of applications' architectural phases and is designed for general applicability, limiting its precision for security mechanisms.

Method: The authors introduce PhaseSeed, a technique that dynamically analyzes an application's initialization phase to collect runtime points-to relationships. These are then incorporated into a subsequent static pointer analysis confined to the processing phase, utilizing the more precise, phase-specific data.

Result: PhaseSeed yields significant precision improvements, delivering up to a 92.6% increase in precision for control flow integrity (CFI) and identifying nine additional security-critical system calls for filtering when generating Seccomp profiles, compared to purely static call graph construction.

Conclusion: PhaseSeed effectively enhances the accuracy of pointer analysis for split-phase applications, leading to more precise call graphs, which benefits several security mechanisms and filters more security-critical system calls.

Abstract: Precise and sound call graph construction is crucial for many software security mechanisms. Unfortunately, traditional static pointer analysis techniques used to generate application call graphs suffer from imprecision. These techniques are agnostic to the application's architecture and are designed for broad applicability. To mitigate this precision problem, we propose PhaseSeed, a novel technique that improves the accuracy of pointer analysis for split-phase applications, which have distinct initialization and processing phases. PhaseSeed analyzes the initialization phase dynamically, collecting the points-to relationships established at runtime. At the end of the initialization phase, it then seeds this information to a static analysis stage that performs pointer analysis for all code that stays in scope during the processing phase, improving precision. Our observations show that, given the same runtime configuration options, the points-to relationships established during the initialization phase remain constant across multiple runs. Therefore, PhaseSeed is sound with respect to a given initial configuration. We apply PhaseSeed to three security mechanisms: control flow integrity (CFI), software debloating, and system call filtering. PhaseSeed provides up to 92.6% precision improvement for CFI compared to static call graph construction techniques, and filters nine additional security-critical system calls when used to generate Seccomp profiles.

</details>


### [22] [Structural Enforcement of Statistical Rigor in AI-Driven Discovery: A Functional Architecture](https://arxiv.org/abs/2511.06701)
*Karen Sargsyan*

Main category: cs.SE

TL;DR: The paper presents a Haskell-based functional system that ensures statistical integrity in LLM-driven automated research. Through monads and scaffolding, it prevents errors like data leakage and demonstrates robust protocol enforcement in simulations and a case study.


<details>
  <summary>Details</summary>
Motivation: LLM-driven automated research systems ('AI Scientists') face challenges with statistical rigor and risk methodological errors like data leakage, especially in the context of dynamic hypothesis testing. There is a need for architectures that enforce correct statistical protocols and defend against integrity risks.

Method: The paper proposes a functional architecture using a Haskell eDSL called the Research monad, employing a monad transformer stack to enforce sequential statistical protocols (such as online FDR control). It introduces Declarative Scaffolding to constrain code execution and prevent errors, validating the approach via large-scale simulation and a detailed case study.

Result: The functional architecture with the Research monad and Declarative Scaffolding successfully enforces statistical protocols and robustly prevents methodological errors in hybrid (LLM + imperative code) AI research systems. Simulation (N=2000 hypotheses) and a case study confirm its effectiveness.

Conclusion: Structurally enforcing statistical rigor via functional programming—specifically with monads and declarative scaffolding—provides essential defense-in-depth for automated scientific integrity in AI-driven research systems.

Abstract: Sequential statistical protocols require meticulous state management and robust error handling -- challenges naturally suited to functional programming. We present a functional architecture for structural enforcement of statistical rigor in automated research systems (AI-Scientists). These LLM-driven systems risk generating spurious discoveries through dynamic hypothesis testing. We introduce the Research monad, a Haskell eDSL that enforces sequential statistical protocols (e.g., Online FDR (false discovery rate) control) using a monad transformer stack. To address risks in hybrid architectures where LLMs generate imperative code, we employ Declarative Scaffolding -- generating rigid harnesses that structurally constrain execution and prevent methodological errors like data leakage. We validate this approach through large-scale simulation (N=2000 hypotheses) and an end-to-end case study, demonstrating essential defense-in-depth for automated science integrity.

</details>


### [23] [Minimizing Breaking Changes and Redundancy in Mitigating Technical Lag for Java Projects](https://arxiv.org/abs/2511.06762)
*Rui Lu,Lyuye Zhang,Kaixuan Li,Min Zhang,Yixiang Chen*

Main category: cs.SE

TL;DR: This paper introduces DepUpdater, an automated tool that upgrades open-source libraries more effectively than current solutions by minimizing technical lag, ensuring compatibility, and avoiding redundant dependencies. Experimental results and an ablation study validate its effectiveness and highlight new considerations for future research on dependency management.


<details>
  <summary>Details</summary>
Motivation: Developers often use open-source software (OSS) libraries, but failing to keep them updated leads to missing features, bugs, and vulnerabilities. However, upgrades can cause incompatibilities or introduce redundant dependencies, making the process difficult, discouraging timely updates. There is a need for an automated approach to manage library upgrades effectively.

Method: The authors introduce DepUpdater, an automated solution for upgrading OSS libraries that balances minimizing technical lag, avoiding incompatibility issues, and pruning redundant dependencies. The tool's effectiveness is evaluated via comparison with existing dependency management tools and an ablation study.

Result: DepUpdater reduces technical lag more efficiently than existing tools, while simultaneously maintaining compatibility and minimizing redundant dependencies. The ablation study shows that pruning requirements further mitigates incompatibility. Additionally, an investigation into transitive dependency upgrades offers new insights into their impact on client compatibility.

Conclusion: DepUpdater offers a more balanced and automated approach to OSS library upgrades, successfully reducing technical lag, ensuring compatibility, and avoiding unnecessary dependencies. It demonstrates potential for further research into the relationship between dependency upgrades and software compatibility.

Abstract: Re-using open-source software (OSS) can avoid reinventing the wheel, but failing to keep it up-to-date can lead to missing new features and persistent bugs or vulnerabilities that have already been resolved. The use of outdated OSS libraries introduces technical lag, necessitating timely upgrades. However, maintaining up-to-date libraries is challenging, as it may introduce incompatibility issues that break the project or redundant dependencies that unnecessarily increase the size of the project. These issues discourage developers from upgrading libraries, highlighting the need for a fully automated solution that balances version upgrades, reduces technical lag, ensures compatibility, and avoids redundant dependencies.
  To this end, we propose DepUpdater, which ensures that upgrades minimize technical lag as much as possible while avoiding incompatibility issues and redundant dependencies. The comparison with existing dependency management tools demonstrates that DepUpdater more effectively reduces technical lag while ensuring compatibility and pruning redundant dependencies. Additionally, an ablation study highlights the potential benefits of considering pruning requirements during upgrades to mitigate incompatibility issues. Finally, leveraging DepUpdater, we investigate the impact of transitive dependency upgrades on client compatibility, providing insights for future research.

</details>


### [24] [MetricSynth: Framework for Aggregating DORA and KPI Metrics Across Multi-Platform Engineering](https://arxiv.org/abs/2511.06864)
*Pallav Jain,Yuvraj Agrawal,Ashutosh Nigam,Pushpak Patil*

Main category: cs.SE

TL;DR: A centralized, secure framework was built to aggregate and visualize DevEx and KPI metrics for software teams, reducing reporting time, aiding faster decision-making, and enhancing visibility into team and system health.


<details>
  <summary>Details</summary>
Motivation: Engineering leaders find it difficult to obtain a holistic, data-driven perspective on team and system performance due to data fragmentation across multiple tools and the labor-intensive nature of manual reporting.

Method: The paper presents the architecture and implementation of a centralized framework that aggregates data from various tools, computes metrics, and visualizes them using Metabase, with cron-based ingestion, dual-schema storage, pre-computation processing, a proactive alerting system, and RBAC security.

Result: The implemented system significantly reduced manual reporting, saving approximately 20 person-hours weekly, and enabled quicker bottleneck identification.

Conclusion: The centralized framework improves efficiency and decision-making by providing near-real-time visibility into DevEx and KPIs, demonstrating value as a scalable engineering intelligence platform, though trade-offs are discussed.

Abstract: In modern, large-scale software development, engineering leaders face the significant challenge of gaining a holistic and data-driven view of team performance and system health. Data is often siloed across numerous disparate tools, making manual report generation time-consuming and prone to inconsistencies. This paper presents the architecture and implementation of a centralized framework designed to provide near-real-time visibility into developer experience (DevEx) and Key Performance Indicator (KPI) metrics for a software ecosystem. By aggregating data from various internal tools and platforms, the system computes and visualizes metrics across key areas such as Developer Productivity, Quality, and Operational Efficiency. The architecture features a cron-based data ingestion layer, a dual-schema data storage approach, a processing engine for metric pre-computation, a proactive alerting system, and utilizes the open-source BI tool Metabase for visualization, all secured with role-based access control (RBAC). The implementation resulted in a significant reduction in manual reporting efforts, saving an estimated 20 person-hours per week, and enabled faster, data-driven bottleneck identification. Finally, we evaluate the system's scalability and discuss its trade-offs, positioning it as a valuable contribution to engineering intelligence platforms.

</details>


### [25] [A Collaborative Model for Improving Information Sharing among Cancer Care Groups using Software Engineering Principles](https://arxiv.org/abs/2511.06885)
*Davis Byamugisha,Francis Kamuganga,Adones Rukundo,John Businge*

Main category: cs.SE

TL;DR: The paper proposes a cancer case management model inspired by software engineering (GitHub version control) principles, showing through simulation that these principles can reduce delays, improve coordination, and enhance outcomes in cancer care by improving information sharing among all involved groups.


<details>
  <summary>Details</summary>
Motivation: Early cancer diagnosis and treatment is hampered by delays caused by limited skilled manpower and poorly organized health information systems that are not integrated, leading to information gaps and miscommunication between care groups. Existing methods for data sharing often leave out crucial stakeholders, worsening the problem.

Method: The authors propose and design a model for more effective information sharing among cancer care groups, inspired by information management and versioning principles from software engineering, specifically drawing on GitHub bug resolution and version control practices. They use Any-Logic simulation software to create a virtual environment to test their approach.

Result: Simulation results demonstrate that adopting software engineering principles—such as those used in GitHub for bug tracking and version control—can enhance collaboration and information sharing among all stakeholders in cancer case management, thereby reducing delays and miscommunication.

Conclusion: The application of software engineering principles, especially those related to version control, can improve coordination and information flow in cancer care. This inclusive approach involving all stakeholders may lead to earlier diagnoses and better treatment outcomes.

Abstract: Effective treatment of cancer requires early diagnosis which involves the patient's awareness of the early signs and symptoms, leading to a consultation with a health provider, who would then promptly refer the patient for confirmation of the diagnosis and thereafter treatment. However, this is not always the case because of delays arising from limited skilled manpower and health information management systems that are neither integrated nor organized in their design hence leading to information gap among care groups. Existing methods focus on using accumulated data to support decision making, enhancing the sharing of secondary data while others exclude some critical stakeholders like patient caretakers and administrators thus, leaving an information gap that creates delays and miscommunication during case management. We however notice some similarities between cancer treatment and software engineering information management especially when progress history needs to be maintained (versioning).
  We analyze the similarities and propose a model for information sharing among cancer care groups using the software engineering principles approach. We model for reducing delays and improving coordination among care groups in cancer case management. Model design was guided by software engineering principles adopted in GitHub version control system for bug fixing in open-source code projects. Any-Logic simulation software was used to mimic the model realism in a virtual environment. Results show that bug resolution principles from software engineering and GitHub version control system can be adopted to coordinate collaboration and information sharing among care groups in a cancer case management environment while involving all stakeholders to improve care treatment outcomes, ensure early diagnosis and increase patient's survival chances.

</details>


### [26] [Benchmarking LLMs for Fine-Grained Code Review with Enriched Context in Practice](https://arxiv.org/abs/2511.07017)
*Ruida Hu,Xinchen Wang,Xin-Cheng Wen,Zhao Zhang,Bo Jiang,Pengfei Gao,Chao Peng,Cuiyun Gao*

Main category: cs.SE

TL;DR: ContextCRBench is a novel, context-rich benchmark for code review by LLMs, addressing major flaws in prior work. By enriching reviews with issue descriptions and rigorous filtering, it provides fine-grained samples that reveal LLMs perform better with textual context, though still below human-level. Industrial use yielded significant review improvements.


<details>
  <summary>Details</summary>
Motivation: Existing code review benchmarks for LLMs lack semantic context, suffer from poor data quality, and overlook fine-grained, line-level review, which limits the progress and reliability of automation in code review.

Method: The authors created ContextCRBench through a multi-stage pipeline: crawling large-scale data (issues and pull requests), linking code and textual context, and applying rigorous rule-based and LLM-based data filtering to ensure high quality. The benchmark supports three review tasks and evaluated eight leading LLMs across them.

Result: ContextCRBench contains 67,910 well-validated, context-enriched review samples and supports hunk-level, line-level localization, and comment generation tasks. Benchmarked LLMs benefit more from added textual context than code context alone but are still well below human performance. Deployment in industry improved automated review by 61.98%.

Conclusion: The ContextCRBench benchmark addresses key shortcomings in LLM-based code review evaluations by providing context-rich, high-quality, and fine-grained samples. Textual context significantly boosts LLM performance, but current models still lag behind human reviewers. ContextCRBench proves valuable in industrial deployment, substantially improving automated review efficiency.

Abstract: Code review is a cornerstone of software quality assurance, and recent advances in Large Language Models (LLMs) have shown promise in automating this process. However, existing benchmarks for LLM-based code review face three major limitations. (1) Lack of semantic context: most benchmarks provide only code diffs without textual information such as issue descriptions, which are crucial for understanding developer intent. (2) Data quality issues: without rigorous validation, many samples are noisy-e.g., reviews on outdated or irrelevant code-reducing evaluation reliability. (3) Coarse granularity: most benchmarks operate at the file or commit level, overlooking the fine-grained, line-level reasoning essential for precise review.
  We introduce ContextCRBench, a high-quality, context-rich benchmark for fine-grained LLM evaluation in code review. Our construction pipeline comprises: (1) Raw Data Crawling, collecting 153.7K issues and pull requests from top-tier repositories; (2) Comprehensive Context Extraction, linking issue-PR pairs for textual context and extracting the full surrounding function or class for code context; and (3) Multi-stage Data Filtering, combining rule-based and LLM-based validation to remove outdated, malformed, or low-value samples, resulting in 67,910 context-enriched entries.
  ContextCRBench supports three evaluation scenarios aligned with the review workflow: (1) hunk-level quality assessment, (2) line-level defect localization, and (3) line-level comment generation. Evaluating eight leading LLMs (four closed-source and four open-source) reveals that textual context yields greater performance gains than code context alone, while current LLMs remain far from human-level review ability. Deployed at ByteDance, ContextCRBench drives a self-evolving code review system, improving performance by 61.98% and demonstrating its robustness and industrial utility.

</details>


### [27] [Bridging the Prototype-Production Gap: A Multi-Agent System for Notebooks Transformation](https://arxiv.org/abs/2511.07257)
*Hanya Elhashemy,Youssef Lotfy,Yongjian Tang*

Main category: cs.SE

TL;DR: Codelevate is a multi-agent system that automatically restructures Jupyter notebooks into production-ready Python code, significantly improving code quality and maintainability while preserving notebook functionality.


<details>
  <summary>Details</summary>
Motivation: Jupyter notebooks are widely used for prototyping in data science, but their lack of proper software engineering practices makes it difficult to convert them into production-ready code. Addressing this prototype-to-production gap is crucial for efficient workflow and code maintainability.

Method: The paper introduces Codelevate, a multi-agent system consisting of three agents—Architect, Developer, and Structure—which collaboratively transform Jupyter notebooks into organized, maintainable Python code repositories. This is achieved by coordinating through a shared dependency tree to maintain architecture and code quality.

Result: Experiments show that Codelevate can autonomously transform prototype notebook code into structured Python repositories, resulting in demonstrable improvements in code quality metrics without altering computational results.

Conclusion: Codelevate effectively bridges the gap between exploratory Jupyter notebook development and maintainable, production-level Python codebases, streamlining the transition with minimal manual intervention and enhanced code quality.

Abstract: The increasing adoption of Jupyter notebooks in data science and machine learning workflows has created a gap between exploratory code development and production-ready software systems. While notebooks excel at iterative development and visualization, they often lack proper software engineering principles, making their transition to production environments challenging. This paper presents Codelevate, a novel multi-agent system that automatically transforms Jupyter notebooks into well-structured, maintainable Python code repositories. Our system employs three specialized agents - Architect, Developer, and Structure - working in concert through a shared dependency tree to ensure architectural coherence and code quality. Our experimental results validate Codelevate's capability to bridge the prototype-to-production gap through autonomous code transformation, yielding quantifiable improvements in code quality metrics while preserving computational semantics.

</details>
