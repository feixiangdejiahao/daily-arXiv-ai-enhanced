<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 33]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AI Exchange Platforms](https://arxiv.org/abs/2510.17839)
*Johannes Schneider,Rene Abraham*

Main category: cs.SE

TL;DR: The paper proposes a taxonomy to classify and better understand AI model exchange platforms, offering insights into their features, usage, and the challenges and opportunities they present. This aids both practitioners and researchers in navigating and advancing the field.


<details>
  <summary>Details</summary>
Motivation: There is a gap in understanding and categorizing the rapidly evolving landscape of AI exchange platforms, which are becoming critical as foundational AI models proliferate across organizations. A comprehensive framework for these platforms is missing, hindering effective utilization and further research.

Method: The paper develops a taxonomy—a structured classification framework—to categorize and analyze AI model exchange platforms. The authors examine key platform characteristics, interaction patterns (such as peer review or model customization), and differences between public and organizational use.

Result: The taxonomy identifies important dimensions and operational features of various AI model exchange platforms, uncovering differences in quality control (e.g., peer review), model deployment, and adaptability for users. It also highlights interactions between public institutions and private organizations in platform dynamics.

Conclusion: The taxonomy provides a foundation for practical and academic understanding of AI model exchange platforms, shedding light on industry practices and setting the stage for future research on their evolution, impact, and best practices as AI continues to transform organizations.

Abstract: The rapid integration of Artificial Intelligence (AI) into organizational
technology frameworks has transformed how organizations engage with AI-driven
models, influencing both operational performance and strategic innovation. With
the advent of foundation models, the importance of structured platforms for AI
model exchange has become paramount for organizational efficacy and
adaptability. However, a comprehensive framework to categorize and understand
these platforms remains underexplored. To address this gap, our taxonomy
provides a structured approach to categorize AI exchange platforms, examining
key dimensions and characteristics, as well as revealing interesting
interaction patterns between public research institutions and organizations:
Some platforms leverage peer review as a mechanism for quality control, and
provide mechanisms for online testing, deploying, and customization of models.
Our paper is beneficial to practitioners seeking to understand challenges and
opportunities that arise from AI exchange platforms. For academics, the
taxonomy serves as a foundation for further research into the evolution,
impact, and best practices associated with AI model sharing and utilization in
different contexts. Additionally, our study provides insights into the evolving
role of AI in various industries, highlighting the importance of adaptability
and innovation in platform design. This paper serves as a critical resource for
understanding the dynamic interplay between technology, business models, and
user engagement in the rapidly growing domain of AI model exchanges pointing
also towards possible future evolution.

</details>


### [2] [Vibe Coding: Toward an AI-Native Paradigm for Semantic and Intent-Driven Programming](https://arxiv.org/abs/2510.17842)
*Vinay Bamil*

Main category: cs.SE

TL;DR: Vibe coding is a new AI-driven programming paradigm where developers specify both functional intent and qualitative 'vibes' for code generation. The paper formalizes the concept, proposes a reference architecture, explores its impacts and challenges, and sets a research agenda for addressing issues like bias, alignment, and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Recent progress in large language models enables more conversational, AI-driven software generation, motivating the search for novel programming paradigms that leverage these capabilities.

Method: This paper formalizes the concept of 'vibe coding,' proposes a reference architecture for it, describes a hypothetical implementation, and compares it to other paradigms (declarative, functional, prompt-based). It also reviews related studies, discusses implications, reports on gains and risks, and identifies challenges and open research questions.

Result: The paper establishes vibe coding as an AI-native programming paradigm, highlights its potential to enhance productivity and democratize software development, and identifies challenges like alignment, reproducibility, bias, explainability, maintainability, and security.

Conclusion: Vibe coding represents a promising direction for integrating AI into software development, but considerable research is needed to address its vulnerabilities and ensure responsible, effective adoption.

Abstract: Recent advances in large language models have enabled developers to generate
software by conversing with artificial intelligence systems rather than writing
code directly. This paper introduces vibe coding, an emerging AI-native
programming paradigm in which a developer specifies high-level functional
intent along with qualitative descriptors of the desired "vibe" (tone, style,
or emotional resonance). An intelligent agent then transforms those
specifications into executable software. We formalize the definition of vibe
coding and propose a reference architecture that includes an intent parser, a
semantic embedding engine, an agentic code generator, and an interactive
feedback loop. A hypothetical implementation is described. We compare vibe
coding with declarative, functional, and prompt-based programming, and we
discuss its implications for software engineering, human-AI collaboration, and
responsible AI practice. Finally, we examine reported productivity gains and
democratizing effects, review recent studies that highlight vulnerabilities and
potential slowdowns, identify key challenges such as alignment,
reproducibility, bias, explainability, maintainability, and security, and
outline future directions and open research questions.

</details>


### [3] [Smart Contracts Formal Verification: A Systematic Literature Review](https://arxiv.org/abs/2510.17865)
*Rene Davila,Everardo Barcenas,Rocio Aldeco-Perez*

Main category: cs.SE

TL;DR: This paper surveys smart contract verification literature and introduces a new formal verification method using description logic.


<details>
  <summary>Details</summary>
Motivation: Smart contracts, which automate agreements on blockchains, often suffer from operational or specification errors. This paper is motivated by the need for reliable verification techniques to ensure smart contracts function as intended.

Method: The authors conduct a survey of existing publications that address the specifications, verification tools, and experiments for smart contract verification. They then propose a novel formal verification approach based on description logic.

Result: The paper reviews a range of previous works on smart contract verification and introduces an alternative verification method utilizing description logic.

Conclusion: A comprehensive analysis of existing smart contract verification approaches is provided, and a new description logic-based verification framework is proposed as an alternative approach.

Abstract: Formal verification entails testing software to ensure it operates as
specified. Smart contracts are self-executing contracts with the terms of the
agreement directly written into lines of code. They run on blockchain platforms
and automatically enforce and execute the terms of an agreement when meeting
predefined conditions. However, Smart Contracts, as software models, often
contain notable errors in their operation or specifications. This observation
prompts us to conduct a focused study examining related works published across
various sources. These publications detail specifications, verification tools,
and relevant experiments. Subsequently, this survey proposes an alternative
formal verification based on description logic.

</details>


### [4] [UniCode: A Framework for Generating High Quality Competitive Coding Problems](https://arxiv.org/abs/2510.17868)
*Xinyue Zheng,Haowei Lin,Shaofei Cai,Zilong Zheng,Yitao Liang*

Main category: cs.SE

TL;DR: UniCode is a new LLM-powered framework for automatically creating diverse and robust algorithmic coding problems and test cases. It addresses problems of data contamination and scalability in traditional benchmarks, and produces highly challenging benchmarks that even top models struggle to solve completely.


<details>
  <summary>Details</summary>
Motivation: Competitive coding benchmarks suffer from data contamination and are difficult to scale because they rely on static, human-authored problems.

Method: The paper proposes UniCode, a framework using Large Language Models (LLMs) to automatically generate algorithmic problems and robust, contamination-resistant test cases. The problem generation process involves three diversification strategies: single problem extension, same-type fusion, and cross-type fusion. The framework includes a stress-driven test case synthesis pipeline, which generates reliable test suites by combining brute-force grounding on small inputs and consensus-based validation on large inputs, all without needing a canonical ground-truth solution.

Result: UniCode generated a benchmark set of 492 problems and evaluated 19 state-of-the-art LLMs. The results show that the benchmark is challenging and discriminative: the best-performing model only achieved a 70.3% pass rate.

Conclusion: UniCode provides a scalable, dynamic, and contamination-resistant approach to generating high-quality coding benchmark datasets, significantly improving on static, human-authored benchmarks.

Abstract: The reliance of competitive coding benchmarks on static, human-authored
problems creates significant challenges, including data contamination and
limited scalability. To address these issues, we introduce UniCode, a novel
framework that automatically generates high-quality algorithmic problems
alongside robust, contamination-resistant test cases. Inspired by biological
evolution that creates better and diverse offspring, our framework leverages
Large Language Models (LLMs) to systematically diversify problems through three
strategies: single problem extension, same-type fusion, and cross-type fusion.
A key innovation is our stress-driven test case synthesis pipeline, which
generates reliable test suites without requiring a canonical ground-truth
solution. This pipeline combines brute-force grounding for small-scale inputs
with a consensus-based validation mechanism for large-scale inputs to ensure
high correctness and coverage. We demonstrate effectiveness of our framework by
curating a benchmark of 492 problems and evaluating 19 state-of-the-art LLMs.
The results reveal that UniCode is highly challenging and discriminative, with
the top-performing model, o4-mini, achieving a pass rate of only 70.3%. Our
framework provides a scalable and reliable solution for generating dynamic
evaluation datasets in coding domain.

</details>


### [5] [Repairing Tool Calls Using Post-tool Execution Reflection and RAG](https://arxiv.org/abs/2510.17874)
*Jason Tsay,Zidane Wright,Gaodan Fang,Kiran Kate,Saurabh Jha,Yara Rizk*

Main category: cs.SE

TL;DR: The paper introduces a system using LLMs and RAG with troubleshooting docs to better fix errors in Kubernetes tool calls, leading to higher success and accuracy rates in automated command execution.


<details>
  <summary>Details</summary>
Motivation: Tool calls in agentic systems frequently fail due to semantic and syntactic reasons, with some errors only detected after tool execution. There is a need to repair these errors efficiently to enhance automation and reliability.

Method: Developed a post-tool execution reflection component that combines LLM-based reflection and domain-specific retrieval-augmented generation, leveraging both official and troubleshooting documents. Evaluated through empirical and manual studies.

Result: RAG-based reflection repairs kubectl commands, increasing successful executions (pass rate) for 55% of models and making them 36% more likely to answer user queries correctly. Troubleshooting documents further improve pass rates by 10% compared to official documentation.

Conclusion: Reflection using RAG and troubleshooting documents significantly improves the repair and success rate of tool calls, especially with kubectl commands.

Abstract: Agentic systems interact with external systems by calling tools such as
Python functions, REST API endpoints, or command line tools such as kubectl in
Kubernetes. These tool calls often fail for various syntactic and semantic
reasons. Some less obvious semantic errors can only be identified and resolved
after analyzing the tool's response. To repair these errors, we develop a
post-tool execution reflection component that combines large language model
(LLM)-based reflection with domain-specific retrieval-augmented generation
(RAG) using documents describing both the specific tool being called and
troubleshooting documents related to the tool. For this paper, we focus on the
use case of the kubectl command line tool to manage Kubernetes, a platform for
orchestrating cluster applications. Through a larger empirical study and a
smaller manual evaluation, we find that our RAG-based reflection will repair
kubectl commands such that they are both more likely to successfully execute
(pass rate) for 55% of our models evaluated and 36% more likely to correctly
answer the user query on average. We find that troubleshooting documents
improve pass rate compared to official documentation by an average of 10%.

</details>


### [6] [TritonRL: Training LLMs to Think and Code Triton Without Cheating](https://arxiv.org/abs/2510.17891)
*Jiin Woo,Shaowei Zhu,Allen Nie,Zhen Jia,Yida Wang,Youngsuk Park*

Main category: cs.SE

TL;DR: The paper introduces TritonRL, a specialized LLM for Triton kernel generation using a novel reinforcement learning framework. By tackling data scarcity and reward hacking, TritonRL achieves state-of-the-art correctness and speed, outperforming existing solutions in generating reliable, high-performance kernels.


<details>
  <summary>Details</summary>
Motivation: Large language models are rapidly advancing, but generating system kernels—especially for frameworks like Triton—faces significant challenges due to scarce training data and unclear evaluation metrics, which can lead to reward hacking and unreliable models.

Method: The authors developed TritonRL, a domain-specialized large language model for generating Triton kernels. The model is trained using a new training framework that combines supervised fine-tuning on curated datasets with reinforcement learning (RL). The RL approach includes robust, verifiable rewards and hierarchical reward assignment to detect and prevent reward hacking, ensuring the model learns effectively. The framework also uses fine-grained verification and hierarchical reward decomposition to guide model outputs.

Result: TritonRL generated Triton kernels that achieved higher correctness and speed compared to all existing Triton-specific models. The model successfully replaced existing modules and performed robustly on KernelBench, thanks to its novel RL-based training paradigm.

Conclusion: The proposed TritonRL and its RL-based training approach effectively address challenges in Triton kernel generation, setting a new state-of-the-art in correctness and performance, and demonstrating a robust solution for domain-specific kernel synthesis.

Abstract: With the rapid evolution of large language models (LLMs), the demand for
automated, high-performance system kernels has emerged as a key enabler for
accelerating development and deployment. We introduce TritonRL, a
domain-specialized LLM for Triton kernel generation, trained with a novel
training framework that enables robust and automated kernel synthesis. Unlike
general-purpose programming languages, Triton kernel generation faces unique
challenges due to data scarcity and incomplete evaluation criteria, vulnerable
to reward hacking. Our approach addresses these challenges end-to-end by
distilling Triton-specific knowledge through supervised fine-tuning on curated
datasets, and further improving code quality via reinforcement learning (RL)
with robust, verifiable rewards and hierarchical reward assignment. Our RL
framework robustly detects reward hacking and guides both reasoning traces and
code tokens through fine-grained verification and hierarchical reward
decomposition, enabling the model to generate high-quality Triton kernels that
can truly replace existing modules. With robust and fine-grained evaluation,
our experiments on KernelBench demonstrate that TritonRL achieves
state-of-the-art correctness and speedup, surpassing all other Triton-specific
models and underscoring the effectiveness of our RL-based training paradigm.

</details>


### [7] [A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension: Implications for Computing Education Research and Practice](https://arxiv.org/abs/2510.17894)
*Yunhan Qiao,Md Istiak Hossain Shihab,Christopher Hundhausen*

Main category: cs.SE

TL;DR: A review of recent literature shows that while GenAI tools can help with code comprehension, their explanations are often flawed and novices struggle with prompt creation. More research and improved tool design are needed for effective educational use.


<details>
  <summary>Details</summary>
Motivation: With the rise of generative AI assistants in software engineering, there is a growing need to understand how these tools impact code comprehension, especially in educational contexts. Educators and learners face challenges and opportunities with GenAI's role in explaining and generating code.

Method: The authors conducted a systematic literature review (SLR) of 31 recent studies (2022-2024) focused on GenAI tools used to enhance code comprehension. The review classified different approaches and tools, studied research methods, and summarized empirical evaluations.

Result: The SLR found that GenAI assistants can improve code comprehension but often provide inaccurate or unclear explanations. Novice programmers have difficulty creating effective prompts. The review classified existing GenAI-based tools, outlined study methodologies, and summarized their efficacy as reported in the literature.

Conclusion: While GenAI tools present significant potential to aid code comprehension, challenges such as explanation quality and prompt effectiveness limit their usefulness, especially for beginners. The study highlights the need for better GenAI interfaces and more research into their educational impact.

Abstract: The ability to comprehend code has long been recognized as an essential skill
in software engineering. As programmers lean more heavily on generative
artificial intelligence (GenAI) assistants to develop code solutions, it is
becoming increasingly important for programmers to comprehend GenAI solutions
so that they can verify their appropriateness and properly integrate them into
existing code. At the same time, GenAI tools are increasingly being enlisted to
provide programmers with tailored explanations of code written both by GenAI
and humans. Thus, in computing education, GenAI presents new challenges and
opportunities for learners who are trying to comprehend computer programs. To
provide computing educators with evidence-based guidance on the use of GenAI to
facilitate code comprehension and to identify directions for future research,
we present a systematic literature review (SLR) of state-of-the-art approaches
and tools that leverage GenAI to enhance code comprehension. Our SLR focuses on
31 studies published between 2022 and 2024. Despite their potential, GenAI
assistants often yield inaccurate or unclear explanations, and novice
programmers frequently struggle to craft effective prompts, thereby impeding
their ability to leverage GenAI to aid code comprehension. Our review
classifies GenAI-based approaches and tools, identifies methods used to study
them, and summarizes the empirical evaluations of their effectiveness. We
consider the implications of our findings for computing education research and
practice, and identify directions for future research.

</details>


### [8] [SpecAgent: A Speculative Retrieval and Forecasting Agent for Code Completion](https://arxiv.org/abs/2510.17925)
*George Ma,Anurag Koul,Qi Chen,Yawen Wu,Sachit Kuhar,Yu Yu,Aritra Sengupta,Varun Kumar,Murali Krishna Ramanathan*

Main category: cs.SE

TL;DR: SpecAgent is an agent for LLMs that speculatively indexes repository files to generate anticipatory context, improving code-generation quality and latency for realistic code tasks. It also introduces a new benchmark addressing context leakage, and demonstrates significant gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM approaches for code tasks struggle with project-specific APIs and dependencies in real-world repositories. Retrieval-augmented approaches help, but low latency budgets lead to trade-offs between retrieval quality and user experience. Additionally, current benchmarks suffer from future context leakage, falsely inflating performance metrics.

Method: SpecAgent proactively explores repository files during indexing, constructing speculative contexts that anticipate future edits, thus masking latency by shifting context computation earlier. Additionally, the paper develops a synthetic, leakage-free benchmark for fair evaluation.

Result: SpecAgent delivers consistent absolute improvements of 9-11% over baselines regarding code-generation quality, and substantially cuts inference latency. The improved benchmark provides more realistic evaluation.

Conclusion: SpecAgent outperforms baselines in code generation quality and latency, with absolute gains of 9-11% and significantly reduced inference latency.

Abstract: Large Language Models (LLMs) excel at code-related tasks but often struggle
in realistic software repositories, where project-specific APIs and cross-file
dependencies are crucial. Retrieval-augmented methods mitigate this by
injecting repository context at inference time. The low inference-time latency
budget affects either retrieval quality or the added latency adversely impacts
user experience. We address this limitation with SpecAgent, an agent that
improves both latency and code-generation quality by proactively exploring
repository files during indexing and constructing speculative context that
anticipates future edits in each file. This indexing-time asynchrony allows
thorough context computation, masking latency, and the speculative nature of
the context improves code-generation quality. Additionally, we identify the
problem of future context leakage in existing benchmarks, which can inflate
reported performance. To address this, we construct a synthetic, leakage-free
benchmark that enables a more realistic evaluation of our agent against
baselines. Experiments show that SpecAgent consistently achieves absolute gains
of 9-11% (48-58% relative) compared to the best-performing baselines, while
significantly reducing inference latency.

</details>


### [9] [From Charts to Code: A Hierarchical Benchmark for Multimodal Models](https://arxiv.org/abs/2510.17932)
*Jiahao Tang,Henry Hengyuan Zhao,Lijian Wu,Yifei Tao,Dongxing Mao,Yang Wan,Jingru Tan,Min Zeng,Min Li,Alex Jinpeng Wang*

Main category: cs.SE

TL;DR: Chart2Code is a new benchmark for testing how well large multimodal models handle chart-related code generation and editing in real-world scenarios. It presents a significant challenge, with top models struggling to achieve high scores, highlighting the need for further progress.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for rigorous evaluation of large multimodal models (LMMs) in tasks involving chart understanding and code generation, especially considering practical, user-driven scenarios. Existing benchmarks do not sufficiently capture the complexity and variability of real-world chart-to-code tasks.

Method: The authors constructed a hierarchical benchmark, Chart2Code, with three increasingly difficult levels: chart reproduction, chart editing, and long-table to chart generation. They designed 2,023 tasks across 22 chart types, paired with multi-level evaluation metrics, and tested 25 state-of-the-art LMMs on these tasks.

Result: The experimental evaluation revealed that even the best-performing model, GPT-5, scored only 0.57 on code correctness and 0.22 on chart-quality for editing tasks, indicating the substantial challenge posed by Chart2Code.

Conclusion: Chart2Code serves as a challenging benchmark that exposes current limitations in multimodal reasoning and chart-to-code generation for LMMs. The authors expect it to encourage further research and improvements in this domain.

Abstract: We introduce Chart2Code, a new benchmark for evaluating the chart
understanding and code generation capabilities of large multimodal models
(LMMs). Chart2Code is explicitly designed from a user-driven perspective,
capturing diverse real-world scenarios and progressively increasing task
difficulty. It consists of three levels: Level 1 (Chart Reproduction)
reproduces charts from a reference figure and user query; Level 2 (Chart
Editing) involves complex modifications such as changing chart types or adding
elements; and Level 3 (Long-Table to Chart Generation) requires models to
transform long, information-dense tables into faithful charts following user
instructions. To our knowledge, this is the first hierarchical benchmark that
reflects practical chart2code usage while systematically scaling task
complexity. In total, Chart2Code contains 2,023 tasks across 22 chart types,
paired with multi-level evaluation metrics that assess both code correctness
and the visual fidelity of rendered charts. We benchmark 25 state-of-the-art
(SoTA) LMMs, including both proprietary and the latest open-source models such
as GPT-5, Qwen2.5-VL, InternVL3/3.5, MiMo-VL, and Seed-1.6-VL. Experimental
results demonstrate that even the SoTA model GPT-5 averages only 0.57 on
code-based evaluation and 0.22 on chart-quality assessment across the editing
tasks, underscoring the difficulty of Chart2Code. We anticipate this benchmark
will drive advances in multimodal reasoning and foster the development of more
robust and general-purpose LMMs. Our code and data are available on Chart2Code.

</details>


### [10] [JunoBench: A Benchmark Dataset of Crashes in Python Machine Learning Jupyter Notebooks](https://arxiv.org/abs/2510.18013)
*Yiran Wang,José Antonio Hernández López,Ulf Nilsson,Dániel Varró*

Main category: cs.SE

TL;DR: JunoBench introduces a benchmark dataset of real crash-and-fix pairs from Python ML notebooks, supporting bug detection and repair research in interactive notebook workflows.


<details>
  <summary>Details</summary>
Motivation: There are few debugging tools for ML code in Jupyter notebooks, partly due to a lack of benchmarks for real-world crashes.

Method: The authors created JunoBench, a curated benchmark dataset featuring 111 reproducible crashes from public Kaggle notebooks, each with a verified fix. The dataset covers popular ML libraries and notebook-specific issues.

Result: JunoBench provides a unified environment for reliably reproducing crashes and fixes, making it easier to study bug detection, localization, and repair for notebook-based ML development.

Conclusion: JunoBench is the first benchmark for real-world ML notebook crashes. It enables reproducible studies and tool development for debugging in notebook environments.

Abstract: Jupyter notebooks are widely used for machine learning (ML) prototyping. Yet
few debugging tools are designed for ML code in notebooks, potentially due to
the lack of benchmarks. We introduce JunoBench, the first benchmark dataset of
real-world crashes in Python-based ML notebooks. JunoBench has 111 curated and
reproducible crashes from public Kaggle notebooks, each paired with a
verifiable fix, ranging over popular ML libraries, including TensorFlow/Keras,
PyTorch, Scikit-learn, Pandas, and NumPy, as well as notebook-specific
out-of-order execution issue. To support reproducibility and ease of use,
JunoBench offers a unified execution environment where crashes and fixes can be
reliably reproduced. By providing realistic crashes and their resolutions,
JunoBench facilitates bug detection, localization, and repair tailored to the
interactive and iterative nature of notebook-based ML development.

</details>


### [11] [DIP-AI: A Discovery Framework for AI Innovation Projects](https://arxiv.org/abs/2510.18017)
*Mariana Crisostomo Martins,Lucas Elias Cardoso Rocha,Lucas Cordeiro Romao,Taciana Novo Kudo,Marcos Kalinowski,Renato de Freitas Bulcao-Neto*

Main category: cs.SE

TL;DR: The paper introduces DIP-AI, a tailored framework to improve early problem discovery in AI projects, blending established standards and design thinking. Case study results show that DIP-AI effectively facilitates problem exploration and is beneficial for both academia and industry.


<details>
  <summary>Details</summary>
Motivation: Requirements Engineering (RE) faces difficulties with early-stage problem discovery in AI projects, which are increasingly data-intensive. There is a recognized gap in structured support for identifying project problems and requirements during the exploration phase of AI initiatives.

Method: The authors conducted a literature review and combined elements from standards ISO 12207, 5338, and Design Thinking to propose DIP-AI, a discovery framework for AI innovation projects. DIP-AI was evaluated through a case study in an industry-academia collaboration, where participants used the framework during the project discovery phase and provided feedback.

Result: Participants found DIP-AI to be relevant and useful for facilitating problem discovery in AI projects. The results showed improved problem identification, acceptance of the framework, and valuable suggestions for its application in real settings.

Conclusion: DIP-AI is an effective framework for guiding early-stage problem discovery in AI innovation projects. It enhances project outcomes by supporting more thorough problem identification and stakeholder satisfaction. DIP-AI is recommended for use in both academic and industry AI projects.

Abstract: Despite the increasing development of Artificial Intelligence (AI) systems,
Requirements Engineering (RE) activities face challenges in this new
data-intensive paradigm. We identified a lack of support for problem discovery
within AI innovation projects. To address this, we propose and evaluate DIP-AI,
a discovery framework tailored to guide early-stage exploration in such
initiatives. Based on a literature review, our solution proposal combines
elements of ISO 12207, 5338, and Design Thinking to support the discovery of AI
innovation projects, aiming at promoting higher quality deliveries and
stakeholder satisfaction. We evaluated DIP-AI in an industry-academia
collaboration (IAC) case study of an AI innovation project, in which
participants applied DIP-AI to the discovery phase in practice and provided
their perceptions about the approach's problem discovery capability,
acceptance, and suggestions. The results indicate that DIP-AI is relevant and
useful, particularly in facilitating problem discovery in AI projects. This
research contributes to academia by sharing DIP-AI as a framework for AI
problem discovery. For industry, we discuss the use of this framework in a real
IAC program that develops AI innovation projects.

</details>


### [12] [A Benchmark Dataset And LLMs Comparison For NFR Classification With Explainable AI](https://arxiv.org/abs/2510.18096)
*Esrat Ebtida Sakib,MD Ahnaf Akib,Md Muktadir Mazumder,Maliha Noushin Raida,Md. Mohsinul Kabir*

Main category: cs.SE

TL;DR: The paper creates an enriched NFR dataset, tests several LLMs for automated classification, and finds Gemma-2 and Phi-3 provide the best results. Automated methods are shown to be accurate and superior to manual identification for NFRs.


<details>
  <summary>Details</summary>
Motivation: Accurate identification and classification of Non-Functional Requirements (NFRs) is crucial for ensuring software systems meet quality standards such as performance, usability, and reliability. Manual methods for extracting NFRs are inefficient and error-prone, driving the need for automated solutions.

Method: The paper collected NFRs from Project Charters and Open Source Documentation to extend an existing NFR dataset. They further categorized NFRs into sub-classes and used multiple Large Language Models (RoBERTa, CodeBERT, Gemma-2, Phi-3, Mistral-8B, Llama-3.1-8B) to automate classification, evaluating outcomes via precision, recall, F1-score, and lime scores.

Result: Gemma-2 performed best among the tested LLMs, achieving precision 0.87, recall 0.89, F1-score 0.88, and lime hit score of 78/80. Phi-3 was the next best, with precision 0.85, recall 0.87, F1-score 0.86, and a lime hit score of 79.

Conclusion: Automated NFR classification, powered by improved data context and advanced LLMs, significantly outperforms manual methods and enhances both technical and user requirement comprehension. Gemma-2 and Phi-3 are particularly effective for this task.

Abstract: Non-Functional Requirements (NFRs) play a critical role in determining the
overall quality and user satisfaction of software systems. Accurately
identifying and classifying NFRs is essential to ensure that software meets
performance, usability, and reliability expectations. However, manual
identification of NFRs from documentation is time-consuming and prone to
errors, necessitating automated solutions. Before implementing any automated
solution, a robust and comprehensive dataset is essential. To build such a
dataset, we collected NFRs from various Project Charters and Open Source
Software Documentation. This enhanced the technical depth and usability of an
already existing NFR dataset. We categorized NFRs into sub-classes and
identified needs using widely used Large Language Models to facilitate
automation. After classifying the NFRs, we compared the classification results
of the selected LLMs: RoBERTa, CodeBERT, Gemma-2, Phi-3, Mistral-8B, and
Llama-3.1-8B using various evaluation metrics, including precision, recall,
F1-score, and lime scores. Among these models, Gemma-2 achieved the best
results with a precision of 0.87, recall of 0.89, and F1-score of 0.88,
alongside a lime hit score of 78 out of 80. Phi-3 closely followed with a
precision of 0.85, recall of 0.87, F1-score of 0.86, and the highest lime hit
score of 79. By improving the contextual foundation, this integration enhanced
the model's comprehension of technical aspects and user requirements.

</details>


### [13] [BlueCodeAgent: A Blue Teaming Agent Enabled by Automated Red Teaming for CodeGen AI](https://arxiv.org/abs/2510.18131)
*Chengquan Guo,Yuzhou Nie,Chulin Xie,Zinan Lin,Wenbo Guo,Bo Li*

Main category: cs.SE

TL;DR: The paper introduces BlueCodeAgent, a system that combines automated red and blue teaming to detect and defend against security risks in LLM-generated code. It significantly improves risk detection accuracy and reduces false positives compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: Security concerns are increasing as LLMs are used for code generation, but most research focuses on red teaming (finding vulnerabilities) rather than blue teaming (defense). There is a need for methods that can reliably identify and defend against unsafe code outputs from these models.

Method: The paper proposes BlueCodeAgent, an end-to-end blue teaming agent supported by automated red teaming. Red teaming generates risky instances, and the blue teaming agent uses these to detect both known and novel risks through constitution and code analysis, using agentic integration for layered defense. For vulnerable code, dynamic analysis is used to reduce false positives.

Result: BlueCodeAgent outperforms the base models and safety-prompt defenses on three tasks: bias detection, malicious instruction detection, and vulnerable code detection, achieving an average improvement of 12.7% in F1 score across four datasets. It is particularly effective at reducing false positives in vulnerable code detection.

Conclusion: BlueCodeAgent successfully integrates red and blue teaming to enhance LLM code generation safety, demonstrating that continuous vulnerability discovery (red teaming) strengthens defense (blue teaming) and improves risk detection effectiveness.

Abstract: As large language models (LLMs) are increasingly used for code generation,
concerns over the security risks have grown substantially. Early research has
primarily focused on red teaming, which aims to uncover and evaluate
vulnerabilities and risks of CodeGen models. However, progress on the blue
teaming side remains limited, as developing defense requires effective semantic
understanding to differentiate the unsafe from the safe. To fill in this gap,
we propose BlueCodeAgent, an end-to-end blue teaming agent enabled by automated
red teaming. Our framework integrates both sides: red teaming generates diverse
risky instances, while the blue teaming agent leverages these to detect
previously seen and unseen risk scenarios through constitution and code
analysis with agentic integration for multi-level defense. Our evaluation
across three representative code-related tasks--bias instruction detection,
malicious instruction detection, and vulnerable code detection--shows that
BlueCodeAgent achieves significant gains over the base models and safety
prompt-based defenses. In particular, for vulnerable code detection tasks,
BlueCodeAgent integrates dynamic analysis to effectively reduce false
positives, a challenging problem as base models tend to be over-conservative,
misclassifying safe code as unsafe. Overall, BlueCodeAgent achieves an average
12.7\% F1 score improvement across four datasets in three tasks, attributed to
its ability to summarize actionable constitutions that enhance context-aware
risk detection. We demonstrate that the red teaming benefits the blue teaming
by continuously identifying new vulnerabilities to enhance defense performance.

</details>


### [14] [When Old Meets New: Evaluating the Impact of Regression Tests on SWE Issue Resolution](https://arxiv.org/abs/2510.18270)
*Yang Chen,Toufique Ahmed,Reyhaneh Jabbarvand,Martin Hirzel*

Main category: cs.SE

TL;DR: TestPrune is an automated tool that intelligently reduces the size of regression test suites for LLM-driven bug reproduction and patch validation. It improves both reproduction and resolution rates by 6-13% with negligible extra cost, making debugging workflows more efficient.


<details>
  <summary>Details</summary>
Motivation: Despite having extensive test suites with high coverage, many real-world projects still fail to catch all bugs, as evidenced by unresolved issues in open-source trackers. Slow or ineffective bug reproduction and patch validation processes hinder debugging and repair pipelines.

Method: TestPrune is a fully automated technique that utilizes issue tracker reports and reuses regression tests for bug reproduction and patch validation. It automatically minimizes regression suites to the most relevant subset of tests, making large suites manageable for LLM-based debugging agents.

Result: TestPrune achieves a 6.2%-9.0% relative increase in issue reproduction rate in the Otter framework and a 9.4%-12.9% relative increase in issue resolution rate in the Agentless framework on SWE-Bench Lite and SWE-Bench Verified benchmarks. The additional computational cost is low ($0.02 - $0.05) per instance using GPT-4o and Claude-3.7-Sonnet.

Conclusion: TestPrune significantly enhances the effectiveness of bug reproduction and patch validation in automated debugging workflows by pruning test suites to relevant subsets, improving performance and efficiency with minimal costs.

Abstract: Test suites in real-world projects are often large and achieve high code
coverage, yet they remain insufficient for detecting all bugs. The abundance of
unresolved issues in open-source project trackers highlights this gap. While
regression tests are typically designed to ensure past functionality is
preserved in the new version, they can also serve a complementary purpose:
debugging the current version. Specifically, regression tests can (1) enhance
the generation of reproduction tests for newly reported issues, and (2)
validate that patches do not regress existing functionality. We present
TestPrune, a fully automated technique that leverages issue tracker reports and
strategically reuses regression tests for both bug reproduction and patch
validation.
  A key contribution of TestPrune is its ability to automatically minimize the
regression suite to a small, highly relevant subset of tests. Due to the
predominance of LLM-based debugging techniques, this minimization is essential
as large test suites exceed context limits, introduce noise, and inflate
inference costs. TestPrune can be plugged into any agentic bug repair pipeline
and orthogonally improve overall performance. As a proof of concept, we show
that TestPrune leads to a 6.2%-9.0% relative increase in issue reproduction
rate within the Otter framework and a 9.4% - 12.9% relative increase in issue
resolution rate within the Agentless framework on SWE-Bench Lite and SWE-Bench
Verified benchmarks, capturing fixes that were correctly produced by agents but
not submitted as final patches. Compared to the benefits, the cost overhead of
using TestPrune is minimal, i.e., \$0.02 and \$0.05 per SWE-Bench instance,
using GPT-4o and Claude-3.7-Sonnet models, respectively.

</details>


### [15] [Ensuring Robustness in ML-enabled Software Systems: A User Survey](https://arxiv.org/abs/2510.18292)
*Hala Abdelkader,Mohamed Abdelrazek,Priya Rani,Rajesh Vasa,Jean-Guy Schneider*

Main category: cs.SE

TL;DR: ML-enabled software systems have key robustness gaps. The ML-On-Rails protocol addresses these by integrating safeguards and standardizing communication, validated through a practitioner survey. More support for ML engineers is needed.


<details>
  <summary>Details</summary>
Motivation: ML-enabled software systems face unique robustness challenges such as silent failures, out-of-distribution data, and adversarial attacks, which traditional software engineering cannot fully address.

Method: The authors propose ML-On-Rails, a unified protocol that incorporates OOD detection, adversarial attack detection, input validation, explainability, and a model-to-software communication framework using HTTP status codes. They also conducted a practitioner survey to validate real-world needs.

Result: The survey identified major robustness issues and gaps in current practice, showing that ML-On-Rails can address these and improve system robustness. It also indicated a need for more support and resources for engineers working with ML systems.

Conclusion: ML-On-Rails offers a comprehensive and standardized solution for improving the robustness and trustworthiness of ML-enabled software systems, with future development guided by practitioner feedback and real-world application.

Abstract: Ensuring robustness in ML-enabled software systems requires addressing
critical challenges, such as silent failures, out-of-distribution (OOD) data,
and adversarial attacks. Traditional software engineering practices, which rely
on predefined logic, are insufficient for ML components that depend on data and
probabilistic decision-making. To address these challenges, we propose the
ML-On-Rails protocol, a unified framework designed to enhance the robustness
and trustworthiness of ML-enabled systems in production. This protocol
integrates key safeguards such as OOD detection, adversarial attack detection,
input validation, and explainability. It also includes a model-to-software
communication framework using HTTP status codes to enhance transparency in
reporting model outcomes and errors. To align our approach with real-world
challenges, we conducted a practitioner survey, which revealed major robustness
issues, gaps in current solutions, and highlighted how a standardised protocol
such as ML-On-Rails can improve system robustness. Our findings highlight the
need for more support and resources for engineers working with ML systems.
Finally, we outline future directions for refining the proposed protocol,
leveraging insights from the survey and real-world applications to continually
enhance its effectiveness.

</details>


### [16] [InspectCoder: Dynamic Analysis-Enabled Self Repair through interactive LLM-Debugger Collaboration](https://arxiv.org/abs/2510.18327)
*Yunkun Wang,Yue Zhang,Guochang Li,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: InspectCoder lets LLMs use debuggers interactively for smarter bug fixing, greatly boosting both accuracy and efficiency compared to previous methods, paving the way for more advanced automated debugging tools.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based program repair tools struggle to diagnose and fix complex bugs due to lack of dynamic analysis and interactive debugging, unlike human debuggers who can actively explore state and behavior.

Method: A dual-agent framework where LLMs strategically place breakpoints, perform targeted inspection of program states, and conduct incremental runtime experiments using debugger feedback, rather than relying solely on static analysis or execution logs.

Result: InspectCoder delivers 5.10%-60.37% relative improvement in repair accuracy and 1.67x-2.24x better bug-fix efficiency vs. previous state-of-the-art, validated across two challenging benchmarks.

Conclusion: InspectCoder significantly improves LLM-based code repair by enabling dynamic, interactive debugging, outperforming existing methods in both accuracy and efficiency. Their open-source middleware, InspectWare, facilitates reproducible research and practical adoption.

Abstract: Large Language Models (LLMs) frequently generate buggy code with complex
logic errors that are challenging to diagnose. While existing LLM-based
self-repair approaches conduct intensive static semantic analysis or reply on
superficial execution logs, they miss the in-depth runtime behaviors that often
expose bug root causes-lacking the interactive dynamic analysis capabilities
that make human debugging effective. We present InspectCoder, the first agentic
program repair system that empowers LLMs to actively conduct dynamic analysis
via interactive debugger control. Our dual-agent framework enables strategic
breakpoint placement, targeted state inspection, and incremental runtime
experimentation within stateful debugger sessions. Unlike existing methods that
follow fixed log collection procedures, InspectCoder adaptively inspects and
perturbs relevant intermediate states at runtime, and leverages immediate
process rewards from debugger feedback to guide multi-step reasoning,
transforming LLM debugging paradigm from blind trial-and-error into systematic
root cause diagnosis. We conduct comprehensive experiments on two challenging
self-repair benchmarks: BigCodeBench-R and LiveCodeBench-R. InspectCoder
achieves 5.10%-60.37% relative improvements in repair accuracy over the
strongest baseline, while delivering 1.67x-2.24x superior bug-fix efficiency
respectively. We also contribute InspectWare, an open-source middleware that
abstracts debugger complexities and maintains stateful debugging sessions
across mainstream Python testing frameworks. Our work provides actionable
insight into the interactive LLM-debugger systems, demonstrating the
significant potential of LLM-driven dynamic analysis for automated software
engineering.

</details>


### [17] [Human to Document, AI to Code: Three Case Studies of Comparing GenAI for Notebook Competitions](https://arxiv.org/abs/2510.18430)
*Tasha Settewong,Youmei Fan,Raula Gaikovina Kula,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: This paper compares gold medal-winning human-written and GenAI-generated computational notebooks, finding human notebooks excel in documentation detail and innovation, while GenAI notebooks demonstrate better code quality. The research sets the stage for future work on optimizing human-AI collaboration in data science notebooks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is the growing use of computational notebooks by data scientists, coupled with the rising presence of Generative AI (GenAI) technologies. With increased competition and collaboration, there is a need to understand the distinguishing features of human-written and GenAI-generated notebooks.

Method: The authors conducted three case studies focusing on coding and documenting activities in computational notebooks. They analyzed 25 features from human-written (particularly Kaggle medal-winning) and GenAI-generated notebooks, comparing aspects like documentation length, code quality, and structural diversity.

Result: Gold medal-winning Kaggle notebooks are noted for their longer and more detailed documentation. GenAI-generated notebooks generally have higher code quality, with fewer code smells and reduced technical debt, but human-authored notebooks reveal greater structural diversity, complexity, and innovative problem-solving.

Conclusion: The study highlights the respective strengths of humans and GenAI in notebook creation. The authors propose four research agendas to further explore how GenAI can be integrated to enhance collaboration and results in computational notebooks.

Abstract: Computational notebooks have become the preferred tool of choice for data
scientists and practitioners to perform analyses and share results. Notebooks
uniquely combine scripts with documentation. With the emergence of generative
AI (GenAI) technologies, it is increasingly important, especially in
competitive settings, to distinguish the characteristics of human-written
versus GenAI.
  In this study, we present three case studies to explore potential strengths
of both humans and GenAI through the coding and documenting activities in
notebooks. We first characterize differences between 25 code and documentation
features in human-written, medal-winning Kaggle notebooks. We find that gold
medalists are primarily distinguished by longer and more detailed
documentation. Second, we analyze the distinctions between human-written and
GenAI notebooks. Our results show that while GenAI notebooks tend to achieve
higher code quality (as measured by metrics like code smells and technical
debt), human-written notebooks display greater structural diversity,
complexity, and innovative approaches to problem-solving. Based on these
results, we envision the work as groundwork that highlight four agendas to
further investigate how GenAI could be utilized in notebooks that maximizes the
potential collaboration between human and AI.

</details>


### [18] [Real-World Usability of Vulnerability Proof-of-Concepts: A Comprehensive Study](https://arxiv.org/abs/2510.18448)
*Wenjing Dang,Kaixuan Li,Sen Chen,Zhenwei Zhuo,Lyuye Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: This paper presents the largest study on vulnerability PoC reports, revealing most CVEs lack PoCs and many reports are incomplete or hard to reproduce. It analyzes factors behind these issues and offers solutions to make PoCs more effective in real-world security practices.


<details>
  <summary>Details</summary>
Motivation: Proof-of-Concept (PoC) reports are critical for validating vulnerabilities and informing security responses, but research on PoCs is underdeveloped compared to vulnerability data studies. Challenges include dispersed PoCs, inconsistent formats, and difficulty in reproduction.

Method: The authors conducted the first large-scale study of PoCs, collecting 470,921 PoCs from 13 platforms. They used pattern-matching with a fine-tuned BERT-NER model to extract key report components and recruited participants to manually test the reproducibility of vulnerabilities using PoC reports.

Result: The study found that 78.9% of CVE vulnerabilities do not have available PoCs. Additionally, existing PoC reports lack around 30% of the key information necessary for reproduction, and there are various reasons for failed reproductions. The paper identifies these factors and proposes strategies to improve PoC usability.

Conclusion: There is a significant gap in PoC report availability and completeness, impeding effective vulnerability understanding and mitigation. The proposed strategies can help stakeholders improve PoC quality and software security.

Abstract: The Proof-of-Concept (PoC) for a vulnerability is crucial in validating its
existence, mitigating false positives, and illustrating the severity of the
security threat it poses. However, research on PoCs significantly lags behind
studies focusing on vulnerability data. This discrepancy can be directly
attributed to several challenges, including the dispersion of real-world PoCs
across multiple platforms, the diversity in writing styles, and the difficulty
associated with PoC reproduction. To fill this gap, we conduct the first
large-scale study on PoCs in the wild, assessing their report availability,
completeness, reproducibility. Specifically, 1) to investigate PoC reports
availability for CVE vulnerability, we collected an extensive dataset of
470,921 PoCs and their reports from 13 platforms, representing the broadest
collection of publicly available PoCs to date. 2) To assess the completeness of
PoC report at a fine-grained level, we proposed a component extraction method,
which combines pattern-matching techniques with a fine-tuned BERT-NER model to
extract 9 key components from PoC reports. 3) To evaluate the effectiveness of
PoCs, we recruited 8 participants to manually reproduce 150 sampled
vulnerabilities with 32 vulnerability types based on PoC reports, enabling an
in-depth analysis of PoC reproducibility and the factors influencing it. Our
findings reveal that 78.9% of CVE vulnerabilities lack available PoCs, and
existing PoC reports typically miss about 30% of the essential components
required for effective vulnerability understanding and reproduction, with
various reasons identified for the failure to reproduce vulnerabilities using
available PoC reports. Finally, we proposed actionable strategies for
stakeholders to enhance the overall usability of vulnerability PoCs in
strengthening software security.

</details>


### [19] [Large Language Models in Thematic Analysis: Prompt Engineering, Evaluation, and Guidelines for Qualitative Software Engineering Research](https://arxiv.org/abs/2510.18456)
*Cristina Martinez Montes,Robert Feldt,Cristina Miguel Martos,Sofia Ouhbi,Shweta Premanandan,Daniel Graziotin*

Main category: cs.SE

TL;DR: The paper presents a systematic way to integrate and test LLMs in qualitative thematic analysis. LLM outputs were often preferred for coding, but humans still did better with complex interpretations. Guidelines help ensure rigor when LLMs are used, but some human insight remains irreplaceable.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in applying large language models (LLMs) to qualitative research methods in software engineering, but reproducible integration techniques and systematic quality assessment are lacking, especially for the commonly used thematic analysis (TA).

Method: The authors developed and refined prompts tailored to Phases 2-5 of Braun and Clarke’s reflexive TA, then used 15 interviews with software engineers for blind evaluation. Outputs from multiple LLMs were compared against codes/themes generated by experienced researchers, using expert evaluators and rubrics based on established quality criteria.

Result: Expert evaluators preferred LLM-generated codes 61% of the time, finding them useful. However, limitations were noted: LLMs sometimes fragmented data excessively, missed deeper interpretations, and produced vague or bounded themes.

Conclusion: A reproducible framework for LLM-assisted thematic analysis was created, guidelines for effective integration were proposed, and it was clarified that while LLMs can contribute valuably to qualitative analysis, human judgment is still required for nuanced interpretation.

Abstract: As artificial intelligence advances, large language models (LLMs) are
entering qualitative research workflows, yet no reproducible methods exist for
integrating them into established approaches like thematic analysis (TA), one
of the most common qualitative methods in software engineering research.
Moreover, existing studies lack systematic evaluation of LLM-generated
qualitative outputs against established quality criteria. We designed and
iteratively refined prompts for Phases 2-5 of Braun and Clarke's reflexive TA,
then tested outputs from multiple LLMs against codes and themes produced by
experienced researchers. Using 15 interviews on software engineers' well-being,
we conducted blind evaluations with four expert evaluators who applied rubrics
derived directly from Braun and Clarke's quality criteria. Evaluators preferred
LLM-generated codes 61% of the time, finding them analytically useful for
answering the research question. However, evaluators also identified
limitations: LLMs fragmented data unnecessarily, missed latent interpretations,
and sometimes produced themes with unclear boundaries. Our contributions are
threefold. First, a reproducible approach integrating refined, documented
prompts with an evaluation framework to operationalize Braun and Clarke's
reflexive TA. Second, an empirical comparison of LLM- and human-generated codes
and themes in software engineering data. Third, guidelines for integrating LLMs
into qualitative analysis while preserving methodological rigour, clarifying
when and how LLMs can assist effectively and when human interpretation remains
essential.

</details>


### [20] [CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment](https://arxiv.org/abs/2510.18471)
*Xue Jiang,Yihong Dong,Mengyang Liu,Hongyi Deng,Tian Wang,Yongding Tao,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: CodeRL+ improves how LLMs learn code by aligning textual code with its execution through variable-level feedback, outperforming existing methods and showing strong generalizability.


<details>
  <summary>Details</summary>
Motivation: Current LLM code generation relies mainly on textual pattern learning, which creates a semantic gap with functional correctness determined by code execution. RLVR uses test case outcomes but struggles with nuanced logical errors due to crude reward signals.

Method: CodeRL+ uses execution semantics alignment in the RLVR training pipeline, enabling models to infer variable-level execution trajectories and providing direct learning signals. It leverages existing on-policy rollouts and integrates with multiple RL algorithms.

Result: Experiments show CodeRL+ outperforms baselines (RLVR and Distillation) with a 4.6% average improvement in pass@1, 15.5% higher accuracy on code reasoning benchmarks, and 4.4% higher accuracy on test-output-generation tasks, with robust applicability across models and RL algorithms.

Conclusion: CodeRL+ significantly enhances the alignment between code's textual representation and its execution semantics, improving code generation performance in LLMs and generalizing effectively across various tasks and RL algorithms.

Abstract: While Large Language Models (LLMs) excel at code generation by learning from
vast code corpora, a fundamental semantic gap remains between their training on
textual patterns and the goal of functional correctness, which is governed by
formal execution semantics. Reinforcement Learning with Verifiable Rewards
(RLVR) approaches attempt to bridge this gap using outcome rewards from
executing test cases. However, solely relying on binary pass/fail signals is
inefficient for establishing a well-aligned connection between the textual
representation of code and its execution semantics, especially for subtle
logical errors within the code. In this paper, we propose CodeRL+, a novel
approach that integrates execution semantics alignment into the RLVR training
pipeline for code generation. CodeRL+ enables the model to infer variable-level
execution trajectory, providing a direct learning signal of execution
semantics. CodeRL+ can construct execution semantics alignment directly using
existing on-policy rollouts and integrates seamlessly with various RL
algorithms. Extensive experiments demonstrate that CodeRL+ outperforms
post-training baselines (including RLVR and Distillation), achieving a 4.6%
average relative improvement in pass@1. CodeRL+ generalizes effectively to
other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning
and test-output-generation benchmarks, respectively. CodeRL+ shows strong
applicability across diverse RL algorithms and LLMs. Furthermore, probe
analyses provide compelling evidence that CodeRL+ strengthens the alignment
between code's textual representations and its underlying execution semantics.

</details>


### [21] [VAPU: System for Autonomous Legacy Code Modernization](https://arxiv.org/abs/2510.18509)
*Valtteri Ala-Salmi,Zeeshan Rasheed,Abdul Malik Sami,Muhammad Waseem,Kai-Kristian Kemell,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: This paper presents VAPU, an LLM-powered multi-agent system for modernizing legacy applications, showing that it fulfills more code update requirements than traditional prompt approaches while maintaining similar error rates.


<details>
  <summary>Details</summary>
Motivation: Modernizing legacy applications is crucial due to compatibility, security, and reliability risks associated with outdated code, but high resource costs discourage companies from updating. LLM-based multi-agent systems have emerged as promising solutions for automating complex, multi-phased modernization tasks.

Method: The study introduces a multi-agent system called Verifying Agent Pipeline Updater (VAPU), which simulates various software development roles to autonomously update code files in phases. The evaluation extends prior work by using five different LLMs at varying temperature settings, and compares the system's performance on 20 open-source Python projects against Zero-Shot and One-Shot Learning prompt-based methods.

Result: VAPU performs comparably to ZSL/OSL in terms of error count, especially at low temperature settings, but achieves a significantly higher rate of fulfilled update requirements—up to 22.5% more—depending on the LLM used.

Conclusion: LLM-based multi-agent systems, such as VAPU, are effective and cost-efficient solutions for autonomously updating legacy application components, achieving greater success in meeting update requirements compared to conventional prompting methods.

Abstract: In this study, we present a solution for the modernization of legacy
applications, an area of code generation where LLM-based multi-agent systems
are proving essential for complex multi-phased tasks. Legacy applications often
contain deprecated components that create compatibility, security, and
reliability risks, but high resource costs make companies hesitate to update.
We take a step forward to integrate an LLM-based multi-agent system as part of
a legacy web application update to provide a cost-effective solution to update
legacy applications autonomously. We propose a multi-agent system named a
Verifying Agent Pipeline Updater (VAPU), which is designed to update code files
in phases while simulating different roles in a software development team. In
our previous study, we evaluated the system for legacy version updates by using
six legacy web application view files by resulting errors and accomplished
requirements. This study extends the previous evaluation of a multi-agent
pipeline system by extending the evaluation of VAPU from a single LLM to five
LLMs and using the temperature parameter in both 0 to 1 settings. Additionally,
we tested the system with 20 open-source Python GitHub projects. The results of
the evaluation were compared to Zero-Shot Learning (ZSL) and One-Shot Learning
(OSL) prompts. The extended evaluation of VAPU showed that particularly in a
low-temperature VAPU can get similar level of error count compared to the
ZSL/OSL prompts but with a higher level of fulfilled requirements, depending on
the LLM. VAPU showed up to 22.5% increase in the succeeding Python file update
requirements compared to ZSL/OSL prompts. The study indicates that an LLM-based
multi-agent system is a capable solution to update components of a legacy
application autonomously.

</details>


### [22] [Mining Service Behavior for Stateful Service Emulation](https://arxiv.org/abs/2510.18519)
*Md Arafat Hossain,Jun Han,Muhammad Ashad Kabir,Steve Versteeg,Jean-Guy Schneider,Jiaojiao Jiang*

Main category: cs.SE

TL;DR: The paper introduces a new method for service virtualization that accounts for service state and dependencies in message data, resulting in better accuracy and efficiency in testing enterprise software systems.


<details>
  <summary>Details</summary>
Motivation: Testing interconnected enterprise software systems is challenging because accessing all connected services is difficult. Service virtualization helps emulate service behaviors for testing, but current methods often ignore service state, reducing accuracy for stateful services.

Method: The paper proposes an approach to model service interactions that includes service state. This is done by identifying contextual dependencies among interaction messages and analyzing the relationships between data values within those messages.

Result: The approach is evaluated with traces from both stateful and stateless services, showing notable improvements in response generation accuracy and efficiency compared to previous methods.

Conclusion: Considering service state in service virtualization models leads to more accurate and realistic test environments, especially for stateful services.

Abstract: Enterprise software systems are increasingly integrating with diverse
services to meet expanding business demands. Testing these highly
interconnected systems presents a challenge due to the need for access to the
connected services. Service virtualization has emerged as a widely used
technique to derive service models from recorded interactions, for service
response generation during system testing. Various methods have been proposed
to emulate actual service behavior based on these interactions, but most fail
to account for the service's state, which reduces the accuracy of service
emulation and the realism of the testing environment, especially when dealing
with stateful services. This paper proposes an approach to deriving service
models from service interactions, which enhance the accuracy of response
generation by considering service state. This is achieved by uncovering
contextual dependencies among interaction messages and analyzing the
relationships between message data values. The approach is evaluated using
interaction traces collected from both stateful and stateless services, and the
results reveal notable enhancements in accuracy and efficiency over existing
approaches in service response generation.

</details>


### [23] [Demonstrators for Industrial Cyber-Physical System Research: A Requirements Hierarchy Driven by Software-Intensive Design](https://arxiv.org/abs/2510.18534)
*Uraz Odyurt,Richard Loendersloot,Tiedo Tinga*

Main category: cs.SE

TL;DR: The paper introduces a new framework to clarify and structure requirements for project demonstrators, helping reduce uncertainty in research projects. Although only partially validated, its use showed promising results in existing projects.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from confusion and mismatches in research projects due to poorly defined demonstrator requirements during proposal writing. Existing reliance on the TRL scale is inadequate for precisely describing what needs to be demonstrated.

Method: The paper proposes a demonstrator requirements elaboration framework, featuring 5 hierarchical levels of demonstration connected to work package interactions and industrial use-cases. It is applied to two research projects (one early stage, one final stage) for partial validation.

Result: Application of the framework to two projects shows its effectiveness, even though complete validation is not possible within the scope of this study.

Conclusion: A structured framework for elaborating demonstrator requirements can clarify expectations, improve feasibility assessment, and reduce confusion in research projects focused on software-intensive and cyber-physical systems.

Abstract: One of the challenges apparent in the organisation of research projects is
the uncertainties around the subject of demonstrators. A precise and detailed
elicitation of the coverage for project demonstrators is often an afterthought
and not sufficiently detailed during proposal writing. This practice leads to
continuous confusion and a mismatch between targeted and achievable
demonstration of results, hindering progress. The reliance on the TRL scale as
a loose descriptor does not help either. We propose a demonstrator requirements
elaboration framework aiming to evaluate the feasibility of targeted
demonstrations, making realistic adjustments, and assist in describing
requirements. In doing so, we define 5 hierarchical levels of demonstration,
clearly connected to expectations, e.g., work package interaction, and also
connected to the project's industrial use-cases. The considered application
scope in this paper is the domain of software-intensive systems and industrial
cyber-physical systems. A complete validation is not accessible, as it would
require application of our framework at the start of a project and observing
the results at the end, taking 4-5 years. Nonetheless, we have applied it to
two research projects from our portfolio, one at the early and another at the
final stages, revealing its effectiveness.

</details>


### [24] [When Abstraction Breaks Physics: Rethinking Modular Design in Quantum Software](https://arxiv.org/abs/2510.18557)
*Jianjun Zhao*

Main category: cs.SE

TL;DR: Classical software abstractions often break down in quantum programming, violating fundamental quantum constraints. This paper details key failure scenarios, proposes physically sound abstraction principles, and outlines research directions to invent quantum-native engineering abstractions.


<details>
  <summary>Details</summary>
Motivation: Classical software engineering relies on abstraction for modularity, reusability, and scalability, but quantum software has fundamentally different properties that may make classical abstraction methods unsafe or invalid.

Method: The paper analyzes the incompatibilities between classical abstraction and quantum computation by identifying fundamental conflicts and failure cases, and it proposes design principles and research directions specific to quantum software engineering.

Result: The paper presents three classes of failure cases where classical abstraction mechanisms fail in the context of quantum programming, and introduces new design principles, as well as research directions such as quantum-specific type systems, effect annotations, and contract-based module design.

Conclusion: Abstraction in quantum software engineering requires a rethinking of conventional methods, with new principles and tools tailored to the unique constraints of quantum computation.

Abstract: Abstraction is a fundamental principle in classical software engineering,
which enables modularity, reusability, and scalability. However, quantum
programs adhere to fundamentally different semantics, such as unitarity,
entanglement, the no-cloning theorem, and the destructive nature of
measurement, which introduce challenges to the safe use of classical
abstraction mechanisms. This paper identifies a fundamental conflict in quantum
software engineering: abstraction practices that are syntactically valid may
violate the physical constraints of quantum computation. We present three
classes of failure cases where naive abstraction breaks quantum semantics and
propose a set of design principles for physically sound abstraction mechanisms.
We further propose research directions, including quantum-specific type
systems, effect annotations, and contract-based module design. Our goal is to
initiate a systematic rethinking of abstraction in quantum software
engineering, based on quantum semantics and considering engineering
scalability.

</details>


### [25] [WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality](https://arxiv.org/abs/2510.18560)
*Chunyang Li,Yilun Zheng,Xinting Huang,Tianqing Fang,Jiahao Xu,Yangqiu Song,Lihui Chen,Han Hu*

Main category: cs.SE

TL;DR: WebDevJudge is a new benchmark for evaluating LLMs in web development tasks, showing that current LLM 'judges' fall short compared to humans, especially in complex, dynamic scenarios. The work points out key areas for improvement and lays groundwork for future robust automated evaluation systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations and unexplored reliability of using LLMs as judges for open-ended, complex tasks in dynamic environments, specifically in web development. The current paradigm is successful for well-defined tasks, but its application to more complex scenarios needs systematic evaluation.

Method: The paper introduces WebDevJudge, a comprehensive benchmark for assessing the performance of LLM-as-a-judge in web development. The method includes both static, non-interactive evaluation and dynamic, interactive evaluation, using human preference labels and structured rubrics. The authors evaluate a range of automated evaluators (LLMs, MLLMs, agentic workflows) and analyze the paradigms and guidance mechanisms.

Result: The experiments reveal a significant gap between the performance of LLM-based evaluators and human experts. The gap is attributed to model limitations such as failure in recognizing functional equivalence, verifying task feasibility, and mitigating bias.

Conclusion: WebDevJudge poses a formidable challenge for LLM-as-a-judge applications, highlighting their current shortcomings in complex web development scenarios and providing valuable insights for future research to enhance automated evaluator reliability and capability.

Abstract: The paradigm of LLM-as-a-judge is emerging as a scalable and efficient
alternative to human evaluation, demonstrating strong performance on
well-defined tasks. However, its reliability in open-ended tasks with dynamic
environments and complex interactions remains unexplored. To bridge the gap, we
introduce WebDevJudge, a systematic benchmark for assessing LLM-as-a-judge
performance in web development, with support for both non-interactive
evaluation based on static observations and continuous interactive evaluation
with a dynamic web environment. WebDevJudge comprises human preference labels
over paired web implementations, annotated with structured and query-grounded
rubrics to ensure high-quality ground truth. Using this benchmark, we
comprehensively evaluate various evaluators, including LLMs, MLLMs, and agentic
workflows. We systematically investigate the impact of different paradigms and
guidance mechanisms. Our experiments reveal a significant gap between LLM
judges and human experts. In-depth analysis indicates this gap stems from
fundamental model limitations, including failures in recognizing functional
equivalence, verifying task feasibility, and mitigating bias. Overall,
WebDevJudge presents a significant challenge to LLM-as-a-judge, offering
insights to guide future research toward developing more reliable and capable
automated evaluators for complicated scenarios. Code and data are available at
https://github.com/lcy2723/WebDevJudge.

</details>


### [26] [A Structured Evaluation Framework for Low-Code Platform Selection: A Multi-Criteria Decision Model for Enterprise Digital Transformation](https://arxiv.org/abs/2510.18590)
*Antonio Lamanna*

Main category: cs.SE

TL;DR: This paper proposes a practical, criteria-based evaluation model to help businesses systematically compare and select low-code platforms, addressing key areas like orchestration, UI/UX, integration, security, and AI features. Empirical tests show it leads to smarter choices and reduces platform selection risks.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption of Low-Code Development Platforms (LCDPs) has made it essential for organizations to have systematic, reliable methods for evaluating and choosing the most suitable platform.

Method: The paper introduces a comprehensive evaluation framework built upon five key criteria: Business Process Orchestration, UI/UX Customization, Integration and Interoperability, Governance and Security, and AI-Enhanced Automation. It utilizes a weighted scoring model to quantitatively compare platforms according to an organization's unique needs.

Result: Empirical validation in enterprise environments shows that the proposed framework helps organizations make better decisions, reducing risks of platform lock-in and unsuitable solutions.

Conclusion: A structured, criteria-based evaluation framework significantly enhances the platform selection process for LCDPs, improving strategic alignment and decision outcomes.

Abstract: The rapid adoption of Low-Code Development Platforms (LCDPs) has created a
critical need for systematic evaluation methodologies that enable organizations
to make informed platform selection decisions. This paper presents a
comprehensive evaluation framework based on five key criteria: Business Process
Orchestration, UI/UX Customization, Integration and Interoperability,
Governance and Security, and AI-Enhanced Automation. We propose a weighted
scoring model that allows organizations to quantitatively assess and compare
different low-code platforms based on their specific requirements and strategic
priorities. The framework addresses the gap between marketing-driven platform
comparisons and rigorous, context-specific evaluation methodologies. Through
empirical validation in enterprise environments, we demonstrate how this
structured approach can significantly improve decision-making outcomes and
reduce the risk of platform lock-in or inadequate solution selection.

</details>


### [27] [CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent](https://arxiv.org/abs/2510.18596)
*Haojia Lin,Xiaoyu Tan,Yulei Qin,Zihan Xu,Yuchen Shi,Zongyi Li,Gang Li,Shaofei Cai,Siqi Cai,Chaoyou Fu,Ke Li,Xing Sun*

Main category: cs.SE

TL;DR: This paper introduces CUARewardBench, the first comprehensive benchmark and dataset for evaluating reward models in computer-using agent tasks. Extensive experiments show general vision-language models outperform specialized ones, and highlight gaps in current model capabilities. The proposed Unanimous Prompt Ensemble (UPE) method sets a new standard for reliable reward evaluation, greatly exceeding previous approaches.


<details>
  <summary>Details</summary>
Motivation: Script-based verifiers used for computer-using agent (CUA) evaluation are limited in scalability and do not provide detailed, step-wise assessments. Reward models are a potential alternative, but their effectiveness in evaluating CUAs is not well-studied.

Method: The authors introduce CUARewardBench, a benchmark and dataset for evaluating reward models in CUA tasks. It includes expertly annotated trajectories from a diverse array of software and agent architectures. They perform extensive experiments with multiple vision-language models (VLMs) and prompt templates, analyze performance, and propose the Unanimous Prompt Ensemble (UPE) method to improve reward model reliability.

Result: Comprehensive analysis reveals limitations of current CUA reward models, such as weak visual reasoning and knowledge gaps. General VLMs outperform specialized models. The proposed UPE method dramatically improves reliability and performance for both outcome and process reward model evaluation, significantly surpassing single models and conventional ensembles.

Conclusion: CUARewardBench provides a systematic, scalable framework and dataset for evaluating reward models on computer-using agent tasks. The new UPE ensemble method sets a strong baseline for reliable reward model assessment, pushing forward research in this under-explored field.

Abstract: Computer-using agents (CUAs) enable task completion through natural
interaction with operating systems and software interfaces. While script-based
verifiers are widely adopted for evaluation, they suffer from limited
scalability and inability to provide step-wise assessment. Reward models offer
promising alternatives, but their effectiveness on CUA evaluation remains
largely underexplored. To address this gap, we present CUARewardBench,
comprising four key contributions: (1) First-ever Comprehensive CUA Reward
Benchmark: We introduce the first benchmark for evaluating both outcome reward
models (ORM) and process reward models (PRM) on CUA tasks, enabling systematic
assessment across trajectory-level and step-level evaluation. (2) Diverse,
Practical and Reliable Dataset: CUARewardBench encompasses trajectories from 10
software categories and 7 agent architectures with varying performance levels
(25.9%-50.8% success rates). All trajectories are expertly annotated through
carefully designed protocols, with rigorous quality control to ensure
reliability and practical applicability. (3) Comprehensive Analysis and
Insights: Through extensive experiments across 7 vision-language models and 3
prompt templates, we reveal critical limitations of current CUA RMs, including
insufficient visual reasoning capabilities, knowledge deficiencies, and the
superiority of general VLMs over specialized CUA models for reward evaluation.
(4) Unanimous Prompt Ensemble (UPE): Based on the insights from our
comprehensive analysis, we propose UPE, a novel ensemble method that
significantly enhances reward model reliability through strict unanimous voting
and strategic prompt-template configurations. UPE achieves 89.8% precision and
93.3% NPV for ORM, and 81.7% precision and 85.1% NPV for PRM, substantially
outperforming single VLMs and traditional ensemble approaches.

</details>


### [28] [An overview of the use of alternative funding and contracting approaches relevant for agile software development: A systematic review of real-life experiences](https://arxiv.org/abs/2510.18711)
*Bertha Ngereja,Magne Jørgensen*

Main category: cs.SE

TL;DR: Traditional approaches clash with agile principles, so this review identifies alternative funding and contracting models that better support agility. These alternatives offer enhanced satisfaction and efficiency but pose challenges like scope creep and cultural resistance. Hybrid models may ease the transition to more flexible methods.


<details>
  <summary>Details</summary>
Motivation: Traditional funding and contracting methods are often at odds with agile software development due to their rigidity, which hinders flexibility, collaboration, and profitability. The motivation is to explore alternatives that better support agile principles.

Method: A systematic literature review analyzing 38 peer-reviewed empirical studies from both private and public sector contexts, sourced from SCOPUS, Web of Science, and Google Scholar.

Result: Four alternative funding and four alternative contracting approaches aligning better with agile principles were identified. These alternatives brought higher client satisfaction, reduced contractor risk, and improved resource utilization. However, organizations may face cultural and structural challenges, increased risks (e.g., scope creep), and need extra resources to adopt them.

Conclusion: Organizations should consider hybrid approaches as a starting point, balancing flexibility and control, then gradually transition to fully flexible alternatives customized to their context and readiness.

Abstract: Agile software development emphasizes flexibility and iterative processes,
which may conflict with the more linear, rigid, and time-consuming traditional
funding and contracting approaches. This review synthesizes real-life
experiences of using alternative (non-traditional) contracting and funding
approaches. The focus is on identifying approaches that align better with agile
principles and understanding the motivations, benefits, and challenges these
alternatives present. A systematic literature review was conducted in SCOPUS,
Web of Science, and Google Scholar, where we identified 38 relevant
peer-reviewed empirical studies from private and public sector contexts. Four
alternative funding and four alternative contracting approaches were
identified. Organizations were motivated to adopt these alternative approaches
because traditional approaches often proved too rigid, conflicted with agile
principles, hindered effective client-contractor collaboration, and limited
profitability. The benefits of these alternatives included higher client
satisfaction, reduced contractor risk, and more efficient resource utilization.
Adopting alternative funding and contracting approaches may promote flexibility
and efficiency in agile projects but also presents cultural and structural
challenges, increases the risk of scope creep and analysis paralysis, and
requires additional effort in terms of time and resources. The context of the
organization matters highly in selecting a suitable approach, such as the
organizational readiness in terms of its leaders, people, and systems. Thus,
instead of wholly adopting alternative approaches and introducing changes
abruptly, organizations may benefit from starting with hybrid approaches that
balance flexibility and control and progressively transition to fully flexible
approaches tailored to their needs

</details>


### [29] [Causally Perturbed Fairness Testing](https://arxiv.org/abs/2510.18719)
*Chengwen Du,Tao Chen*

Main category: cs.SE

TL;DR: CausalFT enhances fairness testing in AI by applying causal inference to guide sample perturbations, outperforming correlation-based methods in revealing fairness bugs and improving bias resilience with minimal runtime overhead.


<details>
  <summary>Details</summary>
Motivation: Fairness testing in AI systems is essential to prevent discrimination over sensitive features such as gender, age, or race. However, current methods often overlook the value of data characteristics in guiding perturbation, limiting their effectiveness in discovering fairness bugs.

Method: The authors introduce CausalFT, a generic framework that leverages causal inference to identify and perturb non-sensitive features which are causally linked to sensitive ones, thereby exposing fairness bugs more effectively. CausalFT integrates with various sample generators, guiding their perturbations through the discovered causal relationships.

Result: Extensive experiments across 1296 cases show CausalFT improves the ability of arbitrary sample generators to reveal fairness bugs in over 93% of cases, with modest additional runtime overhead. It surpasses state-of-the-art correlation-based methods in 64% of cases while being more efficient, and generally enhances bias resilience.

Conclusion: CausalFT offers a versatile and more effective solution for fairness testing in AI models handling tabular data by integrating causal relationships into sample perturbation, outperforming existing correlation-based approaches.

Abstract: To mitigate unfair and unethical discrimination over sensitive features
(e.g., gender, age, or race), fairness testing plays an integral role in
engineering systems that leverage AI models to handle tabular data. A key
challenge therein is how to effectively reveal fairness bugs under an
intractable sample size using perturbation. Much current work has been focusing
on designing the test sample generators, ignoring the valuable knowledge about
data characteristics that can help guide the perturbation and hence limiting
their full potential. In this paper, we seek to bridge such a gap by proposing
a generic framework of causally perturbed fairness testing, dubbed CausalFT.
Through causal inference, the key idea of CausalFT is to extract the most
directly and causally relevant non-sensitive feature to its sensitive
counterpart, which can jointly influence the prediction of the label. Such a
causal relationship is then seamlessly injected into the perturbation to guide
a test sample generator. Unlike existing generator-level work, CausalFT serves
as a higher-level framework that can be paired with diverse base generators.
Extensive experiments on 1296 cases confirm that CausalFT can considerably
improve arbitrary base generators in revealing fairness bugs over 93% of the
cases with acceptable extra runtime overhead. Compared with a state-of-the-art
approach that ranks the non-sensitive features solely based on correlation,
CausalFT performs significantly better on 64% cases while being much more
efficient. Further, CausalFT can better improve bias resilience in nearly all
cases.

</details>


### [30] [ShaRE your Data! Characterizing Datasets for LLM-based Requirements Engineering](https://arxiv.org/abs/2510.18787)
*Quim Motger,Carlota Catot,Xavier Franch*

Main category: cs.SE

TL;DR: The paper systematically reviews datasets used for LLM-driven Requirements Engineering tasks, finding fragmentation and gaps, and proposes a public catalogue for better visibility and reuse.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) need domain-specific data for strong performance, but in Requirements Engineering (RE), there is a known problem of limited and poorly characterized datasets. This limits LLM-based work in RE (LLM4RE), affecting reuse and comparability of research.

Method: A systematic mapping study was conducted. The research identified and analyzed 62 publicly available datasets referenced in 43 primary LLM4RE studies, characterizing each dataset by artifact type, granularity, RE stage, task, domain, and language.

Result: The study found several gaps: lack of datasets for elicitation tasks, few datasets for management activities beyond traceability, and limited multilingual resources.

Conclusion: This work provides a public catalogue and structured dataset characterization to help researchers select, compare, and reuse datasets in LLM4RE, and suggests future extensions including integration with more repositories and grey literature.

Abstract: [Context] Large Language Models (LLMs) rely on domain-specific datasets to
achieve robust performance across training and inference stages. However, in
Requirements Engineering (RE), data scarcity remains a persistent limitation
reported in surveys and mapping studies. [Question/Problem] Although there are
multiple datasets supporting LLM-based RE tasks (LLM4RE), they are fragmented
and poorly characterized, limiting reuse and comparability. This research
addresses the limited visibility and characterization of datasets used in
LLM4RE. We investigate which public datasets are employed, how they can be
systematically characterized, and which RE tasks and dataset descriptors remain
under-represented. [Ideas/Results] To address this, we conduct a systematic
mapping study to identify and analyse datasets used in LLM4RE research. A total
of 62 publicly available datasets are referenced across 43 primary studies.
Each dataset is characterized along descriptors such as artifact type,
granularity, RE stage, task, domain, and language. Preliminary findings show
multiple research gaps, including limited coverage for elicitation tasks,
scarce datasets for management activities beyond traceability, and limited
multilingual availability. [Contribution] This research preview offers a public
catalogue and structured characterization scheme to support dataset selection,
comparison, and reuse in LLM4RE research. Future work will extend the scope to
grey literature, as well as integration with open dataset and benchmark
repositories.

</details>


### [31] [FeClustRE: Hierarchical Clustering and Semantic Tagging of App Features from User Reviews](https://arxiv.org/abs/2510.18799)
*Max Tiessler,Quim Motger*

Main category: cs.SE

TL;DR: FeClustRE is a new hybrid framework that combines linguistic parsing with large language models to extract and organize features from app reviews into coherent taxonomies, improving the usefulness and structure of feedback for requirements engineering.


<details>
  <summary>Details</summary>
Motivation: Extracting actionable features from noisy, ambiguous mobile app reviews is vital for requirements engineering (RE) tasks. However, existing methods are limited: syntactic methods miss semantic depth, LLMs struggle with fine granularity and structure, and outcomes lack semantic organization, frustrating practitioners wanting meaningful, structured representations for analysis and comparison.

Method: The paper introduces FeClustRE, a framework that integrates syntactic parsing with LLM-based semantic enrichment for hybrid feature extraction. Features are hierarchically clustered using auto-tuning, and clusters are labelled semantically using LLMs to generate meaningful taxonomies. FeClustRE is evaluated on public extraction benchmarks and real-world app reviews for correctness, clustering quality, semantic coherence, and interpretability.

Result: FeClustRE successfully extracts, clusters, and semantically organizes app features, outperforming traditional flat list extraction methods in accuracy, coherence, and interpretability. Evaluation confirms the effectiveness of the framework in structuring app review feedback for enhanced requirements analysis.

Conclusion: FeClustRE provides a robust, hybrid approach to structured feature extraction from mobile app reviews, bridging gaps in current methods. Its hierarchical, semantically organized outputs enable deeper insights into user feedback, improving requirements analysis and comparison across apps. The open-source, replicable solution advances RE practices using app review data.

Abstract: [Context and motivation.] Extracting features from mobile app reviews is
increasingly important for multiple requirements engineering (RE) tasks.
However, existing methods struggle to turn noisy, ambiguous feedback into
interpretable insights. [Question/problem.] Syntactic approaches lack semantic
depth, while large language models (LLMs) often miss fine-grained features or
fail to structure them coherently. In addition, existing methods output flat
lists of features without semantic organization, limiting interpretation and
comparability. Consequently, current feature extraction approaches do not
provide structured, meaningful representations of app features. As a result,
practitioners face fragmented information that hinder requirement analysis,
prioritization, and cross-app comparison, among other use cases. [Principal
ideas/results.] In this context, we propose FeClustRE, a framework integrating
hybrid feature extraction, hierarchical clustering with auto-tuning and
LLM-based semantic labelling. FeClustRE combines syntactic parsing with LLM
enrichment, organizes features into clusters, and automatically generates
meaningful taxonomy labels. We evaluate FeClustRE on public benchmarks for
extraction correctness and on a sample study of generative AI assistant app
reviews for clustering quality, semantic coherence, and interpretability.
[Contribution.] Overall, FeClustRE delivers (1) a hybrid framework for feature
extraction and taxonomy generation, (2) an auto-tuning mechanism with a
comprehensive evaluation methodology, and (3) open-source and replicable
implementation. These contributions bridge user feedback and feature
understanding, enabling deeper insights into current and emerging requirements.

</details>


### [32] [Streamlining Acceptance Test Generation for Mobile Applications Through Large Language Models: An Industrial Case Study](https://arxiv.org/abs/2510.18861)
*Pedro Luís Fonseca,Bruno Lima,João Pascoal Faria*

Main category: cs.SE

TL;DR: AToMIC uses large language models to automate mobile acceptance testing for cross-platform apps, greatly reducing manual effort. It produces accurate, usable test scripts from requirements and code changes within minutes, proving effective and time-saving in a real-world BMW app scenario.


<details>
  <summary>Details</summary>
Motivation: Acceptance testing is a significant bottleneck in cross-platform mobile development due to the manual effort required to create and maintain acceptance test artifacts, even as automated tools become more popular.

Method: The authors introduce AToMIC, a framework that uses specialized Large Language Models to automatically generate Gherkin scenarios, Page Objects, and executable UI test scripts directly from requirements (such as JIRA tickets) and recent code changes. The framework is evaluated on BMW's MyBMW app on 13 real-world issues across a large codebase.

Result: AToMIC generated test artifacts for each feature in under five minutes, with 93.3% of Gherkin scenarios syntactically correct on first try, 78.8% of PageObjects running without manual edits, and 100% of UI tests executing successfully. Practitioners reported significant time savings (up to a full developer day per feature) and high confidence in the approach.

Conclusion: AToMIC is a scalable and practical solution for automating and streamlining acceptance test creation and maintenance in industrial cross-platform mobile development projects, delivering high-quality artifacts quickly and saving considerable developer effort.

Abstract: Mobile acceptance testing remains a bottleneck in modern software
development, particularly for cross-platform mobile development using
frameworks like Flutter. While developers increasingly rely on automated
testing tools, creating and maintaining acceptance test artifacts still demands
significant manual effort. To help tackle this issue, we introduce AToMIC, an
automated framework leveraging specialized Large Language Models to generate
Gherkin scenarios, Page Objects, and executable UI test scripts directly from
requirements (JIRA tickets) and recent code changes. Applied to BMW's MyBMW
app, covering 13 real-world issues in a 170+ screen codebase, AToMIC produced
executable test artifacts in under five minutes per feature on standard
hardware. The generated artifacts were of high quality: 93.3% of Gherkin
scenarios were syntactically correct upon generation, 78.8% of PageObjects ran
without manual edits, and 100% of generated UI tests executed successfully. In
a survey, all practitioners reported time savings (often a full developer-day
per feature) and strong confidence in adopting the approach. These results
confirm AToMIC as a scalable, practical solution for streamlining acceptance
test creation and maintenance in industrial mobile projects.

</details>


### [33] [EffiReasonTrans: RL-Optimized Reasoning for Code Translation](https://arxiv.org/abs/2510.18863)
*Yanlin Wang,Rongyi Ou,Yanli Wang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: EffiReasonTrans is a new training framework for code translation that enhances both accuracy and efficiency. By combining reasoning-augmented data and a two-stage training process, it surpasses previous models in accuracy and speed, with open-source code and data available.


<details>
  <summary>Details</summary>
Motivation: Automated code translation is valuable for software development and maintenance, but improvements in accuracy using large language models (LLMs) come with higher inference latency, which negatively impacts practical workflows that require quick, human-in-the-loop inspection.

Method: The authors propose EffiReasonTrans, a framework that first builds a high-quality, reasoning-augmented code translation dataset using a strong LLM. Each (source code, reasoning, target code) triplet is validated automatically for syntax and functionality. The model is then trained using supervised fine-tuning on this dataset, followed by reinforcement learning to boost accuracy while controlling inference latency.

Result: EffiReasonTrans achieves higher translation accuracy (up to +49.2% CA and +27.8% CodeBLEU over the base model), reduces output token count (up to -19.3%), and decreases inference latency (up to -29.0%) on most benchmarks. Ablation studies validate the importance of each training stage, and the framework also performs well in agent-based setups.

Conclusion: EffiReasonTrans effectively bridges the gap between translation accuracy and inference efficiency in LLM-based code translation, enabling more practical, real-time application. The approach is validated over multiple pairs and tasks, and code/data is released for the community.

Abstract: Code translation is a crucial task in software development and maintenance.
While recent advancements in large language models (LLMs) have improved
automated code translation accuracy, these gains often come at the cost of
increased inference latency, hindering real-world development workflows that
involve human-in-the-loop inspection. To address this trade-off, we propose
EffiReasonTrans, a training framework designed to improve translation accuracy
while balancing inference latency. We first construct a high-quality
reasoning-augmented dataset by prompting a stronger language model,
DeepSeek-R1, to generate intermediate reasoning and target translations. Each
(source code, reasoning, target code) triplet undergoes automated syntax and
functionality checks to ensure reliability. Based on this dataset, we employ a
two-stage training strategy: supervised fine-tuning on reasoning-augmented
samples, followed by reinforcement learning to further enhance accuracy and
balance inference latency. We evaluate EffiReasonTrans on six translation
pairs. Experimental results show that it consistently improves translation
accuracy (up to +49.2% CA and +27.8% CodeBLEU compared to the base model) while
reducing the number of generated tokens (up to -19.3%) and lowering inference
latency in most cases (up to -29.0%). Ablation studies further confirm the
complementary benefits of the two-stage training framework. Additionally,
EffiReasonTrans demonstrates improved translation accuracy when integrated into
agent-based frameworks. Our code and data are available at
https://github.com/DeepSoftwareAnalytics/EffiReasonTrans.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [34] [Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp](https://arxiv.org/abs/2510.17889)
*Eilene Tomkins-Flanagan,Mary A. Kelly*

Main category: cs.PL

TL;DR: This paper proves that key elements of Lisp can be encoded in a vector-symbolic architecture (using holographic reduced representations), achieving Turing-completeness, and emphasizes the role of cleanup memory in such systems.


<details>
  <summary>Details</summary>
Motivation: Kanerva (2014) proposed constructing a complete Lisp using vector-symbolic architecture. The motivation here is to demonstrate, in concrete terms, how the core elements of Lisp (elementary functions, lambda expressions, etc.) can be represented within such an architecture and achieve Turing-completeness.

Method: The paper presents a general vector-symbolic representation for the five elementary Lisp functions, lambda expressions, and auxiliary functions, closely following the Lisp 1.5 specification. Specifically, holographic reduced representations are used along with a lookup table cleanup memory to implement this system.

Result: They demonstrate that their vector-symbolic implementation is sufficient for Turing-completeness, aligning with the properties of a Cartesian closed category. The paper also highlights the necessity of cleanup memories for practical implementation.

Conclusion: The study shows that a minimal Lisp, sufficient for computation, can be fully encoded within a vector-symbolic architecture, underlining the mathematical and computational relevance of such architectures. Explicit inclusion of cleanup memory is necessary for effective functioning.

Abstract: Kanerva (2014) suggested that it would be possible to construct a complete
Lisp out of a vector-symbolic architecture. We present the general form of a
vector-symbolic representation of the five Lisp elementary functions, lambda
expressions, and other auxiliary functions, found in the Lisp 1.5 specification
McCarthy (1960), which is near minimal and sufficient for Turing-completeness.
Our specific implementation uses holographic reduced representations Plate
(1995), with a lookup table cleanup memory. Lisp, as all Turing-complete
languages, is a Cartesian closed category, unusual in its proximity to the
mathematical abstraction. We discuss the mathematics, the purpose, and the
significance of demonstrating vector-symbolic architectures' Cartesian-closure,
as well as the importance of explicitly including cleanup memories in the
specification of the architecture.

</details>


### [35] [ZipLex: Verified Invertible Lexing with Memoized Derivatives and Zippers](https://arxiv.org/abs/2510.18479)
*Samuel Chassot,Viktor Kunčak*

Main category: cs.PL

TL;DR: ZipLex is a verified lexer framework that guarantees the output can be inverted (printing is the inverse of lexing), using novel token sequence abstractions and verified optimizations. It achieves practical performance and substantive correctness guarantees beyond prior work.


<details>
  <summary>Details</summary>
Motivation: Past verified lexers only guarantee regular expression semantics and maximal munch property, but do not ensure that lexing and printing are mutual inverses, which is important for reversible transformations and correctness.

Method: ZipLex introduces a new abstraction for token sequences to model separability and efficient manipulation, uses verified data structures like Huet's zippers and memoized derivatives, and implements and verifies the framework in Scala using the Stainless verifier.

Result: ZipLex was implemented and verified for correctness and invertibility. It supports practical tasks (e.g., JSON, programming languages), is 4x slower than Coqlex, but much faster than Verbatim++, showing verified invertibility is feasible without much overhead.

Conclusion: ZipLex provides mutual invertibility in lexical analysis, a property previously lacking in verified lexers, and demonstrates both practical performance and full verification.

Abstract: We present ZipLex, a verified framework for invertible lexical analysis.
Unlike past verified lexers that focus only on satisfying the semantics of
regular expressions and the maximal munch property, ZipLex also guarantees that
lexing and printing are mutual inverses. Our design relies on two sets of
ideas: (1) a new abstraction of token sequences that captures the separability
of tokens in a sequence while supporting their efficient manipulation, and (2)
a combination of verified data structures and optimizations, including Huet's
zippers and memoized derivatives, to achieve practical performance. We
implemented ZipLex in Scala and verified its correctness, including
invertibility, using the Stainless verifier. Our evaluation demonstrates that
ZipLex supports realistic applications such as JSON processing and lexers of
programming languages. In comparison to other verified lexers (which do not
enforce invertibility), ZipLex is 4x slower than Coqlex and two orders of
magnitude faster than Verbatim++, showing that verified invertibility can be
achieved without prohibitive cost.

</details>


### [36] [CPSLint: A Domain-Specific Language Providing Data Validation and Sanitisation for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2510.18651)
*Uraz Odyurt,Ömer Sayilir,Mariëlle Stoelinga,Vadim Zaytsev*

Main category: cs.PL

TL;DR: CPSLint is a new language that streamlines and standardizes data preparation in industrial CPS, adding robust checks and structure, especially for ML and FDI applications, with demonstrated proof-of-concept results.


<details>
  <summary>Details</summary>
Motivation: Raw datasets in industrial Cyber-Physical Systems (CPS) are massive, unstructured, and require extensive preprocessing before being usable, especially for Machine Learning (ML) applications such as Fault Detection and Identification (FDI). There is a need for efficient, domain-specific tools to standardize and automate this data preparation process.

Method: The authors introduce CPSLint, a Domain-Specific Language (DSL) tailored for preparing industrial CPS data. CPSLint features type checking, constraint enforcement, data validation, remediation (e.g., imputing missing values), and advanced inference of CPS-specific data structures. They present these features through a proof-of-concept implementation.

Result: CPSLint provides a structured approach to CPS data preparation, enabling easier consumption by ML-based workflows. It facilitates sanity checks, preprocessing, type safety, and the inference of both column-wise and row-wise structures (e.g., execution phases for FDI tasks).

Conclusion: CPSLint effectively aids in preparing and cleaning CPS data, addressing both common and domain-specific requirements, and making industrial CPS data more readily accessible for machine learning-driven fault detection and identification.

Abstract: Raw datasets are often too large and unstructured to work with directly, and
require a data preparation process. The domain of industrial Cyber-Physical
Systems (CPS) is no exception, as raw data typically consists of large amounts
of time-series data logging the system's status in regular time intervals. Such
data has to be sanity checked and preprocessed to be consumable by data-centric
workflows. We introduce CPSLint, a Domain-Specific Language designed to provide
data preparation for industrial CPS. We build up on the fact that many raw data
collections in the CPS domain require similar actions to render them suitable
for Machine-Learning (ML) solutions, e.g., Fault Detection and Identification
(FDI) workflows, yet still vary enough to hope for one universally applicable
solution.
  CPSLint's main features include type checking and enforcing constraints
through validation and remediation for data columns, such as imputing missing
data from surrounding rows. More advanced features cover inference of extra
CPS-specific data structures, both column-wise and row-wise. For instance, as
row-wise structures, descriptive execution phases are an effective method of
data compartmentalisation are extracted and prepared for ML-assisted FDI
workflows. We demonstrate CPSLint's features through a proof of concept
implementation.

</details>
