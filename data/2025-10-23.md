<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation](https://arxiv.org/abs/2510.18895)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore introduces emotionally-inspired RL for LLM code generation, drastically reducing buggy code and improving self-correction speed. Its combination of valence/surprise tagging and prioritized replay enhances learning, validated both in benchmarks and practical environments. The approach points to emotionally tuned code assistants for broader applications.


<details>
  <summary>Details</summary>
Motivation: Inspired by human and animal learning, especially how negative emotional feedback (embarrassment) leads to rapid error correction. The aim is to improve code generation in LLMs by using neuroscience concepts to enhance RL methods.

Method: CosmoCore uses a neuroscience-inspired RL architecture. It tags code outputs with valence (emotional value) and surprise via a lightweight MLP. 'Cringe' (high-negative valence) mistakes are replayed more often, while low-surprise successes are pruned, supporting faster learning and avoiding overconfidence. Benchmarks and custom simulations validate performance.

Result: CosmoCore achieves a 48% reduction in hallucinated code (errors and bugs) and accelerates self-correction by 45%. Local testing in PySpark environments using Hugging Face models supports these claims. Ablation studies show that valence tagging encourages exploration, while pruning improves efficiency.

Conclusion: CosmoCore extends RLHF frameworks with emotional awareness for code assistants, offering improvements for IDEs and data pipeline applications. Code and simulation tools are released for reproducibility.

Abstract: We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL)
architecture that integrates affective signals to enhance code generation in
large language models (LLMs). Motivated by human and animal learning where
embarrassment from mistakes drives rapid correction, as observed in training a
puppy to avoid repeating errors after a single scolding CosmoCore tags code
generation trajectories with valence and surprise using a lightweight
multi-layer perceptron (MLP). High-negative valence (cringe) episodes, such as
buggy code outputs, are prioritized in a Dream Queue for five-fold replay
during off-policy updates, while low-surprise successes are pruned to prevent
overconfidence and buffer bloat. Evaluated on code generation benchmarks like
HumanEval and BigCodeBench, alongside simulations with a custom data pipeline
environment, CosmoCore reduces hallucinated code (e.g., syntax errors or
logical bugs) by 48\% and accelerates self-correction by 45\%. Local
experiments using Hugging Face models in a PySpark environment validate these
gains, with code snippets provided for replication. Ablations confirm valence
tagging boosts curiosity in exploration, and pruning mitigates inefficiency.
This framework extends RL from human feedback (RLHF) for more emotionally aware
code assistants, with applications in IDEs and data pipelines. Code and the
custom mini-world simulation are released.

</details>


### [2] [A Survey on Feedback Types in Automated Programming Assessment Systems](https://arxiv.org/abs/2510.18923)
*Eduard Frankford,Tobias Antensteiner,Michael Vierhauser,Clemens Sauerwein,Vivien Wallner,Iris Groher,Reinhold Plösch,Ruth Breu*

Main category: cs.SE

TL;DR: The study evaluates how feedback types in automated programming assessment systems affect student experience and performance. While unit test feedback is rated as most helpful by students, AI-driven feedback substantially improves their results. Integrating both approaches is recommended for better programming education.


<details>
  <summary>Details</summary>
Motivation: The increased digitization in industries has led to a surge in demand for programming education, requiring more effective resources for teaching and assessing diverse student groups. Traditional automated assessment systems mainly use unit tests, which can limit feedback quality, but advancements in Large Language Models (LLMs) present new potential for enhancing feedback.

Method: A large-scale study was conducted involving over 200 students from two universities. The study compared three different feedback mechanisms in Automated Programming Assessment Systems: compiler feedback, standard unit test feedback, and LLM-based feedback. The assessment focused on students' perceptions of feedback quality and its impact on their performance.

Result: Students found unit test feedback to be the most helpful. However, those who received AI-generated (LLM-based) feedback showed significantly better performance in programming tasks.

Conclusion: Combining unit test feedback with AI-driven guidance can optimize automated feedback in programming education, resulting in improved student learning outcomes.

Abstract: With the recent rapid increase in digitization across all major industries,
acquiring programming skills has increased the demand for introductory
programming courses. This has further resulted in universities integrating
programming courses into a wide range of curricula, including not only
technical studies but also business and management fields of study.
  Consequently, additional resources are needed for teaching, grading, and
tutoring students with diverse educational backgrounds and skills. As part of
this, Automated Programming Assessment Systems (APASs) have emerged, providing
scalable and high-quality assessment systems with efficient evaluation and
instant feedback. Commonly, APASs heavily rely on predefined unit tests for
generating feedback, often limiting the scope and level of detail of feedback
that can be provided to students. With the rise of Large Language Models (LLMs)
in recent years, new opportunities have emerged as these technologies can
enhance feedback quality and personalization.
  To investigate how different feedback mechanisms in APASs are perceived by
students, and how effective they are in supporting problem-solving, we have
conducted a large-scale study with over 200 students from two different
universities. Specifically, we compare baseline Compiler Feedback, standard
Unit Test Feedback, and advanced LLM-based Feedback regarding perceived quality
and impact on student performance.
  Results indicate that while students rate unit test feedback as the most
helpful, AI-generated feedback leads to significantly better performances.
These findings suggest combining unit tests and AI-driven guidance to optimize
automated feedback mechanisms and improve learning outcomes in programming
education.

</details>


### [3] [Extending Resource Constrained Project Scheduling to Mega-Projects with Model-Based Systems Engineering & Hetero-functional Graph Theory](https://arxiv.org/abs/2510.19035)
*Amirreza Hosseini,Amro M. Farid*

Main category: cs.SE

TL;DR: This paper integrates project scheduling (RCPSP) with model-based systems engineering using hetero-functional graph theory, providing a new framework that preserves classic strengths, enhances monitoring, and supports complex project decisions.


<details>
  <summary>Details</summary>
Motivation: Project scheduling is vital in project management, but the resource-constrained project scheduling problem (RCPSP) is not well-integrated with model-based systems engineering (MBSE). This gap hinders effective design and management of complex systems.

Method: The paper offers a translation pipeline from an activity-on-node network to SysML activity diagrams and then to an operand net. It applies hetero-functional graph theory (HFGT) and the hetero-functional network minimum-cost flow (HFNMCF) formulation to the RCPSP context for systematic analysis.

Result: The proposed method specializes HFNMCF to RCPSP, demonstrating that RCPSP can be recovered as a special case of a broader model. Through an example involving renewable and non-renewable operands, the framework provides schedules similar to traditional RCPSP but with more explicit project state explanations for improved monitoring and control.

Conclusion: The framework maintains classical RCPSP strengths while adding flexibility for real-world constraints and enhanced decision-making in large, complex projects, bridging the gap between RCPSP and MBSE.

Abstract: Within the project management context, project scheduling serves as an
indispensable component, functioning as a fundamental tool for planning,
monitoring, controlling, and managing projects more broadly. Although the
resource-constrained project scheduling problem (RCPSP) lies at the core of
project management activities, it remains largely disconnected from the broader
literature on model-based systems engineering (MBSE), thereby limiting its
integration into the design and management of complex systems. The original
contribution of this paper is twofold. First, the paper seeks to reconcile the
RCPSP with the broader literature and vocabulary of model-based systems
engineering and hetero-functional graph theory (HFGT). A concrete translation
pipeline from an activity-on-node network to a SysML activity diagram, and then
to an operand net is constructed. Using this representation, it specializes the
hetero-functional network minimum-cost flow (HFNMCF) formulation to the RCPSP
context as a systematic means of HFGT for quantitative analysis and proves that
the RCPSP is recoverable as a special case of a broader model. Secondly, on an
illustrative instance with renewable and non-renewable operands, the
specialized HFNMCF, while producing similar schedules, yields explicit
explanations of the project states that enable richer monitoring and control.
Overall, the framework preserves the strengths of the classical RCPSP while
accommodating real-world constraints and enterprise-level decision processes
encountered in large, complex megaprojects.

</details>


### [4] [Docker-based CI/CD for Rocq/OCaml projects](https://arxiv.org/abs/2510.19089)
*Érik Martin-Dorel*

Main category: cs.SE

TL;DR: Three DevOps tools for Docker-based CI/CD in Rocq (Coq)/OCaml projects are described, detailing features and design choices to boost adoption and simplify maintenance.


<details>
  <summary>Details</summary>
Motivation: There is a need for efficient and maintainable CI/CD workflows for Rocq (Coq) and OCaml projects, leveraging Docker-based solutions.

Method: The paper provides a high-level overview of features and underlying requirements/design choices of three DevOps tools: docker-coq, docker-coq-action, and docker-keeper.

Result: The publication describes available features, requirements, and design choices to encourage adoption and facilitate future maintenance of the tools.

Conclusion: The documentation and explanation provided aim to foster broader use of Docker-based CI/CD in Rocq and OCaml projects, and support the tools' maintainers.

Abstract: This paper presents three closely-related software projects, namely:
docker-coq, docker-coq-action, and docker-keeper. It aims at two objectives:
provide a high-level description of the available features -- to foster the use
of a Docker-based CI/CD for Rocq (formerly known as Coq) or OCaml projects --
and document the underlying requirements and the main design choices of these
three DevOps tools -- to help their future maintainers.

</details>


### [5] [Automated Concern Extraction from Textual Requirements of Cyber-Physical Systems: A Multi-solution Study](https://arxiv.org/abs/2510.19237)
*Dongming Jin,Zhi Jin,Xiaohong Chen,Zheng Fang,Linyu Li,Shengxin Zhao,Chuihui Wang,Hongbin Xiao*

Main category: cs.SE

TL;DR: The paper introduces ReqEBench, a thorough benchmark for evaluating automated requirements extraction in cyber-physical systems, showing current solutions (including GPT-4) perform poorly and highlighting areas for improvement.


<details>
  <summary>Details</summary>
Motivation: The extraction of requirements concerns in cyber-physical systems (CPSs) is challenging due to the integration of information and physical domains. While automated solutions exist, their evaluation has been hampered by the lack of fair and comprehensive benchmarks.

Method: The authors developed ReqEBench, a benchmark dataset containing 2,721 requirements from 12 real-world CPSs, covering multiple domains and undergoing rigorous annotation.

Result: Comparative studies using ReqEBench demonstrated the limited performance of current automated extraction solutions, with GPT-4 achieving only a 0.24 F1 score in entity concern extraction. Shortcomings of existing approaches were analyzed and ideas for improvement proposed.

Conclusion: ReqEBench addresses a key evaluation gap, providing a comprehensive resource for benchmarking and guiding the advancement of automated requirements extraction solutions in CPSs.

Abstract: Cyber-physical systems (CPSs) are characterized by a deep integration of the
information space and the physical world, which makes the extraction of
requirements concerns more challenging. Some automated solutions for
requirements concern extraction have been proposed to alleviate the burden on
requirements engineers. However, evaluating the effectiveness of these
solutions, which relies on fair and comprehensive benchmarks, remains an open
question. To address this gap, we propose ReqEBench, a new CPSs requirements
concern extraction benchmark, which contains 2,721 requirements from 12
real-world CPSs. ReqEBench offers four advantages. It aligns with real-world
CPSs requirements in multiple dimensions, e.g., scale and complexity. It covers
comprehensive concerns related to CPSs requirements. It undergoes a rigorous
annotation process. It covers multiple application domains of CPSs, e.g.,
aerospace and healthcare. We conducted a comparative study on three types of
automated requirements concern extraction solutions and revealed their
performance in real-world CPSs using our ReqEBench. We found that the highest
F1 score of GPT-4 is only 0.24 in entity concern extraction. We further analyze
failure cases of popular LLM-based solutions, summarize their shortcomings, and
provide ideas for improving their capabilities. We believe ReqEBench will
facilitate the evaluation and development of automated requirements concern
extraction.

</details>


### [6] [An Empirical Study of Bitwise Operators Intuitiveness through Performance Metrics](https://arxiv.org/abs/2510.19281)
*Shubham Joshi*

Main category: cs.SE

TL;DR: The paper examines how people of differing programming experience understand bitwise operators. Using experimental tasks, it finds that some operators slow down users and are less intuitive, pointing to areas for improvement in programming language design.


<details>
  <summary>Details</summary>
Motivation: The study seeks to understand how easily people with varying programming backgrounds comprehend and use bitwise operators in coding, given the hypothesis that performance changes with exposure to different bitwise operators.

Method: A Within-Subjects Experimental Design was utilized. 23 participants from diverse programming backgrounds were given JavaScript programming tasks involving bitwise operators. Researchers measured task completion time and accuracy.

Result: Operators were found to influence response time, with a small but statistically significant effect (R-squared 0.032, p < .001). Specific operators (OR, NOT, Left Shift) proved less intuitive, resulting in longer task completion times.

Conclusion: While most bitwise operators did not significantly increase task completion time, some (like OR, NOT, and Left Shift) were less intuitive. This highlights the need for further research and possible redesign to enhance operator understandability.

Abstract: Objectives: This study aims to investigate the readability and
understandability of bitwise operators in programming, with the main hypothesis
that there will be a difference in the performance metrics (response time and
error rate) between participants exposed to various bitwise operators related
questions and those who are not.
  Participants: Participants in this human research study include people
without programming background, novice programmers, and university students
with varying programming experience (from freshmen to PhD level). There were 23
participants for this study.
  Study Methods: This study uses an Within-Subjects Experimental Design to
assess how people with diverse programming backgrounds understand and use
bitwise operators. Participants complete tasks in JavaScript program, and their
task completion time and accuracy of the tasks are recorded for analysis.
  Findings: The results indicate that operators can be one of the factors
predicting response time, with a small but significant effect, with R-squared
0.032, (1, 494) = 16.5, p < .001. Additionally, some operators like OR, NOT,
and Left Shift showed statistical significance in task completion times
compared to other operators.
  Conclusions: While the complexity of bitwise operators did not generally
result in longer task completion times, certain operators were found to be less
intuitive, suggesting the need for further investigation and potential redesign
for improved understandability.

</details>


### [7] [A General Solution for the Implementation of CI/CD in Embedded Linux Development](https://arxiv.org/abs/2510.19240)
*Behnam Agahi,Hamed Farbeh*

Main category: cs.SE

TL;DR: An automated Yocto-based workflow for embedded Linux development was built and tested; it proved reproducible, scalable, and efficient, speeding up builds and enabling CI/CD, with potential for real-time Linux apps and further industrial use.


<details>
  <summary>Details</summary>
Motivation: Current embedded systems require efficient and automated development and deployment processes for customized Linux OS, particularly to simplify integration and scalability.

Method: The authors designed and implemented a three-layer architecture using Yocto Project, including main repositories, a custom layer, and a manifest layer for synchronization and reproducibility. CI/CD pipelines with GitLab CI, Docker environments, and local caching were utilized.

Result: Three sample projects were successfully developed and integrated. Automated workflows reduced build times, and multiple boot tests in QEMU verified system stability and functionality. The infrastructure proved reproducible and scalable.

Conclusion: The proposed infrastructure is effective for automated, reproducible, and scalable Linux OS development in embedded systems. It can be extended to advanced use cases such as real-time Linux continuous deployment.

Abstract: With the growing use of embedded systems in various industries, the need for
automated platforms for the development and deployment of customized
Linux-based operating systems has become more important. This research was
conducted with the aim of designing and implementing an integrated and
reproducible infrastructure for the development, building, and testing of a
Linux-based operating system using the Yocto Project. The proposed structure
was implemented based on a three-layer architecture consisting of the main
Yocto repositories, a custom layer (meta-custom), and a coordinating manifest
layer to ensure version synchronization, scalability, and reproducibility.
Three sample projects, including libhelloworld, helloworld, and the kernel
module hello mod, were developed and integrated into the build process.
Continuous Integration and Continuous Deployment pipelines were implemented
with GitLab CI and combined with an isolated Docker environment to automate and
streamline the build and testing workflows. Using a local cache server
containing hashserv, downloads and sstate cache significantly reduced the build
time. The functionality and stability of the system were verified through six
boot test scenarios in the QEMU simulator. The results show that the proposed
design not only ensures reproducibility but also can be extended to advanced
applications such as continuous deployment of real-time Linux versions. Future
recommendations include expanding automated tests, implementing system
monitoring with Prometheus and Grafana, using distributed builds, optimizing
with Docker multi-stage builds, and enabling continuous deployment of real-time
Linux changes to provide a stable and scalable model for industrial and
research projects in embedded systems with a rapid and reliable development
cycle.

</details>


### [8] [Trace: Securing Smart Contract Repository Against Access Control Vulnerability](https://arxiv.org/abs/2510.19254)
*Chong Chen,Jiachi Chen,Lingfeng Bao,David Lo,Yanlin Wang,Zhenyu Shan,Ting Chen,Guangqiang Yin,Jianxing Yu,Zibin Zheng*

Main category: cs.SE

TL;DR: TRACE is a new tool that uses LLMs to analyze non-compilable smart contract repositories and detect access control vulnerabilities with much higher precision than existing tools. It can identify critical flaws in source code—before contracts are deployed—making smart contract development on platforms like GitHub safer.


<details>
  <summary>Details</summary>
Motivation: Smart contract vulnerabilities, especially those related to improper access control, have caused massive financial losses. Many developers reuse or reference code from GitHub repositories during smart contract development, which may introduce security risks if the source code contains vulnerabilities. Existing analysis tools are limited since they require the target contracts to be compilable for analysis, leaving non-compilable repositories less secure.

Method: TRACE is a tool that secures non-compilable smart contract repositories against Access Control vulnerabilities. It leverages Large Language Models (LLMs) to identify sensitive functions in contract code and autocompletes incomplete functions into a compilable contract. TRACE then builds a function call graph using the abstract syntax tree (AST) and analyzes the control flow graph (CFG) of each function to detect potential access control vulnerabilities.

Result: Experimental evaluation shows TRACE detects 14 out of 15 CVEs in an open-source dataset, achieves 89.2% precision on 5,000 recent on-chain contracts (better than the previous best of 76.9%), and records 87.0% precision on 83 real-world repositories, significantly outperforming DeepSeek-R1's 14.3%.

Conclusion: TRACE effectively addresses the limitations of existing smart contract vulnerability detection tools, offering high precision and performance on access control vulnerabilities even for non-compilable repositories, thereby improving smart contract security.

Abstract: Smart contract vulnerabilities, particularly improper Access Control that
allows unauthorized execution of restricted functions, have caused billions of
dollars in losses. GitHub hosts numerous smart contract repositories containing
source code, documentation, and configuration files-these serve as intermediate
development artifacts that must be compiled and packaged before deployment.
Third-party developers often reference, reuse, or fork code from these
repositories during custom development. However, if the referenced code
contains vulnerabilities, it can introduce significant security risks. Existing
tools for detecting smart contract vulnerabilities are limited in their ability
to handle complex repositories, as they typically require the target contract
to be compilable to generate an abstract representation for further analysis.
This paper presents TRACE, a tool designed to secure non-compilable smart
contract repositories against access control vulnerabilities. TRACE employs
LLMs to locate sensitive functions involving critical operations (e.g.,
transfer) within the contract and subsequently completes function snippets into
a fully compilable contract. TRACE constructs a function call graph from the
abstract syntax tree (AST) of the completed contract. It uses the control flow
graph (CFG) of each function as node information. The nodes of the sensitive
functions are then analyzed to detect Access Control vulnerabilities.
Experimental results demonstrate that TRACE outperforms state-of-the-art tools
on an open-sourced CVE dataset, detecting 14 out of 15 CVEs. In addition, it
achieves 89.2% precision on 5,000 recent on-chain contracts, far exceeding the
best existing tool at 76.9%. On 83 real-world repositories, TRACE achieves
87.0% precision, significantly surpassing DeepSeek-R1's 14.3%.

</details>


### [9] [From Specification to Service: Accelerating API-First Development Using Multi-Agent Systems](https://arxiv.org/abs/2510.19274)
*Saurabh Chauhan,Zeeshan Rasheed,Malik Abdul Sami,Kai-Kristian Kemell,Muhammad Waseem,Zheying Zhang,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: This paper introduces a system using LLM agents to automate RESTful microservice development from OpenAPI specs. By incorporating log analysis and iterative refinement, the system efficiently generates robust code that matches the specification, especially for smaller APIs. Results show significant automation potential for API-first workflows.


<details>
  <summary>Details</summary>
Motivation: Traditional API-first development of RESTful web services is often labor-intensive, requiring manual creation of specifications, code generation, and iterative debugging. There is a need for automated solutions that can streamline this process and improve productivity.

Method: The paper presents a system based on LLM-powered agents that automate API-first microservice development. The system creates OpenAPI specs, generates server code, and automatically refines the code via a feedback loop that uses execution logs and error messages. The approach was tested using the PRAB benchmark.

Result: When the OpenAPI specification remains small and focused, the system enables LLMs to generate complete functional code with appropriate business logic that aligns with the specification, with fewer development iterations required.

Conclusion: LLM-based multi-agent systems can successfully automate much of the API-first development process for RESTful microservices, especially when specifications are well-defined and focused.

Abstract: This paper presents a system that uses Large Language Models (LLMs)-based
agents to automate the API-first development of RESTful microservices. This
system helps to create an OpenAPI specification, generate server code from it,
and refine the code through a feedback loop that analyzes execution logs and
error messages. The integration of log analysis enables the LLM to detect and
address issues efficiently, reducing the number of iterations required to
produce functional and robust services. This study's main goal is to advance
API-first development automation for RESTful web services and test the
capability of LLM-based multi-agent systems in supporting the API-first
development approach. To test the proposed system's potential, we utilized the
PRAB benchmark. The results indicate that if we keep the OpenAPI specification
small and focused, LLMs are capable of generating complete functional code with
business logic that aligns to the specification. The code for the system is
publicly available at https://github.com/sirbh/code-gen

</details>


### [10] [Bytecode-centric Detection of Known-to-be-vulnerable Dependencies in Java Projects](https://arxiv.org/abs/2510.19393)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: Java projects mainly consist of OSS dependencies, posing security risks. Jaralyzer is a new bytecode-based scanner that surpasses existing tools in finding vulnerabilities, especially in modified dependencies, and reduces false alarms.


<details>
  <summary>Details</summary>
Motivation: Modern Java projects rely heavily on open-source software (OSS) dependencies, which account for 71% of their code base. This high dependency poses significant security risks due to the potential inclusion of vulnerable OSS components. Existing dependency scanners struggle to detect vulnerabilities in modified dependencies, such as those that are re-compiled, re-bundled, or re-packaged, which are common in the Java ecosystem.

Method: The authors introduce Jaralyzer, a bytecode-centric dependency scanner for Java. Unlike traditional scanners that depend on metadata or source code, Jaralyzer operates directly on bytecode, allowing it to analyze dependencies even when source or metadata is unavailable or altered.

Result: Jaralyzer outperformed other popular dependency scanners, especially in identifying vulnerabilities in modified dependencies. It is the only tool capable of detecting vulnerabilities across all studied forms of dependency modification. In tests with unmodified dependencies, Jaralyzer detected 28 more true vulnerabilities and generated 29 fewer false positives compared to the leading code-centric scanner, Eclipse Steady.

Conclusion: Jaralyzer represents a significant advancement in Java dependency scanning by using bytecode analysis. It effectively detects vulnerabilities in both modified and unmodified OSS dependencies, addressing key shortcomings of existing scanners and improving codebase security.

Abstract: On average, 71% of the code in typical Java projects comes from open-source
software (OSS) dependencies, making OSS dependencies the dominant component of
modern software code bases. This high degree of OSS reliance comes with a
considerable security risk of adding known security vulnerabilities to a code
base. To remedy this risk, researchers and companies have developed various
dependency scanners, which try to identify inclusions of known-to-be-vulnerable
OSS dependencies. However, there are still challenges that modern dependency
scanners do not overcome, especially when it comes to dependency modifications,
such as re-compilations, re-bundlings or re-packagings, which are common in the
Java ecosystem. To overcome these challenges, we present Jaralyzer, a
bytecode-centric dependency scanner for Java. Jaralyzer does not rely on the
metadata or the source code of the included OSS dependencies being available
but directly analyzes a dependency's bytecode. Our evaluation across 56 popular
OSS components demonstrates that Jaralyzer outperforms other popular dependency
scanners in detecting vulnerabilities within modified dependencies. It is the
only scanner capable of identifying vulnerabilities across all the above
mentioned types of modifications. But even when applied to unmodified
dependencies, Jaralyzer outperforms the current state-of-the-art code-centric
scanner Eclipse Steady by detecting 28 more true vulnerabilities and yielding
29 fewer false warnings.

</details>


### [11] [AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of Autonomous Driving Systems](https://arxiv.org/abs/2510.19438)
*Linfeng Liang,Chenkai Tan,Yao Deng,Yingfeng Cai,T. Y Chen,Xi Zheng*

Main category: cs.SE

TL;DR: AutoMT is an automated multi-agent testing framework for autonomous driving systems that leverages Large Language Models to extract traffic rule-based test scenarios, generating far more diverse and effective follow-up cases than traditional manual approaches. It improves fault detection and coverage of critical scenarios, making it suitable for industrial adoption.


<details>
  <summary>Details</summary>
Motivation: Autonomous Driving Systems (ADS) are safety-critical, and failures can have severe consequences. Current Metamorphic Testing (MT) approaches for ADS are effective but rely heavily on manual effort and lack automation, limiting scalability and efficiency.

Method: The paper introduces AutoMT, a multi-agent Metamorphic Testing framework driven by Large Language Models (LLMs). AutoMT automates the extraction of Metamorphic Relations (MRs) from local traffic rules using a predefined ontology. It uses a vision-language agent to analyze scenarios and a search agent to retrieve suitable MRs from a RAG-based database, enabling the automated generation of follow-up test cases via computer vision.

Result: AutoMT achieves up to 5x higher test diversity in follow-up case generation compared to the best baseline (manual expert-defined MRs) in terms of validation rate. It also detects up to 20.55% more behavioral violations. AutoMT automatically extracts diverse MRs, augmenting real-world datasets and helping uncover corner cases often missed during traditional testing and data collection.

Conclusion: AutoMT significantly automates and improves Metamorphic Testing for ADS by leveraging LLMs, providing higher test diversity, better fault detection, and facilitating coverage of safety-critical scenarios with minimal manual intervention. Its modular design supports industrial integration and enhances simulation-based testing.

Abstract: Autonomous Driving Systems (ADS) are safety-critical, where failures can be
severe. While Metamorphic Testing (MT) is effective for fault detection in ADS,
existing methods rely heavily on manual effort and lack automation. We present
AutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that
automates the extraction of Metamorphic Relations (MRs) from local traffic
rules and the generation of valid follow-up test cases. AutoMT leverages LLMs
to extract MRs from traffic rules in Gherkin syntax using a predefined
ontology. A vision-language agent analyzes scenarios, and a search agent
retrieves suitable MRs from a RAG-based database to generate follow-up cases
via computer vision. Experiments show that AutoMT achieves up to 5 x higher
test diversity in follow-up case generation compared to the best baseline
(manual expert-defined MRs) in terms of validation rate, and detects up to
20.55% more behavioral violations. While manual MT relies on a fixed set of
predefined rules, AutoMT automatically extracts diverse metamorphic relations
that augment real-world datasets and help uncover corner cases often missed
during in-field testing and data collection. Its modular architecture
separating MR extraction, filtering, and test generation supports integration
into industrial pipelines and potentially enables simulation-based testing to
systematically cover underrepresented or safety-critical scenarios.

</details>


### [12] [Mapping and Evolving Interoperability Testing in European Energy Systems: The int:net Perspective](https://arxiv.org/abs/2510.19460)
*Thomas I. Strasser,Edmund Widl,Carlos Ayon Mac Gregor,Mirko Ginocchi,Rene Kuchenbuch*

Main category: cs.SE

TL;DR: This paper surveys 30 European interoperability testing facilities in the energy sector, categorizes their capabilities, and proposes a blueprint for future testing environments, thus supporting better collaboration and innovation in Europe’s energy transition.


<details>
  <summary>Details</summary>
Motivation: The integration of renewable energy sources, digital technologies, and decentralized systems in Europe's energy landscape requires high interoperability among systems to ensure a reliable, flexible, and efficient energy supply. However, there is currently a lack of a dedicated, structured overview of interoperability testing facilities across Europe.

Method: The authors conducted a structured survey of 30 interoperability testing facilities across Europe. They categorized these facilities, their methodologies, and referenced test cases, and developed a blueprint for future testing environments.

Result: The survey provided a categorized inventory of European interoperability testing infrastructures, methodologies, and test cases. A blueprint for future testing environments was also presented.

Conclusion: The study fills an existing gap by offering a harmonized overview of interoperability testing facilities in Europe, thereby supporting the creation of a coordinated ecosystem for interoperability testing. This facilitates collaboration, innovation, and the advancement of Europe’s energy transition goals.

Abstract: The ongoing transformation of the European energy landscape, driven by the
integration of renewable energy sources, digital technologies, and
decentralized systems, requires a high degree of interoperability across
diverse components and systems. Ensuring that these elements can exchange
information and operate together reliably is essential for achieving a secure,
flexible, and efficient energy supply infrastructure. While several initiatives
have contributed to the development of smart grid testing infrastructures, they
do not provide a dedicated or comprehensive focus on interoperability testing.
A structured and harmonized overview of interoperability testing capabilities
across Europe is therefore still missing. This work therefore presents a novel
contribution by analyzing the European interoperability testing facility
landscape through a structured survey of 30 facilities. It provides a
categorized inventory of testing infrastructures, applied methodologies, and
reference test cases, and introduces a blueprint for the development of future
testing environments. The findings contribute to the establishment of a
coordinated European ecosystem for interoperability testing, supporting
collaboration, innovation, and alignment with the goals of the energy
transition.

</details>


### [13] [A Goal-Driven Survey on Root Cause Analysis](https://arxiv.org/abs/2510.19593)
*Aoyang Fang,Haowen Yang,Haoze Dong,Qisheng Lu,Junjielong Xu,Pinjia He*

Main category: cs.SE

TL;DR: This paper introduces a novel, goal-based framework for surveying Root Cause Analysis research in cloud incident management, categorizing 135 papers by their specific objectives instead of their input data types. This approach reveals clearer research gaps, aids different audiences, and discusses future challenges in the field.


<details>
  <summary>Details</summary>
Motivation: Traditional surveys on Root Cause Analysis (RCA) in cloud services have primarily categorized research by input data types, overlooking distinctions in underlying goals. This results in grouped works with fundamentally different objectives, obscuring actual progress and gaps. There is a strong demand for an RCA survey organized by research goals, aiding both laymen seeking an overview and researchers seeking studies with similar purposes.

Method: The paper proposes a goal-driven framework that categorizes and integrates 135 RCA-related papers based on their specific goals. This method acknowledges the diversity of objectives in RCA research, ranging from rapid faulty service localization to definitive bug identification.

Result: The framework successfully classifies RCA research by goals rather than data types and reveals the true progress and gaps in the field. It also defines the ultimate goal that encompasses various RCA formulations and highlights open challenges and future research directions.

Conclusion: A goal-driven categorization of RCA research provides a clearer understanding of the field, helping both novices and experts. This approach is more meaningful than traditional input-type-based surveys and identifies areas for further investigation.

Abstract: Root Cause Analysis (RCA) is a crucial aspect of incident management in
large-scale cloud services. While the term root cause analysis or RCA has been
widely used, different studies formulate the task differently. This is because
the term "RCA" implicitly covers tasks with distinct underlying goals. For
instance, the goal of localizing a faulty service for rapid triage is
fundamentally different from identifying a specific functional bug for a
definitive fix. However, previous surveys have largely overlooked these
goal-based distinctions, conventionally categorizing papers by input data types
(e.g., metric-based vs. trace-based methods). This leads to the grouping of
works with disparate objectives, thereby obscuring the true progress and gaps
in the field. Meanwhile, the typical audience of an RCA survey is either laymen
who want to know the goals and big picture of the task or RCA researchers who
want to figure out past research under the same task formulation. Thus, an RCA
survey that organizes the related papers according to their goals is in high
demand. To this end, this paper presents a goal-driven framework that
effectively categorizes and integrates 135 papers on RCA in the context of
cloud incident management based on their diverse goals, spanning the period
from 2014 to 2025. In addition to the goal-driven categorization, it discusses
the ultimate goal of all RCA papers as an umbrella covering different RCA
formulations. Moreover, the paper discusses open challenges and future
directions in RCA.

</details>


### [14] [Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1](https://arxiv.org/abs/2510.19600)
*Qianli Ma,Siyu Wang,Yilin Chen,Yinhao Tang,Yixiang Yang,Chang Guo,Bingjie Gao,Zhening Xing,Yanan Sun,Zhipeng Zhang*

Main category: cs.SE

TL;DR: AutoPage is an automated, multi-agent system that quickly and reliably turns research papers into interactive project webpages, verified by agents and optional human review. It reduces manual workload, boosts efficiency, and achieves high page quality, as validated by their new PageBench benchmark.


<details>
  <summary>Details</summary>
Motivation: Creating project webpages for research papers is essential for accessibility and dissemination but is a repetitive, manual task that is not addressed well by automation, especially for dynamic and interactive content.

Method: Introduces AutoPage, a multi-agent hierarchical system that breaks down webpage generation into steps from planning, content generation, to rendering. It uses "Checker" agents for verification against the source paper and allows optional human reviews. The team also developed PageBench, a benchmark for this new task.

Result: AutoPage efficiently generates high-quality, visually appealing interactive project webpages from papers in under 15 minutes and at a low cost ($<0.1), as validated by experiments using the PageBench benchmark.

Conclusion: AutoPage transforms project webpage creation from a tedious manual task into a streamlined collaborative process, outperforming prior approaches by providing efficiency, quality, and reliability through multi-agent collaboration and verification.

Abstract: In the quest for scientific progress, communicating research is as vital as
the discovery itself. Yet, researchers are often sidetracked by the manual,
repetitive chore of building project webpages to make their dense papers
accessible. While automation has tackled static slides and posters, the
dynamic, interactive nature of webpages has remained an unaddressed challenge.
To bridge this gap, we reframe the problem, arguing that the solution lies not
in a single command, but in a collaborative, hierarchical process. We introduce
$\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.
AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline
from narrative planning to multimodal content generation and interactive
rendering. To combat AI hallucination, dedicated "Checker" agents verify each
step against the source paper, while optional human checkpoints ensure the
final product aligns perfectly with the author's vision, transforming the
system from a mere tool into a powerful collaborative assistant. To rigorously
validate our approach, we also construct $\textbf{PageBench}$, the first
benchmark for this new task. Experiments show AutoPage not only generates
high-quality, visually appealing pages but does so with remarkable efficiency
in under 15 minutes for less than \$0.1. Code and dataset will be released at
$\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.

</details>


### [15] [FidelityGPT: Correcting Decompilation Distortions with Retrieval Augmented Generation](https://arxiv.org/abs/2510.19615)
*Zhiping Zhou,Xiaohong Li,Ruitao Feng,Yao Zhang,Yuekang Li,Wenbu Feng,Yunqian Wang,Yuqing Li*

Main category: cs.SE

TL;DR: FidelityGPT improves the accuracy and readability of decompiled code using advanced prompt engineering, context handling, and semantic correction, outperforming previous methods and showing promise for reverse engineering tasks.


<details>
  <summary>Details</summary>
Motivation: Decompilation is essential for analyzing and debugging software without source code, but existing approaches struggle with semantic fidelity and readability, especially for closed-source binaries.

Method: FidelityGPT uses distortion-aware prompt templates, combines Retrieval-Augmented Generation (RAG), and introduces a dynamic semantic intensity algorithm to detect and correct distorted code lines. It also employs a variable dependency algorithm to address long-context issues and improve prompt relevance.

Result: In tests on 620 function pairs, FidelityGPT achieved 89% detection accuracy and 83% precision. It markedly outperformed DeGPT with a 94% Fix Rate and 64% Corrected Fix Rate, improving both accuracy and readability of decompiled code.

Conclusion: FidelityGPT significantly advances the fidelity and readability of LLM-based decompilation, offering robust semantic correction for challenging closed-source binaries.

Abstract: Decompilation converts machine code into human-readable form, enabling
analysis and debugging without source code. However, fidelity issues often
degrade the readability and semantic accuracy of decompiled output. Existing
methods, such as variable renaming or structural simplification, provide
partial improvements but lack robust detection and correction, particularly for
complex closed-source binaries. We present FidelityGPT, a framework that
enhances decompiled code accuracy and readability by systematically detecting
and correcting semantic distortions. FidelityGPT introduces distortion-aware
prompt templates tailored to closed-source settings and integrates
Retrieval-Augmented Generation (RAG) with a dynamic semantic intensity
algorithm to locate distorted lines and retrieve semantically similar code from
a database. A variable dependency algorithm further mitigates long-context
limitations by analyzing redundant variables and integrating their dependencies
into the prompt context. Evaluated on 620 function pairs from a binary
similarity benchmark, FidelityGPT achieved an average detection accuracy of 89%
and a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,
Corrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating
significant gains in accuracy and readability. These results highlight its
potential to advance LLM-based decompilation and reverse engineering.

</details>


### [16] [Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary](https://arxiv.org/abs/2510.19692)
*Rashina Hoda*

Main category: cs.SE

TL;DR: The paper highlights the need to expand agentic AI's role in software engineering from code-centric to a process-wide perspective, proposes guiding principles, and recommends clear vocabulary, aiming to foster a thoughtful, collaborative approach for future developments in agentic SE.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the rise of agentic AI in software engineering and ensure that its integration is thoughtful and encompasses the broader socio-technical aspects beyond just coding.

Method: The paper recommends expanding the focus of agentic SE beyond code, anchors proposals in foundational SE concepts, proposes values and principles, and provides guidance on establishing a clear vocabulary for the field.

Result: The result is the introduction of a broader framework for agentic SE, values and principles for its development, and vocabulary guidelines to help standardize the discourse.

Conclusion: The paper concludes by advocating for deliberate and collaborative development of agentic SE, enabling the SE community to build strong foundations for agentic AI integration in a desirable and sustainable way.

Abstract: Agentic AI is poised to usher in a seismic paradigm shift in Software
Engineering (SE). As technologists rush head-along to make agentic AI a
reality, SE researchers are driven to establish agentic SE as a research area.
While early visions of agentic SE are primarily focused on code-related
activities, early empirical evidence calls for a consideration of a range of
socio-technical concerns to make it work in practice. This paper contributes to
the emerging community vision by: (a) recommending an expansion of its scope
beyond code, toward a 'whole of process' vision, grounding it in SE foundations
and evolution and emerging agentic SE frameworks, (b) proposing a preliminary
set of values and principles to guide efforts, and (c) sharing guidance on
designing/using well-defined vocabulary for agentic SE. It is hoped that these
ideas will encourage community collaborations and steer the SE community
towards laying strong foundations of agentic SE so its not only inevitable but
also deliberate and desirable in the long run.

</details>


### [17] [Review of Tools for Zero-Code LLM Based Application Development](https://arxiv.org/abs/2510.19747)
*Priyaranjan Pattnayak,Hussain Bohra*

Main category: cs.SE

TL;DR: Survey of LLM-driven zero code platforms shows they make app-building accessible, but challenges remain in flexibility and reliability; taxonomy and comparative analysis included, plus future directions suggested.


<details>
  <summary>Details</summary>
Motivation: To understand how LLM-powered platforms are transforming software development, making it accessible to users without coding experience, and to document the current landscape, strengths, limitations, and future directions.

Method: Broad survey methodology, including categorization and detailed comparison, was used to analyze various platforms’ characteristics (interface, backend, output type, extensibility, features).

Result: A taxonomy and comparison of platforms reveal diverse approaches, strengths, and trade-offs, with future opportunities identified in multimodal interfaces and improved orchestration. Key features and comparison with traditional development approaches are highlighted.

Conclusion: Zero code platforms powered by LLMs can significantly lower the barrier for application development, especially for non-programmers; however, there are challenges regarding flexibility and reliability.

Abstract: Large Language Models (LLMs) are transforming software creation by enabling
zero code development platforms. Our survey reviews recent platforms that let
users build applications without writing code, by leveraging LLMs as the brains
of the development process. We adopt a broad survey methodology, categorizing
platforms based on key dimensions such as interface style, backend integration,
output type, and extensibility. We analyze both dedicated LLM based app
builders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and
general no code platforms (e.g., Bubble, Glide) that integrate LLM
capabilities. We present a taxonomy categorizing these platforms by their
interface (conversational, visual, etc.), supported LLM backends, output type
(chatbot, full application, workflow), and degree of extensibility. Core
features such as autonomous agents, memory management, workflow orchestration,
and API integrations are in scope of the survey. We provide a detailed
comparison, highlighting each platform's strengths and limitations. Trade offs
(customizability, scalability, vendor lock-in) are discussed in comparison with
traditional and low code development approaches. Finally, we outline future
directions, including multimodal interfaces, on device LLMs, and improved
orchestration for democratizing app creation with AI. Our findings indicate
that while zero code LLM platforms greatly reduce the barrier to creating AI
powered applications, they still face challenges in flexibility and
reliability. Overall, the landscape is rapidly evolving, offering exciting
opportunities to empower non programmers to create sophisticated software.

</details>


### [18] [BOSQTGEN: Breaking the Sound Barrier in Test Generation](https://arxiv.org/abs/2510.19777)
*S M Sadrul Islam Asif,James Chen,Earl T. Barr,Mark Marron*

Main category: cs.SE

TL;DR: BOSQTGEN is a new black-box API test generation tool leveraging LLMs and combinatorial sampling. It delivers higher coverage and quality compared to prior approaches, facilitating automated and reliable API testing without needing source access.


<details>
  <summary>Details</summary>
Motivation: Modern software relies heavily on APIs, and failures due to inadequate API contracts are common. Existing test generation methods struggle with issues like handling multiple programming languages, inaccessible source code, and generating valid structured inputs, making robust conformance testing difficult.

Method: The authors present BOSQTGEN, a black-box API test generation tool. BOSQTGEN decomposes API specifications into basic components, uses Large Language Models (LLMs) to organize these components into logical strata, and applies combinatorial testing to smartly sample input values, maximizing coverage and reducing redundant tests.

Result: BOSQTGEN achieves an average of 82% code coverage on RESTful API benchmarks, demonstrating a 20% or greater improvement over previous state-of-the-art systems and approaching the results of hand-written test suites.

Conclusion: BOSQTGEN provides an efficient, fully API-driven mechanism for automatically generating robust API test cases, aiding in both validation and test-driven development, and improving software reliability.

Abstract: Modern software is increasingly built by composing APIs, elevating the API
contract to a critical role. Inadequate contracts, however, lead to mismatched
expectations and failures, creating a pressing need for robust conformance
testing. Current test generation techniques are hindered by key challenges:
polyglot systems, source code inaccessibility, a cost-reliability trade-off,
and, most critically, the difficulty of generating structured inputs.
  We introduce BOSQTGEN, a novel black-box methodology and tool for API test
generation. BOSQTGEN utilizes a novel approach for decomposing API
specifications into primitives, using LLMs to suggest coherent strata for them,
and employing combinatorial testing to efficiently sample over these values.
This approach ensures coverage of critical interactions while avoiding the
redundancy of random sampling.
  The resulting BOSQTGEN system achieves an average of 82% code coverage on
RESTful benchmarks, often a 20% or more increase over prior state-of-the-art
systems and nearing parity with hand-written test suites. Providing a fully
API-driven approach to test generation, enables developers to automatically
create high-quality test cases for validation or test-driven development.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Dependent Session Types for Verified Concurrent Programming](https://arxiv.org/abs/2510.19129)
*Qiancheng Fu,Hongwei Xi,Ankush Das*

Main category: cs.PL

TL;DR: TLLC builds on dependent types to directly support and verify concurrent programs via session types, is sound in theory, and is practical with a compiling implementation.


<details>
  <summary>Details</summary>
Motivation: Current dependent type theories like TLL lack built-in support for session-based concurrency, and verifying correctness of concurrent programs remains challenging.

Method: The authors extend the Two-Level Linear dependent type theory (TLL) to introduce TLLC, which incorporates session-based concurrency using Martin-Löf style dependency. They introduce novel intuitionistic session types and develop the theory, proofs of soundness as term and process calculi, and implement a compiler from TLLC to concurrent C code.

Result: TLLC enables relational verification between sequential and concurrent programs, making it possible to transfer correctness properties from sequential proofs to concurrent settings. The approach is shown to be practical with a working prototype compiler and extensive evaluation.

Conclusion: TLLC extends dependent types to support session-based concurrency, making session types an intrinsic tool for verifying both data structures and concurrent algorithms, and provides a foundation for integrating session types into other systems.

Abstract: We present TLLC which extends the Two-Level Linear dependent type theory
(TLL) with session-based concurrency. Equipped with Martin-L\"{o}f style
dependency, the session types of TLLC allow protocols to specify properties of
communicated messages. When used in conjunction with the dependent type
machinery already present in TLL, dependent session types facilitate a form of
relational verification by relating concurrent programs with their idealized
sequential counterparts. Correctness properties proven for sequential programs
can be easily lifted to their corresponding concurrent implementations. TLLC
makes session types a powerful tool for intrinsically verifying the correctness
of data structures such as queues and concurrent algorithms such as map-reduce.
To extend TLL with session types, we develop a novel formulation of
intuitionistic session type which we believe to be widely applicable for
integrating session types into other type systems beyond the context of TLLC.
We study the meta-theory of our language, proving its soundness as both a term
calculus and a process calculus. To demonstrate the practicality of TLLC, we
have implemented a prototype compiler that translates TLLC programs into
concurrent C code, which has been extensively evaluated.

</details>
