{"id": "2510.12803", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.12803", "abs": "https://arxiv.org/abs/2510.12803", "authors": ["Shang Zhou", "Zihan Zheng", "Kaiyuan Liu", "Zeyu Shen", "Zerui Cheng", "Zexing Chen", "Hansen He", "Jianzhu Yao", "Huanzhi Mao", "Qiuyang Mang", "Tianfu Fu", "Beichen Li", "Dongruixuan Li", "Wenhao Chai", "Zhuang Liu", "Aleksandra Korolova", "Peter Henderson", "Natasha Jaques", "Pramod Viswanath", "Saining Xie", "Jingbo Shang"], "title": "AutoCode: LLMs as Problem Setters for Competitive Programming", "comment": "Project page: https://livecodebenchpro.com/projects/autocode/overview", "summary": "Writing competitive programming problems is exacting. Authors must: set\nconstraints, input distributions, and edge cases that rule out shortcuts;\ntarget specific algorithms (e.g., max-flow, dynamic programming, data\nstructures); and calibrate complexity beyond the reach of most competitors. We\nargue that this makes for an ideal test of general large language model\ncapabilities and study whether they can do this reliably. We introduce\nAutoCode, which uses multiple rounds of validation to yield competition-grade\nproblem statements and test cases. On held-out problems, AutoCode test suites\napproach 99% consistency with official judgments, a significant improvement\nover current state-of-the-art methods like HardTests, which achieve less than\n81%. Furthermore, starting with a random seed problem, AutoCode can create\nnovel variants with reference and brute-force solutions. By cross-verifying\nthese generated solutions against test cases, we can further filter out\nmalformed problems. Our system ensures high correctness, as verified by human\nexperts. AutoCode successfully produces novel problems judged by\nGrandmaster-level (top 0.3%) competitive programmers to be of contest quality.", "AI": {"tldr": "AutoCode is a new system for automatically creating and validating competitive programming problems, achieving near-perfect consistency with official judgments and producing contest-quality problems as confirmed by top programmers. It significantly outperforms previous methods.", "motivation": "Competitive programming problem creation is demanding, requiring careful calibration and validation, and previous automated solutions have had limited reliability. The authors aim to see if general large language models can reliably generate competition-grade problems.", "method": "They introduce AutoCode, a system that uses multiple rounds of validation and cross-verification (including brute-force solution checks) to generate, test, and filter competitive programming problems, ensuring high correctness and quality.", "result": "AutoCode\u2019s output is nearly as consistent with official judgments as human-produced problems (99% consistency vs. less than 81% for previous methods). Grandmaster-level programmers have judged the system\u2019s novel problems to be of contest quality.", "conclusion": "AutoCode effectively generates high-quality competitive programming problems, with test suites approaching 99% consistency with official judgments, outperforming existing methods. The system is confirmed by human experts and top competitive programmers to produce contest-level problems."}}
{"id": "2510.12948", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12948", "abs": "https://arxiv.org/abs/2510.12948", "authors": ["Minh Nguyen"], "title": "SpareCodeSearch: Searching for Code Context When You Have No Spare GPU", "comment": "4 pages, 3 figures, 4 tables. Accepted to Context Collection Workshop\n  co-located with ASE'25", "summary": "Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language\nModels (CLMs) by including another module for retrieving relevant context to\nconstruct the input prompt. However, these retrieval modules commonly use\nsemantic search, requiring substantial computational resources for training and\nhosting these embedded models, making them infeasible to integrate into\nlightweight applications such as in-IDE AI-based code completion. In this\nsolution paper, we prove that using keyword-search is sufficient to retrieve\nrelevant and useful code context inside large codebases, without the need for\nextensive GPU resources. The usefulness of code contexts found by our solution\nis demonstrated through their completion results on the Code Context\nCompetition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and\nPython tracks, respectively.", "AI": {"tldr": "This paper shows that keyword-search can replace semantic search in code context retrieval for RAG frameworks, delivering strong performance and significantly lowering computational costs, making it suitable for in-IDE code completion.", "motivation": "The motivation is to overcome the computational resource demands of semantic search in Retrieval-Augmented Generation (RAG) setups for Code Language Models (CLMs), which limits their deployability in lightweight scenarios like in-IDE code completion.", "method": "The paper proposes replacing semantic search-based retrieval modules with a simpler keyword-search technique for retrieving relevant code context from large codebases.", "result": "The keyword-search solution achieved competitive completion results on the Code Context Competition benchmark, with chRF scores of 0.748 for Kotlin and 0.725 for Python.", "conclusion": "Keyword-search is an effective and resource-efficient alternative to semantic search for retrieving code context in RAG systems, enabling practical integration in lightweight applications without the need for extensive GPU resources."}}
{"id": "2510.13078", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13078", "abs": "https://arxiv.org/abs/2510.13078", "authors": ["Tri Minh-Triet Pham", "Diego Elias Costa", "Weiyi Shang", "Jinqiu Yang"], "title": "ADPerf: Investigating and Testing Performance in Autonomous Driving Systems", "comment": "13 pages, accepted by ASE 2025", "summary": "Obstacle detection is crucial to the operation of autonomous driving systems,\nwhich rely on multiple sensors, such as cameras and LiDARs, combined with code\nlogic and deep learning models to detect obstacles for time-sensitive\ndecisions. Consequently, obstacle detection latency is critical to the safety\nand effectiveness of autonomous driving systems. However, the latency of the\nobstacle detection module and its resilience to various changes in the LiDAR\npoint cloud data are not yet fully understood. In this work, we present the\nfirst comprehensive investigation on measuring and modeling the performance of\nthe obstacle detection modules in two industry-grade autonomous driving\nsystems, i.e., Apollo and Autoware. Learning from this investigation, we\nintroduce ADPerf, a tool that aims to generate realistic point cloud data test\ncases that can expose increased detection latency. Increasing latency decreases\nthe availability of the detected obstacles and stresses the capabilities of\nsubsequent modules in autonomous driving systems, i.e., the modules may be\nnegatively impacted by the increased latency in obstacle detection.\n  We applied ADPerf to stress-test the performance of widely used 3D obstacle\ndetection modules in autonomous driving systems, as well as the propagation of\nsuch tests on trajectory prediction modules. Our evaluation highlights the need\nto conduct performance testing of obstacle detection components, especially 3D\nobstacle detection, as they can be a major bottleneck to increased latency of\nthe autonomous driving system. Such an adverse outcome will also further\npropagate to other modules, reducing the overall reliability of autonomous\ndriving systems.", "AI": {"tldr": "The paper investigates how latency in obstacle detection modules, especially with 3D data, affects autonomous driving systems. Using their tool ADPerf, the authors stress-test industry-grade systems and show that increased latency causes reliability issues throughout the system, emphasizing the need for dedicated performance testing.", "motivation": "Autonomous driving systems rely on fast and accurate obstacle detection using sensors like cameras and LiDARs, making detection latency a critical safety and reliability factor. Current understanding of latency and resilience to LiDAR data variability in obstacle detection modules is lacking.", "method": "Comprehensive measurement and modeling of the obstacle detection performance in two industry-grade systems (Apollo and Autoware). Introduction of ADPerf, a tool to generate realistic point cloud test cases aimed at revealing potential latency increases.", "result": "ADPerf was used to stress-test 3D obstacle detection modules and analyze the impact of increased latency on trajectory prediction modules. The performance bottleneck caused by latency in 3D obstacle detection is highlighted and shown to negatively affect downstream modules.", "conclusion": "Performance testing, especially for 3D obstacle detection modules, is essential as detection latency can significantly reduce the reliability and availability of autonomous driving systems by impacting other crucial modules."}}
{"id": "2510.13106", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13106", "abs": "https://arxiv.org/abs/2510.13106", "authors": ["Ruoyu Sun", "Da Song", "Jiayang Song", "Yuheng Huang", "Lei Ma"], "title": "TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models", "comment": "4 pages, 2 figures, To appear in ASE 2025 Demo Track", "summary": "As Large Language Models (LLMs) continue to revolutionize Natural Language\nProcessing (NLP) applications, critical concerns about their trustworthiness\npersist, particularly in safety and robustness. To address these challenges, we\nintroduce TRUSTVIS, an automated evaluation framework that provides a\ncomprehensive assessment of LLM trustworthiness. A key feature of our framework\nis its interactive user interface, designed to offer intuitive visualizations\nof trustworthiness metrics. By integrating well-known perturbation methods like\nAutoDAN and employing majority voting across various evaluation methods,\nTRUSTVIS not only provides reliable results but also makes complex evaluation\nprocesses accessible to users. Preliminary case studies on models like\nVicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our\nframework in identifying safety and robustness vulnerabilities, while the\ninteractive interface allows users to explore results in detail, empowering\ntargeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g", "AI": {"tldr": "TRUSTVIS is an automated framework for evaluating large language model trustworthiness, using perturbation methods and intuitive visualizations. It enables users to identify and address safety issues in models like Vicuna-7b, Llama2-7b, and GPT-3.5, making evaluation more reliable and actionable.", "motivation": "Concerns about the trustworthiness, safety, and robustness of Large Language Models (LLMs) persist as these models become integral to NLP applications. There is a need for comprehensive and accessible tools to evaluate and visualize LLM trustworthiness.", "method": "The paper introduces TRUSTVIS, an automated evaluation framework equipped with an interactive user interface for intuitive visualization of trustworthiness metrics. It integrates established perturbation methods (e.g., AutoDAN) and applies majority voting across various evaluation approaches to reliably assess LLMs.", "result": "Preliminary case studies on Vicuna-7b, Llama2-7b, and GPT-3.5 show that TRUSTVIS effectively identifies safety and robustness vulnerabilities. The interactive interface enables users to examine results in detail, facilitating targeted improvements in model design and deployment.", "conclusion": "TRUSTVIS streamlines and enhances the evaluation of LLM trustworthiness, making sophisticated assessments more accessible and actionable for users, and supports the development of safer, more robust NLP systems."}}
{"id": "2510.13082", "categories": ["cs.PL", "cs.SE", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.13082", "abs": "https://arxiv.org/abs/2510.13082", "authors": ["Mark Koch", "Agust\u00edn Borgna", "Craig Roy", "Alan Lawrence", "Kartik Singhal", "Seyon Sivarajah", "Ross Duncan"], "title": "Imperative Quantum Programming with Ownership and Borrowing in Guppy", "comment": "Presented at the Fifth International Workshop on Programming\n  Languages for Quantum Computing (PLanQC 2025)", "summary": "Linear types enforce no-cloning and no-deleting theorems in functional\nquantum programming. However, in imperative quantum programming, they have not\ngained widespread adoption. This work aims to develop a quantum type system\nthat combines ergonomic linear typing with imperative semantics and maintains\nsafety guarantees. All ideas presented here have been implemented in\nQuantinuum's Guppy programming language.", "AI": {"tldr": "The paper introduces a quantum type system that integrates linear type safety into imperative programming, making it user-friendly and reliable. All concepts have been implemented in the Guppy quantum language.", "motivation": "Linear types are essential for enforcing quantum-specific constraints (no-cloning, no-deleting), but have not been widely adopted in imperative quantum programming due to ergonomic or semantic limitations. The paper seeks to bridge this gap.", "method": "The method involves designing and developing a type system that blends linear type theory (traditionally for functional languages) with imperative programming paradigms, and practical implementation within the Guppy quantum programming language.", "result": "The developed quantum type system combines the benefits of linear typing (enforcing quantum constraints) with the usability and semantics of imperative programming. This system has been fully implemented in Guppy.", "conclusion": "The paper concludes by presenting a quantum type system that integrates linear typing with imperative semantics, achieving safety guarantees and practical ergonomics, and notes successful implementation in the Guppy language."}}
{"id": "2510.13128", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13128", "abs": "https://arxiv.org/abs/2510.13128", "authors": ["Yujie Liu", "Mingxuan Zhu", "Shengyu Cheng", "Dan Hao"], "title": "Isolating Compiler Bugs through Compilation Steps Analysis", "comment": null, "summary": "Compilers are essential to software systems, and their bugs can propagate to\ndependent software. Ensuring compiler correctness is critical. However,\nisolating compiler bugs remains challenging due to the internal complexity of\ncompiler execution. Existing techniques primarily mutate compilation inputs to\ngenerate passing and failing tests, but often lack causal analysis of internal\nsteps, limiting their effectiveness.\n  To address this limitation, we propose CompSCAN, a novel compiler bug\nisolation technique that applies analysis over the sequence of compilation\nsteps. CompSCAN follows a three-stage process: (1) extracting the array of\ncompilation steps that leads to the original failure, (2) identifying\nbug-causing steps and collecting corresponding compiler code elements, and (3)\ncalculating suspicious scores for each code element and outputting a suspicious\nranking list as the bug isolation result.\n  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that\nCompSCAN outperforms state-of-the-art techniques in both effectiveness and\nefficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the\nTop-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two\nstate-of-the-art compiler bug isolation techniques, CompSCAN achieves relative\nimprovements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /\n49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs\nfaster on average per bug than both baselines.", "AI": {"tldr": "CompSCAN is a new technique for isolating compiler bugs that analyzes internal compilation steps, outperforming existing methods in accuracy and speed when tested on LLVM and GCC bugs.", "motivation": "Compiler bugs have significant downstream impacts, but isolating them is hard due to compiler complexity and lack of causal analysis in prior work.", "method": "CompSCAN uses a three-stage process: extracting failing compilation steps, identifying bug-causing steps and code elements, and ranking suspicious elements for isolation.", "result": "CompSCAN isolated more bugs within top rank positions than state-of-the-art techniques ETEM and ODFL (e.g., isolating 123 bugs in top-10), and showed 24.49%-50.18% relative improvements. It was also faster than the baselines.", "conclusion": "CompSCAN effectively improves compiler bug isolation both in accuracy and speed compared to existing techniques."}}
{"id": "2510.13236", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.13236", "abs": "https://arxiv.org/abs/2510.13236", "authors": ["Sebastian mateos Nicolajsen"], "title": "Extensibility in Programming Languages: An overview", "comment": null, "summary": "I here conduct an exploration of programming language extensibility, making\nan argument for an often overlooked component of conventional language design.\nNow, this is not a technical detailing of these components, rather, I attempt\nto provide an overview as I myself have lacked during my time investigating\nprogramming languages. Thus, read this as an introduction to the magical world\nof extensibility. Through a literature review, I identify key extensibility\nthemes - Macros, Modules, Types, and Reflection - highlighting diverse\nstrategies for fostering extensibility. The analysis extends to cross-theme\nproperties such as Parametricism and First-class citizen behaviour, introducing\nlayers of complexity by highlighting the importance of customizability and\nflexibility in programming language constructs. By outlining these facets of\nexisting programming languages and research, I aim to inspire future language\ndesigners to assess and consider the extensibility of their creations\ncritically.", "AI": {"tldr": "This paper reviews and summarizes key aspects and strategies of programming language extensibility, aiming to encourage designers to pay more attention to making languages flexible and customizable.", "motivation": "The paper is motivated by the lack of accessible overviews on programming language extensibility and seeks to present an introductory exploration for designers and learners.", "method": "The author conducts a literature review to identify and analyze key themes and strategies related to extensibility in programming languages, focusing on areas like macros, modules, types, and reflection.", "result": "The analysis highlights how various extensibility mechanisms (macros, modules, types, and reflection), along with cross-cutting properties such as parametricism and first-class citizen behavior, contribute to customizability and flexibility in language design.", "conclusion": "The author concludes by urging future language designers to critically evaluate and prioritize extensibility in their designs, using the overview provided as inspiration and guidance."}}
{"id": "2510.13176", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13176", "abs": "https://arxiv.org/abs/2510.13176", "authors": ["Haolin Pan", "Chao Zha", "Jinyuan Dong", "Mingjie Xing", "Yanjun Wu"], "title": "GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning", "comment": null, "summary": "Compiler pass selection and phase ordering present a significant challenge in\nachieving optimal program performance, particularly for objectives like code\nsize reduction. Standard compiler heuristics offer general applicability but\noften yield suboptimal, program-specific results due to their one-size-fits-all\nnature. While iterative compilation can find tailored solutions, its\nprohibitive search cost limits practical use. Machine learning approaches\npromise faster inference but frequently struggle with generalization to unseen\nprograms. This paper introduces GRACE, a novel framework for compiler\nauto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE\neffectively curtails the search space by leveraging pass synergies and a\nweighted scoring method to generate initial high-quality candidate sequences\nand a pass pool. It then employs contrastive learning, using pass\nsequence-based data augmentation, to create program embeddings that facilitate\nsimilarity-aware clustering. Evolutionary search within these clusters yields a\ncoreset of $k$ specialized pass sequences designed for robust generalization to\nunseen programs. At test time, GRACE efficiently selects the best coreset\nsequence and refines it using lightweight techniques. Experimental results on\nseven diverse datasets show that GRACE reduces LLVM IR instruction count by an\naverage of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,\nwhile incurring an average tuning time of less than 1s per program,\ndemonstrating its state-of-the-art performance and practical effectiveness.", "AI": {"tldr": "GRACE is a new compiler auto-tuning method that robustly cuts code size and tuning time by mixing pass synergy analysis, machine learning clustering, and evolutionary search. It outperforms typical optimization (opt -Oz) on LLVM, generalizes well, and is practically fast.", "motivation": "Optimizing compiler passes and phase ordering is crucial for improving program performance, particularly in reducing code size. Existing solutions, such as standard heuristics and iterative compilation, are either too generic or too costly. Machine learning methods have promise but struggle to generalize well to new programs.", "method": "The paper introduces GRACE, a compiler auto-tuning framework aiming to optimize LLVM IR instruction count. GRACE narrows the search space by analyzing pass synergies and uses a weighted scoring system to pick promising initial sequences and pass pools. It then utilizes contrastive learning with pass-sequence-based data augmentation to build program embeddings, enabling similarity-aware clustering. Within each cluster, evolutionary search identifies $k$ specialized pass sequences. At deployment, GRACE selects and refines the most suitable sequence for each new program efficiently.", "result": "GRACE achieves a reduction in LLVM IR instruction count by an average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to the standard opt -Oz, with an average tuning time under 1 second per program.", "conclusion": "GRACE delivers fast and robust compiler optimization, yielding state-of-the-art code size reductions efficiently. Its innovative blend of synergy analysis, machine learning-driven clustering, and evolutionary search enables practical, generalizable, and high-quality auto-tuning for unseen programs."}}
{"id": "2510.13426", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.13426", "abs": "https://arxiv.org/abs/2510.13426", "authors": ["Sehyeok Park", "Santosh Nagarakatte"], "title": "Fast Trigonometric Functions using the RLIBM Approach", "comment": "In Proceedings VSS 2025, arXiv:2510.12314", "summary": "This paper describes our experience developing polynomial approximations for\ntrigonometric functions that produce correctly rounded results for multiple\nrepresentations and rounding modes using the RLIBM approach. A key challenge\nwith trigonometric functions concerns range reduction with \"pi\", which reduces\na given input in the domain of a 32-bit float to a small domain. Any rounding\nerror in the value of \"pi\" is amplified during range reduction, which can\nresult in wrong results. We describe our experience implementing fast range\nreduction techniques that maintain a large number of bits of \"pi\" both with\nfloating-point and integer computations. The resulting implementations for\ntrigonometric functions are fast and produce correctly rounded results for all\ninputs for multiple representations up to 32-bits with a single implementation.", "AI": {"tldr": "This paper presents efficient polynomial approximations for trigonometric functions using advanced range reduction techniques that maintain high precision for 'pi,' enabling fast and correctly rounded results for multiple floating-point formats and modes with a single implementation.", "motivation": "Accurately computing trigonometric functions in floating-point arithmetic is challenging due to errors magnified during range reduction with the irrational number 'pi.' Correct rounding across various representations and modes is difficult.", "method": "The paper uses the RLIBM approach to develop polynomial approximations for trigonometric functions and implements fast range reduction techniques that preserve a large number of bits of 'pi,' employing both floating-point and integer computations.", "result": "The developed implementations for trigonometric functions are fast and produce correctly rounded results for all inputs and for multiple floating-point representations up to 32 bits in one implementation.", "conclusion": "The authors successfully created efficient and broadly applicable polynomial approximations for trigonometric functions with correct rounding, overcoming the critical challenge of range reduction with high-precision 'pi.'"}}
{"id": "2510.13184", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13184", "abs": "https://arxiv.org/abs/2510.13184", "authors": ["Haolin Pan", "Jinyuan Dong", "Mingjie Xing", "Yanjun Wu"], "title": "Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines", "comment": null, "summary": "Compiler optimization relies on sequences of passes to improve program\nperformance. Selecting and ordering these passes automatically, known as\ncompiler auto-tuning, is challenging due to the large and complex search space.\nExisting approaches generally assume a linear sequence of passes, a model\ncompatible with legacy compilers but fundamentally misaligned with the\nhierarchical design of the LLVM New Pass Manager. This misalignment prevents\nthem from guaranteeing the generation of syntactically valid optimization\npipelines. In this work, we present a new auto-tuning framework built from the\nground up for the New Pass Manager. We introduce a formal grammar to define the\nspace of valid nested pipelines and a forest-based data structure for their\nnative representation. Upon this foundation, we develop a structure-aware\nGenetic Algorithm whose operators manipulate these forests directly, ensuring\nthat all candidate solutions are valid by construction. The framework first\nmines synergistic pass relationships to guide the search. An optional\nrefinement stage further explores subtle performance variations arising from\ndifferent valid structural arrangements.\n  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The\ndiscovered pipelines achieve an average of 13.62% additional instruction count\nreduction compared to the standard opt -Oz optimization level, showing that our\nframework is capable of navigating this complex, constrained search space to\nidentify valid and effective pass pipelines.", "AI": {"tldr": "This paper presents a novel auto-tuning framework for LLVM's New Pass Manager, leveraging a structure-aware Genetic Algorithm and a formal grammar to generate valid hierarchical optimization pipelines. The method outperforms traditional approaches, reducing instruction count by 13.62% compared to standard optimization on several benchmarks.", "motivation": "Current auto-tuning approaches for compiler optimization assume linear pass sequences, which do not align with the hierarchical structure of the LLVM New Pass Manager, resulting in syntactically invalid pipelines and suboptimal optimization.", "method": "A formal grammar and forest-based data structure are used to represent valid pipelines, with a structure-aware Genetic Algorithm manipulating pipeline structures directly. The algorithm mines synergistic relationships among compiler passes and includes an optional refinement phase.", "result": "The method achieved an average 13.62% reduction in instruction count over the standard opt -Oz optimization level across seven benchmark datasets, demonstrating its effectiveness in finding valid and high-performing optimization pipelines.", "conclusion": "The proposed auto-tuning framework for the LLVM New Pass Manager can generate syntactically valid and effective optimization pipelines, achieving better performance than the standard optimization level."}}
{"id": "2510.13725", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.13725", "abs": "https://arxiv.org/abs/2510.13725", "authors": ["Celia Mengyue Li", "Sophie Pull", "Steven Ramsay"], "title": "A Complementary Approach to Incorrectness Typing", "comment": "Version conditionally accepted to POPL'26, with reviewer suggestions\n  incorporated", "summary": "We introduce a new two-sided type system for verifying the correctness and\nincorrectness of functional programs with atoms and pattern matching. A key\nidea in the work is that types should range over sets of normal forms, rather\nthan sets of values, and this allows us to define a complement operator on\ntypes that acts as a negation on typing formulas. We show that the complement\nallows us to derive a wide range of refutation principles within the system,\nincluding the type-theoretic analogue of co-implication, and we use them to\ncertify that a number of Erlang-like programs go wrong. An expressive\naxiomatisation of the complement operator via subtyping is shown decidable, and\nthe type system as a whole is shown to be not only sound, but also complete for\nnormal forms.", "AI": {"tldr": "A new type system is described that covers both program correctness and incorrectness by introducing a negation-like complement operator, enabling thorough verification and soundness for functional programs that use atoms and pattern matching.", "motivation": "Traditional type systems for functional programming struggle with expressing both correctness and incorrectness, especially in the presence of atoms and pattern matching. A new approach is needed to better certify when programs fail.", "method": "The paper introduces a two-sided type system where types range over sets of normal forms (not just values). They define a complement operator on types that acts as a negation, giving rise to refutation principles. The complement's axiomatisation via subtyping is formally shown to be decidable.", "result": "Their system can express refutations (incorrectness) as well as correctness, and they demonstrate it on Erlang-like programs, showing cases where programs can be certified to 'go wrong.' The whole system is proven to be sound and complete for normal forms.", "conclusion": "They present a two-sided type system with a decidable and expressive complement operator, enabling verification of both correctness and refutation (incorrectness) in functional programs with atoms and pattern matching."}}
{"id": "2510.13423", "categories": ["cs.SE", "cs.MS"], "pdf": "https://arxiv.org/pdf/2510.13423", "abs": "https://arxiv.org/abs/2510.13423", "authors": ["Matthew Sottile", "Mohit Tekriwal", "John Sarracino"], "title": "Towards Richer Challenge Problems for Scientific Computing Correctness", "comment": "In Proceedings VSS 2025, arXiv:2510.12314", "summary": "Correctness in scientific computing (SC) is gaining increasing attention in\nthe formal methods (FM) and programming languages (PL) community. Existing\nPL/FM verification techniques struggle with the complexities of realistic SC\napplications. Part of the problem is a lack of a common understanding between\nthe SC and PL/FM communities of machine-verifiable correctness challenges and\ndimensions of correctness in SC applications.\n  To address this gap, we call for specialized challenge problems to inform the\ndevelopment and evaluation of FM/PL verification techniques for correctness in\nSC. These specialized challenges are intended to augment existing problems\nstudied by FM/PL researchers for general programs to ensure the needs of SC\napplications can be met. We propose several dimensions of correctness relevant\nto scientific computing, and discuss some guidelines and criteria for designing\nchallenge problems to evaluate correctness in scientific computing.", "AI": {"tldr": "The paper identifies a disconnect between scientific computing and formal methods communities regarding correctness verification. It proposes creating specialized challenge problems and guidelines to better evaluate and improve verification techniques for scientific applications.", "motivation": "There is a growing concern about ensuring correctness in scientific computing, but current formal methods and programming languages verification techniques are not well-suited for the unique challenges of scientific computing applications. This is due in part to a lack of shared understanding between the scientific computing and formal methods/programming languages communities.", "method": "The paper suggests developing specialized challenge problems tailored for scientific computing. These problems would help inform the creation and evaluation of verification techniques sensitive to the needs of scientific computing. The authors propose several key dimensions of correctness and provide guidelines for designing such challenge problems.", "result": "The outcome is a proposal for targeted challenge problems and design guidelines that can guide future work in creating effective verification tools for scientific computing. This aims to bridge the current gap between the communities and deepen understanding of correctness in scientific applications.", "conclusion": "A set of specialized challenge problems and evaluation criteria is needed to address the unique correctness challenges of scientific computing, improving communication between relevant research communities and enabling better FM/PL tool development."}}
{"id": "2510.13424", "categories": ["cs.SE", "D.2.5; G.1.3"], "pdf": "https://arxiv.org/pdf/2510.13424", "abs": "https://arxiv.org/abs/2510.13424", "authors": ["Alexander C. Wilton"], "title": "Verifying a Sparse Matrix Algorithm Using Symbolic Execution", "comment": "In Proceedings VSS 2025, arXiv:2510.12314", "summary": "Scientific software is, by its very nature, complex. It is mathematical and\nhighly optimized which makes it prone to subtle bugs not as easily detected by\ntraditional testing. We outline how symbolic execution can be used to write\ntests similar to traditional unit tests while providing stronger verification\nguarantees and apply this methodology to a sparse matrix algorithm.", "AI": {"tldr": "Symbolic execution improves testing for scientific software, enabling stronger verification and better bug detection than traditional testing, as shown in a sparse matrix algorithm application.", "motivation": "Scientific software faces challenges due to its complexity and optimization, which often result in subtle bugs that are hard to catch with regular testing methods.", "method": "The authors propose using symbolic execution as a testing methodology. This approach enables the writing of tests similar to unit tests, but with enhanced verification guarantees compared to traditional testing.", "result": "The methodology is applied to a sparse matrix algorithm, demonstrating its practical utility in detecting bugs and verifying correctness.", "conclusion": "Symbolic execution offers a more powerful verification mechanism for scientific software, helping to uncover bugs that standard unit tests might miss."}}
{"id": "2510.13561", "categories": ["cs.SE", "cs.AI", "68N30"], "pdf": "https://arxiv.org/pdf/2510.13561", "abs": "https://arxiv.org/abs/2510.13561", "authors": ["Peng Di", "Faqiang Chen", "Xiao Bai", "Hongjun Yang", "Qingfeng Li", "Ganglin Wei", "Jian Mou", "Feng Shi", "Keting Chen", "Peng Tang", "Zhitao Shen", "Zheng Li", "Wenhui Shi", "Junwei Guo", "Hang Yu"], "title": "OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies", "comment": "23 pages", "summary": "The escalating complexity of modern software imposes an unsustainable\noperational burden on Site Reliability Engineering (SRE) teams, demanding\nAI-driven automation that can emulate expert diagnostic reasoning. Existing\nsolutions, from traditional AI methods to general-purpose multi-agent systems,\nfall short: they either lack deep causal reasoning or are not tailored for the\nspecialized, investigative workflows unique to SRE. To address this gap, we\npresent OpenDerisk, a specialized, open-source multi-agent framework\narchitected for SRE. OpenDerisk integrates a diagnostic-native collaboration\nmodel, a pluggable reasoning engine, a knowledge engine, and a standardized\nprotocol (MCP) to enable specialist agents to collectively solve complex,\nmulti-domain problems. Our comprehensive evaluation demonstrates that\nOpenDerisk significantly outperforms state-of-the-art baselines in both\naccuracy and efficiency. This effectiveness is validated by its large-scale\nproduction deployment at Ant Group, where it serves over 3,000 daily users\nacross diverse scenarios, confirming its industrial-grade scalability and\npractical impact. OpenDerisk is open source and available at\nhttps://github.com/derisk-ai/OpenDerisk/", "AI": {"tldr": "OpenDerisk is an open-source, multi-agent framework tailored to SRE teams, offering advanced diagnostic reasoning and collaboration. It surpasses current solutions in accuracy and efficiency and is proven effective in large-scale, real-world deployment.", "motivation": "Modern software complexity places excessive operational demand on SRE teams. There's a need for AI-driven tools that can mimic expert diagnostic reasoning, as current solutions lack deep causal reasoning and do not fit specialized SRE workflows.", "method": "The authors introduce OpenDerisk, an open-source multi-agent framework designed specifically for SRE. It features a diagnostic-native collaboration model, pluggable reasoning engine, knowledge engine, and a standardized protocol for agent communication, enabling specialist agents to collaborate on complex problems.", "result": "OpenDerisk outperforms current state-of-the-art solutions in both accuracy and efficiency. It has been successfully deployed at Ant Group, supporting over 3,000 daily users in various production scenarios, demonstrating its scalability and real-world effectiveness.", "conclusion": "OpenDerisk fills a critical gap in SRE AI tooling, offering superior diagnostic reasoning, collaboration, and scalability compared to existing approaches. Its open-source nature and production validation signify its practical impact and accessibility for industry adoption."}}
{"id": "2510.13575", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.13575", "abs": "https://arxiv.org/abs/2510.13575", "authors": ["Han Fu", "Sigrid Eldh", "Kristian Wiklund", "Andreas Ermedahl", "Philipp Haller", "Cyrille Artho"], "title": "Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code", "comment": "9 pages, 4 figures, conference: 2025 28th Euromicro Conference on\n  Digital System Design (DSD)", "summary": "The co-development of hardware and software in industrial embedded systems\nfrequently leads to compilation errors during continuous integration (CI).\nAutomated repair of such failures is promising, but existing techniques rely on\ntest cases, which are not available for non-compilable code.\n  We employ an automated repair approach for compilation errors driven by large\nlanguage models (LLMs). Our study encompasses the collection of more than 40000\ncommits from the product's source code. We assess the performance of an\nindustrial CI system enhanced by four state-of-the-art LLMs, comparing their\noutcomes with manual corrections provided by human programmers. LLM-equipped CI\nsystems can resolve up to 63 % of the compilation errors in our baseline\ndataset. Among the fixes associated with successful CI builds, 83 % are deemed\nreasonable. Moreover, LLMs significantly reduce debugging time, with the\nmajority of successful cases completed within 8 minutes, compared to hours\ntypically required for manual debugging.", "AI": {"tldr": "This paper shows that large language models can automatically fix most compilation errors in embedded system CI pipelines, outperforming manual fixes in speed and efficiency, even without test cases.", "motivation": "Compilation errors in CI pipelines, caused by co-development of hardware and software, slow down development and require manual intervention. Traditional automated repair methods depend on test cases, which aren't available for non-compilable code, prompting exploration of LLM-driven solutions.", "method": "The approach uses large language models to automatically repair compilation errors without test cases. The researchers collected over 40,000 commits from industrial products and compared the performance of CI systems enhanced with four different LLMs to manual human corrections.", "result": "LLM-enhanced CI systems resolved up to 63% of compilation errors. Of the CI builds fixed by LLMs, 83% were judged reasonable. Successful cases also completed debugging much faster (under 8 minutes) than manual debugging (hours).", "conclusion": "LLMs can effectively automate the repair of compilation errors in industrial embedded systems' CI pipelines, resolving a majority of issues and significantly reducing debugging time."}}
{"id": "2510.13692", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13692", "abs": "https://arxiv.org/abs/2510.13692", "authors": ["Deepak A. Cherian"], "title": "Property Testing for Ocean Models. Can We Specify It? (Invited Talk)", "comment": "In Proceedings VSS 2025, arXiv:2510.12314", "summary": "I take inspiration from the property-testing literature, particularly the\nwork of Prof. John Hughes, and explore how such ideas might be applied to\nnumerical models of the ocean. Specifically, I ask whether geophysical fluid\ndynamics (GFD) theory, expressed as property tests, might be used to address\nthe oracle problem of testing the correctness of ocean models. I propose that a\nnumber of simple idealized GFD problems can be framed as property tests. These\nexamples clearly illustrate how physics naturally lends itself to specifying\nproperty tests. Which of these proposed tests might be most feasible and\nuseful, remains to be seen.", "AI": {"tldr": "The paper suggests using property-based testing methods from computer science, specifically applying geophysical fluid dynamics theory as property tests, to improve the validation of ocean models. Initial examples show promise, but further research is needed to determine practicality and utility.", "motivation": "Testing the correctness of ocean models is challenging due to the oracle problem, where it is difficult to determine whether outputs are 'correct.' The author seeks novel ways to address this challenge.", "method": "The author proposes applying the concept of property-based testing, inspired by computer science literature, specifically transforming geophysical fluid dynamics (GFD) theory into property tests for model validation.", "result": "The author demonstrates that simple, idealized GFD problems can indeed be reframed as property tests, and that physics principles align well with this approach. The paper identifies several candidate tests but leaves the determination of their feasibility and utility as an open question.", "conclusion": "Physics-based property tests may offer a natural and potentially effective approach for addressing the oracle problem in ocean model validation, though further work is needed to evaluate which specific tests are most valuable."}}
{"id": "2510.13697", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13697", "abs": "https://arxiv.org/abs/2510.13697", "authors": ["Maksim Sapronov", "Evgeniy Glukhov"], "title": "On Pretraining for Project-Level Code Completion", "comment": null, "summary": "Repository-level pretraining is commonly used to enable large language models\nfor code to leverage codebase-wide context. This enhances their ability to\ngenerate accurate and context-aware code completions. In this work, we\ninvestigate how different repository-processing strategies affect in-context\nlearning in OpenCoder, a 1.5B-parameter model. We extend its context window\nfrom 4,096 to 16,384 tokens by training on additional 1B tokens of curated\nrepository-level data. Despite relying on a smaller dataset than competing\nmodels (which often use hundreds of billions of tokens), our model achieves\ncomparable performance on the Long Code Arena benchmark. We find that various\nrepository-processing techniques yield similarly strong results, with the\nprimary gain coming from adapting to a new rotary positional embedding (RoPE)\nscaling parameter. Finally, we show that a simpler file-level training approach\nat the original sequence length remains highly effective, opening up\nrepository-level code completion research to settings with more constrained\ndata and compute resources.", "AI": {"tldr": "Expanding context window and trying different data processing strategies for OpenCoder, a code LLM, shows competitive results even with less data. The main gain results from an adapted RoPE scaling. Simple file-level training also remains effective, helping lower data and compute barriers for code completion research.", "motivation": "Enable large language models for code to better leverage codebase-wide context for more accurate and context-aware code completion.", "method": "Investigated different repository-processing strategies on OpenCoder (1.5B parameters), extended context window from 4,096 to 16,384 tokens via training on an extra 1B tokens of curated repository-level data. Compared repository-level with file-level training and adapted rotary positional embedding (RoPE) scaling.", "result": "Despite using much less data than competitors, their model performed comparably on the Long Code Arena benchmark. All repository-processing techniques tested produced strong results, with the main improvement coming from adapting the RoPE scaling. Simpler file-level training at original sequence lengths also remained very effective.", "conclusion": "Repository-level training is efficient and less dependent on massive amounts of data/computing resources than previously thought, especially when carefully adjusted for position embeddings. File-level training is also surprisingly competitive."}}
