{"id": "2506.19897", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19897", "abs": "https://arxiv.org/abs/2506.19897", "authors": ["Christopher Glasz", "Emily Escamilla", "Eric O. Scott", "Anand Patel", "Jacob Zimmer", "Colin Diggs", "Michael Doyle", "Scott Rosen", "Nitin Naik", "Justin F. Brunelle", "Samruddhi Thaker", "Parthav Poudel", "Arun Sridharan", "Amit Madan", "Doug Wendt", "William Macke", "Thomas Schill"], "title": "Can LLMs Replace Humans During Code Chunking?", "comment": null, "summary": "Large language models (LLMs) have become essential tools in computer science,\nespecially for tasks involving code understanding and generation. However,\nexisting work does not address many of the unique challenges presented by code\nwritten for government applications. In particular, government enterprise\nsoftware is often written in legacy languages like MUMPS or assembly language\ncode (ALC) and the overall token lengths of these systems exceed the context\nwindow size for current commercially available LLMs. Additionally, LLMs are\nprimarily trained on modern software languages and have undergone limited\ntesting with legacy languages, making their ability to understand legacy\nlanguages unknown and, hence, an area for empirical study. This paper examines\nthe application of LLMs in the modernization of legacy government code written\nin ALC and MUMPS, addressing the challenges of input limitations. We\ninvestigate various code-chunking methods to optimize the generation of summary\nmodule comments for legacy code files, evaluating the impact of code-chunking\nmethods on the quality of documentation produced by different LLMs, including\nGPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs\ncan select partition points closely aligned with human expert partitioning. We\nalso find that chunking approaches have significant impact on downstream tasks\nsuch as documentation generation. LLM-created partitions produce comments that\nare up to 20% more factual and up to 10% more useful than when humans create\npartitions. Therefore, we conclude that LLMs can be used as suitable\nreplacements for human partitioning of large codebases during LLM-aided\nmodernization."}
{"id": "2506.20063", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.20063", "abs": "https://arxiv.org/abs/2506.20063", "authors": ["Zixuan Feng", "Thomas Zimmermann", "Lorenzo Pisani", "Christopher Gooley", "Jeremiah Wander", "Anita Sarma"], "title": "When Domains Collide: An Activity Theory Exploration of Cross-Disciplinary Collaboration", "comment": "Cross-disciplinary Collaboration, Activity Theory, Mixed-Methods", "summary": "Background: Software development teams are increasingly diverse, embedded,\nand cross-disciplinary. Domain experts (DEs) from different disciplines\ncollaborate with professional software developers (SDEs), bringing\ncomplementary expertise in creating and maintaining complex production\nsoftware. However, contested expectations, divergent problem-solving\nperspectives, and conflicting priorities lead to friction. Aims: This study\naims to investigate the dynamics of emerging collaboration of\ncross-disciplinary software development (CDSD) by exploring the expectations\nheld by DEs and SDEs and understanding how these frictions manifest in\npractice. Method: We utilize Activity Theory (AT), a well-established\nsocio-technical framework, as an analytical lens in a grounded, empirical\ninvestigation, conducted through a mixed-method study involving 24 interviews\n(12 DEs and 12 SDEs) and a large-scale validation survey with 293 participants\n(161 DEs and 132 SDEs). Results: We conceptualize and empirically ground the\nCDSD dynamics. We identified eight expectations held by SDEs and six by DEs. By\nmapping these expectations to AT components, we revealed 21 frictions in CDSD\nand illustrated where and how they arise. Conclusions: This study offers a\ntheoretical lens for understanding the dynamics and frictions in CDSD and\nprovides actionable insights for future research, practitioners, and\ninfrastructure design."}
{"id": "2506.20159", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20159", "abs": "https://arxiv.org/abs/2506.20159", "authors": ["Tomas Herda", "Victoria Pichler", "Zheying Zhang", "Pekka Abrahamsson", "Geir K. Hanssen"], "title": "AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary", "comment": null, "summary": "The full-day workshop on AI and Agile at XP 2025 convened a diverse group of\nresearchers and industry practitioners to address the practical challenges and\nopportunities of integrating Artificial Intelligence into Agile software\ndevelopment. Through interactive sessions, participants identified shared\nfrustrations related to integrating AI into Agile Software Development\npractices, including challenges with tooling, governance, data quality, and\ncritical skill gaps. These challenges were systematically prioritized and\nanalyzed to uncover root causes. The workshop culminated in the collaborative\ndevelopment of a research roadmap that pinpoints actionable directions for\nfuture work, including both immediate solutions and ambitious long-term goals.\nThe key outcome is a structured agenda designed to foster joint\nindustry-academic efforts to move from identified frustrations to successful\nimplementation."}
{"id": "2506.20217", "categories": ["cs.SE", "cs.CE", "cs.CY", "68-01", "K.6.3"], "pdf": "https://arxiv.org/pdf/2506.20217", "abs": "https://arxiv.org/abs/2506.20217", "authors": ["Stuart M. Allen", "Neil Chue Hong", "Stephan Druskat", "Toby Hodges", "Daniel S. Katz", "Jan Linxweiler", "Frank Löffler", "Lars Grunske", "Heidi Seibold", "Jan Philipp Thiele", "Samantha Wittke"], "title": "Ten simple rules for PIs to integrate Research Software Engineering into their research group", "comment": "10 pages, submitted to PLOS Computational Biology", "summary": "Research Software Engineering (RSEng) is a key success factor in producing\nhigh-quality research software, which in turn enables and improves research\noutcomes. However, as a principal investigator or leader of a research group\nyou may not know what RSEng is, where to get started with it, or how to use it\nto maximize its benefit for your research. RSEng also often comes with\ntechnical complexity, and therefore reduced accessibility to some researchers.\nThe ten simple rules presented in this paper aim to improve the accessibility\nof RSEng, and provide practical and actionable advice to PIs and leaders for\nintegrating RSEng into their research group. By following these rules, readers\ncan improve the quality, reproducibility, and trustworthiness of their research\nsoftware, ultimately leading to better, more reproducible and more trustworthy\nresearch outcomes."}
{"id": "2506.20435", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.20435", "abs": "https://arxiv.org/abs/2506.20435", "authors": ["Mennatullah T. Khedr", "John S. Fitzgerald"], "title": "The Composition of Digital Twins for Systems-of-Systems: a Systematic Literature Review", "comment": "15 pages, 3 figures, Presented at the 23rd Overture workshop, June\n  2025 (arXiv:cs/2506.08680)", "summary": "Digital Twins (DTs) are increasingly used to model complex systems,\nespecially in Cyber-Physical Systems (CPS) and System-of-Systems (SoS), where\neffective integration is key. This systematic literature review investigates DT\ncomposition and verification and validation (V&V) methodologies. Analyzing 21\nstudies from 2022-2024, we examined composition mechanisms, SoS\ncharacteristics, and V&V formality, scope, and challenges. While composition is\ndiscussed, formalization is limited. V&V approaches vary, with semi-formal\nmethods and simulations dominating; formal verification is underutilized. Key\ntechnical challenges include model uncertainty and integration complexity.\nMethodological challenges highlight the lack of standardized DT-specific V&V\nframeworks. There is a need to move beyond model validation to address\nintegration and cyber-physical consistency. This review contributes a\nstructured classification of V&V approaches and emphasizes the need for\nstandardized, scalable V&V and rigorous composition methodologies for complex\nDT implementations."}
{"id": "2506.20444", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.20444", "abs": "https://arxiv.org/abs/2506.20444", "authors": ["Xiang Lan", "Tim Menzies", "Bowen Xu"], "title": "Smart Cuts: Enhance Active Learning for Vulnerability Detection by Pruning Bad Seeds", "comment": null, "summary": "Vulnerability detection is crucial for identifying security weaknesses in\nsoftware systems. However, the effectiveness of machine learning models in this\ndomain is often hindered by low-quality training datasets, which contain noisy,\nmislabeled, or imbalanced samples. This paper proposes a novel dataset\nmaps-empowered approach that systematically identifies and mitigates\nhard-to-learn outliers, referred to as \"bad seeds\", to improve model training\nefficiency. Our approach can categorize training examples based on learning\ndifficulty and integrate this information into an active learning framework.\nUnlike traditional methods that focus on uncertainty-based sampling, our\nstrategy prioritizes dataset quality by filtering out performance-harmful\nsamples while emphasizing informative ones. Our experimental results show that\nour approach can improve F1 score over random selection by 45.36% (DeepGini)\nand 45.91% (K-Means) and outperforms standard active learning by 61.46%\n(DeepGini) and 32.65% (K-Means) for CodeBERT on the Big-Vul dataset,\ndemonstrating the effectiveness of integrating dataset maps for optimizing\nsample selection in vulnerability detection. Furthermore, our approach also\nenhances model robustness, improves sample selection by filtering bad seeds,\nand stabilizes active learning performance across iterations. By analyzing the\ncharacteristics of these outliers, we provide insights for future improvements\nin dataset construction, making vulnerability detection more reliable and\ncost-effective."}
{"id": "2506.20551", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20551", "abs": "https://arxiv.org/abs/2506.20551", "authors": ["Soumya Madireddy", "Lu Gao", "Zia Din", "Kinam Kim", "Ahmed Senouci", "Zhe Han", "Yunpeng Zhang"], "title": "Large Language Model-Driven Code Compliance Checking in Building Information Modeling", "comment": null, "summary": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects."}
{"id": "2506.20558", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2506.20558", "abs": "https://arxiv.org/abs/2506.20558", "authors": ["Renyi Zhong", "Yintong Huo", "Wenwei Gu", "Jinxi Kuang", "Zhihan Jiang", "Guangba Yu", "Yichen Li", "David Lo", "Michael R. Lyu"], "title": "CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency", "comment": "This manuscript is under review", "summary": "Comments within code serve as a crucial foundation for software\ndocumentation, facilitating developers to communicate and understand the code\neffectively. However, code-comment inconsistency (CCI) can negatively affect\nsoftware development, testing, and maintenance. Recent efforts to mitigate this\nissue have emerged, but existing studies often suffer from inaccurate datasets\nand inadequate solutions, weakening their practical effectiveness. In this\nstudy, we first conduct a quantitative analysis of existing datasets, revealing\na substantial portion of sampled data are mislabeled. To address these data\nlimitations, we introduce CCIBench, a refined dataset comprising high-quality\ndata, to support the training and evaluation of method-level CCI methods.\nFurthermore, we present an innovative end-to-end LLM-based framework,\nCCISolver, designed to improve code quality by identifying and rectifying CCIs.\nComprehensive evaluations demonstrate CCISolver's superior performance. For\ndetection, it establishes a new state-of-the-art with an F1-score of 89.54%. In\nfixing task, it achieves a remarkable 18.84% relative improvement in GLEU score\nover the strongest baseline. This superiority is confirmed by human evaluation,\nwhere CCISolver's fixing success rate of 0.6533 significantly surpasses\nexisting methods. Critically, in a practical end-to-end setting, CCISolver's\ninnovative architecture is approximately 36% faster for inference than the\nbaseline model, underscoring its scalability and real-world applicability."}
{"id": "2506.20621", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20621", "abs": "https://arxiv.org/abs/2506.20621", "authors": ["Silvio Alonso", "Antonio Pedro Santos Alves", "Lucas Romao", "Hélio Lopes", "Marcos Kalinowski"], "title": "Define-ML: An Approach to Ideate Machine Learning-Enabled Systems", "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "[Context] The increasing adoption of machine learning (ML) in software\nsystems demands specialized ideation approaches that address ML-specific\nchallenges, including data dependencies, technical feasibility, and alignment\nbetween business objectives and probabilistic system behavior. Traditional\nideation methods like Lean Inception lack structured support for these ML\nconsiderations, which can result in misaligned product visions and unrealistic\nexpectations. [Goal] This paper presents Define-ML, a framework that extends\nLean Inception with tailored activities - Data Source Mapping, Feature-to-Data\nSource Mapping, and ML Mapping - to systematically integrate data and technical\nconstraints into early-stage ML product ideation. [Method] We developed and\nvalidated Define-ML following the Technology Transfer Model, conducting both\nstatic validation (with a toy problem) and dynamic validation (in a real-world\nindustrial case study). The analysis combined quantitative surveys with\nqualitative feedback, assessing utility, ease of use, and intent of adoption.\n[Results] Participants found Define-ML effective for clarifying data concerns,\naligning ML capabilities with business goals, and fostering cross-functional\ncollaboration. The approach's structured activities reduced ideation ambiguity,\nthough some noted a learning curve for ML-specific components, which can be\nmitigated by expert facilitation. All participants expressed the intention to\nadopt Define-ML. [Conclusion] Define-ML provides an openly available, validated\napproach for ML product ideation, building on Lean Inception's agility while\naligning features with available data and increasing awareness of technical\nfeasibility."}
