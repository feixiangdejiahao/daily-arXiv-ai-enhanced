{"id": "2511.09794", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09794", "abs": "https://arxiv.org/abs/2511.09794", "authors": ["Wasique Islam Shafin", "Md Nakhla Rafi", "Zhenhao Li", "Tse-Hsun Chen"], "title": "Evaluating Software Process Models for Multi-Agent Class-Level Code Generation", "comment": null, "summary": "Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\\% for GPT-4o-mini and -39.8\\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation.", "AI": {"tldr": "Imposing a Waterfall-style workflow among multiple LLMs leads to more maintainable code but often lowers correctness, with the nature of errors shifting. The Testing stage is most impactful. Claude-3.5-Haiku benefits, others do not. The underlying trade-off is between workflow discipline and reasoning flexibility.", "motivation": "Most previous work focuses on single-agent, function-level code generation with LLMs. The authors aim to understand how collaborative, multi-agent workflows and process structure (like Waterfall) impact the quality, maintainability, and correctness of generated code.", "method": "The paper simulates a multi-agent workflow for class-level code generation in Python using a Waterfall-style development cycle (Requirement, Design, Implementation, Testing) with three different LLMs: GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku. The study evaluates these workflows across 100 tasks from the ClassEval benchmark.", "result": "Multi-agent workflows based on Waterfall structure produce code that is cleaner and more maintainable but tend to reduce functional correctness for most models (notably, -37.8% for GPT-4o-mini, -39.8% for DeepSeek-Chat). Claude-3.5-Haiku is an exception, with a +9.5% improvement in correctness. Process constraints change error types: missing code issues decrease while semantic/validation errors increase. The Testing stage notably improves verification coverage but can cause new reasoning failures.", "conclusion": "Process structure fundamentally affects LLM collaboration and failure patterns in code generation. Rigid workflows like Waterfall can reorganize outcomes, promote maintainability, and alter error profiles, but may sacrifice functional correctness for some models. Careful consideration is needed when designing LLM-based software development pipelines, as there are inherent trade-offs between workflow rigidity and flexible, creative problem-solving."}}
{"id": "2511.09964", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09964", "abs": "https://arxiv.org/abs/2511.09964", "authors": ["Noah van der Vleuten", "Anthony Flores", "Shray Mathur", "Max Rakitin", "Thomas Hopkins", "Kevin G. Yager", "Esther H. R. Tsai"], "title": "EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines", "comment": null, "summary": "Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI.", "AI": {"tldr": "EnvTrace is a new simulation-based tool for testing and scoring control code generated by large language models in physical systems, demonstrating strong performance among top LLMs and paving the way for more autonomous AI applications.", "motivation": "Standard algorithmic benchmarks cannot adequately evaluate control code for physical systems due to their stateless nature, motivating the need for a trace-based simulation method to more accurately assess the capabilities of LLMs for instrument control.", "method": "The paper uses EnvTrace\u2014a simulation-based trace alignment technique\u2014on a digital twin for beamline control, evaluating over 30 LLMs on code generation for instrument control and scoring their functional correctness across several behavioral metrics.", "result": "The paper introduces EnvTrace, a simulation-based method for assessing the semantic equivalence of instrument control code generated by large language models (LLMs). EnvTrace evaluates execution traces rather than relying solely on stateless benchmarks, providing a richer assessment for control logic involving physical systems. Using a beamline digital twin, the authors test over 30 LLMs, scoring them on functional correctness across behavioral dimensions and showing that some models approach human-level code generation for control tasks.", "conclusion": "Many current LLMs can generate control code for physical systems at near human-level performance when evaluated using trace alignment in simulated environments, suggesting LLMs may play an agentic role alongside digital twins in future autonomous AI systems."}}
{"id": "2511.10049", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10049", "abs": "https://arxiv.org/abs/2511.10049", "authors": ["Divyanshu Saxena", "Rishikesh Maurya", "Xiaoxuan Ou", "Gagan Somashekar", "Shachee Mishra Gupta", "Arun Iyer", "Yu Kang", "Chetan Bansal", "Aditya Akella", "Saravan Rajmohan"], "title": "Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents", "comment": "5 pages", "summary": "The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements.", "AI": {"tldr": "Instead of relying on fixed benchmarks, the paper proposes evolving benchmarks using LLMs and enterprise documentation, aiming for better evaluation of AI agents in real-world, changing conditions.", "motivation": "Fixed benchmarks are inadequate for enterprise-scale AI agents due to continuous changes in requirements and infrequent ground-truth data. The authors aim to address this gap by enabling benchmarks to evolve with enterprise needs, improving the robustness of agent evaluation.", "method": "The process involves extracting high-level intents from semi-structured documents created by developers, and using large language models to automatically generate corresponding benchmarks. This is then tested in a service migration case study.", "result": "The paper introduces an approach for dynamic benchmark generation tailored to evaluating AI agents in enterprise environments where requirements and services are constantly evolving. The authors demonstrate their method using a case study of service migration at a large enterprise, leveraging semi-structured developer documents and LLMs to create benchmarks.", "conclusion": "The proposed method enables a maintainable and adaptable evaluation framework for AI agents. This leads to more accurate and actionable feedback for agent improvement in enterprise-scale scenarios."}}
{"id": "2511.10271", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10271", "abs": "https://arxiv.org/abs/2511.10271", "authors": ["Xin Sun", "Daniel St\u00e5hl", "Kristian Sandahl", "Christoph Kessler"], "title": "Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics", "comment": null, "summary": "In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.", "AI": {"tldr": "Most research on LLM code focuses only on whether code works, not on quality. Academics care about security and performance; industry cares about maintainability. LLM code quality varies a lot between models and goals. There's a mismatch between what research, industry, and LLMs actually deliver.", "motivation": "Although LLMs can generate functionally correct code, there is little understanding of their ability to meet industry-relevant, non-functional quality standards. Bridging the gap between academic research, industry needs, and LLM output is critical for reliable software engineering tools.", "method": "Three approaches: (1) systematic literature review of 108 papers; (2) industry workshops with engineers from various organizations; (3) empirical evaluation of real-world patches generated by three LLMs, assessing non-functional qualities like security, maintainability, and performance.", "result": "The paper presents a comprehensive evaluation of non-functional qualities in LLM-generated code. Researchers conducted a systematic review of 108 publications, ran industry workshops, and empirically evaluated code patches created by three LLMs. They focused on the qualities of security, maintainability, and performance efficiency. Their results reveal that security and performance are emphasized in academic work, while practitioners prioritize maintainability and readability. The empirical analysis shows improvements in one quality often degrade another, with models showing highly variable performance across non-functional requirements. The study also uncovers a clear gap between academic research, industry needs, and the actual capabilities of LLMs in code quality.", "conclusion": "The study concludes that there is an urgent need for systematic quality assurance in LLM-based code generation to ensure generated code is not only correct but also high quality, especially in the non-functional aspects prioritized by practitioners. Current research and model performance do not align with industry expectations."}}
{"id": "2511.09987", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09987", "abs": "https://arxiv.org/abs/2511.09987", "authors": ["Shiv Sundram", "Akhilesh Balasingam", "Nathan Zhang", "Kunle Olukotun", "Fredrik Kjolstad"], "title": "Cyclotron: Compilation of Recurrences to Distributed and Systolic Architectures", "comment": null, "summary": "We present Cyclotron, a framework and compiler for using recurrence equations to express streaming dataflow algorithms, which then get portably compiled to distributed topologies of interlinked processors. Our framework provides an input language of recurrences over logical tensors, which then gets lowered into an intermediate language of recurrences over logical iteration spaces, and finally into programs of send, receive, and computation operations specific to each individual processor. In Cyclotron's IR, programs are optimized such that external memory interactions are confined to the boundaries of the iteration space. Within inner iteration spaces, all data accesses become local: data accesses target values residing in local fast memory or on neighboring processing units, avoiding costly memory movement. We provide a scheduling language allowing users to define how data gets streamed and broadcasted between processors, enabling pipelined execution of computation kernels over distributed topologies of processing elements. We demonstrate the portability of our approach by compiling our IR to a reconfigurable simulator of systolic arrays and chiplet style distributed hardware, as well as to distributed-memory CPU clusters. In the simulated reconfigurable setting, we use our compiler for hardware design space exploration in which link costs and latencies can be specified. In the distributed CPU setting, we show how to use recurrences and our scheduling language to express various matrix multiplication routines (Cannon, SUMMA, PUMMA, weight stationary) and solvers (Triangular solve and Cholesky). For matrix multiplication and the triangular solve, we generate distributed implementations competitive with ScaLAPACK.", "AI": {"tldr": "Cyclotron is a framework and compiler that takes high-level recurrence equations for streaming dataflow algorithms and automatically compiles them into distributed processor topologies, optimizing for locality and minimizing costly memory interactions.", "motivation": "Distributed streaming dataflow algorithms are difficult to express and optimize, especially in regard to efficient data movement and portability across hardware. Cyclotron aims to simplify this by offering a unifying recurrence-based language and compilation pathway that automates these aspects.", "method": "The paper introduces Cyclotron, which translates recurrences over tensors into distributed computation. It employs an input language for recurrences, transforms it through intermediate representations, and finally outputs processor-specific operations, optimizing memory locality and providing a scheduling language for data movement. The compiled IR is demonstrated on both simulated reconfigurable hardware and real distributed CPU clusters.", "result": "Using Cyclotron, efficient distributed implementations of matrix multiplication and solver routines are generated. These are portable across simulated hardware and CPU clusters, and match or outperform existing libraries like ScaLAPACK under certain conditions.", "conclusion": "Cyclotron successfully generates distributed implementations of common matrix routines (like Cannon, SUMMA, PUMMA, etc.) competitive with existing solutions such as ScaLAPACK, across varying distributed hardware backends."}}
{"id": "2511.10323", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10323", "abs": "https://arxiv.org/abs/2511.10323", "authors": ["D\u00e1vid K\u00f3sz\u00f3", "Tam\u00e1s Aladics", "Rudolf Ferenc", "P\u00e9ter Heged\u0171s"], "title": "A Large-Scale Collection Of (Non-)Actionable Static Code Analysis Reports", "comment": "Under publication to Nature Scientific Data journal", "summary": "Static Code Analysis (SCA) tools, while invaluable for identifying potential coding problems, functional bugs, or vulnerabilities, often generate an overwhelming number of warnings, many of which are non-actionable. This overload of alerts leads to ``alert fatigue'', a phenomenon where developers become desensitized to warnings, potentially overlooking critical issues and ultimately hindering productivity and code quality. Analyzing these warnings and training machine learning models to identify and filter them requires substantial datasets, which are currently scarce, particularly for Java. This scarcity impedes efforts to improve the accuracy and usability of SCA tools and mitigate the effects of alert fatigue. In this paper, we address this gap by introducing a novel methodology for collecting and categorizing SCA warnings, effectively distinguishing actionable from non-actionable ones. We further leverage this methodology to generate a large-scale dataset of over 1 million entries of Java source code warnings, named NASCAR: (Non-)Actionable Static Code Analysis Reports. To facilitate follow-up research in this domain, we make both the dataset and the tools used to generate it publicly available.", "AI": {"tldr": "The paper introduces a new method and resulting large-scale dataset (NASCAR) of Java SCA warnings categorized by actionability, aiming to address alert fatigue and support future research through open tools and data.", "motivation": "Static Code Analysis (SCA) tools overwhelm developers with too many warnings, most of which are not actionable, causing alert fatigue and reducing productivity. There is also a lack of large, labeled datasets (especially for Java) necessary for improving machine learning-based filtering of these alerts.", "method": "The authors propose a novel methodology to collect and categorize SCA warnings, distinguishing actionable from non-actionable alerts. Using this approach, they systematically compile a large-scale dataset focused on Java warnings.", "result": "They create and publicly release the NASCAR dataset\u2014over 1 million Java SCA warnings, categorized as actionable or non-actionable, along with the tools used to produce it.", "conclusion": "By providing a large, well-categorized dataset and associated tools, this work lays the groundwork for research and improvements in filtering SCA warnings, addressing both usability concerns and alert fatigue."}}
{"id": "2511.10343", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.10343", "abs": "https://arxiv.org/abs/2511.10343", "authors": ["Alistair O'Brien", "Didier R\u00e9my", "Gabriel Scherer"], "title": "Omnidirectional type inference for ML: principality any way", "comment": "39 pages + appendices", "summary": "The Damas-Hindley-Milner (ML) type system owes its success to principality, the property that every well-typed expression has a unique most general type. This makes inference predictable and efficient. Unfortunately, many extensions of ML (GADTs, higher-rank polymorphism, and static overloading) endanger princpality by introducing _fragile_ constructs that resist principal inference. Existing approaches recover principality through directional inference algorithms, which propagate _known_ type information in a fixed (or static) order (e.g. as in bidirectional typing) to disambiguate such constructs. However, the rigidity of a static inference order often causes otherwise well-typed programs to be rejected.\n  We propose _omnidirectional_ type inference, where type information flows in a dynamic order. Typing constraints may be solved in any order, suspending when progress requires known type information and resuming once it becomes available, using _suspended match constraints_. This approach is straightforward for simply typed systems, but extending it to ML is challenging due to let-generalization. Existing ML inference algorithms type let-bindings (let x = e1 in e2) in a fixed order: type e1, generalize its type, and then type e2. To overcome this, we introduce _incremental instantiation_, allowing partially solved type schemes containing suspended constraints to be instantiated, with a mechanism to incrementally update instances as the scheme is refined.\n  Omnidirectionality provides a general framework for restoring principality in the presence of fragile features. We demonstrate its versatility on two fundamentally different features of OCaml: static overloading of record labels and datatype constructors and semi-explicit first-class polymorphism. In both cases, we obtain a principal type inference algorithm that is more expressive than OCaml's current typechecker.", "AI": {"tldr": "The paper introduces omnidirectional type inference to dynamically propagate type information in ML-extended languages, enabling principal types in the presence of complex features and accepting more programs than current approaches.", "motivation": "The motivation is to address the loss of principality in ML-type systems when extended with features like GADTs, higher-rank polymorphism, and static overloading, as current inference algorithms with static order often reject valid programs.", "method": "They propose omnidirectional type inference, where typing constraints are solved in dynamic order, using suspended match constraints and incremental instantiation to handle let-generalization and partially solved type schemes.", "result": "The result is a more expressive, principal type inference algorithm that restores principality with features like static overloading and first-class polymorphism, and is more flexible than OCaml's current typechecker.", "conclusion": "Omnidirectional type inference restores principality for ML extensions by allowing dynamic flow of type information and handling fragile features, enabling more programs to be accepted with principal types."}}
{"id": "2511.10326", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10326", "abs": "https://arxiv.org/abs/2511.10326", "authors": ["Shuangyu Lyu", "Chuan Luo", "Ruizhi Shi", "Wei Wu", "Chanjuan Liu", "Chunming Hu"], "title": "Towards Comprehensive Sampling of SMT Solutions", "comment": null, "summary": "This work focuses on effectively generating diverse solutions for satisfiability modulo theories (SMT) formulas, targeting the theories of bit-vectors, arrays, and uninterpreted functions, which is a critical task in software and hardware testing. Generating diverse SMT solutions helps uncover faults and detect safety violations during the verification and testing process, resulting in the SMT sampling problem, i.e., constructing a small number of solutions while achieving comprehensive coverage of the constraint space. While high coverage is crucial for exploring system behaviors, reducing the number of solutions is of great importance, as excessive solutions increase testing time and resource usage, undermining efficiency. In this work, we introduce PanSampler, a novel SMT sampler that achieves high coverage with a small number of solutions. It incorporates three novel techniques, i.e., diversity-aware SMT algorithm, abstract syntax tree (AST)-guided scoring function and post-sampling optimization technology, enhancing its practical performance. It iteratively samples solutions, evaluates candidates, and employs local search to refine solutions, ensuring high coverage with a small number of samples. Extensive experiments on practical benchmarks demonstrate that PanSampler exhibits a significantly stronger capability to reach high target coverage, while requiring fewer solutions than current samplers to achieve the same coverage level. Furthermore, our empirical evaluation on practical subjects, which are collected from real-world software systems, shows that PanSampler achieves higher fault detection capability and reduces the number of required test cases from 32.6\\% to 76.4\\% to reach the same fault detection effectiveness, leading to a substantial improvement in testing efficiency. PanSampler advances SMT sampling, reducing the cost of software testing and hardware verification.", "AI": {"tldr": "PanSampler is a new SMT solution sampler that uses diversity-aware algorithms and local search to ensure comprehensive coverage with fewer test cases, resulting in more efficient software/hardware testing and verification.", "motivation": "Effective generation of diverse SMT solutions is important for software/hardware testing to uncover faults and ensure safety. Achieving high coverage is necessary, but having too many solutions increases testing time and resource consumption, reducing efficiency.", "method": "PanSampler leverages three techniques: diversity-aware SMT solving, AST-guided scoring, and post-sampling optimization. It samples iteratively, scores candidates on coverage, and uses local search to refine results, maximizing effectiveness with minimal redundancy.", "result": "PanSampler, the proposed tool, obtains high coverage with fewer solutions compared to current state-of-the-art samplers. Experiments show PanSampler can reach high fault detection rates while reducing the number of required test cases by 32.6% to 76.4%.", "conclusion": "PanSampler offers substantial improvements in SMT sampling, fault detection, and testing efficiency, lowering testing costs while maintaining high-quality verification."}}
{"id": "2511.10361", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.10361", "abs": "https://arxiv.org/abs/2511.10361", "authors": ["Rodrigo Mesquita", "Bernardo Toninho"], "title": "Lazy Linearity for a Core Functional Language", "comment": "Extended version of POPL 2026 paper", "summary": "Traditionally, in linearly typed languages, consuming a linear resource is synonymous with its syntactic occurrence in the program. However, under the lens of non-strict evaluation, linearity can be further understood semantically, where a syntactic occurrence of a resource does not necessarily entail using that resource when the program is executed. While this distinction has been largely unexplored, it turns out to be inescapable in Haskell's optimising compiler, which heavily rewrites the source program in ways that break syntactic linearity but preserve the program's semantics. We introduce Linear Core, a novel system which accepts the lazy semantics of linearity statically and is suitable for lazy languages such as the Core intermediate language of the Glasgow Haskell Compiler. We prove that Linear Core is sound, guaranteeing linear resource usage, and that multiple optimising transformations preserve linearity in Linear Core while failing to do so in Core. We have implemented Linear Core as a compiler plugin to validate the system against linearity-heavy libraries, including linear-base.", "AI": {"tldr": "Linear Core is a new system that guarantees linear resource usage in lazy languages like Haskell, even after compiler optimizations, addressing limitations of previous approaches.", "motivation": "In linearly typed languages, resource consumption is tracked at the syntactic level, but in non-strict (lazy) languages, the actual use of resources depends on runtime evaluation. Existing systems do not correctly enforce linear resource usage in the presence of compiler optimizations that break syntactic linearity. There is a need for a system that can guarantee linearity under lazy evaluation and compiler transformations.", "method": "The approach involves designing and formalizing the Linear Core system for lazy linearity, proving soundness and preservation of linearity under compiler optimizations, and practically validating the system through a compiler plugin tested with linearity-heavy libraries.", "result": "Linear Core was proposed, providing a static semantics for linearity suitable for lazy languages like Haskell's Core intermediate language. The authors proved that Linear Core is sound with respect to linear resource usage and demonstrated that various compiler optimizations preserve linearity in Linear Core but may fail in the original Core system. A compiler plugin was implemented to validate these results against real-world libraries.", "conclusion": "Linear Core bridges the gap between semantic and syntactic linearity in lazy evaluation contexts, ensuring sound linear resource usage despite program transformations by the compiler. Its validation shows practical applicability to linearity-heavy libraries."}}
{"id": "2511.10374", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.10374", "abs": "https://arxiv.org/abs/2511.10374", "authors": ["Somashekaracharya G Bhaskaracharya", "Aravind Acharya", "Bastian Hagedorn", "Vinod Grover"], "title": "Modeling Layout Abstractions Using Integer Set Relations", "comment": null, "summary": "Modern deep learning compilers rely on layout abstractions to manage the complex mapping between logical tensor structures and physical memory arrangements. CuTe layouts and Triton linear layouts are widely adopted industry standards. However, these layout systems operate independently with distinct mathematical underpinnings, preventing unified formal analysis and cross-system reasoning. We bridge this gap by introducing a novel approach that leverages the Integer Set Library (ISL) to create a unified mathematical representation for both layout systems through integer set relations, thereby enabling rigorous formal analysis, correctness verification, and the foundation for future cross-system optimization strategies. Our approach models CuTe layouts through integer set relations that encode the transformation from multi-dimensional coordinates to linear indices using stride-based calculations, including sophisticated swizzle operations that perform bit-level manipulations for enhanced memory access patterns. For Triton linear layouts, we construct integer set relations that model the binary vector space transformations where arithmetic operations follow finite field F_2 rules. We implement a complete suite of layout manipulation algorithms for composition, inversion, complement using built-in operations in ISL to ensure mathematical correctness and preserve layout semantics. Experimental evaluation shows that the system handles the full spectrum of layout complexity, from elementary identity transformations to sophisticated multi-dimensional tensor arrangements with complex stride configurations and swizzle patterns, validating the mathematical modeling approach across different layout paradigms.", "AI": {"tldr": "The paper presents a unified mathematical framework using ISL for analyzing and manipulating both CuTe and Triton deep learning compiler layouts. This enables formal verification and cross-system optimization, handling a broad range of real-world layout complexities.", "motivation": "Existing layout abstractions in deep learning compilers, such as CuTe and Triton, use fundamentally different mathematical frameworks. This prevents unified analysis and cross-system optimization, making it hard to reason formally or transform layouts between systems.", "method": "The authors leverage the Integer Set Library (ISL) to create a unified mathematical representation of both CuTe and Triton layouts through integer set relations. This approach models the different underlying mathematical operations (stride-based for CuTe, finite field arithmetic for Triton) and implements layout manipulation algorithms\u2014like composition and inversion\u2014using ISL's built-in operations.", "result": "The system successfully models layout transformations ranging from simple identity mappings to complex multidimensional tensor arrangements with advanced stride and swizzle operations. Experimental results demonstrate that the unified approach can handle all tested layout complexities, verifying correctness and the potential for future cross-system optimization.", "conclusion": "A unified ISL-based mathematical framework enables rigorous formal analysis and manipulation of industry-standard tensor layouts (CuTe and Triton), validating the approach across diverse layout complexities and laying the groundwork for future optimizations between systems."}}
{"id": "2511.10565", "categories": ["cs.PL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10565", "abs": "https://arxiv.org/abs/2511.10565", "authors": ["Rahul Krishnan", "Ashley Samuelson", "Emily Yao", "Ethan Cecchetti"], "title": "zkStruDul: Programming zkSNARKs with Structural Duality", "comment": null, "summary": "Non-Interactive Zero Knowledge (NIZK) proofs, such as zkSNARKS, let one prove knowledge of private data without revealing it or interacting with a verifier. While existing tooling focuses on specifying the predicate to be proven, real-world applications optimize predicate definitions to minimize proof generation overhead, but must correspondingly transform predicate inputs. Implementing these two steps separately duplicates logic that must precisely match to avoid catastrophic security flaws. We address this shortcoming with zkStruDul, a language that unifies input transformations and predicate definitions into a single combined abstraction from which a compiler can project both procedures, eliminating duplicate code and problematic mismatches. zkStruDul provides a high-level abstraction to layer on top of existing NIZK technology and supports important features like recursive proofs. We provide a source-level semantics and prove its behavior is identical to the projected semantics, allowing straightforward standard reasoning.", "AI": {"tldr": "zkStruDul merges input transformation and predicate specification in NIZK proofs, preventing security flaws from mismatched logic and redundant coding. Its compiler guarantees correct behavior and supports powerful features like recursion.", "motivation": "Current workflows for NIZK proof systems split input transformations and predicate definitions, requiring manual duplication of logic and risking security flaws if mismatched. The motivation is to simplify this process, reduce errors and overhead, and allow safe and efficient optimization in real-world applications.", "method": "The paper introduces zkStruDul, a domain-specific language that combines the specification of predicate definitions and input transformations. The language is equipped with source-level semantics and a compiler that projects both procedures from the combined abstraction. Formal proof is provided that projected semantics matches the source-level semantics.", "result": "zkStruDul provides a high-level abstraction over existing NIZK backends, supports advanced features like recursive proofs, and is proven formally to maintain behavioral consistency between source and projected code.", "conclusion": "zkStruDul successfully unifies input transformation and predicate specification for NIZK proofs, ensuring consistency and eliminating duplicate logic, which enhances security and reliability."}}
