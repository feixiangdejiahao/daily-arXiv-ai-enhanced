{"id": "2510.18895", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.18895", "abs": "https://arxiv.org/abs/2510.18895", "authors": ["Santhosh Kumar Ravindran"], "title": "CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation", "comment": "12 pages", "summary": "We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL)\narchitecture that integrates affective signals to enhance code generation in\nlarge language models (LLMs). Motivated by human and animal learning where\nembarrassment from mistakes drives rapid correction, as observed in training a\npuppy to avoid repeating errors after a single scolding CosmoCore tags code\ngeneration trajectories with valence and surprise using a lightweight\nmulti-layer perceptron (MLP). High-negative valence (cringe) episodes, such as\nbuggy code outputs, are prioritized in a Dream Queue for five-fold replay\nduring off-policy updates, while low-surprise successes are pruned to prevent\noverconfidence and buffer bloat. Evaluated on code generation benchmarks like\nHumanEval and BigCodeBench, alongside simulations with a custom data pipeline\nenvironment, CosmoCore reduces hallucinated code (e.g., syntax errors or\nlogical bugs) by 48\\% and accelerates self-correction by 45\\%. Local\nexperiments using Hugging Face models in a PySpark environment validate these\ngains, with code snippets provided for replication. Ablations confirm valence\ntagging boosts curiosity in exploration, and pruning mitigates inefficiency.\nThis framework extends RL from human feedback (RLHF) for more emotionally aware\ncode assistants, with applications in IDEs and data pipelines. Code and the\ncustom mini-world simulation are released.", "AI": {"tldr": "CosmoCore introduces emotionally-inspired RL for LLM code generation, drastically reducing buggy code and improving self-correction speed. Its combination of valence/surprise tagging and prioritized replay enhances learning, validated both in benchmarks and practical environments. The approach points to emotionally tuned code assistants for broader applications.", "motivation": "Inspired by human and animal learning, especially how negative emotional feedback (embarrassment) leads to rapid error correction. The aim is to improve code generation in LLMs by using neuroscience concepts to enhance RL methods.", "method": "CosmoCore uses a neuroscience-inspired RL architecture. It tags code outputs with valence (emotional value) and surprise via a lightweight MLP. 'Cringe' (high-negative valence) mistakes are replayed more often, while low-surprise successes are pruned, supporting faster learning and avoiding overconfidence. Benchmarks and custom simulations validate performance.", "result": "CosmoCore achieves a 48% reduction in hallucinated code (errors and bugs) and accelerates self-correction by 45%. Local testing in PySpark environments using Hugging Face models supports these claims. Ablation studies show that valence tagging encourages exploration, while pruning improves efficiency.", "conclusion": "CosmoCore extends RLHF frameworks with emotional awareness for code assistants, offering improvements for IDEs and data pipeline applications. Code and simulation tools are released for reproducibility."}}
{"id": "2510.18923", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18923", "abs": "https://arxiv.org/abs/2510.18923", "authors": ["Eduard Frankford", "Tobias Antensteiner", "Michael Vierhauser", "Clemens Sauerwein", "Vivien Wallner", "Iris Groher", "Reinhold Pl\u00f6sch", "Ruth Breu"], "title": "A Survey on Feedback Types in Automated Programming Assessment Systems", "comment": null, "summary": "With the recent rapid increase in digitization across all major industries,\nacquiring programming skills has increased the demand for introductory\nprogramming courses. This has further resulted in universities integrating\nprogramming courses into a wide range of curricula, including not only\ntechnical studies but also business and management fields of study.\n  Consequently, additional resources are needed for teaching, grading, and\ntutoring students with diverse educational backgrounds and skills. As part of\nthis, Automated Programming Assessment Systems (APASs) have emerged, providing\nscalable and high-quality assessment systems with efficient evaluation and\ninstant feedback. Commonly, APASs heavily rely on predefined unit tests for\ngenerating feedback, often limiting the scope and level of detail of feedback\nthat can be provided to students. With the rise of Large Language Models (LLMs)\nin recent years, new opportunities have emerged as these technologies can\nenhance feedback quality and personalization.\n  To investigate how different feedback mechanisms in APASs are perceived by\nstudents, and how effective they are in supporting problem-solving, we have\nconducted a large-scale study with over 200 students from two different\nuniversities. Specifically, we compare baseline Compiler Feedback, standard\nUnit Test Feedback, and advanced LLM-based Feedback regarding perceived quality\nand impact on student performance.\n  Results indicate that while students rate unit test feedback as the most\nhelpful, AI-generated feedback leads to significantly better performances.\nThese findings suggest combining unit tests and AI-driven guidance to optimize\nautomated feedback mechanisms and improve learning outcomes in programming\neducation.", "AI": {"tldr": "The study evaluates how feedback types in automated programming assessment systems affect student experience and performance. While unit test feedback is rated as most helpful by students, AI-driven feedback substantially improves their results. Integrating both approaches is recommended for better programming education.", "motivation": "The increased digitization in industries has led to a surge in demand for programming education, requiring more effective resources for teaching and assessing diverse student groups. Traditional automated assessment systems mainly use unit tests, which can limit feedback quality, but advancements in Large Language Models (LLMs) present new potential for enhancing feedback.", "method": "A large-scale study was conducted involving over 200 students from two universities. The study compared three different feedback mechanisms in Automated Programming Assessment Systems: compiler feedback, standard unit test feedback, and LLM-based feedback. The assessment focused on students' perceptions of feedback quality and its impact on their performance.", "result": "Students found unit test feedback to be the most helpful. However, those who received AI-generated (LLM-based) feedback showed significantly better performance in programming tasks.", "conclusion": "Combining unit test feedback with AI-driven guidance can optimize automated feedback in programming education, resulting in improved student learning outcomes."}}
{"id": "2510.19035", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19035", "abs": "https://arxiv.org/abs/2510.19035", "authors": ["Amirreza Hosseini", "Amro M. Farid"], "title": "Extending Resource Constrained Project Scheduling to Mega-Projects with Model-Based Systems Engineering & Hetero-functional Graph Theory", "comment": null, "summary": "Within the project management context, project scheduling serves as an\nindispensable component, functioning as a fundamental tool for planning,\nmonitoring, controlling, and managing projects more broadly. Although the\nresource-constrained project scheduling problem (RCPSP) lies at the core of\nproject management activities, it remains largely disconnected from the broader\nliterature on model-based systems engineering (MBSE), thereby limiting its\nintegration into the design and management of complex systems. The original\ncontribution of this paper is twofold. First, the paper seeks to reconcile the\nRCPSP with the broader literature and vocabulary of model-based systems\nengineering and hetero-functional graph theory (HFGT). A concrete translation\npipeline from an activity-on-node network to a SysML activity diagram, and then\nto an operand net is constructed. Using this representation, it specializes the\nhetero-functional network minimum-cost flow (HFNMCF) formulation to the RCPSP\ncontext as a systematic means of HFGT for quantitative analysis and proves that\nthe RCPSP is recoverable as a special case of a broader model. Secondly, on an\nillustrative instance with renewable and non-renewable operands, the\nspecialized HFNMCF, while producing similar schedules, yields explicit\nexplanations of the project states that enable richer monitoring and control.\nOverall, the framework preserves the strengths of the classical RCPSP while\naccommodating real-world constraints and enterprise-level decision processes\nencountered in large, complex megaprojects.", "AI": {"tldr": "This paper integrates project scheduling (RCPSP) with model-based systems engineering using hetero-functional graph theory, providing a new framework that preserves classic strengths, enhances monitoring, and supports complex project decisions.", "motivation": "Project scheduling is vital in project management, but the resource-constrained project scheduling problem (RCPSP) is not well-integrated with model-based systems engineering (MBSE). This gap hinders effective design and management of complex systems.", "method": "The paper offers a translation pipeline from an activity-on-node network to SysML activity diagrams and then to an operand net. It applies hetero-functional graph theory (HFGT) and the hetero-functional network minimum-cost flow (HFNMCF) formulation to the RCPSP context for systematic analysis.", "result": "The proposed method specializes HFNMCF to RCPSP, demonstrating that RCPSP can be recovered as a special case of a broader model. Through an example involving renewable and non-renewable operands, the framework provides schedules similar to traditional RCPSP but with more explicit project state explanations for improved monitoring and control.", "conclusion": "The framework maintains classical RCPSP strengths while adding flexibility for real-world constraints and enhanced decision-making in large, complex projects, bridging the gap between RCPSP and MBSE."}}
{"id": "2510.19089", "categories": ["cs.SE", "D.2.4; K.6.3"], "pdf": "https://arxiv.org/pdf/2510.19089", "abs": "https://arxiv.org/abs/2510.19089", "authors": ["\u00c9rik Martin-Dorel"], "title": "Docker-based CI/CD for Rocq/OCaml projects", "comment": "26 pages, 17 figures, 3 tables, 16 references", "summary": "This paper presents three closely-related software projects, namely:\ndocker-coq, docker-coq-action, and docker-keeper. It aims at two objectives:\nprovide a high-level description of the available features -- to foster the use\nof a Docker-based CI/CD for Rocq (formerly known as Coq) or OCaml projects --\nand document the underlying requirements and the main design choices of these\nthree DevOps tools -- to help their future maintainers.", "AI": {"tldr": "Three DevOps tools for Docker-based CI/CD in Rocq (Coq)/OCaml projects are described, detailing features and design choices to boost adoption and simplify maintenance.", "motivation": "There is a need for efficient and maintainable CI/CD workflows for Rocq (Coq) and OCaml projects, leveraging Docker-based solutions.", "method": "The paper provides a high-level overview of features and underlying requirements/design choices of three DevOps tools: docker-coq, docker-coq-action, and docker-keeper.", "result": "The publication describes available features, requirements, and design choices to encourage adoption and facilitate future maintenance of the tools.", "conclusion": "The documentation and explanation provided aim to foster broader use of Docker-based CI/CD in Rocq and OCaml projects, and support the tools' maintainers."}}
{"id": "2510.19129", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19129", "abs": "https://arxiv.org/abs/2510.19129", "authors": ["Qiancheng Fu", "Hongwei Xi", "Ankush Das"], "title": "Dependent Session Types for Verified Concurrent Programming", "comment": null, "summary": "We present TLLC which extends the Two-Level Linear dependent type theory\n(TLL) with session-based concurrency. Equipped with Martin-L\\\"{o}f style\ndependency, the session types of TLLC allow protocols to specify properties of\ncommunicated messages. When used in conjunction with the dependent type\nmachinery already present in TLL, dependent session types facilitate a form of\nrelational verification by relating concurrent programs with their idealized\nsequential counterparts. Correctness properties proven for sequential programs\ncan be easily lifted to their corresponding concurrent implementations. TLLC\nmakes session types a powerful tool for intrinsically verifying the correctness\nof data structures such as queues and concurrent algorithms such as map-reduce.\nTo extend TLL with session types, we develop a novel formulation of\nintuitionistic session type which we believe to be widely applicable for\nintegrating session types into other type systems beyond the context of TLLC.\nWe study the meta-theory of our language, proving its soundness as both a term\ncalculus and a process calculus. To demonstrate the practicality of TLLC, we\nhave implemented a prototype compiler that translates TLLC programs into\nconcurrent C code, which has been extensively evaluated.", "AI": {"tldr": "TLLC builds on dependent types to directly support and verify concurrent programs via session types, is sound in theory, and is practical with a compiling implementation.", "motivation": "Current dependent type theories like TLL lack built-in support for session-based concurrency, and verifying correctness of concurrent programs remains challenging.", "method": "The authors extend the Two-Level Linear dependent type theory (TLL) to introduce TLLC, which incorporates session-based concurrency using Martin-L\u00f6f style dependency. They introduce novel intuitionistic session types and develop the theory, proofs of soundness as term and process calculi, and implement a compiler from TLLC to concurrent C code.", "result": "TLLC enables relational verification between sequential and concurrent programs, making it possible to transfer correctness properties from sequential proofs to concurrent settings. The approach is shown to be practical with a working prototype compiler and extensive evaluation.", "conclusion": "TLLC extends dependent types to support session-based concurrency, making session types an intrinsic tool for verifying both data structures and concurrent algorithms, and provides a foundation for integrating session types into other systems."}}
{"id": "2510.19237", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19237", "abs": "https://arxiv.org/abs/2510.19237", "authors": ["Dongming Jin", "Zhi Jin", "Xiaohong Chen", "Zheng Fang", "Linyu Li", "Shengxin Zhao", "Chuihui Wang", "Hongbin Xiao"], "title": "Automated Concern Extraction from Textual Requirements of Cyber-Physical Systems: A Multi-solution Study", "comment": "27 pages, 3 figures", "summary": "Cyber-physical systems (CPSs) are characterized by a deep integration of the\ninformation space and the physical world, which makes the extraction of\nrequirements concerns more challenging. Some automated solutions for\nrequirements concern extraction have been proposed to alleviate the burden on\nrequirements engineers. However, evaluating the effectiveness of these\nsolutions, which relies on fair and comprehensive benchmarks, remains an open\nquestion. To address this gap, we propose ReqEBench, a new CPSs requirements\nconcern extraction benchmark, which contains 2,721 requirements from 12\nreal-world CPSs. ReqEBench offers four advantages. It aligns with real-world\nCPSs requirements in multiple dimensions, e.g., scale and complexity. It covers\ncomprehensive concerns related to CPSs requirements. It undergoes a rigorous\nannotation process. It covers multiple application domains of CPSs, e.g.,\naerospace and healthcare. We conducted a comparative study on three types of\nautomated requirements concern extraction solutions and revealed their\nperformance in real-world CPSs using our ReqEBench. We found that the highest\nF1 score of GPT-4 is only 0.24 in entity concern extraction. We further analyze\nfailure cases of popular LLM-based solutions, summarize their shortcomings, and\nprovide ideas for improving their capabilities. We believe ReqEBench will\nfacilitate the evaluation and development of automated requirements concern\nextraction.", "AI": {"tldr": "The paper introduces ReqEBench, a thorough benchmark for evaluating automated requirements extraction in cyber-physical systems, showing current solutions (including GPT-4) perform poorly and highlighting areas for improvement.", "motivation": "The extraction of requirements concerns in cyber-physical systems (CPSs) is challenging due to the integration of information and physical domains. While automated solutions exist, their evaluation has been hampered by the lack of fair and comprehensive benchmarks.", "method": "The authors developed ReqEBench, a benchmark dataset containing 2,721 requirements from 12 real-world CPSs, covering multiple domains and undergoing rigorous annotation.", "result": "Comparative studies using ReqEBench demonstrated the limited performance of current automated extraction solutions, with GPT-4 achieving only a 0.24 F1 score in entity concern extraction. Shortcomings of existing approaches were analyzed and ideas for improvement proposed.", "conclusion": "ReqEBench addresses a key evaluation gap, providing a comprehensive resource for benchmarking and guiding the advancement of automated requirements extraction solutions in CPSs."}}
{"id": "2510.19281", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19281", "abs": "https://arxiv.org/abs/2510.19281", "authors": ["Shubham Joshi"], "title": "An Empirical Study of Bitwise Operators Intuitiveness through Performance Metrics", "comment": "15 pages, 10 tables, 9 Figures", "summary": "Objectives: This study aims to investigate the readability and\nunderstandability of bitwise operators in programming, with the main hypothesis\nthat there will be a difference in the performance metrics (response time and\nerror rate) between participants exposed to various bitwise operators related\nquestions and those who are not.\n  Participants: Participants in this human research study include people\nwithout programming background, novice programmers, and university students\nwith varying programming experience (from freshmen to PhD level). There were 23\nparticipants for this study.\n  Study Methods: This study uses an Within-Subjects Experimental Design to\nassess how people with diverse programming backgrounds understand and use\nbitwise operators. Participants complete tasks in JavaScript program, and their\ntask completion time and accuracy of the tasks are recorded for analysis.\n  Findings: The results indicate that operators can be one of the factors\npredicting response time, with a small but significant effect, with R-squared\n0.032, (1, 494) = 16.5, p < .001. Additionally, some operators like OR, NOT,\nand Left Shift showed statistical significance in task completion times\ncompared to other operators.\n  Conclusions: While the complexity of bitwise operators did not generally\nresult in longer task completion times, certain operators were found to be less\nintuitive, suggesting the need for further investigation and potential redesign\nfor improved understandability.", "AI": {"tldr": "The paper examines how people of differing programming experience understand bitwise operators. Using experimental tasks, it finds that some operators slow down users and are less intuitive, pointing to areas for improvement in programming language design.", "motivation": "The study seeks to understand how easily people with varying programming backgrounds comprehend and use bitwise operators in coding, given the hypothesis that performance changes with exposure to different bitwise operators.", "method": "A Within-Subjects Experimental Design was utilized. 23 participants from diverse programming backgrounds were given JavaScript programming tasks involving bitwise operators. Researchers measured task completion time and accuracy.", "result": "Operators were found to influence response time, with a small but statistically significant effect (R-squared 0.032, p < .001). Specific operators (OR, NOT, Left Shift) proved less intuitive, resulting in longer task completion times.", "conclusion": "While most bitwise operators did not significantly increase task completion time, some (like OR, NOT, and Left Shift) were less intuitive. This highlights the need for further research and possible redesign to enhance operator understandability."}}
{"id": "2510.19240", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19240", "abs": "https://arxiv.org/abs/2510.19240", "authors": ["Behnam Agahi", "Hamed Farbeh"], "title": "A General Solution for the Implementation of CI/CD in Embedded Linux Development", "comment": null, "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.", "AI": {"tldr": "An automated Yocto-based workflow for embedded Linux development was built and tested; it proved reproducible, scalable, and efficient, speeding up builds and enabling CI/CD, with potential for real-time Linux apps and further industrial use.", "motivation": "Current embedded systems require efficient and automated development and deployment processes for customized Linux OS, particularly to simplify integration and scalability.", "method": "The authors designed and implemented a three-layer architecture using Yocto Project, including main repositories, a custom layer, and a manifest layer for synchronization and reproducibility. CI/CD pipelines with GitLab CI, Docker environments, and local caching were utilized.", "result": "Three sample projects were successfully developed and integrated. Automated workflows reduced build times, and multiple boot tests in QEMU verified system stability and functionality. The infrastructure proved reproducible and scalable.", "conclusion": "The proposed infrastructure is effective for automated, reproducible, and scalable Linux OS development in embedded systems. It can be extended to advanced use cases such as real-time Linux continuous deployment."}}
{"id": "2510.19254", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19254", "abs": "https://arxiv.org/abs/2510.19254", "authors": ["Chong Chen", "Jiachi Chen", "Lingfeng Bao", "David Lo", "Yanlin Wang", "Zhenyu Shan", "Ting Chen", "Guangqiang Yin", "Jianxing Yu", "Zibin Zheng"], "title": "Trace: Securing Smart Contract Repository Against Access Control Vulnerability", "comment": null, "summary": "Smart contract vulnerabilities, particularly improper Access Control that\nallows unauthorized execution of restricted functions, have caused billions of\ndollars in losses. GitHub hosts numerous smart contract repositories containing\nsource code, documentation, and configuration files-these serve as intermediate\ndevelopment artifacts that must be compiled and packaged before deployment.\nThird-party developers often reference, reuse, or fork code from these\nrepositories during custom development. However, if the referenced code\ncontains vulnerabilities, it can introduce significant security risks. Existing\ntools for detecting smart contract vulnerabilities are limited in their ability\nto handle complex repositories, as they typically require the target contract\nto be compilable to generate an abstract representation for further analysis.\nThis paper presents TRACE, a tool designed to secure non-compilable smart\ncontract repositories against access control vulnerabilities. TRACE employs\nLLMs to locate sensitive functions involving critical operations (e.g.,\ntransfer) within the contract and subsequently completes function snippets into\na fully compilable contract. TRACE constructs a function call graph from the\nabstract syntax tree (AST) of the completed contract. It uses the control flow\ngraph (CFG) of each function as node information. The nodes of the sensitive\nfunctions are then analyzed to detect Access Control vulnerabilities.\nExperimental results demonstrate that TRACE outperforms state-of-the-art tools\non an open-sourced CVE dataset, detecting 14 out of 15 CVEs. In addition, it\nachieves 89.2% precision on 5,000 recent on-chain contracts, far exceeding the\nbest existing tool at 76.9%. On 83 real-world repositories, TRACE achieves\n87.0% precision, significantly surpassing DeepSeek-R1's 14.3%.", "AI": {"tldr": "TRACE is a new tool that uses LLMs to analyze non-compilable smart contract repositories and detect access control vulnerabilities with much higher precision than existing tools. It can identify critical flaws in source code\u2014before contracts are deployed\u2014making smart contract development on platforms like GitHub safer.", "motivation": "Smart contract vulnerabilities, especially those related to improper access control, have caused massive financial losses. Many developers reuse or reference code from GitHub repositories during smart contract development, which may introduce security risks if the source code contains vulnerabilities. Existing analysis tools are limited since they require the target contracts to be compilable for analysis, leaving non-compilable repositories less secure.", "method": "TRACE is a tool that secures non-compilable smart contract repositories against Access Control vulnerabilities. It leverages Large Language Models (LLMs) to identify sensitive functions in contract code and autocompletes incomplete functions into a compilable contract. TRACE then builds a function call graph using the abstract syntax tree (AST) and analyzes the control flow graph (CFG) of each function to detect potential access control vulnerabilities.", "result": "Experimental evaluation shows TRACE detects 14 out of 15 CVEs in an open-source dataset, achieves 89.2% precision on 5,000 recent on-chain contracts (better than the previous best of 76.9%), and records 87.0% precision on 83 real-world repositories, significantly outperforming DeepSeek-R1's 14.3%.", "conclusion": "TRACE effectively addresses the limitations of existing smart contract vulnerability detection tools, offering high precision and performance on access control vulnerabilities even for non-compilable repositories, thereby improving smart contract security."}}
{"id": "2510.19274", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19274", "abs": "https://arxiv.org/abs/2510.19274", "authors": ["Saurabh Chauhan", "Zeeshan Rasheed", "Malik Abdul Sami", "Kai-Kristian Kemell", "Muhammad Waseem", "Zheying Zhang", "Jussi Rasku", "Mika Saari", "Pekka Abrahamsson"], "title": "From Specification to Service: Accelerating API-First Development Using Multi-Agent Systems", "comment": "9 Figures, 6Tables", "summary": "This paper presents a system that uses Large Language Models (LLMs)-based\nagents to automate the API-first development of RESTful microservices. This\nsystem helps to create an OpenAPI specification, generate server code from it,\nand refine the code through a feedback loop that analyzes execution logs and\nerror messages. The integration of log analysis enables the LLM to detect and\naddress issues efficiently, reducing the number of iterations required to\nproduce functional and robust services. This study's main goal is to advance\nAPI-first development automation for RESTful web services and test the\ncapability of LLM-based multi-agent systems in supporting the API-first\ndevelopment approach. To test the proposed system's potential, we utilized the\nPRAB benchmark. The results indicate that if we keep the OpenAPI specification\nsmall and focused, LLMs are capable of generating complete functional code with\nbusiness logic that aligns to the specification. The code for the system is\npublicly available at https://github.com/sirbh/code-gen", "AI": {"tldr": "This paper introduces a system using LLM agents to automate RESTful microservice development from OpenAPI specs. By incorporating log analysis and iterative refinement, the system efficiently generates robust code that matches the specification, especially for smaller APIs. Results show significant automation potential for API-first workflows.", "motivation": "Traditional API-first development of RESTful web services is often labor-intensive, requiring manual creation of specifications, code generation, and iterative debugging. There is a need for automated solutions that can streamline this process and improve productivity.", "method": "The paper presents a system based on LLM-powered agents that automate API-first microservice development. The system creates OpenAPI specs, generates server code, and automatically refines the code via a feedback loop that uses execution logs and error messages. The approach was tested using the PRAB benchmark.", "result": "When the OpenAPI specification remains small and focused, the system enables LLMs to generate complete functional code with appropriate business logic that aligns with the specification, with fewer development iterations required.", "conclusion": "LLM-based multi-agent systems can successfully automate much of the API-first development process for RESTful microservices, especially when specifications are well-defined and focused."}}
{"id": "2510.19393", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19393", "abs": "https://arxiv.org/abs/2510.19393", "authors": ["Stefan Schott", "Serena Elisa Ponta", "Wolfram Fischer", "Jonas Klauke", "Eric Bodden"], "title": "Bytecode-centric Detection of Known-to-be-vulnerable Dependencies in Java Projects", "comment": "To be published in: ICSE 2026 Proceedings", "summary": "On average, 71% of the code in typical Java projects comes from open-source\nsoftware (OSS) dependencies, making OSS dependencies the dominant component of\nmodern software code bases. This high degree of OSS reliance comes with a\nconsiderable security risk of adding known security vulnerabilities to a code\nbase. To remedy this risk, researchers and companies have developed various\ndependency scanners, which try to identify inclusions of known-to-be-vulnerable\nOSS dependencies. However, there are still challenges that modern dependency\nscanners do not overcome, especially when it comes to dependency modifications,\nsuch as re-compilations, re-bundlings or re-packagings, which are common in the\nJava ecosystem. To overcome these challenges, we present Jaralyzer, a\nbytecode-centric dependency scanner for Java. Jaralyzer does not rely on the\nmetadata or the source code of the included OSS dependencies being available\nbut directly analyzes a dependency's bytecode. Our evaluation across 56 popular\nOSS components demonstrates that Jaralyzer outperforms other popular dependency\nscanners in detecting vulnerabilities within modified dependencies. It is the\nonly scanner capable of identifying vulnerabilities across all the above\nmentioned types of modifications. But even when applied to unmodified\ndependencies, Jaralyzer outperforms the current state-of-the-art code-centric\nscanner Eclipse Steady by detecting 28 more true vulnerabilities and yielding\n29 fewer false warnings.", "AI": {"tldr": "Java projects mainly consist of OSS dependencies, posing security risks. Jaralyzer is a new bytecode-based scanner that surpasses existing tools in finding vulnerabilities, especially in modified dependencies, and reduces false alarms.", "motivation": "Modern Java projects rely heavily on open-source software (OSS) dependencies, which account for 71% of their code base. This high dependency poses significant security risks due to the potential inclusion of vulnerable OSS components. Existing dependency scanners struggle to detect vulnerabilities in modified dependencies, such as those that are re-compiled, re-bundled, or re-packaged, which are common in the Java ecosystem.", "method": "The authors introduce Jaralyzer, a bytecode-centric dependency scanner for Java. Unlike traditional scanners that depend on metadata or source code, Jaralyzer operates directly on bytecode, allowing it to analyze dependencies even when source or metadata is unavailable or altered.", "result": "Jaralyzer outperformed other popular dependency scanners, especially in identifying vulnerabilities in modified dependencies. It is the only tool capable of detecting vulnerabilities across all studied forms of dependency modification. In tests with unmodified dependencies, Jaralyzer detected 28 more true vulnerabilities and generated 29 fewer false positives compared to the leading code-centric scanner, Eclipse Steady.", "conclusion": "Jaralyzer represents a significant advancement in Java dependency scanning by using bytecode analysis. It effectively detects vulnerabilities in both modified and unmodified OSS dependencies, addressing key shortcomings of existing scanners and improving codebase security."}}
{"id": "2510.19438", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19438", "abs": "https://arxiv.org/abs/2510.19438", "authors": ["Linfeng Liang", "Chenkai Tan", "Yao Deng", "Yingfeng Cai", "T. Y Chen", "Xi Zheng"], "title": "AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of Autonomous Driving Systems", "comment": null, "summary": "Autonomous Driving Systems (ADS) are safety-critical, where failures can be\nsevere. While Metamorphic Testing (MT) is effective for fault detection in ADS,\nexisting methods rely heavily on manual effort and lack automation. We present\nAutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that\nautomates the extraction of Metamorphic Relations (MRs) from local traffic\nrules and the generation of valid follow-up test cases. AutoMT leverages LLMs\nto extract MRs from traffic rules in Gherkin syntax using a predefined\nontology. A vision-language agent analyzes scenarios, and a search agent\nretrieves suitable MRs from a RAG-based database to generate follow-up cases\nvia computer vision. Experiments show that AutoMT achieves up to 5 x higher\ntest diversity in follow-up case generation compared to the best baseline\n(manual expert-defined MRs) in terms of validation rate, and detects up to\n20.55% more behavioral violations. While manual MT relies on a fixed set of\npredefined rules, AutoMT automatically extracts diverse metamorphic relations\nthat augment real-world datasets and help uncover corner cases often missed\nduring in-field testing and data collection. Its modular architecture\nseparating MR extraction, filtering, and test generation supports integration\ninto industrial pipelines and potentially enables simulation-based testing to\nsystematically cover underrepresented or safety-critical scenarios.", "AI": {"tldr": "AutoMT is an automated multi-agent testing framework for autonomous driving systems that leverages Large Language Models to extract traffic rule-based test scenarios, generating far more diverse and effective follow-up cases than traditional manual approaches. It improves fault detection and coverage of critical scenarios, making it suitable for industrial adoption.", "motivation": "Autonomous Driving Systems (ADS) are safety-critical, and failures can have severe consequences. Current Metamorphic Testing (MT) approaches for ADS are effective but rely heavily on manual effort and lack automation, limiting scalability and efficiency.", "method": "The paper introduces AutoMT, a multi-agent Metamorphic Testing framework driven by Large Language Models (LLMs). AutoMT automates the extraction of Metamorphic Relations (MRs) from local traffic rules using a predefined ontology. It uses a vision-language agent to analyze scenarios and a search agent to retrieve suitable MRs from a RAG-based database, enabling the automated generation of follow-up test cases via computer vision.", "result": "AutoMT achieves up to 5x higher test diversity in follow-up case generation compared to the best baseline (manual expert-defined MRs) in terms of validation rate. It also detects up to 20.55% more behavioral violations. AutoMT automatically extracts diverse MRs, augmenting real-world datasets and helping uncover corner cases often missed during traditional testing and data collection.", "conclusion": "AutoMT significantly automates and improves Metamorphic Testing for ADS by leveraging LLMs, providing higher test diversity, better fault detection, and facilitating coverage of safety-critical scenarios with minimal manual intervention. Its modular design supports industrial integration and enhances simulation-based testing."}}
{"id": "2510.19460", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19460", "abs": "https://arxiv.org/abs/2510.19460", "authors": ["Thomas I. Strasser", "Edmund Widl", "Carlos Ayon Mac Gregor", "Mirko Ginocchi", "Rene Kuchenbuch"], "title": "Mapping and Evolving Interoperability Testing in European Energy Systems: The int:net Perspective", "comment": "2025 IEEE PES Innovative Smart Grid Technologies Conference Europe\n  (ISGT Europe)", "summary": "The ongoing transformation of the European energy landscape, driven by the\nintegration of renewable energy sources, digital technologies, and\ndecentralized systems, requires a high degree of interoperability across\ndiverse components and systems. Ensuring that these elements can exchange\ninformation and operate together reliably is essential for achieving a secure,\nflexible, and efficient energy supply infrastructure. While several initiatives\nhave contributed to the development of smart grid testing infrastructures, they\ndo not provide a dedicated or comprehensive focus on interoperability testing.\nA structured and harmonized overview of interoperability testing capabilities\nacross Europe is therefore still missing. This work therefore presents a novel\ncontribution by analyzing the European interoperability testing facility\nlandscape through a structured survey of 30 facilities. It provides a\ncategorized inventory of testing infrastructures, applied methodologies, and\nreference test cases, and introduces a blueprint for the development of future\ntesting environments. The findings contribute to the establishment of a\ncoordinated European ecosystem for interoperability testing, supporting\ncollaboration, innovation, and alignment with the goals of the energy\ntransition.", "AI": {"tldr": "This paper surveys 30 European interoperability testing facilities in the energy sector, categorizes their capabilities, and proposes a blueprint for future testing environments, thus supporting better collaboration and innovation in Europe\u2019s energy transition.", "motivation": "The integration of renewable energy sources, digital technologies, and decentralized systems in Europe's energy landscape requires high interoperability among systems to ensure a reliable, flexible, and efficient energy supply. However, there is currently a lack of a dedicated, structured overview of interoperability testing facilities across Europe.", "method": "The authors conducted a structured survey of 30 interoperability testing facilities across Europe. They categorized these facilities, their methodologies, and referenced test cases, and developed a blueprint for future testing environments.", "result": "The survey provided a categorized inventory of European interoperability testing infrastructures, methodologies, and test cases. A blueprint for future testing environments was also presented.", "conclusion": "The study fills an existing gap by offering a harmonized overview of interoperability testing facilities in Europe, thereby supporting the creation of a coordinated ecosystem for interoperability testing. This facilitates collaboration, innovation, and the advancement of Europe\u2019s energy transition goals."}}
{"id": "2510.19593", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19593", "abs": "https://arxiv.org/abs/2510.19593", "authors": ["Aoyang Fang", "Haowen Yang", "Haoze Dong", "Qisheng Lu", "Junjielong Xu", "Pinjia He"], "title": "A Goal-Driven Survey on Root Cause Analysis", "comment": null, "summary": "Root Cause Analysis (RCA) is a crucial aspect of incident management in\nlarge-scale cloud services. While the term root cause analysis or RCA has been\nwidely used, different studies formulate the task differently. This is because\nthe term \"RCA\" implicitly covers tasks with distinct underlying goals. For\ninstance, the goal of localizing a faulty service for rapid triage is\nfundamentally different from identifying a specific functional bug for a\ndefinitive fix. However, previous surveys have largely overlooked these\ngoal-based distinctions, conventionally categorizing papers by input data types\n(e.g., metric-based vs. trace-based methods). This leads to the grouping of\nworks with disparate objectives, thereby obscuring the true progress and gaps\nin the field. Meanwhile, the typical audience of an RCA survey is either laymen\nwho want to know the goals and big picture of the task or RCA researchers who\nwant to figure out past research under the same task formulation. Thus, an RCA\nsurvey that organizes the related papers according to their goals is in high\ndemand. To this end, this paper presents a goal-driven framework that\neffectively categorizes and integrates 135 papers on RCA in the context of\ncloud incident management based on their diverse goals, spanning the period\nfrom 2014 to 2025. In addition to the goal-driven categorization, it discusses\nthe ultimate goal of all RCA papers as an umbrella covering different RCA\nformulations. Moreover, the paper discusses open challenges and future\ndirections in RCA.", "AI": {"tldr": "This paper introduces a novel, goal-based framework for surveying Root Cause Analysis research in cloud incident management, categorizing 135 papers by their specific objectives instead of their input data types. This approach reveals clearer research gaps, aids different audiences, and discusses future challenges in the field.", "motivation": "Traditional surveys on Root Cause Analysis (RCA) in cloud services have primarily categorized research by input data types, overlooking distinctions in underlying goals. This results in grouped works with fundamentally different objectives, obscuring actual progress and gaps. There is a strong demand for an RCA survey organized by research goals, aiding both laymen seeking an overview and researchers seeking studies with similar purposes.", "method": "The paper proposes a goal-driven framework that categorizes and integrates 135 RCA-related papers based on their specific goals. This method acknowledges the diversity of objectives in RCA research, ranging from rapid faulty service localization to definitive bug identification.", "result": "The framework successfully classifies RCA research by goals rather than data types and reveals the true progress and gaps in the field. It also defines the ultimate goal that encompasses various RCA formulations and highlights open challenges and future research directions.", "conclusion": "A goal-driven categorization of RCA research provides a clearer understanding of the field, helping both novices and experts. This approach is more meaningful than traditional input-type-based surveys and identifies areas for further investigation."}}
{"id": "2510.19600", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19600", "abs": "https://arxiv.org/abs/2510.19600", "authors": ["Qianli Ma", "Siyu Wang", "Yilin Chen", "Yinhao Tang", "Yixiang Yang", "Chang Guo", "Bingjie Gao", "Zhening Xing", "Yanan Sun", "Zhipeng Zhang"], "title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1", "comment": null, "summary": "In the quest for scientific progress, communicating research is as vital as\nthe discovery itself. Yet, researchers are often sidetracked by the manual,\nrepetitive chore of building project webpages to make their dense papers\naccessible. While automation has tackled static slides and posters, the\ndynamic, interactive nature of webpages has remained an unaddressed challenge.\nTo bridge this gap, we reframe the problem, arguing that the solution lies not\nin a single command, but in a collaborative, hierarchical process. We introduce\n$\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.\nAutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline\nfrom narrative planning to multimodal content generation and interactive\nrendering. To combat AI hallucination, dedicated \"Checker\" agents verify each\nstep against the source paper, while optional human checkpoints ensure the\nfinal product aligns perfectly with the author's vision, transforming the\nsystem from a mere tool into a powerful collaborative assistant. To rigorously\nvalidate our approach, we also construct $\\textbf{PageBench}$, the first\nbenchmark for this new task. Experiments show AutoPage not only generates\nhigh-quality, visually appealing pages but does so with remarkable efficiency\nin under 15 minutes for less than \\$0.1. Code and dataset will be released at\n$\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.", "AI": {"tldr": "AutoPage is an automated, multi-agent system that quickly and reliably turns research papers into interactive project webpages, verified by agents and optional human review. It reduces manual workload, boosts efficiency, and achieves high page quality, as validated by their new PageBench benchmark.", "motivation": "Creating project webpages for research papers is essential for accessibility and dissemination but is a repetitive, manual task that is not addressed well by automation, especially for dynamic and interactive content.", "method": "Introduces AutoPage, a multi-agent hierarchical system that breaks down webpage generation into steps from planning, content generation, to rendering. It uses \"Checker\" agents for verification against the source paper and allows optional human reviews. The team also developed PageBench, a benchmark for this new task.", "result": "AutoPage efficiently generates high-quality, visually appealing interactive project webpages from papers in under 15 minutes and at a low cost ($<0.1), as validated by experiments using the PageBench benchmark.", "conclusion": "AutoPage transforms project webpage creation from a tedious manual task into a streamlined collaborative process, outperforming prior approaches by providing efficiency, quality, and reliability through multi-agent collaboration and verification."}}
{"id": "2510.19615", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.19615", "abs": "https://arxiv.org/abs/2510.19615", "authors": ["Zhiping Zhou", "Xiaohong Li", "Ruitao Feng", "Yao Zhang", "Yuekang Li", "Wenbu Feng", "Yunqian Wang", "Yuqing Li"], "title": "FidelityGPT: Correcting Decompilation Distortions with Retrieval Augmented Generation", "comment": null, "summary": "Decompilation converts machine code into human-readable form, enabling\nanalysis and debugging without source code. However, fidelity issues often\ndegrade the readability and semantic accuracy of decompiled output. Existing\nmethods, such as variable renaming or structural simplification, provide\npartial improvements but lack robust detection and correction, particularly for\ncomplex closed-source binaries. We present FidelityGPT, a framework that\nenhances decompiled code accuracy and readability by systematically detecting\nand correcting semantic distortions. FidelityGPT introduces distortion-aware\nprompt templates tailored to closed-source settings and integrates\nRetrieval-Augmented Generation (RAG) with a dynamic semantic intensity\nalgorithm to locate distorted lines and retrieve semantically similar code from\na database. A variable dependency algorithm further mitigates long-context\nlimitations by analyzing redundant variables and integrating their dependencies\ninto the prompt context. Evaluated on 620 function pairs from a binary\nsimilarity benchmark, FidelityGPT achieved an average detection accuracy of 89%\nand a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,\nCorrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating\nsignificant gains in accuracy and readability. These results highlight its\npotential to advance LLM-based decompilation and reverse engineering.", "AI": {"tldr": "FidelityGPT improves the accuracy and readability of decompiled code using advanced prompt engineering, context handling, and semantic correction, outperforming previous methods and showing promise for reverse engineering tasks.", "motivation": "Decompilation is essential for analyzing and debugging software without source code, but existing approaches struggle with semantic fidelity and readability, especially for closed-source binaries.", "method": "FidelityGPT uses distortion-aware prompt templates, combines Retrieval-Augmented Generation (RAG), and introduces a dynamic semantic intensity algorithm to detect and correct distorted code lines. It also employs a variable dependency algorithm to address long-context issues and improve prompt relevance.", "result": "In tests on 620 function pairs, FidelityGPT achieved 89% detection accuracy and 83% precision. It markedly outperformed DeGPT with a 94% Fix Rate and 64% Corrected Fix Rate, improving both accuracy and readability of decompiled code.", "conclusion": "FidelityGPT significantly advances the fidelity and readability of LLM-based decompilation, offering robust semantic correction for challenging closed-source binaries."}}
{"id": "2510.19692", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.19692", "abs": "https://arxiv.org/abs/2510.19692", "authors": ["Rashina Hoda"], "title": "Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary", "comment": "5 pages", "summary": "Agentic AI is poised to usher in a seismic paradigm shift in Software\nEngineering (SE). As technologists rush head-along to make agentic AI a\nreality, SE researchers are driven to establish agentic SE as a research area.\nWhile early visions of agentic SE are primarily focused on code-related\nactivities, early empirical evidence calls for a consideration of a range of\nsocio-technical concerns to make it work in practice. This paper contributes to\nthe emerging community vision by: (a) recommending an expansion of its scope\nbeyond code, toward a 'whole of process' vision, grounding it in SE foundations\nand evolution and emerging agentic SE frameworks, (b) proposing a preliminary\nset of values and principles to guide efforts, and (c) sharing guidance on\ndesigning/using well-defined vocabulary for agentic SE. It is hoped that these\nideas will encourage community collaborations and steer the SE community\ntowards laying strong foundations of agentic SE so its not only inevitable but\nalso deliberate and desirable in the long run.", "AI": {"tldr": "The paper highlights the need to expand agentic AI's role in software engineering from code-centric to a process-wide perspective, proposes guiding principles, and recommends clear vocabulary, aiming to foster a thoughtful, collaborative approach for future developments in agentic SE.", "motivation": "The motivation is to address the rise of agentic AI in software engineering and ensure that its integration is thoughtful and encompasses the broader socio-technical aspects beyond just coding.", "method": "The paper recommends expanding the focus of agentic SE beyond code, anchors proposals in foundational SE concepts, proposes values and principles, and provides guidance on establishing a clear vocabulary for the field.", "result": "The result is the introduction of a broader framework for agentic SE, values and principles for its development, and vocabulary guidelines to help standardize the discourse.", "conclusion": "The paper concludes by advocating for deliberate and collaborative development of agentic SE, enabling the SE community to build strong foundations for agentic AI integration in a desirable and sustainable way."}}
{"id": "2510.19747", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19747", "abs": "https://arxiv.org/abs/2510.19747", "authors": ["Priyaranjan Pattnayak", "Hussain Bohra"], "title": "Review of Tools for Zero-Code LLM Based Application Development", "comment": "Accepted in 6th World Conference on Artificial Intelligence: Advances\n  and Applications (WCAIAA 2025)", "summary": "Large Language Models (LLMs) are transforming software creation by enabling\nzero code development platforms. Our survey reviews recent platforms that let\nusers build applications without writing code, by leveraging LLMs as the brains\nof the development process. We adopt a broad survey methodology, categorizing\nplatforms based on key dimensions such as interface style, backend integration,\noutput type, and extensibility. We analyze both dedicated LLM based app\nbuilders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and\ngeneral no code platforms (e.g., Bubble, Glide) that integrate LLM\ncapabilities. We present a taxonomy categorizing these platforms by their\ninterface (conversational, visual, etc.), supported LLM backends, output type\n(chatbot, full application, workflow), and degree of extensibility. Core\nfeatures such as autonomous agents, memory management, workflow orchestration,\nand API integrations are in scope of the survey. We provide a detailed\ncomparison, highlighting each platform's strengths and limitations. Trade offs\n(customizability, scalability, vendor lock-in) are discussed in comparison with\ntraditional and low code development approaches. Finally, we outline future\ndirections, including multimodal interfaces, on device LLMs, and improved\norchestration for democratizing app creation with AI. Our findings indicate\nthat while zero code LLM platforms greatly reduce the barrier to creating AI\npowered applications, they still face challenges in flexibility and\nreliability. Overall, the landscape is rapidly evolving, offering exciting\nopportunities to empower non programmers to create sophisticated software.", "AI": {"tldr": "Survey of LLM-driven zero code platforms shows they make app-building accessible, but challenges remain in flexibility and reliability; taxonomy and comparative analysis included, plus future directions suggested.", "motivation": "To understand how LLM-powered platforms are transforming software development, making it accessible to users without coding experience, and to document the current landscape, strengths, limitations, and future directions.", "method": "Broad survey methodology, including categorization and detailed comparison, was used to analyze various platforms\u2019 characteristics (interface, backend, output type, extensibility, features).", "result": "A taxonomy and comparison of platforms reveal diverse approaches, strengths, and trade-offs, with future opportunities identified in multimodal interfaces and improved orchestration. Key features and comparison with traditional development approaches are highlighted.", "conclusion": "Zero code platforms powered by LLMs can significantly lower the barrier for application development, especially for non-programmers; however, there are challenges regarding flexibility and reliability."}}
{"id": "2510.19777", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19777", "abs": "https://arxiv.org/abs/2510.19777", "authors": ["S M Sadrul Islam Asif", "James Chen", "Earl T. Barr", "Mark Marron"], "title": "BOSQTGEN: Breaking the Sound Barrier in Test Generation", "comment": null, "summary": "Modern software is increasingly built by composing APIs, elevating the API\ncontract to a critical role. Inadequate contracts, however, lead to mismatched\nexpectations and failures, creating a pressing need for robust conformance\ntesting. Current test generation techniques are hindered by key challenges:\npolyglot systems, source code inaccessibility, a cost-reliability trade-off,\nand, most critically, the difficulty of generating structured inputs.\n  We introduce BOSQTGEN, a novel black-box methodology and tool for API test\ngeneration. BOSQTGEN utilizes a novel approach for decomposing API\nspecifications into primitives, using LLMs to suggest coherent strata for them,\nand employing combinatorial testing to efficiently sample over these values.\nThis approach ensures coverage of critical interactions while avoiding the\nredundancy of random sampling.\n  The resulting BOSQTGEN system achieves an average of 82% code coverage on\nRESTful benchmarks, often a 20% or more increase over prior state-of-the-art\nsystems and nearing parity with hand-written test suites. Providing a fully\nAPI-driven approach to test generation, enables developers to automatically\ncreate high-quality test cases for validation or test-driven development.", "AI": {"tldr": "BOSQTGEN is a new black-box API test generation tool leveraging LLMs and combinatorial sampling. It delivers higher coverage and quality compared to prior approaches, facilitating automated and reliable API testing without needing source access.", "motivation": "Modern software relies heavily on APIs, and failures due to inadequate API contracts are common. Existing test generation methods struggle with issues like handling multiple programming languages, inaccessible source code, and generating valid structured inputs, making robust conformance testing difficult.", "method": "The authors present BOSQTGEN, a black-box API test generation tool. BOSQTGEN decomposes API specifications into basic components, uses Large Language Models (LLMs) to organize these components into logical strata, and applies combinatorial testing to smartly sample input values, maximizing coverage and reducing redundant tests.", "result": "BOSQTGEN achieves an average of 82% code coverage on RESTful API benchmarks, demonstrating a 20% or greater improvement over previous state-of-the-art systems and approaching the results of hand-written test suites.", "conclusion": "BOSQTGEN provides an efficient, fully API-driven mechanism for automatically generating robust API test cases, aiding in both validation and test-driven development, and improving software reliability."}}
