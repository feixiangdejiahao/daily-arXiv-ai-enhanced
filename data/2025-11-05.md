<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Detecting Vulnerabilities from Issue Reports for Internet-of-Things](https://arxiv.org/abs/2511.01941)
*Sogol Masoumzadeh*

Main category: cs.SE

TL;DR: This paper explores, for the first time, the automatic detection of IoT vulnerability-related issue reports using ML, LLMs, and NLP methods. SVM with BERT-derived features performed best (AUC 0.65), indicating initial promise, though accuracy is still limited.


<details>
  <summary>Details</summary>
Motivation: Timely discovery of software vulnerabilities through issue reports is critical for security, but IoT systems lag behind non-IoT systems in analysis capabilities. Existing ML and LLM approaches are mainly applied to non-IoT contexts, leaving a gap in IoT vulnerability detection from issue reports.

Method: The paper proposes two approaches: (1) integrating traditional Machine Learning (ML), Large Language Models (LLMs), and NLP techniques to detect vulnerability-related issues across 21 Eclipse IoT projects; (2) fine-tuning a pre-trained BERT Masked Language Model on 11,000 GitHub issues for vulnerability classification, and experimenting with classifiers such as Support Vector Machine (SVM) using BERT-extracted NLP features.

Result: The best result was obtained using an SVM trained on BERT-derived features, with an AUC of 0.65. The fine-tuned BERT model achieved an accuracy of 0.26, highlighting the need for comprehensive data during model training.

Conclusion: This is the first study to systematically apply and evaluate ML, LLMs, and NLP-based approaches for detecting vulnerability-related issues in IoT projects, bridging the gap with non-IoT vulnerability detection and laying groundwork for further advancements.

Abstract: Timely identification of issue reports reflecting software vulnerabilities is
crucial, particularly for Internet-of-Things (IoT) where analysis is slower
than non-IoT systems. While Machine Learning (ML) and Large Language Models
(LLMs) detect vulnerability-indicating issues in non-IoT systems, their IoT use
remains unexplored. We are the first to tackle this problem by proposing two
approaches: (1) combining ML and LLMs with Natural Language Processing (NLP)
techniques to detect vulnerability-indicating issues of 21 Eclipse IoT projects
and (2) fine-tuning a pre-trained BERT Masked Language Model (MLM) on 11,000
GitHub issues for classifying \vul. Our best performance belongs to a Support
Vector Machine (SVM) trained on BERT NLP features, achieving an Area Under the
receiver operator characteristic Curve (AUC) of 0.65. The fine-tuned BERT
achieves 0.26 accuracy, emphasizing the importance of exposing all data during
training. Our contributions set the stage for accurately detecting IoT
vulnerabilities from issue reports, similar to non-IoT systems.

</details>


### [2] [Metamorphic Testing of Large Language Models for Natural Language Processing](https://arxiv.org/abs/2511.02108)
*Steven Cho,Stefano Ruberto,Valerio Terragni*

Main category: cs.SE

TL;DR: This paper conducts the largest study to date of metamorphic testing (MT) for LLMs, compiling 191 metamorphic relations and experimentally testing 36 on three LLMs with over half a million test cases. The study demonstrates MT’s capacity and limitations for automatic fault detection in LLMs without relying on labeled data.


<details>
  <summary>Details</summary>
Motivation: There is a growing use of large language models (LLMs) for NLP tasks, but these models sometimes produce incorrect results. Identifying such errors is important, and current barriers include the lack of labeled datasets (oracles) for determining correctness.

Method: The paper uses metamorphic testing (MT), a technique that detects errors without explicit oracles by utilizing metamorphic relations (MRs) across outputs of related inputs. A comprehensive literature review identified 191 MRs, and 36 representative MRs were implemented to test three popular LLMs in about 560,000 experiments.

Result: The experiments reveal the strengths, weaknesses, and limitations of applying MT to LLMs, providing new insights into the reliability and fault detection capabilities of these models when evaluated using MT.

Conclusion: Metamorphic testing is a promising approach for evaluating LLMs without labeled datasets, enabling the systematic identification of faulty behaviors. The comprehensive study advances understanding of MT’s potentials and limitations for pragmatic LLM evaluation.

Abstract: Using large language models (LLMs) to perform natural language processing
(NLP) tasks has become increasingly pervasive in recent times. The versatile
nature of LLMs makes them applicable to a wide range of such tasks. While the
performance of recent LLMs is generally outstanding, several studies have shown
that they can often produce incorrect results. Automatically identifying these
faulty behaviors is extremely useful for improving the effectiveness of LLMs.
One obstacle to this is the limited availability of labeled datasets, which
necessitates an oracle to determine the correctness of LLM behaviors.
Metamorphic testing (MT) is a popular testing approach that alleviates this
oracle problem. At the core of MT are metamorphic relations (MRs), which define
relationships between the outputs of related inputs. MT can expose faulty
behaviors without the need for explicit oracles (e.g., labeled datasets). This
paper presents the most comprehensive study of MT for LLMs to date. We
conducted a literature review and collected 191 MRs for NLP tasks. We
implemented a representative subset (36 MRs) to conduct a series of experiments
with three popular LLMs, running approximately 560,000 metamorphic tests. The
results shed light on the capabilities and opportunities of MT for LLMs, as
well as its limitations.

</details>


### [3] [Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs](https://arxiv.org/abs/2511.02197)
*Shufan Wang,Xing Hu,Junkai Chen,Zhiyuan Pan,Xin Xia*

Main category: cs.SE

TL;DR: This paper analyzes and improves how confident large language models are in code-reasoning tasks, finding that a hybrid strategy significantly enhances model reliability, though challenges remain for tough tasks. The work guides future use of confidence metrics in AI-driven software development.


<details>
  <summary>Details</summary>
Motivation: With LLMs being widely used in code intelligence, there is growing concern about the reliability and controllability of their outputs, especially in code reasoning tasks. Assessing and improving the confidence of these models is crucial for their use in software engineering.

Method: The paper introduces a confidence analysis and enhancement framework for LLMs on code reasoning tasks. It empirically evaluates the confidence reliability of leading LLMs by testing them on different tasks, and investigates the effect of prompt strategies and mathematical calibrations (like Platt Scaling) on confidence reliability.

Result: DeepSeek-Reasoner model outperforms others in key metrics (ECE, Brier Score, Performance Score), and a hybrid approach combining reassessment prompt strategy with Platt Scaling yields significant improvements across models. The results also reveal that reasoning-capable models have better confidence reliability, but there is still room for improvement, especially for complex reasoning tasks. Factors like task complexity, model size, and strategy substantially affect confidence reliability.

Conclusion: The study establishes a solid foundation and provides technical guidance for integrating confidence estimation in LLM-driven software engineering. It identifies the hybrid strategy as most effective in boosting confidence reliability, and emphasizes continuing efforts to optimize confidence mechanisms for deployment in complex engineering scenarios.

Abstract: With the widespread application of large language models (LLMs) in the field
of code intelligence, increasing attention has been paid to the reliability and
controllability of their outputs in code reasoning tasks. Confidence estimation
serves as an effective and convenient approach for evaluating these aspects.
This paper proposes a confidence analysis and enhancement framework for LLMs
tailored to code reasoning tasks. We conduct a comprehensive empirical study on
the confidence reliability of mainstream LLMs across different tasks, and
further evaluate the effectiveness of techniques such as prompt strategy
optimisation and mathematical calibration (e.g., Platt Scaling) in improving
confidence reliability. Our results show that DeepSeek-Reasoner achieves the
best performance across various tasks, outperforming other models by up to
$0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance
Score, respectively. The hybrid strategy combining the reassess prompt strategy
and Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$
over the original performance in the aforementioned three metrics. These
results indicate that models with reasoning capabilities demonstrate superior
confidence reliability, and that the hybrid strategy is the most effective in
enhancing the confidence reliability of various models. Meanwhile, we elucidate
the impact of different task complexities, model scales, and strategies on
confidence performance, and highlight that the confidence of current LLMs in
complex reasoning tasks still has considerable room for improvement. This study
not only provides a research foundation and technical reference for the
application of confidence in LLM-assisted software engineering, but also points
the way for future optimisation and engineering deployment of confidence
mechanisms.

</details>


### [4] [LLMs as Judges: Toward The Automatic Review of GSN-compliant Assurance Cases](https://arxiv.org/abs/2511.02203)
*Gerhard Yu,Mithila Sivakumar,Alvine B. Belle,Soude Ghari,Song Wang,Timothy C. Lethbridge*

Main category: cs.SE

TL;DR: This paper introduces a method to automate assurance case reviews for safety-critical systems through custom prompts for large language models, showing strong performance, especially by DeepSeek-R1, but still requires human refinement for best results.


<details>
  <summary>Details</summary>
Motivation: Assurance cases for mission-critical systems like autonomous driving and avionics are essential for verifying non-functional requirements (safety, security, reliability), but their extensive and complex nature makes review processes error-prone and inefficient. There is a need for (semi-)automated solutions to improve the efficiency and reliability of these reviews.

Method: The paper proposes a novel approach termed 'LLM-as-a-judge', which uses predicate-based rules reflecting assurance case review criteria to craft prompts for large language models (LLMs). Experiments are conducted with several advanced LLMs, including GPT-4o, GPT-4.1, DeepSeek-R1, and Gemini 2.0 Flash, to evaluate their performance in automating assurance case reviews.

Result: DeepSeek-R1 and GPT-4.1 exhibit the best performance in automated review tasks, with DeepSeek-R1 outperforming GPT-4.1. Overall, most tested LLMs show competent review abilities. However, human reviewers are still necessary for final refinement of the LLM-generated reviews.

Conclusion: Automating assurance case reviews with the LLM-as-a-judge paradigm using well-formulated, rule-based prompts improves the review process's efficiency and accuracy. Despite advances, human oversight remains crucial to ensure highest quality.

Abstract: Assurance cases allow verifying the correct implementation of certain
non-functional requirements of mission-critical systems, including their
safety, security, and reliability. They can be used in the specification of
autonomous driving, avionics, air traffic control, and similar systems. They
aim to reduce risks of harm of all kinds including human mortality,
environmental damage, and financial loss. However, assurance cases often tend
to be organized as extensive documents spanning hundreds of pages, making their
creation, review, and maintenance error-prone, time-consuming, and tedious.
Therefore, there is a growing need to leverage (semi-)automated techniques,
such as those powered by generative AI and large language models (LLMs), to
enhance efficiency, consistency, and accuracy across the entire assurance-case
lifecycle. In this paper, we focus on assurance case review, a critical task
that ensures the quality of assurance cases and therefore fosters their
acceptance by regulatory authorities. We propose a novel approach that
leverages the \textit{LLM-as-a-judge} paradigm to automate the review process.
Specifically, we propose new predicate-based rules that formalize
well-established assurance case review criteria, allowing us to craft LLM
prompts tailored to the review task. Our experiments on several
state-of-the-art LLMs (GPT-4o, GPT-4.1, DeepSeek-R1, and Gemini 2.0 Flash) show
that, while most LLMs yield relatively good review capabilities, DeepSeek-R1
and GPT-4.1 demonstrate superior performance, with DeepSeek-R1 ultimately
outperforming GPT-4.1. However, our experimental results also suggest that
human reviewers are still needed to refine the reviews LLMs yield.

</details>


### [5] [SWE-Sharp-Bench: A Reproducible Benchmark for C# Software Engineering Tasks](https://arxiv.org/abs/2511.02352)
*Sanket Mhatre,Yasharth Bajpai,Sumit Gulwani,Emerson Murphy-Hill,Gustavo Soares*

Main category: cs.SE

TL;DR: This paper introduces SWE-Sharp-Bench, a new open-source benchmark for C# programming tasks to evaluate AI coding agents. The results show that current AI models perform significantly worse on C# tasks than on Python, revealing a gap in capabilities across languages and promoting further research in this area.


<details>
  <summary>Details</summary>
Motivation: C# is a widely used enterprise language and ranks high in popularity, but is not represented in current AI coding benchmarks, which primarily focus on languages like Python, Java, and C.

Method: Introduced SWE-Sharp-Bench, a new and reproducible benchmark for C# software engineering tasks. The benchmark comprises 150 instances from 17 repositories. The study systematically evaluates the same AI model-agent configurations across different programming languages including C# and Python.

Result: There is a significant performance gap: AI agents solve 70% of Python tasks but only 40% of C# tasks in benchmark tests.

Conclusion: AI coding agents currently lag behind in C# compared to Python, highlighting an important gap and the need for further research and development in C# code generation. The SWE-Sharp-Bench and its curation pipeline are made open-source for the community.

Abstract: AI coding agents have shown great progress on Python software engineering
benchmarks like SWE-Bench, and for other languages like Java and C in
benchmarks like Multi-SWE-Bench. However, C# -- a prominent enterprise language
ranking #5 in the TIOBE index -- remains absent from such benchmarks. We
introduce SWE-Sharp-Bench, a reproducible software engineering benchmark for
C\# featuring 150 instances from 17 repositories. Evaluating identical
model-agent configurations across languages reveals a significant performance
gap: while 70% of Python tasks in SWE-Bench Verified are solved, $only 40% of
our C\# tasks are resolved. We open-source SWE-Sharp-Bench and our entire
curation pipeline.

</details>


### [6] [EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents](https://arxiv.org/abs/2511.02399)
*Junwei Liu,Chen Xu,Chong Wang,Tong Bai,Weitong Chen,Kaseng Wong,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: EvoDev is an iterative, feature-driven LLM framework that outperforms linear pipelines in software development by modeling dependencies and propagating multi-level context, substantially improving results over previous LLM-based approaches.


<details>
  <summary>Details</summary>
Motivation: Current LLM-agent approaches to automated software development are limited by their reliance on simplistic, linear (waterfall) pipelines, which do not accommodate the iterative, complex, and dependency-rich nature of real software projects.

Method: The paper introduces EvoDev, an LLM-driven, iterative software development framework inspired by feature-driven development. EvoDev decomposes requirements into user-valued features, builds a Feature Map (a directed acyclic graph) to capture dependencies, and maintains multi-level information at each node (business logic, design, code) that propagates through the development process, providing context-aware, iterative refinement.

Result: EvoDev was evaluated on challenging Android development tasks. It outperformed the best baseline (Claude Code) by 56.8% and improved single-agent performance by 16.0%-76.6% across different LLMs, demonstrating improved handling of dependencies, context propagation, and iterative workflows.

Conclusion: Modeling feature dependencies, propagating context, and designing workflow-aware LLM agents substantially improves LLM-driven software development. EvoDev provides a practical, iterative framework, offering insights for future LLM training and iterative agent-based development systems.

Abstract: Recent advances in large language model agents offer the promise of
automating end-to-end software development from natural language requirements.
However, existing approaches largely adopt linear, waterfall-style pipelines,
which oversimplify the iterative nature of real-world development and struggle
with complex, large-scale projects. To address these limitations, we propose
EvoDev, an iterative software development framework inspired by feature-driven
development. EvoDev decomposes user requirements into a set of user-valued
features and constructs a Feature Map, a directed acyclic graph that explicitly
models dependencies between features. Each node in the feature map maintains
multi-level information, including business logic, design, and code, which is
propagated along dependencies to provide context for subsequent development
iterations. We evaluate EvoDev on challenging Android development tasks and
show that it outperforms the best-performing baseline, Claude Code, by a
substantial margin of 56.8%, while improving single-agent performance by
16.0%-76.6% across different base LLMs, highlighting the importance of
dependency modeling, context propagation, and workflow-aware agent design for
complex software projects. Our work summarizes practical insights for designing
iterative, LLM-driven development frameworks and informs future training of
base LLMs to better support iterative software development.

</details>


### [7] [Who's Who? LLM-assisted Software Traceability with Architecture Entity Recognition](https://arxiv.org/abs/2511.02434)
*Dominik Fuchß,Haoyu Liu,Sophie Corallo,Tobias Hey,Jan Keim,Johannes von Geisau,Anne Koziolek*

Main category: cs.SE

TL;DR: This paper shows that using large language models to automatically extract architectural entities from text and source code enables effective, automated architecture-documentation-to-code traceability, matching or exceeding the performance of existing manual and heuristic approaches.


<details>
  <summary>Details</summary>
Motivation: Manual creation of Software Architecture Models (SAMs) is labor-intensive, yet necessary for effective traceability between architecture documentation and source code. Automating this process would make traceability link recovery (TLR) more practical and accessible.

Method: The paper introduces two approaches based on Large Language Models (LLMs): ExArch, which extracts component names from SAD and code to form simple SAMs, and ArTEMiS, which identifies and matches architectural entities in documentation with entities in SAMs. These approaches are evaluated against existing methods.

Result: ExArch reaches a strong F1 score of 0.86, comparable to TransArC (F1: 0.87) but without requiring manual SAMs. ArTEMiS matches the heuristic-based SWATTR (F1: 0.81) and, when integrated with TransArC, can replace SWATTR. The combination of ArTEMiS and ExArch outperforms ArDoCode, the best prior baseline that does not use manual SAMs.

Conclusion: LLMs can effectively automate the identification of architectural entities, enabling fully automated SAM generation and TLR. This advancement improves the practicality and accessibility of architecture-code traceability.

Abstract: Identifying architecturally relevant entities in textual artifacts is crucial
for Traceability Link Recovery (TLR) between Software Architecture
Documentation (SAD) and source code. While Software Architecture Models (SAMs)
can bridge the semantic gap between these artifacts, their manual creation is
time-consuming. Large Language Models (LLMs) offer new capabilities for
extracting architectural entities from SAD and source code to construct SAMs
automatically or establish direct trace links. This paper presents two
LLM-based approaches: ExArch extracts component names as simple SAMs from SAD
and source code to eliminate the need for manual SAM creation, while ArTEMiS
identifies architectural entities in documentation and matches them with
(manually or automatically generated) SAM entities. Our evaluation compares
against state-of-the-art approaches SWATTR, TransArC and ArDoCode. TransArC
achieves strong performance (F1: 0.87) but requires manually created SAMs;
ExArch achieves comparable results (F1: 0.86) using only SAD and code. ArTEMiS
is on par with the traditional heuristic-based SWATTR (F1: 0.81) and can
successfully replace it when integrated with TransArC. The combination of
ArTEMiS and ExArch outperforms ArDoCode, the best baseline without manual SAMs.
Our results demonstrate that LLMs can effectively identify architectural
entities in textual artifacts, enabling automated SAM generation and TLR,
making architecture-code traceability more practical and accessible.

</details>


### [8] [When Continuous Delivery Is Not an Option: Practical Paths to Continuous Engineering in Complex Organizations](https://arxiv.org/abs/2511.02445)
*Eriks Klotins,Magnus Ahlgren,Nicolas Martin Vivaldi,Even-Andre Karlsson*

Main category: cs.SE

TL;DR: This paper analyzes real-world barriers to fully adopting Continuous Software Engineering (CSE) in four industries, updating an existing readiness model to offer practical guidance for organizations facing constraints. Although full CSE transformation is rarely achievable, partial adoption can still yield substantial benefits.


<details>
  <summary>Details</summary>
Motivation: Continuous Software Engineering (CSE) offers potential benefits in efficiency, quality, and responsiveness, but organizations face significant constraints to full adoption—such as complex products, legacy systems, inertia, and regulations. The paper seeks to understand how these challenges impact CSE adoption across different industries.

Method: The researchers analyze four industrial cases from diverse sectors (automation, automotive, retail, chemical) using an extended version of the CSE Industry Readiness Model. Data was collected via expert interviews and synthesized narratively, with focus on identifying common drivers and barriers to adoption.

Result: The study identifies key barriers—organizational preparedness, cross-organizational dependencies, and limited customer demand—as well as driving forces. An updated readiness model is proposed, introducing new feedback levels and distinguishing between market-facing and organization-facing constraints to better guide practitioners.

Conclusion: Full CSE adoption is often not feasible due to practical constraints, but significant internal improvements can still be achieved. The updated readiness model provides actionable, empirically grounded guidance for organizations navigating partial or constrained CSE transformations.

Abstract: Purpose: Continuous Software Engineering (CSE) promises improved efficiency,
quality, and responsiveness in software-intensive organizations. However, fully
adopting CSE is often constrained by complex products, legacy systems,
organizational inertia, and regulatory requirements. In this paper, we examine
four industrial cases from the automation, automotive, retail, and chemical
sectors to explore how such constraints shape CSE adoption in practice.
Methods: We apply and extend a previously proposed CSE Industry Readiness Model
to assess the current and potential levels of adoption in each case. Through
expert interviews and narrative synthesis, we identify common driving forces
and adoption barriers, including organizational preparedness,
cross-organizational dependencies, and limited customer demand for continuous
delivery. Results: Based on our findings, we propose an updated readiness model
that introduces additional levels of internal and external feedback,
distinguishes market- and organization-facing constraints, and better guides
practitioners in setting realistic CSE adoption goals. Conclusions: Our results
highlight that while full end-to-end CSE adoption may not always be feasible,
meaningful internal improvements are still possible and beneficial. This study
provides empirically grounded guidance for organizations navigating partial or
constrained CSE transformations.

</details>


### [9] [Lost in Code Generation: Reimagining the Role of Software Models in AI-driven Software Engineering](https://arxiv.org/abs/2511.02475)
*Jürgen Cito,Dominik Bork*

Main category: cs.SE

TL;DR: The paper argues for using software models recovered from AI-generated code to improve understanding, reliability, and maintainability of such software, addressing the risks inherent in rapid 'vibe coding' enabled by generative AI.


<details>
  <summary>Details</summary>
Motivation: Generative AI allows for easy creation of software from natural language, but this development blurs the line between quick prototypes and properly engineered software, causing issues like fragility and lack of robustness.

Method: The paper proposes recovering software models after code has been generated by AI, rather than only using models as original design blueprints. These post-hoc models help in understanding, assessing, and refining AI-generated systems.

Result: Post-hoc models can clarify the intent behind the code, reveal hidden risks, and support sustainable improvement of AI-created software.

Conclusion: Using software models as ongoing mediators—between human intent, AI generation, and long-term evolution—can make AI-generated software more robust, comprehensible, and maintainable.

Abstract: Generative AI enables rapid ``vibe coding," where natural language prompts
yield working software systems. While this lowers barriers to software
creation, it also collapses the boundary between prototypes and engineered
software, leading to fragile systems that lack robustness, security, and
maintainability. We argue that this shift motivates a reimagining of software
models. Rather than serving only as upfront blueprints, models can be recovered
post-hoc from AI-generated code to restore comprehension, expose risks, and
guide refinement. In this role, models serve as mediators between human intent,
AI generation, and long-term system evolution, providing a path toward
sustainable AI-driven software engineering.

</details>


### [10] [ReleaseEval: A Benchmark for Evaluating Language Models in Automated Release Note Generation](https://arxiv.org/abs/2511.02713)
*Qianru Meng,Zhaochun Ren,Joost Visser*

Main category: cs.SE

TL;DR: ReleaseEval provides a robust new benchmark for automated release note generation, showing LLMs excel in structured tasks but still face hurdles with detailed code diff abstraction.


<details>
  <summary>Details</summary>
Motivation: Manual creation of software release notes is labor-intensive and error-prone. Existing automated approaches face challenges due to limited datasets, lack of licensing, poor reproducibility, and incomplete use of available context.

Method: The paper proposes ReleaseEval, an openly licensed and reproducible benchmark dataset for evaluating automated release note generation. The benchmark draws from 94,987 release notes from 3,369 repositories over 6 languages and supports three granular task settings: (1) commit2sum, (2) tree2sum, (3) diff2sum.

Result: Large language models (LLMs) outperform traditional methods, especially when provided with structured commit tree information (tree2sum), but show weakness in handling detailed code diffs (diff2sum).

Conclusion: LLMs are effective at automated release note generation, especially when using structured input, but more work is needed to improve their ability to abstract information from fine-grained code changes.

Abstract: Automated release note generation addresses the challenge of documenting
frequent software updates, where manual efforts are time-consuming and prone to
human error. Although recent advances in language models further enhance this
process, progress remains hindered by dataset limitations, including the lack
of explicit licensing and limited reproducibility, and incomplete task design
that relies mainly on commit messages for summarization while overlooking
fine-grained contexts such as commit hierarchies and code changes. To fill this
gap, we introduce ReleaseEval, a reproducible and openly licensed benchmark
designed to systematically evaluate language models for automated release note
generation. ReleaseEval comprises 94,987 release notes from 3,369 repositories
across 6 programming languages, and supports three task settings with three
levels of input granularity: (1) commit2sum, which generates release notes from
commit messages; (2) tree2sum, which incorporates commit tree structures; and
(3) diff2sum, which leverages fine-grained code diffs. Both automated and human
evaluations show that large language models consistently outperform traditional
baselines across all tasks, achieving substantial gains on tree2sum, while
still struggling on diff2sum. These findings highlight LLMs' proficiency in
leveraging structured information while revealing challenges in abstracting
from long code diffs.

</details>


### [11] [Investigating the Experience of Autistic Individuals in Software Engineering](https://arxiv.org/abs/2511.02736)
*Madalena Sasportes,Grischa Liebel,Miguel Goulão*

Main category: cs.SE

TL;DR: Autistic software engineers have valuable strengths like attention to detail, logical thinking, and hyperfocus. They prefer written communication, remote work, and are comfortable with AI tools. This research shifts focus from challenges to strengths, showing these attributes can benefit software engineering.


<details>
  <summary>Details</summary>
Motivation: Autistic individuals often face unemployment and mental health challenges, despite possessing notable strengths compared to non-autistic individuals. Prior research in software engineering (SE) has focused more on challenges and accommodations rather than leveraging autistic strengths.

Method: The paper combines Social-Technical Grounded Theory using semi-structured interviews with 16 autistic software engineers and a survey with 49 respondents (including 5 autistic participants). Themes from this data are compared with existing theory on neurodivergent cognitive dysfunctions in SE performance.

Result: Autistic software engineers often excel at logical thinking, attention to detail, and hyperfocus during programming. They also display strong enthusiasm for learning new programming languages and technologies. Consistent with previous findings, they prefer written communication and remote work, and report high comfort levels with AI-based systems.

Conclusion: The study reinforces and extends previous knowledge by documenting and providing further evidence for the distinctive strengths of autistic software engineers, shifting focus towards their capabilities and potential contributions in SE.

Abstract: Context: Autism spectrum disorder (ASD) leads to various issues in the
everyday life of autistic individuals, often resulting in unemployment and
mental health problems. To improve the inclusion of autistic adults, existing
studies have highlighted the strengths these individuals possess in comparison
to non-autistic individuals, e.g., high attention to detail or excellent
logical reasoning skills. If fostered, these strengths could be valuable in
software engineering activities, such for identifying specific kinds of bugs in
code. However, existing work in SE has primarily studied the challenges of
autistic individuals and possible accommodations, with little attention their
strengths. Objective: Our goal is to analyse the experiences of autistic
individuals in software engineering activities, such as code reviews, with a
particular emphasis on strengths. Methods: This study combines Social-Technical
Grounded Theory through semi-structured interviews with 16 autistic software
engineers and a survey with 49 respondents, including 5 autistic participants.
We compare the emerging themes with the theory by Gama et al. on the Effect of
Neurodivergent Cognitive Dysfunctions in Software Engineering Performance.
Results: Our results suggest that autistic software engineers are often skilled
in logical thinking, attention to detail, and hyperfocus in programming; and
they enjoy learning new programming languages and programming-related
technologies. Confirming previous work, they tend to prefer written
communication and remote work. Finally, we report a high comfort level in
interacting with AI-based systems. Conclusions: Our findings extend existing
work by providing further evidence on the strengths of autistic software
engineers.

</details>


### [12] [Formalizing Regression Testing for Agile and Continuous Integration Environments](https://arxiv.org/abs/2511.02810)
*Suddhasvatta Das,Kevin Gary*

Main category: cs.SE

TL;DR: This paper formalizes continuous regression testing in agile development, modeling builds as a chain and representing algorithms without extra assumptions, and validates the approach with soundness and completeness proofs.


<details>
  <summary>Details</summary>
Motivation: Agile software development produces frequent software versions, necessitating ongoing regression testing rather than infrequent testing near release, challenging classical regression-testing assumptions.

Method: The authors formalize continuous regression testing as a time-ordered sequence of builds (with program, requirements, and tests), introducing the concept of a regression test window to model limited time budgets. They validate their model by expressing two modern agile regression testing algorithms as build-tuple operations and proving soundness and completeness.

Result: The formalization successfully represents state-of-the-art agile regression testing algorithms directly, without extra assumptions, and soundness and completeness proofs confirm the validity of the approach.

Conclusion: The proposed formal model generalizes and preserves classical regression testing semantics while accommodating agile development's demands for continual regression testing.

Abstract: Software developed using modern agile practices delivers a stream of software
versions that require continuous regression testing rather than testing once
close to the delivery or maintenance phase, as assumed by classical
regression-testing theory. In this work, we formalize the phenomenon of
continuous or near-continuous regression testing using successive builds as a
time-ordered chain, where each build contains the program, requirements, and
the accompanying tests. We also formalize the regression test window between
any two builds, which captures the limited time budget available for regression
testing. As the time limit is set to infinity and the chain is closed to two
builds, the model degenerates to retest-all, thereby preserving semantics for
the classical two-version case. The formalization is validated by directly
representing two state-of-the-art agile regression testing algorithms in terms
of build-tuple operations without requiring auxiliary assumptions, followed by
proof of the soundness and completeness of our formalization.

</details>


### [13] [From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu](https://arxiv.org/abs/2511.02827)
*Mohamed Almukhtar,Anwar Ghammam,Marouane Kessentini,Hua Ming*

Main category: cs.SE

TL;DR: The paper introduces PyQu, a tool for assessing quality-enhancing code changes in Python ML projects, analyzing 3,340 projects and revealing 61 change types (41% newly identified). PyQu achieves high accuracy and F1 score, offering new insights and a foundation for automated quality assessment and best practices in ML software.


<details>
  <summary>Details</summary>
Motivation: There is a growing concern in the software industry regarding the quality of Python-based Machine Learning systems, especially given the rapid advancements in Generative AI for code generation. A significant challenge is understanding how specific code changes affect software quality, hampered by a lack of assessment tools and clear mappings between code changes and their impact.

Method: The authors conduct a comprehensive empirical study on 3,340 open-source Python ML projects, analyzing over 3.7 million commits and 2.7 trillion lines of code. They develop a new tool, PyQu, which uses low-level software metrics to identify commits that improve quality. Additionally, a thematic analysis is performed to categorize the types of quality-enhancing code changes.

Result: PyQu demonstrates strong performance, with an average accuracy, precision, and recall of 0.84 and an average F1 score of 0.85. The study identifies 61 distinct quality-enhancing code changes across 13 categories, with 41% being newly discovered and not recognized by existing tools.

Conclusion: This study provides important new insights into the relationship between code changes and software quality in Python-based ML systems. The PyQu tool and the newly identified categories of improvements establish a foundation for future research, tool development, and best practices aimed at automated quality assessment of ML code.

Abstract: In an era shaped by Generative Artificial Intelligence for code generation
and the rising adoption of Python-based Machine Learning systems (MLS),
software quality has emerged as a major concern. As these systems grow in
complexity and importance, a key obstacle lies in understanding exactly how
specific code changes affect overall quality-a shortfall aggravated by the lack
of quality assessment tools and a clear mapping between ML systems code changes
and their quality effects. Although prior work has explored code changes in
MLS, it mostly stops at what the changes are, leaving a gap in our knowledge of
the relationship between code changes and the MLS quality. To address this gap,
we conducted a large-scale empirical study of 3,340 open-source Python ML
projects, encompassing more than 3.7 million commits and 2.7 trillion lines of
code. We introduce PyQu, a novel tool that leverages low level software metrics
to identify quality-enhancing commits with an average accuracy, precision, and
recall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic
analysis, we identified 61 code changes, each demonstrating a direct impact on
enhancing software quality, and we classified them into 13 categories based on
contextual characteristics. 41% of the changes are newly discovered by our
study and have not been identified by state-of-the-art Python changes detection
tools. Our work offers a vital foundation for researchers, practitioners,
educators, and tool developers, advancing the quest for automated quality
assessment and best practices in Python-based ML software.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [14] [Oriented Metrics for Bottom-Up Enumerative Synthesis](https://arxiv.org/abs/2511.02491)
*Roland Meyer,Jakob Tepe*

Main category: cs.PL

TL;DR: This paper introduces 'oriented metrics' for program synthesis, enabling powerful search space reduction techniques. Their approach, implemented in a new solver, improves synthesis efficiency by more than an order of magnitude on benchmark tasks, surpassing current methods.


<details>
  <summary>Details</summary>
Motivation: In syntax-guided synthesis, the main challenge is the very large search space when trying to find suitable programs that meet a specification. Existing approaches often don't take advantage of inherent structure in the search space, especially when operations are not symmetric.

Method: The authors introduce the concept of 'oriented metrics,' which measure directed distances between programs considering the directionality of operations (especially in string and bitvector domains). They develop several oriented metrics and design four search space reduction techniques: (1) pruning to a ball around the ground truth, (2) factorizing by oriented metric-induced equivalence, (3) abstracting and refining the oriented metric, and (4) learning to improve enumeration order. These techniques are then integrated into a new, generic synthesis algorithm and solver.

Result: The proposed solver and techniques, tested on string and bitvector synthesis tasks, significantly improve performance compared to existing methods—by more than an order of magnitude.

Conclusion: Oriented metrics provide a novel and effective way to reduce and structure the search space in syntax-guided synthesis, leading to substantial improvements in efficiency and broad applicability.

Abstract: In syntax-guided synthesis, one of the challenges is to reduce the enormous
size of the search space. We observe that most search spaces are not just flat
sets of programs, but can be endowed with a structure that we call an oriented
metric. Oriented metrics measure the distance between programs, like ordinary
metrics do, but are designed for settings in which operations have an
orientation. Our focus is on the string and the bitvector domains, where
operations like concatenation and bitwise conjunction transform an input into
an output in a way that is not symmetric. We develop several new oriented
metrics for these domains. Oriented metrics are designed for search space
reduction, and we present four techniques: (i) pruning the search space to a
ball around the ground truth, (ii) factorizing the search space by an
equivalence that is induced by the oriented metric, (iii) abstracting the
oriented metric (and hence the equivalence) and refining it, and (iv) improving
the enumeration order by learning from abstract information. We acknowledge
that these techniques are inspired by developments in the literature. By
understanding their roots in oriented metrics, we can substantially increase
their applicability and efficiency. We have integrated these techniques into a
new synthesis algorithm and implemented the algorithm in a new solver. Notably,
our solver is generic in the oriented metric over which it computes. We
conducted experiments in the string and the bitvector domains, and consistently
improve the performance over the state-of-the-art by more than an order of
magnitude.

</details>
