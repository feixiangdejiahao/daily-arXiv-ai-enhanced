<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 12]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: This survey reviews and analyzes 150+ papers on LLMs in software engineering, organizing solution methods and benchmarks into a clear taxonomy. It connects over 50 benchmarks to relevant solution strategies, charts the field's evolution to advanced agentic systems, and provides a unified workflow. The paper identifies research gaps and recommends future work, making it a key resource for researchers and practitioners in LLM-enabled software engineering.


<details>
  <summary>Details</summary>
Motivation: The rapid integration of LLMs (Large Language Models) into software engineering has transformed the landscape from traditional rule-based to autonomous agentic systems. However, the field lacks a comprehensive perspective on how benchmarks (evaluations) and solution paradigms relate, limiting systematic development and assessment.

Method: Conducted a large-scale survey and analysis of more than 150 recent research papers on LLM-powered software engineering. Developed a comprehensive taxonomy based on two key axes: solution approaches (prompt-based, fine-tuning-based, agent-based) and benchmark types (code generation, translation, repair, etc.). Connected 50+ benchmarks with associated solution strategies, and presented a unified workflow pipeline to illustrate the software engineering lifecycle using LLMs. Identified research gaps and proposed actionable future research directions.

Result: Mapped the evolution of the field from basic prompt-based methods to advanced agent-based systems with planning, reasoning, memory, and tool augmentation. Provided a comprehensive overview linking evaluation benchmarks and solution methods. Identified current shortcomings—such as lack of multi-agent frameworks and integration with formal verification—and outlined future opportunities (multi-agent collaboration, self-evolving systems, etc.).

Conclusion: This work establishes a holistic framework for understanding and advancing LLM-powered software engineering by thoroughly connecting evaluation benchmarks with solution strategies. It serves as a foundational guide for ongoing and future research, highlighting both the spectrum of current capabilities and the gaps to be addressed moving forward.

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [2] [InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation](https://arxiv.org/abs/2510.09724)
*Qiaosheng Chen,Yang Liu,Lei Li,Kai Chen,Qipeng Guo,Gong Cheng,Fei Yuan*

Main category: cs.SE

TL;DR: This paper presents InteractScience, a new benchmark that tests LLMs' ability to generate interactive scientific demonstrations by integrating domain knowledge with front-end coding. Current LLMs struggle with this task, highlighting an important area for further research and development.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are capable of generating software from natural language, opening opportunities in science and education for interactive demonstrations. However, current benchmarks do not evaluate LLMs’ ability to integrate scientific knowledge with interactive code generation.

Method: The paper introduces a hybrid evaluation framework combining programmatic functional testing for interaction logic and visually-grounded qualitative testing against reference outputs. Using this, they build the InteractScience benchmark: a large set of questions from 5 scientific domains, each with unit tests, snapshots, and checklists.

Result: 30 open- and closed-source LLMs were tested with InteractScience. Results show significant weaknesses in LLMs when integrating scientific domain knowledge with interactive front-end coding.

Conclusion: InteractScience is the first benchmark for systematically and automatically evaluating LLMs’ ability to generate realistic, educational, and interactive scientific demonstration code, laying the foundation for future improvements.

Abstract: Large Language Models (LLMs) are increasingly capable of generating complete
applications from natural language instructions, creating new opportunities in
science and education. In these domains, interactive scientific demonstrations
are particularly valuable for explaining concepts, supporting new teaching
methods, and presenting research findings. Generating such demonstrations
requires models to combine accurate scientific knowledge with the ability to
implement interactive front-end code that behaves correctly and responds to
user actions. This capability goes beyond the scope of existing benchmarks,
which typically evaluate either knowledge question answering without grounding
in code or static web code generation without scientific interactivity. To
evaluate this integrated ability, we design a hybrid framework that combines
programmatic functional testing to rigorously verify interaction logic with
visually-grounded qualitative testing to assess rendered outputs against
reference snapshots. Building on this framework, we present InteractScience, a
benchmark consisting of a substantial set of carefully designed questions
across five scientific domains, each paired with unit tests, reference
snapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs
and report results that highlight ongoing weaknesses in integrating domain
knowledge with interactive front-end coding. Our work positions InteractScience
as the first benchmark to automatically measure this combined capability with
realistic interactive operations, providing a foundation for advancing reliable
and educationally useful scientific demonstration code generation. All code and
data are publicly available at https://github.com/open-compass/InteractScience.

</details>


### [3] [Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem](https://arxiv.org/abs/2510.09907)
*Muhammad Maaz,Liam DeVoe,Zac Hatfield-Dodds,Nicholas Carlini*

Main category: cs.SE

TL;DR: LLMs can automate property-based testing for Python packages by synthesizing tests, analyzing outputs, and generating useful bug reports. Applied to 100 packages, the approach surfaced many valid and actionable bugs—including in major libraries—with several resulting patches merged. This method offers scalable, rigorous automated software testing.


<details>
  <summary>Details</summary>
Motivation: Property-based testing (PBT) is effective but requires manual specification of test properties and input domains. The motivation is to automate and scale PBT by leveraging the capabilities of large language models (LLMs) to analyze code, infer properties, and execute robust test generation and bug reporting.

Method: The authors developed an LLM-based agent that automatically analyzes Python modules, infers properties from both code and documentation, synthesizes and runs property-based tests, interprets the results to confirm bugs, and generates actionable bug reports. The agent was evaluated on 100 popular Python packages.

Result: Manual review found that 56% of bug reports were valid bugs and 32% were valid bugs worth reporting. A ranking rubric was created to prioritize bugs, and among the top 21, 86% were valid and 81% were worth reporting. The team reported 5 bugs (4 with patches), including issues in major libraries like NumPy, and successfully merged 3 patches.

Conclusion: The study demonstrates that combining LLMs with PBT forms an effective and scalable method for autonomous software testing, capable of identifying a broad range of real-world bugs and producing actionable reports and patches. The approach shows promise for improving software robustness with minimal manual effort.

Abstract: Property-based testing (PBT) is a lightweight formal method, typically
implemented as a randomized testing framework. Users specify the input domain
for their test using combinators supplied by the PBT framework, and the
expected properties or invariants as a unit-test function. The framework then
searches for a counterexample, e.g. by generating inputs and calling the test
function. In this work, we demonstrate an LLM-based agent which analyzes Python
modules, infers function-specific and cross-function properties from code and
documentation, synthesizes and executes PBTs, reflects on outputs of these
tests to confirm true bugs, and finally outputs actionable bug reports for the
developer. We perform an extensive evaluation of our agent across 100 popular
Python packages. Of the bug reports generated by the agent, we found after
manual review that 56\% were valid bugs and 32\% were valid bugs that we would
report to maintainers. We then developed a ranking rubric to surface
high-priority valid bugs to developers, and found that of the 21 top-scoring
bugs, 86\% were valid and 81\% we would report. The bugs span diverse failure
modes from serialization failures to numerical precision errors to flawed cache
implementations. We reported 5 bugs, 4 with patches, including to NumPy and
cloud computing SDKs, with 3 patches merged successfully. Our results suggest
that LLMs with PBT provides a rigorous and scalable method for autonomously
testing software. Our code and artifacts are available at:
https://github.com/mmaaz-git/agentic-pbt.

</details>


### [4] [OFP-Repair: Repairing Floating-point Errors via Original-Precision Arithmetic](https://arxiv.org/abs/2510.09938)
*Youshuai Tan,Zishuo Ding,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: Floating-point bugs can be hard to fix, especially if high-precision computation is required. Most tools either fail to identify which errors need high-precision or struggle to fix a wide range of bugs. This paper introduces OFP-Repair, a new method that efficiently repairs floating-point errors, achieving significant improvements over prior tools on benchmarks and real-world cases and being considered by real developers for practical use.


<details>
  <summary>Details</summary>
Motivation: Floating-point errors can have severe consequences in high-stakes domains (military, aerospace, finance), and current repair tools are limited—being either too reliant on high-precision (expensive and difficult) methods or not effective enough in fixing many errors. Distinguishing which errors need high-precision remains a practical challenge, leaving a gap in effective repair strategies.

Method: The paper proposes a new repair method called OFP-Repair, designed to better distinguish and repair errors that can be fixed with original-precision arithmetic versus those requiring high-precision. The approach aims to be practical and efficient, able to fix more errors without always requiring high-precision implementations.

Result: On the ACESO dataset, OFP-Repair shows improvements of 3, 7, 3, and 8 orders of magnitude across four accuracy metrics. In real-world trials, it detects all five original-precision-repairable errors and fixes three (whereas the competing ACESO tool fixes only one). When tested on historical bugs in the GNU Scientific Library, OFP-Repair repairs 5 out of 15, and the maintainers show interest in adoption.

Conclusion: OFP-Repair significantly advances the state of floating-point error repair by fixing more bugs efficiently, often without resorting to high-precision computation, and demonstrates promising results both in benchmark datasets and real-world libraries. Its adoption is being considered in practice, indicating strong practical relevance.

Abstract: Errors in floating-point programs can lead to severe consequences,
particularly in critical domains such as military, aerospace, and financial
systems, making their repair a crucial research problem. In practice, some
errors can be fixed using original-precision arithmetic, while others require
high-precision computation. Developers often avoid addressing the latter due to
excessive computational resources required. However, they sometimes struggle to
distinguish between these two types of errors, and existing repair tools fail
to assist in this differentiation. Most current repair tools rely on
high-precision implementations, which are time-consuming to develop and demand
specialized expertise. Although a few tools do not require high-precision
programs, they can only fix a limited subset of errors or produce suboptimal
results.
  To address these challenges, we propose a novel method, named OFP-Repair.On
ACESO's dataset, our patches achieve improvements of three, seven, three, and
eight orders of magnitude across four accuracy metrics. In real-world cases,
our method successfully detects all five original-precision-repairable errors
and fixes three, whereas ACESO only repairs one. Notably, these results are
based on verified data and do not fully capture the potential of OFP-Repair. To
further validate our method, we deploy it on a decade-old open bug report from
GNU Scientific Library (GSL), successfully repairing five out of 15 bugs. The
developers have expressed interest in our method and are considering
integrating our tool into their development workflow. We are currently working
on applying our patches to GSL. The results are highly encouraging,
demonstrating the practical applicability of our technique.

</details>


### [5] [Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context](https://arxiv.org/abs/2510.09968)
*Stefan Pasch*

Main category: cs.SE

TL;DR: MLOps best practices significantly improve user satisfaction in AI development across organizations of all sizes, even though smaller firms talk about them less.


<details>
  <summary>Details</summary>
Motivation: Organizations face significant challenges in scaling, maintaining, and coordinating AI initiatives. The effectiveness and real-world impact of MLOps practices in overcoming these challenges and improving AI application development are not well understood.

Method: The study analyzed more than 8,000 user reviews from G2.com for various AI development platforms. Zero-shot classification was used to gauge review sentiment toward nine specific MLOps practices, such as CI/CD, orchestration, reproducibility, versioning, collaboration, and monitoring.

Result: Seven out of nine MLOps practices showed a significant positive relationship with user satisfaction, indicating their valuable contribution to AI development. However, smaller firms discuss these practices less often, though firm size does not affect the positive impact of MLOps practices on satisfaction.

Conclusion: MLOps practices deliver tangible benefits for AI development across organizations, regardless of firm size. Their implementation is universally valued by users, even if the emphasis and discussion of these practices vary by organizational context.

Abstract: Organizational efforts to utilize and operationalize artificial intelligence
(AI) are often accompanied by substantial challenges, including scalability,
maintenance, and coordination across teams. In response, the concept of Machine
Learning Operations (MLOps) has emerged as a set of best practices that
integrate software engineering principles with the unique demands of managing
the ML lifecycle. Yet, empirical evidence on whether and how these practices
support users in developing and operationalizing AI applications remains
limited. To address this gap, this study analyzes over 8,000 user reviews of AI
development platforms from G2.com. Using zero-shot classification, we measure
review sentiment toward nine established MLOps practices, including continuous
integration and delivery (CI/CD), workflow orchestration, reproducibility,
versioning, collaboration, and monitoring. Seven of the nine practices show a
significant positive relationship with user satisfaction, suggesting that
effective MLOps implementation contributes tangible value to AI development.
However, organizational context also matters: reviewers from small firms
discuss certain MLOps practices less frequently, suggesting that organizational
context influences the prevalence and salience of MLOps, though firm size does
not moderate the MLOps-satisfaction link. This indicates that once applied,
MLOps practices are perceived as universally beneficial across organizational
settings.

</details>


### [6] [SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study](https://arxiv.org/abs/2510.10010)
*Matheus J. T. Vargas*

Main category: cs.SE

TL;DR: SLEAN is a simple, file-based framework for orchestrating multiple LLMs to safely filter and arbitrate AI-generated code fixes. It efficiently reduces harmful changes, requires minimal expertise, and is broadly applicable to reliable, auditable multi-provider code and document review workflows.


<details>
  <summary>Details</summary>
Motivation: AI-assisted debugging can introduce harmful, unnecessary, or overly complex changes to software code. There is a need for a simple framework to coordinate multiple large language model (LLM) providers for safer and more reliable code fixes without deep technical barriers.

Method: SLEAN is a deterministic text-based orchestration framework that coordinates multiple LLMs using .txt templates. It uses a three-phase protocol: independent analysis, cross-critique, and arbitration, to filter and select appropriate code changes. The framework avoids complex infrastructure and enables a file-driven deployment.

Result: On 15 software bugs and 69 AI-generated fix proposals, SLEAN accepted 22 fixes and rejected 47 potentially harmful ones. It reduced code change surface by 83-90%, prioritized minimal edits, and showed about 20% efficiency gain with minimal inputs. The agreement rate between LLMs was weakly correlated with fix quality, and arbitration occurred only under specific convergence scenarios.

Conclusion: SLEAN is an efficient, provider-agnostic framework for filtering and managing code fixes from multiple LLMs. It requires little technical expertise to deploy and improves safety and minimalism in code changes. It has potential applications in security auditing, code review, and document verification beyond software debugging.

Abstract: We present SLEAN (Simple Lightweight Ensemble Analysis Network), a
deterministic framework for coordinating multiple LLM providers through
text-based prompt orchestration. Unlike complex multi-agent systems requiring
specialized infrastructure, SLEAN operates as a simple prompt bridge between
LLMs using .txt templates, requiring no deep technical knowledge for
deployment. The three-phase protocol formed by independent analysis,
cross-critique, and arbitration, filters harmful AI-generated code suggestions
before production deployment, addressing how AI-assisted debugging increasingly
produces modifications that introduce unnecessary complexity, break existing
functionality, or address problems. Evaluating 15 software bugs, we analyzed 69
AI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95%
CI 20.9-42.9%) while rejecting 47 that would have been harmful if applied
verbatim. The arbitration process reduced code change surface by 83-90%
relative to raw AI outputs, enforcing minimal causal edits over scope-expanding
modifications. Minimal Type 2 inputs proved more efficient than detailed Type 1
inputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus
28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems
showed weak correlation with fix quality: high convergence (at least 80%)
occurred in 4 of 15 cases and improved acceptance by only 2.4% points;
arbitration appeared only at exactly 10% convergence in 2 of 15 cases, although
low convergence alone did not necessitate arbitration. The file-driven,
provider-agnostic architecture enables deployment without specialized coding
expertise, making it applicable to security auditing, code review, document
verification, and other domains requiring reliable multi-provider synthesis
with end-to-end auditability.

</details>


### [7] [OBsmith: Testing JavaScript Obfuscator using LLM-powered sketching](https://arxiv.org/abs/2510.10066)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: OBsmith is a new LLM-powered framework to test if JavaScript obfuscators preserve code correctness. It found bugs that existing fuzzers missed, showing the need for obfuscator-focused correctness testing to ensure reliable protection of intellectual property.


<details>
  <summary>Details</summary>
Motivation: JavaScript obfuscators are commonly used to protect code and intellectual property, but the correctness of these tools—whether they preserve the meaning and function of the original code—remains largely overlooked. Most evaluations focus on how well obfuscators resist deobfuscation rather than whether they change program behavior, which can undermine functionality and security.

Method: The authors introduce OBsmith, a new framework that leverages large language models (LLMs) to automatically generate a wide variety of JavaScript program sketches that cover different constructs, idioms, and edge cases. These sketches are instantiated into executable programs for systematic testing of obfuscators. OBsmith also extracts real-world sketches from existing projects to enable more domain-focused testing. Programs are then obfuscated and analyzed across different configurations to detect semantic changes.

Result: OBsmith identified 11 previously unknown correctness bugs in JavaScript obfuscators. State-of-the-art JavaScript fuzzers failed to detect these bugs, indicating that OBsmith offers complementary capabilities by targeting obfuscation-specific issues. An ablation study found that all system components (except one generic testing strategy) contributed to the detection of unique bug classes, but pointed out the need for more tailored metamorphic relations for different obfuscators.

Conclusion: OBsmith demonstrates that current JavaScript obfuscators may introduce semantic errors, and its systematic approach using LLMs and extracted sketching provides a powerful testing methodology. The results emphasize the importance of correctness testing alongside resilience to deobfuscation and endorse OBsmith as a valuable tool for quality assurance of obfuscators and potentially other transformation tools.

Abstract: JavaScript obfuscators are widely deployed to protect intellectual property
and resist reverse engineering, yet their correctness has been largely
overlooked compared to performance and resilience. Existing evaluations
typically measure resistance to deobfuscation, leaving the critical question of
whether obfuscators preserve program semantics unanswered. Incorrect
transformations can silently alter functionality, compromise reliability, and
erode security-undermining the very purpose of obfuscation. To address this
gap, we present OBsmith, a novel framework to systematically test JavaScript
obfuscators using large language models (LLMs). OBsmith leverages LLMs to
generate program sketches abstract templates capturing diverse language
constructs, idioms, and corner cases-which are instantiated into executable
programs and subjected to obfuscation under different configurations. Besides
LLM-powered sketching, OBsmith also employs a second source: automatic
extraction of sketches from real programs. This extraction path enables more
focused testing of project specific features and lets developers inject domain
knowledge into the resulting test cases. OBsmith uncovers 11 previously unknown
correctness bugs. Under an equal program budget, five general purpose
state-of-the-art JavaScript fuzzers (FuzzJIT, Jsfunfuzz, Superion, DIE,
Fuzzilli) failed to detect these issues, highlighting OBsmith's complementary
focus on obfuscation induced misbehavior. An ablation shows that all components
except our generic MRs contribute to at least one bug class; the negative MR
result suggests the need for obfuscator-specific metamorphic relations. Our
results also seed discussion on how to balance obfuscation presets and
performance cost. We envision OBsmith as an important step towards automated
testing and quality assurance of obfuscators and other semantic-preserving
toolchains.

</details>


### [8] [A Mathematics-Guided Approach to Floating-Point Error Detection](https://arxiv.org/abs/2510.10081)
*Youshuai Tan,Zhanwei Zhang,Zishuo Ding,Lianyu Zheng,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: A new method, MGDE, uses mathematical insight to detect floating-point software bugs far faster and more effectively than the existing best approach, identifying twice as many bugs in a fraction of the time.


<details>
  <summary>Details</summary>
Motivation: Floating-point errors can cause critical failures, especially in high-stakes applications like military systems. Existing tools for finding such errors are slow and ineffective at finding hard-to-locate, error-inducing inputs.

Method: The authors propose MGDE, a detection method based on mathematical guidance using the Newton-Raphson method for more efficient and effective identification of potentially disastrous floating-point errors.

Result: MGDE outperforms the current best method, FPCC, by discovering almost twice as many bugs (89 vs. 48), affecting more programs (44 vs. 29), and operates over six times faster. On multi-input programs, MGDE found nine bugs in under a second each, while FPCC detected none and took 100 seconds per program.

Conclusion: MGDE dramatically improves both the speed and efficacy of finding floating-point bugs, surpassing the state-of-the-art and demonstrating scalability to more complex, multi-input cases.

Abstract: Floating-point program errors can lead to severe consequences, particularly
in critical domains such as military applications. Only a small subset of
inputs may induce substantial floating-point errors, prompting researchers to
develop methods for identifying these error-inducing inputs. Although existing
approaches have achieved some success, they still suffer from two major
limitations: (1) High computational cost: The evaluation of error magnitude for
candidate inputs relies on high-precision programs, which are prohibitively
time-consuming. (2) Limited long-range convergence capability: Current methods
exhibit inefficiency in search, making the process akin to finding a needle in
a haystack.
  To address these two limitations, we propose a novel method, named MGDE, to
detect error-inducing inputs based on mathematical guidance. By employing the
Newton-Raphson method, which exhibits quadratic convergence properties, we
achieve highly effective and efficient results. Since the goal of identifying
error-inducing inputs is to uncover the underlying bugs, we use the number of
bugs detected in floating-point programs as the primary evaluation metric in
our experiments. As FPCC represents the most effective state-of-the-art
approach to date, we use it as the baseline for comparison. The dataset of FPCC
consists of 88 single-input floating-point programs. FPCC is able to detect 48
bugs across 29 programs, whereas our method successfully identifies 89 bugs
across 44 programs. Moreover, FPCC takes 6.4096 times as long as our proposed
method. We also deploy our method to multi-input programs, identifying a total
of nine bugs with an average detection time of 0.6443 seconds per program. In
contrast, FPCC fails to detect any bugs while requiring an average computation
time of 100 seconds per program.

</details>


### [9] [IntrinTrans: LLM-based Intrinsic Code Translator for RISC-V Vector](https://arxiv.org/abs/2510.10119)
*Liutong Han,Zhiyuan Tan,Hongbin Zhang,Pengcheng Wang,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: Manual translation of vectorized code to RISC-V is slow and limited. This paper presents IntrinTrans, an automated LLM-based system that translates and optimizes intrinsic code for RVV, achieving high correctness and up to 5.93x speedup over open-source versions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing need to support RISC-V Vector (RVV) extensions in software libraries as the RISC-V ecosystem expands. Current reliance on manual translation from Arm or x86 SIMD intrinsic code to RVV is time-consuming and error-prone, and rule-based methods are limited. Efficient and accurate automatic translation is in demand to leverage RVV hardware features and improve library performance.

Method: The authors propose IntrinTrans, a multi-agent system based on large language models (LLMs) that automatically translates vectorized intrinsic code from one architecture to another (e.g., from Arm Neon to RVV). The approach incorporates compile-and-test feedback and optimizes RVV intrinsics by analyzing register usage through liveness analysis.

Result: In experiments with 34 open-source vectorized algorithm cases, the system produced semantically correct RVV intrinsic code in most scenarios after a few iterations. Furthermore, the translated code in some cases achieved up to 5.93x higher performance compared to the native RVV implementations from the open-source community.

Conclusion: IntrinTrans successfully automates the translation of intrinsic code across architectures and significantly optimizes RVV-specific performance, reducing reliance on manual rewriting and surpassing community implementations in both correctness and speed.

Abstract: The use of intrinsic functions to exploit hardware-specific capabilities is
an important approach for optimizing library performance. Many mainstream
libraries implement a large number of vectorized algorithms on Arm or x86 SIMD
intrinsic functions. With the rapid expansion of the RISC-V hardware-software
ecosystem, there is a growing demand for support of the RISC-V Vector (RVV)
extension. Translating existing vectorized intrinsic code onto RVV intrinsics
is a practical and effective approach. However, current cross-architecture
translation largely relies on manual rewriting, which is time-consuming and
error-prone. Furthermore, while some rule-based methods can reduce the need for
manual intervention, their translation success rate is limited by incomplete
rule coverage and syntactic constraints, and the performance suffers from
inadequate utilization of RVV-specific features. We present IntrinTrans, a
LLM-based multi-agent approach that utilizes compile-and-test feedback to
translate intrinsic code across architectures automatically, and further
optimizes the generated RVV intrinsics using register-usage information derived
from liveness analysis. To evaluate the effectiveness of our approach, we
collected 34 vectorized algorithm cases from open-source libraries. Each case
includes an Arm Neon intrinsics implementation and a RVV intrinsics
implementation contributed by the open-source community, together with
correctness and performance tests. Our experiments show that advanced LLMs
produce semantically correct RISC-V Vector intrinsics in most cases within a
limited number of iterations, and in some cases achieve up to 5.93x the
performance of the native implementation from the open-source community.

</details>


### [10] [A Systematic Study on Generating Web Vulnerability Proof-of-Concepts Using Large Language Models](https://arxiv.org/abs/2510.10148)
*Mengyao Zhao,Kaixuan Li,Lyuye Zhang,Wenjing Dang,Chenggong Ding,Sen Chen,Zheli Liu*

Main category: cs.SE

TL;DR: This paper studies how well LLMs can automatically create working exploit PoCs for web vulnerabilities using publicly disclosed data. With more code context and better prompting, success rates rise sharply, indicating LLMs could impact security dynamics. DeepSeek-R1 performs better than GPT-4o, and dozens of AI-generated PoCs have already been recognized by major databases.


<details>
  <summary>Details</summary>
Motivation: Advances in LLMs have opened new possibilities for automated code exploitation, especially in generating PoC exploits, which are crucial for vulnerability reproduction and mitigation. With rich public information available for disclosed CVEs, the study investigates whether LLMs can leverage this data to automatically generate valid PoCs.

Method: The paper empirically evaluates two LLMs (GPT-4o and DeepSeek-R1) on 100 real-world CVEs at different disclosure stages (description-only, patch-available, full code context). It measures PoC generation rates and analyzes the impact of supplied context (function-level, file-level) and adaptive prompt refinement.

Result: LLMs can generate valid PoCs in 8%-34% of cases using only public information, with DeepSeek-R1 outperforming GPT-4o. Including code context increases success by 17%-20%, and prompt refinement strategy boosts rates up to 68%-72%. 23 generated PoCs were accepted by NVD/Exploit DB.

Conclusion: LLM-based PoC generation is feasible with public CVE data and improves with more code context and advanced prompting. This capability could alter the landscape of vulnerability exploitation.

Abstract: Recent advances in Large Language Models (LLMs) have brought remarkable
progress in code understanding and reasoning, creating new opportunities and
raising new concerns for software security. Among many downstream tasks,
generating Proof-of-Concept (PoC) exploits plays a central role in
vulnerability reproduction, comprehension, and mitigation. While previous
research has focused primarily on zero-day exploitation, the growing
availability of rich public information accompanying disclosed CVEs leads to a
natural question: can LLMs effectively use this information to automatically
generate valid PoCs? In this paper, we present the first empirical study of
LLM-based PoC generation for web application vulnerabilities, focusing on the
practical feasibility of leveraging publicly disclosed information. We evaluate
GPT-4o and DeepSeek-R1 on 100 real-world and reproducible CVEs across three
stages of vulnerability disclosure: (1) newly disclosed vulnerabilities with
only descriptions, (2) 1-day vulnerabilities with patches, and (3) N-day
vulnerabilities with full contextual code. Our results show that LLMs can
automatically generate working PoCs in 8%-34% of cases using only public data,
with DeepSeek-R1 consistently outperforming GPT-4o. Further analysis shows that
supplementing code context improves success rates by 17%-20%, with
function-level providing 9%-13% improvement than file-level ones. Further
integrating adaptive reasoning strategies to prompt refinement significantly
improves success rates to 68%-72%. Our findings suggest that LLMs could reshape
vulnerability exploitation dynamics. To date, 23 newly generated PoCs have been
accepted by NVD and Exploit DB.

</details>


### [11] [LLMs are All You Need? Improving Fuzz Testing for MOJO with Large Language Models](https://arxiv.org/abs/2510.10179)
*Linghan Huang,Peizhou Zhao,Huaming Chen*

Main category: cs.SE

TL;DR: MOJOFuzzer, an adaptive LLM-based fuzzing framework, addresses challenges in testing the new MOJO language by filtering poor test inputs and using runtime feedback to improve LLM generation. It enhances test validity and bug detection, finding 13 new MOJO bugs, and sets the stage for robust LLM-driven testing in emerging programming languages.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) have improved software testing automation, especially fuzz testing. However, the new MOJO programming language lacks adequate testing frameworks and a comprehensive code corpus. This deficiency causes model hallucination when LLMs generate MOJO code, resulting in semantically incorrect test cases that reduce fuzzing effectiveness.

Method: The paper introduces MOJOFuzzer, an adaptive LLM-based fuzzing framework tailored for MOJO and similar emerging languages where zero-shot learning is required. MOJOFuzzer uses a multi-phase approach to filter out low-quality test inputs before execution and adaptively refines prompts using runtime feedback, enabling iterative learning.

Result: Experiments show that MOJOFuzzer increases the validity of test cases, API coverage, and bug detection performance relative to both traditional fuzz testing and existing LLM-based fuzzers. It was used for a large-scale evaluation of MOJO, discovering 13 previously unknown bugs.

Conclusion: MOJOFuzzer marks a significant advance in applying LLMs to software testing for new languages. It improves fuzzing efficiency and reliability and provides a foundational approach for LLM-based testing of future programming languages.

Abstract: The rapid development of large language models (LLMs) has revolutionized
software testing, particularly fuzz testing, by automating the generation of
diverse and effective test inputs. This advancement holds great promise for
improving software reliability. Meanwhile, the introduction of MOJO, a
high-performance AI programming language blending Python's usability with the
efficiency of C and C++, presents new opportunities to enhance AI model
scalability and programmability. However, as a new language, MOJO lacks
comprehensive testing frameworks and a sufficient corpus for LLM-based testing,
which exacerbates model hallucination. In this case, LLMs will generate
syntactically valid but semantically incorrect code, significantly reducing the
effectiveness of fuzz testing. To address this challenge, we propose
MOJOFuzzer, the first adaptive LLM-based fuzzing framework designed for
zero-shot learning environments of emerging programming languages. MOJOFuzzer
integrates a mutil-phase framework that systematically eliminates low-quality
generated inputs before execution, significantly improving test case validity.
Furthermore, MOJOFuzzer dynamically adapts LLM prompts based on runtime
feedback for test case mutation, enabling an iterative learning process that
continuously enhances fuzzing efficiency and bug detection performance. Our
experimental results demonstrate that MOJOFuzzer significantly enhances test
validity, API coverage, and bug detection performance, outperforming
traditional fuzz testing and state-of-the-art LLM-based fuzzing approaches.
Using MOJOFuzzer, we have conducted a first large-scale fuzz testing evaluation
of MOJO, uncorvering 13 previous unknown bugs. This study not only advances the
field of LLM-driven software testing but also establishes a foundational
methodology for leveraging LLMs in the testing of emerging programming
languages.

</details>


### [12] [Grounded AI for Code Review: Resource-Efficient Large-Model Serving in Enterprise Pipelines](https://arxiv.org/abs/2510.10290)
*Sayan Mandal,Hua Jiang*

Main category: cs.SE

TL;DR: A modular, context-aware code review system using efficient LLM deployment achieves fast, accurate, and auditable remediation and explanation, reducing triage effort in compliance-driven environments.


<details>
  <summary>Details</summary>
Motivation: Automated code review tools are underutilized in compliance-heavy contexts due to overwhelming output from static analyzers and the shortcomings of naive LLM implementations, such as hallucinations and high costs. There is a need for more accurate, grounded, and efficient automated review systems.

Method: The paper introduces a production code review system that integrates static-analysis findings with context derived from AST (Abstract Syntax Tree) guided extraction. It features a lightweight on-demand serving stack (using quantized open-weight LLMs and multi-tier caching) to provide concise explanations and remediation advice. The architecture is modular, enabling independent adoption of prompting/grounding or serving layers.

Result: On safety-focused C/C++ standards, the system delivers sub-minute median first feedback (59.8s for build and LLM) with competitive or improved violation reduction when compared to larger proprietary models. Surveyed internal users (n=8) noted reduced triage effort, moderate perceived grounding, and fewer review cycles.

Conclusion: The proposed architecture effectively streamlines code review in compliance-oriented settings, balancing speed, cost, and output quality while offering modularity for incremental adoption. The authors detail operational lessons, stress reproducibility and auditability, and highlight remaining limitations and future directions for broader standard coverage and automated patching.

Abstract: Automated code review adoption lags in compliance-heavy settings, where
static analyzers produce high-volume, low-rationale outputs, and naive LLM use
risks hallucination and incurring cost overhead. We present a production system
for grounded, PR-native review that pairs static-analysis findings with
AST-guided context extraction and a single-GPU, on-demand serving stack
(quantized open-weight model, multi-tier caching) to deliver concise
explanations and remediation guidance. Evaluated on safety-oriented C/C++
standards, the approach achieves sub-minute median first-feedback (offline p50
build+LLM 59.8s) while maintaining competitive violation reduction and lower
violation rates versus larger proprietary models. The architecture is
decoupled: teams can adopt the grounding/prompting layer or the serving layer
independently. A small internal survey (n=8) provides directional signals of
reduced triage effort and moderate perceived grounding, with participants
reporting fewer human review iterations. We outline operational lessons and
limitations, emphasizing reproducibility, auditability, and pathways to broader
standards and assisted patching.

</details>


### [13] [Prepared for the Unknown: Adapting AIOps Capacity Forecasting Models to Data Changes](https://arxiv.org/abs/2510.10320)
*Lorena Poenaru-Olaru,Wouter van 't Hof,Adrian Stando,Arkadiusz P. Trawinski,Eileen Kapel,Jan S. Rellermeyer,Luis Cruz,Arie van Deursen*

Main category: cs.SE

TL;DR: Retrain forecasting models based on data changes (drift-based) to save computational resources without losing much accuracy, except when data shifts rapidly—then use regular (periodic) retraining for best results.


<details>
  <summary>Details</summary>
Motivation: Software organizations need to predict future resource needs for effective capacity management. Traditional approaches require frequent model retraining, which is costly and hard to scale. There is a need to identify when retraining is necessary without sacrificing accuracy.

Method: The paper compares two retraining strategies for time series forecasting models: drift-based retraining (triggered by data changes) versus periodic retraining (triggered at regular intervals). The impact of both methods on forecasting accuracy and computational cost is investigated empirically.

Result: Drift-based retraining achieves forecasting accuracy comparable to periodic retraining in most cases, offering significant computational savings. In scenarios with rapidly changing data, periodic retraining remains superior in accuracy.

Conclusion: Drift-based retraining is a cost-effective alternative without significant loss in accuracy for most cases, but periodic retraining should be used when data changes quickly. The findings can help software teams make more efficient retraining decisions for forecasting models.

Abstract: Capacity management is critical for software organizations to allocate
resources effectively and meet operational demands. An important step in
capacity management is predicting future resource needs often relies on
data-driven analytics and machine learning (ML) forecasting models, which
require frequent retraining to stay relevant as data evolves. Continuously
retraining the forecasting models can be expensive and difficult to scale,
posing a challenge for engineering teams tasked with balancing accuracy and
efficiency. Retraining only when the data changes appears to be a more
computationally efficient alternative, but its impact on accuracy requires
further investigation. In this work, we investigate the effects of retraining
capacity forecasting models for time series based on detected changes in the
data compared to periodic retraining. Our results show that drift-based
retraining achieves comparable forecasting accuracy to periodic retraining in
most cases, making it a cost-effective strategy. However, in cases where data
is changing rapidly, periodic retraining is still preferred to maximize the
forecasting accuracy. These findings offer actionable insights for software
teams to enhance forecasting systems, reducing retraining overhead while
maintaining robust performance.

</details>


### [14] [Bridging Semantics & Structure for Software Vulnerability Detection using Hybrid Network Models](https://arxiv.org/abs/2510.10321)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: The paper introduces a method that models code as graphs and uses efficient, local LLMs to jointly analyze structure and meaning for vulnerability detection. This approach achieves high accuracy, surpasses existing methods, and explains its findings, making it both practical and interpretable for secure software development.


<details>
  <summary>Details</summary>
Motivation: Traditional static and dynamic analysis methods for software vulnerability detection frequently miss the deeper structural dependencies and interactions in code that underpin insecure behavior. There is a need for techniques that can combine structural and semantic reasoning while being lightweight and privacy-preserving without relying on large cloud-based models.

Method: The authors propose a hybrid framework that models software as heterogeneous graphs to capture control- and data-flow dependencies. This representation is combined with small, locally run LLMs (less than 4 billion parameters) to integrate topological graph features with semantic analysis, allowing accurate and private vulnerability detection without expensive cloud resources.

Result: The proposed method achieves 93.57% accuracy in Java vulnerability detection, outperforming existing baselines by 8.36% over Graph Attention Network embeddings and 17.81% over pretrained large language models (LLMs) like Qwen2.5 Coder 3B. Additionally, the approach enhances interpretability by extracting significant subgraphs and producing natural language explanations for developers.

Conclusion: This novel framework demonstrates that combining graph-based structural analysis with efficient, local LLMs enables scalable, explainable, and privacy-preserving tools for software vulnerability detection. It paves the way for moving beyond simple syntactic checks toward deeper semantic understanding, fostering broader adoption in secure software engineering.

Abstract: Software vulnerabilities remain a persistent risk, yet static and dynamic
analyses often overlook structural dependencies that shape insecure behaviors.
Viewing programs as heterogeneous graphs, we capture control- and data-flow
relations as complex interaction networks. Our hybrid framework combines these
graph representations with light-weight (<4B) local LLMs, uniting topological
features with semantic reasoning while avoiding the cost and privacy concerns
of large cloud models. Evaluated on Java vulnerability detection (binary
classification), our method achieves 93.57% accuracy-an 8.36% gain over Graph
Attention Network-based embeddings and 17.81% over pretrained LLM baselines
such as Qwen2.5 Coder 3B. Beyond accuracy, the approach extracts salient
subgraphs and generates natural language explanations, improving
interpretability for developers. These results pave the way for scalable,
explainable, and locally deployable tools that can shift vulnerability analysis
from purely syntactic checks to deeper structural and semantic insights,
facilitating broader adoption in real-world secure software development.

</details>


### [15] [Testing and Enhancing Multi-Agent Systems for Robust Code Generation](https://arxiv.org/abs/2510.10460)
*Zongyi Lyu,Songqiang Chen,Zhenlan Ji,Liwen Wang,Shuai Wang,Daoyuan Wu,Wenxuan Wang,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: Despite strong performance, multi-agent systems for code generation have critical robustness issues, particularly due to miscommunication between agents. Using fuzzing-based testing, the authors show these systems often fail on mutated tasks. They propose an effective repair method that adds a monitor agent and multi-prompt generation, improving reliability. This research highlights overlooked weaknesses and proposes improvements for trustworthy MAS-based code generation.


<details>
  <summary>Details</summary>
Motivation: While multi-agent systems (MASs) have shown significant progress in automated code generation, their robustness—important for real-world applications—remains insufficiently studied. This paper is motivated by the need to understand and improve the reliability of MASs when deployed in practical coding tasks.

Method: The study uses a fuzzing-based testing approach employing semantic-preserving mutation operators and a novel fitness function. This methodology is applied across several mainstream MASs, different datasets, and language models. Additionally, the authors propose a repairing method that uses multi-prompt generation and a new monitor agent to address identified failures.

Result: The results demonstrate that popular MASs display substantial robustness flaws, as they fail to solve 7.9%-83.3% of previously solved problems after semantic-preserving mutations are applied. Comprehensive analysis points to miscommunication between planning and coding agents as a major cause. The proposed repairing method improves system robustness, resolving 40.0%-88.9% of previously identified failures.

Conclusion: This work identifies significant robustness gaps in current MASs for code generation and offers an effective mitigation strategy. Introducing a new monitor agent and multi-prompt generation substantially enhances MAS robustness, providing valuable directions for future development of reliable code generation systems.

Abstract: Multi-agent systems (MASs) have emerged as a promising paradigm for automated
code generation, demonstrating impressive performance on established benchmarks
by decomposing complex coding tasks across specialized agents with different
roles. Despite their prosperous development and adoption, their robustness
remains pressingly under-explored, raising critical concerns for real-world
deployment. This paper presents the first comprehensive study examining the
robustness of MASs for code generation through a fuzzing-based testing
approach. By designing a fuzzing pipeline incorporating semantic-preserving
mutation operators and a novel fitness function, we assess mainstream MASs
across multiple datasets and LLMs. Our findings reveal substantial robustness
flaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they
initially resolved successfully after applying the semantic-preserving
mutations. Through comprehensive failure analysis, we identify a common yet
largely overlooked cause of the robustness issue: miscommunications between
planning and coding agents, where plans lack sufficient detail and coding
agents misinterpret intricate logic, aligning with the challenges inherent in a
multi-stage information transformation process. Accordingly, we also propose a
repairing method that encompasses multi-prompt generation and introduces a new
monitor agent to address this issue. Evaluation shows that our repairing method
effectively enhances the robustness of MASs by solving 40.0%-88.9% of
identified failures. Our work uncovers critical robustness flaws in MASs and
provides effective mitigation strategies, contributing essential insights for
developing more reliable MASs for code generation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Herb.jl: A Unifying Program Synthesis Library](https://arxiv.org/abs/2510.09726)
*Tilman Hinnerichs,Reuben Gardos Reid,Jaap de Jong,Bart Swinkels,Pamela Wochner,Nicolae Filat,Tudor Magurescu,Issa Hanou,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: Herb.jl is a modular Julia library designed to unify and simplify program synthesis. It allows easier reuse, remixing, and benchmarking of synthesis methods, addressing inefficiencies in existing tools and demonstrating practical benefits through key use cases.


<details>
  <summary>Details</summary>
Motivation: Existing program synthesis tools are difficult to reuse and remix due to their monolithic structures, leading to tedious and time-consuming development. Since many synthesis methods rely on similar algorithmic blocks, there is a need for a unified, modular library to accelerate and simplify tool development.

Method: Herb.jl is implemented in Julia, comprising modular and extendable compartments for synthesis algorithms. The paper demonstrates its utility through use cases: implementing simple problems and grammars, recreating synthesizers with minimal code, and benchmarking synthesizer performance.

Result: Herb.jl enables users to quickly reimplement synthesizers, solve specification problems with minimal setup, and conduct benchmarking studies, showing improved usability and extensibility over previous approaches.

Conclusion: Herb.jl streamlines and unifies program synthesis tasks by modularizing core synthesis components, making reuse and extension easier and more efficient.

Abstract: Program synthesis -- the automatic generation of code given a specification
-- is one of the most fundamental tasks in artificial intelligence (AI) and
many programmers' dream. Numerous synthesizers have been developed to tackle
program synthesis, manifesting different ideas to approach the exponentially
growing program space. While numerous smart program synthesis tools exist,
reusing and remixing previously developed methods is tedious and
time-consuming. We propose Herb.jl, a unifying program synthesis library
written in the Julia programming language, to address these issues. Since
current methods rely on similar building blocks, we aim to modularize the
underlying synthesis algorithm into communicating and fully extendable
sub-compartments, allowing for straightforward reapplication of these modules.
To demonstrate the benefits of using Herb.jl, we show three common use cases:
1. how to implement a simple problem and grammar, and how to solve it, 2. how
to implement a previously developed synthesizer with just a few lines of code,
and 3. how to run a synthesizer against a benchmark.

</details>


### [17] [ACT: Automatically Generating Compiler Backends from Tensor Accelerator ISA Descriptions](https://arxiv.org/abs/2510.09932)
*Devansh Jain,Akash Pardeshi,Marco Frigo,Krut Patel,Kaustubh Khulbe,Jai Arora,Charith Mendis*

Main category: cs.PL

TL;DR: The paper presents ACT, an automated tool for generating high-quality compiler backends for tensor accelerators from just their ISA descriptions. ACT's generated backends are both correct and performant, matching or surpassing hand-tuned code and greatly streamlining software development for emerging accelerator designs.


<details>
  <summary>Details</summary>
Motivation: Most new tensor accelerators lack dedicated compiler backend support, making software development for them challenging and slow. Manual development of these backends is labor-intensive and can't keep up with rapid hardware iteration cycles. There's a need for an automated, agile solution to support fast and efficient code generation for novel accelerators.

Method: The authors introduce ACT, a compiler backend generator that automatically produces compiler backends for tensor accelerators using only their ISA descriptions. ACT introduces a novel ISA specification, employs parameterized equality saturation for instruction selection, and utilizes constraint programming for memory allocation. The generated backends are formally proven to be sound and complete.

Result: ACT successfully generated compiler backends for three different accelerator platforms from both industry and academia. The resulting backends achieved performance on par with or better than hand-optimized kernel libraries, while keeping compilation overheads low.

Conclusion: ACT automates the generation of high-performance and correct compiler backends for tensor accelerators, significantly accelerating software development for new hardware and closing the gap between hardware innovation and usable software ecosystems.

Abstract: Tensor compilers play a key role in enabling high-performance implementations
of deep learning workloads. These compilers rely on existing CPU and GPU code
generation backends to generate device-specific code. Recently, many tensor
accelerators (neural processing units) have been proposed to further accelerate
these workloads. Compared to commodity hardware, however, most of the proposed
tensor accelerators do not have compiler backends with code generation support.
Moreover, the accelerator designs are subject to fast iteration cycles, making
it difficult to manually develop compiler backends similar to commodity
hardware platforms. Therefore, to increase adoption and enable faster software
development cycles for novel tensor accelerator designs, we need to make the
compiler backend construction process more agile.
  To address this gap, we introduce ACT, a compiler backend generator that
automatically generates compiler backends for tensor accelerators, given just
the instruction set architecture (ISA) descriptions. We first formally specify
the compiler backend generation problem that introduces a novel specification
for describing tensor accelerator ISAs. Next, we design ACT such that it
supports user-programmable memories and complex parameterized instructions that
are prevalent in tensor accelerators. ACT uses a novel parameterized equality
saturation-based instruction selection phase and a constraint programming-based
memory allocation phase. We prove that compiler backends generated by ACT are
sound and complete. Finally, we generate compiler backends for three
accelerator platforms from industry and academia, and show that they match or
outperform code written using hand-optimized kernel libraries while maintaining
low compilation overheads.

</details>


### [18] [End-to-end Compositional Verification of Program Safety through Verified and Verifying Compilation](https://arxiv.org/abs/2510.10015)
*Jinhua Wu,Yuting Wang,Liukun Yu,Linglong Meng*

Main category: cs.PL

TL;DR: The paper tackles end-to-end safety verification in modern safe languages like Rust, proposing a new modular safety concept ('open safety') that supports compositional verification across safe and unsafe modules, and demonstrates its practicality with a prototype language and real-world example.


<details>
  <summary>Details</summary>
Motivation: Modern safe programming languages like Rust require mixing safe and unsafe modules for full functionality, but current verification and compilation approaches struggle to ensure end-to-end safety in such modular, heterogeneous environments.

Method: The authors introduce a modular and generic definition of safety—called open safety—framed by open labeled transition systems (LTS) for program semantics. This approach enables safety properties to be composable and modularly preserved through verified compositional compilation. The framework is demonstrated by developing a verified compiler for an ownership-based language (Owlang) and is evaluated using a hash map implemented in Owlang and C.

Result: The proposed framework enables the separate verification of safety for heterogeneous modules and composes the safety results at the target (compiled) level. Open safety also allows for generalization to partial safety. The verified compiler for Owlang shows the practical applicability of their approach in compositional safety verification.

Conclusion: The paper introduces open safety to support compositional, modular, and end-to-end safety verification for programs mixing safe and unsafe modules, bridging gaps between verified and verifying compilation approaches. The approach has been validated with a prototype compiler and use case involving Owlang and C.

Abstract: Program safety (i.e., absence of undefined behaviors) is critical for correct
operation of computer systems. It is usually verified at the source level
(e.g., by separation logics) and preserved to the target by verified compilers
(e.g., CompCert), thereby achieving end-to-end verification of safety. However,
modern safe programming languages like Rust pose new problems in achieving
end-to-end safety. Because not all functionalities can be implemented in the
safe language, mixing safe and unsafe modules is needed. Therefore, verified
compilation must preserve a modular notion of safety which can be composed at
the target level. Furthermore, certain classes of errors (e.g., memory errors)
are automatically excluded by verifying compilation (e.g., borrow checking) for
modules written in safe languages. As a result, verified compilation needs to
cooperate with verifying compilation to ensure end-to-end safety.
  To address the above problems, we propose a modular and generic definition of
safety called open safety based on program semantics described as open labeled
transition systems (LTS). Open safety is composable at the boundary of modules
and can be modularly preserved by verified compositional compilation. Those
properties enable separate verification of safety for heterogeneous modules and
composition of the safety results at the target level. Open safety can be
generalized to partial safety (i.e., only a certain class of errors can occur).
By this we formalized the correctness of verifying compilation as derivation of
total safety from partial safety. We demonstrate how our framework can combine
verified and verifying compilation by developing a verified compiler for an
ownership language (called Owlang) inspired by Rust. We evaluate our approach
on the compositional safety verification using a hash map implemented by Owlang
and C.

</details>


### [19] [LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization](https://arxiv.org/abs/2510.10209)
*Massinissa Merouani,Afif Boudaoud,Riyadh Baghdadi*

Main category: cs.PL

TL;DR: The paper introduces LOOPerSet, a massive and publicly available dataset for polyhedral compiler optimization research, enabling easier, reproducible, and data-driven advancements in the field.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address the lack of large-scale, public performance datasets in the area of machine learning-driven compiler optimization, especially within the polyhedral model. This scarcity hampers innovation and reproducibility in research.

Method: The authors create and release LOOPerSet, a new public dataset that consists of 28 million labeled data points from 220,000 unique, synthetically generated polyhedral programs. For each program, a set of semantics-preserving transformation sequences is mapped to ground truth execution times.

Result: LOOPerSet provides extensive, diverse data for training and evaluating cost models, benchmarking new architectures, and exploring automated polyhedral scheduling in compilers.

Conclusion: LOOPerSet significantly contributes to the field of data-driven compiler optimization by providing a large, permissively licensed dataset, supporting reproducible research and lowering barriers for further innovations in polyhedral scheduling and optimization.

Abstract: The advancement of machine learning for compiler optimization, particularly
within the polyhedral model, is constrained by the scarcity of large-scale,
public performance datasets. This data bottleneck forces researchers to
undertake costly data generation campaigns, slowing down innovation and
hindering reproducible research learned code optimization. To address this gap,
we introduce LOOPerSet, a new public dataset containing 28 million labeled data
points derived from 220,000 unique, synthetically generated polyhedral
programs. Each data point maps a program and a complex sequence of
semantics-preserving transformations (such as fusion, skewing, tiling, and
parallelism)to a ground truth performance measurement (execution time). The
scale and diversity of LOOPerSet make it a valuable resource for training and
evaluating learned cost models, benchmarking new model architectures, and
exploring the frontiers of automated polyhedral scheduling. The dataset is
released under a permissive license to foster reproducible research and lower
the barrier to entry for data-driven compiler optimization.

</details>


### [20] [Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis](https://arxiv.org/abs/2510.10216)
*Zhechong Huang,Zhao Zhang,Ruyi Ji,Tingxuan Xia,Qihao Zhu,Qinxiang Cao,Zeyu Sun,Yingfei Xiong*

Main category: cs.PL

TL;DR: TyFlow helps language models generate code with fewer type errors by teaching them to reason about types internally, resulting in better and more correct code.


<details>
  <summary>Details</summary>
Motivation: Despite strong code generation abilities of language models, ensuring type correctness is still challenging. Existing approaches only externally filter out type errors, and do not help the model learn type reasoning natively.

Method: The paper introduces TyFlow, a system that integrates type reasoning directly into code generation. TyFlow uses a novel type-guided program synthesis technique, establishing an isomorphism between type derivation trees and synthesis derivation trees. This results in a new code representation guided by synthesis decisions instead of regular token sequences.

Result: TyFlow eliminates type errors and significantly enhances the functional correctness of code generated by language models.

Conclusion: Aligning language models with type systems internally, as with TyFlow, leads to better type correctness and functional quality in code generation. This approach allows language models to learn and use type reasoning as part of their generation process.

Abstract: Language models have shown remarkable proficiency in code generation;
nevertheless, ensuring type correctness remains a challenge. Although
traditional methods, such as constrained decoding, alleviate this problem by
externally rejecting untypable code, the model itself does not effectively
learn type reasoning internally, which ultimately limits its overall
performance. This paper introduces TyFlow, a novel system that internalizes
type reasoning within code generation to guide the model to learn the type
system. The core of our approach is a novel type-guided program synthesis
system that maintains an isomorphism between type derivation trees and
synthesis derivation trees, enabling a new code representation based on
synthesis decision sequences rather than traditional text-based token
sequences. By offloading the complexity of type system learning to the
representation itself, models can redirect their computational resources toward
higher-level program semantics. Our evaluation shows that TyFlow not only
eliminates type errors but also significantly improves functional correctness,
highlighting the importance of aligning LMs with type systems internally.

</details>


### [21] [Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc](https://arxiv.org/abs/2510.10219)
*Ruihao Li,Lizy K. John,Neeraja J. Yadwadkar*

Main category: cs.PL

TL;DR: Exgen-Malloc is a new memory allocator tailored for single-threaded applications, delivering significant speed and memory savings compared to legacy allocators by cutting unnecessary complexity and leveraging modern design principles.


<details>
  <summary>Details</summary>
Motivation: Memory allocators are crucial for application performance, but their inefficiencies at datacenter scale can result in significant costs and energy consumption. Most allocators are designed for multi-threaded environments and introduce complexity that may be unnecessary for single-threaded applications, which remain common.

Method: The paper introduces Exgen-Malloc, a memory allocator specialized for single-threaded applications. The design eliminates unnecessary metadata, uses a centralized heap with one free-block list, and balances memory commitment and relocation. It also borrows principles from modern multi-threaded allocators to improve upon legacy single-threaded designs. The authors evaluate Exgen-Malloc on Intel Xeon platforms using SPEC CPU2017, redis-benchmark, and mimalloc-bench.

Result: Exgen-Malloc achieved speedups of 1.17x, 1.10x, and 1.93x over dlmalloc and memory savings of 6.2%, 0.1%, and 25.2% over mimalloc, across various benchmarks.

Conclusion: Exgen-Malloc provides a more efficient memory allocation solution for single-threaded applications, reducing both performance overhead and memory usage compared to existing allocators.

Abstract: Memory allocators hide beneath nearly every application stack, yet their
performance footprint extends far beyond their code size. Even small
inefficiencies in the allocators ripple through caches and the rest of the
memory hierarchy, collectively imposing what operators often call a "datacenter
tax". At hyperscale, even a 1% improvement in allocator efficiency can unlock
millions of dollars in savings and measurable reductions in datacenter energy
consumption. Modern memory allocators are designed to optimize allocation speed
and memory fragmentation in multi-threaded environments, relying on complex
metadata and control logic to achieve high performance. However, the overhead
introduced by this complexity prompts a reevaluation of allocator design.
Notably, such overhead can be avoided in single-threaded scenarios, which
continue to be widely used across diverse application domains.
  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built
for single-threaded applications. By specializing for single-threaded
execution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control
flow, thereby reducing overhead and improving allocation efficiency. Its core
design features include a centralized heap, a single free-block list, and a
balanced strategy for memory commitment and relocation. Additionally,
Exgen-Malloc incorporates design principles in modern multi-threaded
allocators, which do not exist in legacy single-threaded allocators such as
dlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both
systems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over
dlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In
addition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory
savings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,
respectively.

</details>


### [22] [A Trace-based Approach for Code Safety Analysis](https://arxiv.org/abs/2510.10410)
*Hui Xu*

Main category: cs.PL

TL;DR: This paper analyzes real-world Rust projects to build a framework for understanding unsafe code and undefined behavior, summarizes soundness criteria, and offers practical guidance for safely using and encapsulating unsafe code in Rust.


<details>
  <summary>Details</summary>
Motivation: Rust offers strong memory safety guarantees by preventing undefined behavior, which has led to widespread adoption and success. However, the use of 'unsafe' code sections remains a concern, as they can potentially violate Rust's safety promises. This paper is motivated by the need to systematically understand and address the challenges posed by unsafe code in Rust.

Method: The authors review Rust's safety design and conduct an analysis of real-world Rust projects. They establish a structured framework for understanding the concepts of unsafe code and undefined behavior within Rust, and synthesize criteria for code soundness based on this analysis.

Result: The paper provides a systematic framework for interpreting unsafe code and undefined behavior in Rust. It also summarizes the soundness criteria that Rust code should meet, and offers practical guidance for safely encapsulating unsafe code.

Conclusion: Unsafe code is a persistent issue in Rust that requires thorough scrutiny. The proposed framework and derived guidelines assist developers in achieving safe and robust encapsulation of unsafe code, contributing to overall program soundness.

Abstract: Rust is a memory-safe programming language that disallows undefined behavior.
Its safety guarantees have been extensively examined by the community through
empirical studies, which has led to its remarkable success. However, unsafe
code remains a critical concern in Rust. By reviewing the safety design of Rust
and analyzing real-world Rust projects, this paper establishes a systematic
framework for understanding unsafe code and undefined behavior, and summarizes
the soundness criteria for Rust code. It further derives actionable guidance
for achieving sound encapsulation.

</details>


### [23] [ECO: Enhanced Code Optimization via Performance-Aware Prompting for Code-LLMs](https://arxiv.org/abs/2510.10517)
*Su-Hyeon Kim,Joonghyuk Hahn,Sooyoung Cha,Yo-Sub Han*

Main category: cs.PL

TL;DR: ECO is a new performance-aware prompting framework for code optimization that guides code-LLMs to generate faster, efficient code by focusing on root causes of inefficiency, showing significant improvements without model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Code runtime optimization is challenging because it requires understanding complex trade-offs and reasoning about performance rather than simply copying patterns from faster code versions. Existing code-LLM optimization methods often miss the underlying reasons for performance improvements.

Method: ECO is introduced as a performance-aware prompting framework. It works by (1) distilling Runtime Optimization Instructions (ROIs) from slow-fast code pairs, explaining root causes of inefficiency and rationales for improvement; (2) using a symbolic advisor to diagnose code bottlenecks; (3) retrieving relevant ROIs; and (4) composing a model-agnostic, actionable prompt for code-LLMs without fine-tuning.

Result: ECO prompting enables code-LLMs to generate more efficient code in empirical studies, yielding speedups up to 7.81x with low correctness loss.

Conclusion: ECO provides a practical, model-agnostic, and effective approach to boost code-LLMs in generating optimized code, outperforming traditional pair-based optimization prompts by fostering real performance reasoning.

Abstract: Code runtime optimization-the task of rewriting a given code to a faster
one-remains challenging, as it requires reasoning about performance trade-offs
involving algorithmic and structural choices. Recent approaches employ
code-LLMs with slow-fast code pairs provided as optimization guidance, but such
pair-based methods obscure the causal factors of performance gains and often
lead to superficial pattern imitation rather than genuine performance
reasoning. We introduce ECO, a performance-aware prompting framework for code
optimization. ECO first distills runtime optimization instructions (ROIs) from
reference slow-fast code pairs; Each ROI describes root causes of inefficiency
and the rationales that drive performance improvements. For a given input code,
ECO in parallel employs (i) a symbolic advisor to produce a bottleneck
diagnosis tailored to the code, and (ii) an ROI retriever to return related
ROIs. These two outputs are then composed into a performance-aware prompt,
providing actionable guidance for code-LLMs. ECO's prompts are model-agnostic,
require no fine-tuning, and can be easily prepended to any code-LLM prompt. Our
empirical studies highlight that ECO prompting significantly improves
code-LLMs' ability to generate efficient code, achieving speedups of up to
7.81x while minimizing correctness loss.

</details>


### [24] [A Verified High-Performance Composable Object Library for Remote Direct Memory Access (Extended Version)](https://arxiv.org/abs/2510.10531)
*Guillaume Ambal,George Hodgkins,Mark Madler,Gregory Chockler,Brijesh Dongol,Joseph Izraelevitz,Azalea Raad,Viktor Vafeiadis*

Main category: cs.PL

TL;DR: The paper introduces LOCO, a formally verified library for building efficient, multi-node abstractions on RDMA. It leverages a new verification framework, Mowgli, to ensure correctness even with RDMA's challenging memory model, achieving simplicity and high performance.


<details>
  <summary>Details</summary>
Motivation: RDMA offers high performance for distributed computing but suffers from a weak, permissive memory model that is difficult to use and has only recently been formalized. There is a gap in practical, verified tools to build reliable, high-level abstractions (objects) over RDMA, bridging shared memory and distributed system programming paradigms.

Method: The paper introduces LOCO, a formally verified library for building multi-node objects on RDMA, and a novel modular declarative verification framework called Mowgli. Mowgli can model multi-node objects and is memory-consistency-model independent. It is instantiated with the RDMA memory model and used to verify LOCO.

Result: LOCO provides well-encapsulated RDMA objects that leverage both locality and the weak consistency of RDMA. LOCO achieves performance comparable to custom RDMA systems, such as distributed maps, while being easier to program and formally verify for correctness. Mowgli successfully enables verification of LOCO even under RDMA’s weak memory model.

Conclusion: The paper demonstrates that formally verified, high-level, composable abstractions for RDMA are possible without sacrificing performance. Their approach fills the gap between shared memory and distributed system programming and provides a usable, modular, and verifiable foundation for RDMA-based applications.

Abstract: Remote Direct Memory Access (RDMA) is a memory technology that allows remote
devices to directly write to and read from each other's memory, bypassing
components such as the CPU and operating system. This enables low-latency
high-throughput networking, as required for many modern data centres, HPC
applications and AI/ML workloads. However, baseline RDMA comprises a highly
permissive weak memory model that is difficult to use in practice and has only
recently been formalised. In this paper, we introduce the Library of Composable
Objects (LOCO), a formally verified library for building multi-node objects on
RDMA, filling the gap between shared memory and distributed system programming.
LOCO objects are well-encapsulated and take advantage of the strong locality
and the weak consistency characteristics of RDMA. They have performance
comparable to custom RDMA systems (e.g. distributed maps), but with a far
simpler programming model amenable to formal proofs of correctness. To support
verification, we develop a novel modular declarative verification framework,
called Mowgli, that is flexible enough to model multinode objects and is
independent of a memory consistency model. We instantiate Mowgli with the RDMA
memory model, and use it to verify correctness of LOCO libraries.

</details>


### [25] [Abstract String Domain Defined with Word Equations as a Reduced Product (Extended Version)](https://arxiv.org/abs/2510.11007)
*Antonina Nepeivoda,Ilya Afanasyev*

Main category: cs.PL

TL;DR: The paper presents a new abstract domain for static analysis of string-manipulating programs by modeling string values via intervals defined with word equations and disequalities. It defines a novel lattice-based structure and efficient operations, showing practical use in analyzing JavaScript string manipulations.


<details>
  <summary>Details</summary>
Motivation: There is a need for precise and efficient static analysis methods for reasoning about string values in programs, particularly for languages like JavaScript where string manipulation is pervasive and complex.

Method: The paper introduces a string-interval abstract domain, representing string intervals using word equations (for lower bounds) and word disequalities (for upper bounds). The authors define a reduced product construction on a string property semilattice using length-non-increasing morphisms. They explore several reduction strategies for these abstract string objects, analyze the lattice structure, and define abstract operations to support the analysis with minimal computational overhead.

Result: They show that the proposed abstract string object domain, under their reduction strategies, forms a lattice. The basic abstract string operations are defined effectively to keep the reduction process computationally efficient. They demonstrate that the domain is applicable for analyzing properties of JavaScript string manipulating programs.

Conclusion: The string-interval abstract domain provides a theoretically sound and computationally efficient framework for static analysis of programs with complex string operations, with demonstrated applicability to JavaScript.

Abstract: We introduce a string-interval abstract domain, where string intervals are
characterized by systems of word equations (encoding lower bounds on string
values) and word disequalities (encoding upper bounds). Building upon the
lattice structure of string intervals, we define an abstract string object as a
reduced product on a string property semilattice, determined by
length-non-increasing morphisms. We consider several reduction strategies for
abstract string objects and show that upon these strategies the string object
domain forms a lattice. We define basic abstract string operations on the
domain, aiming to minimize computational overheads on the reduction, and show
how the domain can be used to analyse properties of JavaScript string
manipulating programs.

</details>


### [26] [HUGR: A Quantum-Classical Intermediate Representation](https://arxiv.org/abs/2510.11420)
*Mark Koch,Agustín Borgna,Seyon Sivarajah,Alan Lawrence,Alec Edgington,Douglas Wilson,Craig Roy,Luca Mondada,Lukas Heidemann,Ross Duncan*

Main category: cs.PL

TL;DR: HUGR is a new, open graph-based IR for quantum-classical programming, designed for extensibility, safety, and powerful compilation, ensuring adaptability to evolving quantum technologies and paradigms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a new intermediate representation (IR) capable of supporting modern quantum-classical programming needs, accommodating the evolving abstractions and requirements of current and future quantum hardware.

Method: The authors introduce Hierarchical Unified Graph Representation (HUGR), a graph-based IR designed for mixed quantum-classical programs. It borrows ideas from MLIR, focusing on extensibility, safety, and powerful compilation through pattern matching. The design includes strict, static typing and linear quantum types to ensure program safety.

Result: HUGR is shown to be highly expressive and extensible, supporting various abstraction levels and easing the development of compilation tools. Safety is enhanced through typing rules, and a reference implementation is openly available.

Conclusion: HUGR provides a promising solution for the intermediate representation of quantum-classical programs, bridging the gap between current compilation needs and the challenges posed by new quantum programming paradigms.

Abstract: We introduce the Hierarchical Unified Graph Representation (HUGR): a novel
graph based intermediate representation for mixed quantum-classical programs.
HUGR's design features high expressivity and extensibility to capture the
capabilities of near-term and forthcoming quantum computing devices, as well as
new and evolving abstractions from novel quantum programming paradigms. The
graph based structure is machine-friendly and supports powerful pattern
matching based compilation techniques. Inspired by MLIR, HUGR's extensibility
further allows compilation tooling to reason about programs at multiple levels
of abstraction, lowering smoothly between them. Safety guarantees in the
structure including strict, static typing and linear quantum types allow rapid
development of compilation tooling without fear of program invalidation. A full
specification of HUGR and reference implementation are open-source and
available online.

</details>


### [27] [(Dis)Proving Spectre Security with Speculation-Passing Style](https://arxiv.org/abs/2510.11573)
*Santiago Arranz-Olmos,Gilles Barthe,Lionel Blatter,Xingyu Xie,Zhiyuan Zhang*

Main category: cs.PL

TL;DR: The paper provides a formal and practical method (Speculation-Passing Style) for verifying speculative constant-time security using existing constant-time verification tools, showing soundness and completeness, and demonstrating effectiveness on Spectre benchmarks.


<details>
  <summary>Details</summary>
Motivation: Speculative constant-time (SCT) analysis is crucial for detecting Spectre vulnerabilities in cryptographic software, but existing SCT tools are mostly informal extensions of constant-time (CT) verification tools with little formal foundation or analysis.

Method: The paper introduces the Speculation-Passing Style (SPS) program transformation that converts SCT verification problems into CT verification problems. SPS instruments programs to model attacker-controlled predictions, allowing existing CT verification methodologies to cover SCT. The approach is combined with tools such as EasyCrypt, BINSEC, and ctgrind and tested on Spectre-v1 benchmarks.

Result: The SPS transformation is shown to be both sound and complete: SCT in the original program exactly matches CT in the transformed program. This allows existing CT verification tools to be repurposed for SCT. The integrations are demonstrated using EasyCrypt, BINSEC, and ctgrind on Spectre-v1 benchmarks, with discussions of broader applicability to other Spectre variants and leakage models.

Conclusion: Formalizing the lifting from CT to SCT tool verification via SPS enables practical, precise, and reusable analyses of speculative vulnerabilities, using established CT methods and tools. This advances both the theory and practice of cryptographic side-channel security.

Abstract: Constant-time (CT) verification tools are commonly used for detecting
potential side-channel vulnerabilities in cryptographic libraries. Recently, a
new class of tools, called speculative constant-time (SCT) tools, has also been
used for detecting potential Spectre vulnerabilities. In many cases, these SCT
tools have emerged as liftings of CT tools. However, these liftings are seldom
defined precisely and are almost never analyzed formally. The goal of this
paper is to address this gap, by developing formal foundations for these
liftings, and to demonstrate that these foundations can yield practical
benefits.
  Concretely, we introduce a program transformation, coined Speculation-Passing
Style (SPS), for reducing SCT verification to CT verification. Essentially, the
transformation instruments the program with a new input that corresponds to
attacker-controlled predictions and modifies the program to follow them. This
approach is sound and complete, in the sense that a program is SCT if and only
if its SPS transform is CT. Thus, we can leverage existing CT verification
tools to prove SCT; we illustrate this by combining SPS with three standard
methodologies for CT verification, namely reducing it to non-interference,
assertion safety and dynamic taint analysis. We realize these combinations with
three existing tools, EasyCrypt, BINSEC, and ctgrind, and we evaluate them on
Kocher's benchmarks for Spectre-v1. Our results focus on Spectre-v1 in the
standard CT leakage model; however, we also discuss applications of our method
to other variants of Spectre and other leakage models.

</details>
