{"id": "2510.14558", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.14558", "abs": "https://arxiv.org/abs/2510.14558", "authors": ["Amir Mohammad Fadaei Ayyam", "Michael Sammler"], "title": "HITrees: Higher-Order Interaction Trees", "comment": null, "summary": "Recent years have witnessed the rise of compositional semantics as a\nfoundation for formal verification of complex systems. In particular,\ninteraction trees have emerged as a popular denotational semantics. Interaction\ntrees achieve compositionality by providing a reusable library of effects.\nHowever, their notion of effects does not support higher-order effects, i.e.,\neffects that take or return monadic computations. Such effects are essential to\nmodel complex semantic features like parallel composition and call/cc.\n  We introduce Higher-Order Interaction Trees (HITrees), the first variant of\ninteraction trees to support higher-order effects in a non-guarded type theory.\nHITrees accomplish this through two key techniques: first, by designing the\nnotion of effects such that the fixpoints of effects with higher-order input\ncan be expressed as inductive types inside the type theory; and second, using\ndefunctionalization to encode higher-order outputs into a first-order\nrepresentation. We implement HITrees in the Lean proof assistant, accompanied\nby a comprehensive library of effects including concurrency, recursion, and\ncall/cc. Furthermore, we provide two interpretations of HITrees, as state\ntransition systems and as monadic programs. To demonstrate the expressiveness\nof HITrees, we apply them to define the semantics of a language with parallel\ncomposition and call/cc.", "AI": {"tldr": "This paper introduces Higher-Order Interaction Trees (HITrees) for supporting higher-order effects in compositional semantics, implemented in Lean and demonstrated on complex language features like parallelism and call/cc.", "motivation": "Existing interaction trees lack support for higher-order effects, which are crucial for modeling advanced semantic features like parallel composition and call/cc in formal verification.", "method": "The authors introduce Higher-Order Interaction Trees (HITrees), employing two main techniques: (1) redesigning effect notions so that fixpoints of higher-order input effects can be expressed as inductive types, and (2) using defunctionalization to encode higher-order outputs as first-order representations. They implement HITrees in Lean and provide a comprehensive effects library.", "result": "HITrees successfully support higher-order effects in a non-guarded type theory, enabling modeling of concurrency, recursion, and call/cc. The authors also provide state transition system and monadic program interpretations of HITrees.", "conclusion": "HITrees extend interaction trees to handle higher-order effects, enhancing their applicability for formal semantics involving advanced features such as parallel composition and call/cc. The Lean implementation and library demonstrate HITrees\u2019 expressiveness and practical utility."}}
{"id": "2510.14279", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.14279", "abs": "https://arxiv.org/abs/2510.14279", "authors": ["Evangelos Lamprou", "Seong-Heon Jung", "Mayank Keoliya", "Lukas Lazarek", "Konstantinos Kallas", "Michael Greenberg", "Nikos Vasilakis"], "title": "Caruca: Effective and Efficient Specification Mining for Opaque Software Components", "comment": null, "summary": "A wealth of state-of-the-art systems demonstrate impressive improvements in\nperformance, security, and reliability on programs composed of opaque\ncomponents, such as Unix shell commands. To reason about commands, these\nsystems require partial specifications. However, creating such specifications\nis a manual, laborious, and error-prone process, limiting the practicality of\nthese systems. This paper presents Caruca, a system for automatic specification\nmining for opaque commands. To overcome the challenge of language diversity\nacross commands, Caruca first instruments a large language model to translate a\ncommand's user-facing documentation into a structured invocation syntax. Using\nthis representation, Caruca explores the space of syntactically valid command\ninvocations and execution environments. Caruca concretely executes each\ncommand-environment pair, interposing at the system-call and filesystem level\nto extract key command properties such as parallelizability and filesystem pre-\nand post-conditions. These properties can be exported in multiple specification\nformats and are immediately usable by existing systems. Applying Caruca across\n60 GNU Coreutils, POSIX, and third-party commands across several\nspecification-dependent systems shows that Caruca generates correct\nspecifications for all but one case, completely eliminating manual effort from\nthe process and currently powering the full specifications for a\nstate-of-the-art static analysis tool.", "AI": {"tldr": "Manual specification creation for opaque system commands is a major bottleneck for advanced analysis tools. Caruca automates this process using large language models and dynamic execution, producing highly accurate specifications and significantly reducing manual effort.", "motivation": "State-of-the-art systems can improve performance, security, and reliability for programs using opaque commands (like Unix shell commands), but they require partial specifications, which are hard and tedious to create manually.", "method": "Caruca leverages a large language model to translate user documentation into a structured syntax. It systematically generates and executes command invocations in various environments, then monitors system calls and filesystem changes to extract command properties, automating specification mining.", "result": "Caruca was applied to 60 different commands (including GNU Coreutils, POSIX, and third-party ones). It generated correct specifications for all but one case, fully automating specification extraction and supporting a state-of-the-art static analysis tool.", "conclusion": "Caruca effectively eliminates manual specification mining for opaque commands by automatically generating usable specifications with high accuracy, enhancing the practicality of specification-dependent system tools."}}
{"id": "2510.13857", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13857", "abs": "https://arxiv.org/abs/2510.13857", "authors": ["Qiang Xu", "Xiangyu Wen", "Changran Xu", "Zeju Li", "Jianyuan Zhong"], "title": "From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering", "comment": null, "summary": "The advent of powerful Large Language Models (LLMs) has ushered in an ``Age\nof the Agent,'' enabling autonomous systems to tackle complex goals. However,\nthe transition from prototype to production is hindered by a pervasive ``crisis\nof craft,'' resulting in agents that are brittle, unpredictable, and ultimately\nuntrustworthy in mission-critical applications. This paper argues this crisis\nstems from a fundamental paradigm mismatch -- attempting to command inherently\nprobabilistic processors with the deterministic mental models of traditional\nsoftware engineering. To solve this crisis, we introduce a governance-first\nparadigm for principled agent engineering, embodied in a formal architecture we\ncall ArbiterOS.", "AI": {"tldr": "Current LLM-based agents are unreliable due to a mismatch between how they work and how we try to control them. This paper proposes ArbiterOS, a new architecture focused on governance, to make these agents trustworthy and robust for critical use cases.", "motivation": "The rapid advancement of Large Language Models (LLMs) enables autonomous agents to perform complex tasks, but their deployment is hampered by unreliable behavior, making them untrustworthy for critical applications.", "method": "The authors argue that the main issue is a paradigm mismatch between the probabilistic nature of LLM agents and the deterministic approach of traditional software engineering. They propose a governance-first paradigm and introduce a formal architecture called ArbiterOS to address this problem.", "result": "ArbiterOS is presented as a solution for principled agent engineering, aiming to improve reliability, predictability, and trustworthiness of LLM-based autonomous agents.", "conclusion": "Transitioning to a governance-first paradigm, as exemplified by ArbiterOS, can resolve the craft crisis in LLM agents, enabling safer and more reliable deployment in mission-critical settings."}}
{"id": "2510.13859", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13859", "abs": "https://arxiv.org/abs/2510.13859", "authors": ["Ruchit Rawal", "Jeffrey Yang Fan Chiang", "Chihao Shen", "Jeffery Siyuan Tian", "Aastha Mahajan", "Tom Goldstein", "Yizheng Chen"], "title": "Benchmarking Correctness and Security in Multi-Turn Code Generation", "comment": null, "summary": "AI coding assistants powered by large language models (LLMs) have transformed\nsoftware development, significantly boosting productivity. While existing\nbenchmarks evaluate the correctness and security of LLM-generated code, they\nare typically limited to single-turn tasks that do not reflect the iterative\nnature of real-world development. We introduce MT-Sec, the first benchmark to\nsystematically evaluate both correctness and security in multi-turn coding\nscenarios. We construct this using a synthetic data pipeline that transforms\nexisting single-turn tasks into semantically aligned multi-turn interaction\nsequences, allowing reuse of original test suites while modeling the complexity\nof real-world coding processes. We evaluate 32 open- and closed-source models,\nand three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in\n\"correct and secure\" outputs from single-turn to multi-turn settings -- even\namong state-of-the-art models. Beyond full-program generation, we also evaluate\nmodels on multi-turn code-diff generation -- an unexplored yet practically\nrelevant setting -- and find that models perform worse here, with increased\nrates of functionally incorrect and insecure outputs. Finally, we find that\nwhile agent scaffoldings boost single-turn code generation performance, they\nare not quite as effective in multi-turn evaluations. Together, these findings\nhighlight the need for benchmarks that jointly evaluate correctness and\nsecurity in multi-turn, real-world coding workflows.", "AI": {"tldr": "MT-Sec shows that popular AI coding assistants become significantly less correct and secure in multi-turn, realistic coding tasks, highlighting the limitations of current single-turn benchmarks and the need for better multi-turn evaluation methods.", "motivation": "Existing benchmarks for evaluating LLM-generated code focus on single-turn tasks, which don't accurately reflect the iterative and multi-turn nature of real-world software development. There is a need for a systematic assessment of both correctness and security in multi-turn coding scenarios.", "method": "The authors introduce MT-Sec, a benchmark designed to evaluate correctness and security of LLM-assisted coding in multi-turn scenarios. They created MT-Sec using a synthetic data pipeline that converts single-turn tasks into multi-turn sequences, while reusing original test suites. The benchmark is used to assess 32 models (open- and closed-source) and agent scaffoldings in both full-program and code-diff generation.", "result": "Across all models, there was a consistent 20-27% drop in \u201ccorrect and secure\u201d outputs when moving from single-turn to multi-turn settings, including state-of-the-art options. In multi-turn code-diff generation, models performed even worse, producing more incorrect and insecure outputs. Agent scaffoldings help in single-turn cases, but are less effective in multi-turn scenarios.", "conclusion": "Current LLM coding assistants suffer notable drops in both correctness and security in multi-turn coding scenarios, demonstrating that existing single-turn benchmarks underestimate real-world complexity. There is a pressing need for benchmarks like MT-Sec that reflect genuine coding workflows and assess both correctness and security over multiple interaction turns."}}
{"id": "2510.13914", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13914", "abs": "https://arxiv.org/abs/2510.13914", "authors": ["Janghan Yoon", "Jaegwan Cho", "Junhyeok Kim", "Jiwan Chung", "Jaehyun Jeon", "Youngjae Yu"], "title": "A11YN: aligning LLMs for accessible web UI code generation", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating functional and aesthetic web interfaces directly from\ninstructions. However, these models often replicate accessibility flaws from\ntheir training data, resulting in interfaces that exclude users with diverse\nneeds and contexts. To address this gap, we introduce A11yn, the first method\nthat aligns code-generating LLMs to reliably produce accessibility-compliant\nweb UIs. A11yn optimizes a novel reward function that penalizes violations of\nthe Web Content Accessibility Guidelines (WCAG), with penalties scaled to the\nseverity of each violation as identified by an accessibility testing engine. To\nsupport training, we construct UIReq-6.8K, a dataset of 6,800 diverse\ninstructions for web UI generation. For evaluation, we introduce RealUIReq-300,\na benchmark of 300 real-world web UI requests grounded and manually curated\nfrom public web pages, spanning a broad range of use cases. Empirical results\nshow that A11yn significantly outperforms strong baselines, lowering the\nInaccessibility Rate by 60% over the base model while preserving semantic\nfidelity and visual quality of generated UIs. These findings demonstrate that\naccessibility can be systematically optimized within LLMs, showing the\nfeasibility of aligning code generation for accessibility.", "AI": {"tldr": "A11yn is a new method that helps large language models generate much more accessible web UIs by penalizing accessibility violations during training. It shows that big improvements in accessibility are possible without sacrificing utility or appearance.", "motivation": "Large language models can generate web interfaces from text instructions but often reproduce accessibility problems, making their outputs less inclusive for users with varying needs. Addressing these persistent accessibility issues in generated UI code is crucial.", "method": "The authors propose A11yn, a method that aligns code-generating LLMs to produce accessibility-compliant web UIs. A11yn uses an innovative reward function during training that penalizes code violating the Web Content Accessibility Guidelines, with penalties scaled by violation severity. The authors also build a new instruction dataset (UIReq-6.8K) and a real-world benchmarking set (RealUIReq-300) for training and evaluation.", "result": "A11yn reduces the Inaccessibility Rate by 60% compared to the base LLM, while maintaining the intended meaning and appearance of the generated web UIs. The system significantly outperforms existing methods on accessibility compliance.", "conclusion": "It is feasible to systematically optimize LLMs for accessibility in code generation. Methods like A11yn can align large language models to reliably produce accessible and high-quality web UIs."}}
{"id": "2510.13992", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13992", "abs": "https://arxiv.org/abs/2510.13992", "authors": ["Quoc Hung Le", "Thanh Le-Cong", "Bach Le", "Bowen Xu"], "title": "Signature in Code Backdoor Detection, how far are we?", "comment": "20 pages, 3 figures", "summary": "As Large Language Models (LLMs) become increasingly integrated into software\ndevelopment workflows, they also become prime targets for adversarial attacks.\nAmong these, backdoor attacks are a significant threat, allowing attackers to\nmanipulate model outputs through hidden triggers embedded in training data.\nDetecting such backdoors remains a challenge, and one promising approach is the\nuse of Spectral Signature defense methods that identify poisoned data by\nanalyzing feature representations through eigenvectors. While some prior works\nhave explored Spectral Signatures for backdoor detection in neural networks,\nrecent studies suggest that these methods may not be optimally effective for\ncode models. In this paper, we revisit the applicability of Spectral\nSignature-based defenses in the context of backdoor attacks on code models. We\nsystematically evaluate their effectiveness under various attack scenarios and\ndefense configurations, analyzing their strengths and limitations. We found\nthat the widely used setting of Spectral Signature in code backdoor detection\nis often suboptimal. Hence, we explored the impact of different settings of the\nkey factors. We discovered a new proxy metric that can more accurately estimate\nthe actual performance of Spectral Signature without model retraining after the\ndefense.", "AI": {"tldr": "This paper reassesses the effectiveness of Spectral Signature defense methods in detecting backdoor attacks on Large Language Models, especially code-focused models, and introduces a new metric for evaluating their performance.", "motivation": "Backdoor attacks in LLMs used for software development pose significant risks, and current defenses are inadequate\u2014improving detection methods for code models is critical for security.", "method": "Systematic evaluation of Spectral Signature defense effectiveness under diverse attack scenarios and defense configurations on code models; exploration of alternative parameter settings and identification of a proxy metric.", "result": "Varied Spectral Signature settings substantially impact backdoor detection performance, and the proposed proxy metric enhances the evaluation accuracy without requiring model retraining.", "conclusion": "The commonly used Spectral Signature settings for code model backdoor detection are often not optimal; adjusting key parameters and applying the proposed metric allows for more accurate detection without retraining."}}
{"id": "2510.14036", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14036", "abs": "https://arxiv.org/abs/2510.14036", "authors": ["Qiushi Wu", "Yue Xiao", "Dhilung Kirat", "Kevin Eykholt", "Jiyong Jang", "Douglas Lee Schales"], "title": "One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery", "comment": null, "summary": "Fixing bugs in large programs is a challenging task that demands substantial\ntime and effort. Once a bug is found, it is reported to the project\nmaintainers, who work with the reporter to fix it and eventually close the\nissue. However, across the program, there are often similar code segments,\nwhich may also contain the bug, but were missed during discovery. Finding and\nfixing each recurring bug instance individually is labor intensive. Even more\nconcerning, bug reports can inadvertently widen the attack surface as they\nprovide attackers with an exploitable pattern that may be unresolved in other\nparts of the program.\n  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear\nrepeatedly across various code segments of a program or even in different\nprograms, stemming from a same root cause, but are unresolved. Our\ninvestigation reveals that RPBs are widespread and can significantly compromise\nthe security of software programs. This paper introduces BugStone, a program\nanalysis system empowered by LLVM and a Large Language Model (LLM). The key\nobservation is that many RPBs have one patched instance, which can be leveraged\nto identify a consistent error pattern, such as a specific API misuse. By\nexamining the entire program for this pattern, it is possible to identify\nsimilar sections of code that may be vulnerable. Starting with 135 unique RPBs,\nBugStone identified more than 22K new potential issues in the Linux kernel.\nManual analysis of 400 of these findings confirmed that 246 were valid. We also\ncreated a dataset from over 1.9K security bugs reported by 23 recent top-tier\nconference works. We manually annotate the dataset, identify 80 recurring\npatterns and 850 corresponding fixes. Even with a cost-efficient model choice,\nBugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.", "AI": {"tldr": "BugStone uses machine learning and program analysis to automatically find recurring bug patterns in large codebases, reliably discovering thousands of vulnerabilities that may otherwise go unnoticed.", "motivation": "Fixing bugs in large programs is difficult and time-consuming, especially when similar unresolved bugs exist in other parts of the code. Reporting bugs can unintentionally reveal attack patterns to adversaries.", "method": "The authors developed BugStone, a program analysis system using LLVM and a Large Language Model (LLM). BugStone detects Recurring Pattern Bugs (RPBs) by learning from patched instances to identify similar error patterns throughout a codebase.", "result": "BugStone identified over 22,000 potential issues in the Linux kernel, of which manual analysis confirmed 246 valid cases out of 400 sampled. The authors also curated a dataset of 1.9K security bugs, manually annotated 80 recurring patterns, and 850 fixes. BugStone achieved 92.2% precision and 79.1% pairwise accuracy on this dataset.", "conclusion": "Recurring Pattern Bugs are widespread and pose significant security risks. The BugStone system can efficiently detect these bugs, greatly improving the security and quality of large codebases."}}
{"id": "2510.14115", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14115", "abs": "https://arxiv.org/abs/2510.14115", "authors": ["Philipp Bauerfeind", "Amir Salarpour", "David Fernandez", "Pedram MohajerAnsari", "Johannes Reschke", "Mert D. Pes\u00e9"], "title": "David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation", "comment": null, "summary": "Scenario simulation is central to testing autonomous driving systems. Scenic,\na domain-specific language (DSL) for CARLA, enables precise and reproducible\nscenarios, but NL-to-Scenic generation with large language models (LLMs)\nsuffers from scarce data, limited reproducibility, and inconsistent metrics. We\nintroduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a\ndifficulty-stratified 30-case test split, an Example Retriever, and 14\nprompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four\nproprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine\nopen-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using\ntext metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics\n(compilation and generation), and compare them with an expert study (n=11).\nEDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of\nEDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking\nfidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88\npercent of its expert score on local hardware. Retrieval-augmented prompting,\nFew-Shot with Example Retriever (FSER), consistently boosts smaller models, and\nscaling shows diminishing returns beyond mid-size, with Qwen2.5Coder\noutperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a\nstandardized, reproducible basis for evaluating Scenic code generation and\nindicate that mid-size open-source models are practical, cost-effective options\nfor autonomous-driving scenario programming.", "AI": {"tldr": "This paper introduces NL2Scenic, a standardized dataset and evaluation framework for translating natural language (NL) descriptions into Scenic DSL code for autonomous driving simulations. It assesses multiple large language models (LLMs), both proprietary and open-source, using various evaluation metrics and finds that mid-sized open-source models can perform competitively and cost-effectively.", "motivation": "Testing autonomous driving systems with simulated scenarios requires reproducible scenario generation from natural language descriptions. Current methods are hampered by limited data, inconsistency, and evaluation challenges, necessitating a standardized approach.", "method": "The authors constructed a dataset of 146 NL/Scenic code pairs and a 30-case test set, introduced multiple prompting variants (zero-shot, few-shot, CoT, step-by-step, multi-objective), and evaluated model outputs using both text-based and execution-based metrics. Human expert judgments were collected for ground-truth comparison.", "result": "GPT-4o achieved the highest performance, but Qwen2.5Coder-14B reached 88% of expert-level accuracy on local hardware. Retrieval-augmented prompting, especially FSER, improved the performance of smaller models. Scaling beyond mid-size yields diminishing returns. The EDIT-COMP metric correlates well with human judgment and enhances reliable model ranking.", "conclusion": "NL2Scenic and the new EDIT-COMP metric provide reproducible, reliable benchmarks for NL-to-Scenic code generation. While proprietary models like GPT-4o lead in performance, mid-sized open-source models like Qwen2.5Coder-14B are practical alternatives. Retrieval-augmented prompting methods further boost smaller models, making scenario programming for autonomous driving more accessible."}}
{"id": "2510.14292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14292", "abs": "https://arxiv.org/abs/2510.14292", "authors": ["Haolin Pan", "Hongbin Zhang", "Mingjie Xing", "Yanjun Wu"], "title": "A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning", "comment": null, "summary": "Compiler pass auto-tuning is critical for enhancing software performance, yet\nfinding the optimal pass sequence for a specific program is an NP-hard problem.\nTraditional, general-purpose optimization flags like -O3 and -Oz adopt a\none-size-fits-all approach, often failing to unlock a program's full\nperformance potential. To address this challenge, we propose a novel Hybrid,\nKnowledge-Guided Evolutionary Framework. This framework intelligently guides\nonline, personalized optimization using knowledge extracted from a large-scale\noffline analysis phase. During the offline stage, we construct a comprehensive\ncompilation knowledge base composed of four key components: (1) Pass Behavioral\nVectors to quantitatively capture the effectiveness of each optimization; (2)\nPass Groups derived from clustering these vectors based on behavior similarity;\n(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a\nlibrary of Prototype Pass Sequences evolved for distinct program types. In the\nonline stage, a bespoke genetic algorithm leverages this rich knowledge base\nthrough specially designed, knowledge-infused genetic operators. These\noperators transform the search by performing semantically-aware recombination\nand targeted, restorative mutations. On a suite of seven public datasets, our\nframework achieves an average of 11.0% additional LLVM IR instruction reduction\nover the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art\ncapability in discovering personalized, high-performance optimization\nsequences.", "AI": {"tldr": "The study proposes a smarter compiler pass auto-tuning approach using offline knowledge and online genetic algorithms, delivering better-optimized programs than traditional methods.", "motivation": "Standard compiler optimizations like -O3 and -Oz apply generalized settings that miss program-specific performance gains. The need to overcome the NP-hardness of finding optimal pass order motivates the use of a hybrid, knowledge-driven evolutionary approach for auto-tuning compiler passes.", "method": "The method involves a two-stage approach: (1) offline analysis to build a knowledge base with behavioral vectors, pass groups, synergy graphs, and prototype sequences; and (2) online genetic search using operators informed by this knowledge to produce optimized pass sequences for new programs.", "result": "The paper introduces a hybrid, knowledge-guided evolutionary framework for automatic tuning of compiler optimization passes. It achieves significant additional program reduction (11% on average in LLVM IR instructions) over standard optimization (opt -Oz), by leveraging a comprehensive compilation knowledge base and knowledge-infused genetic operators.", "conclusion": "Using a hybrid framework that combines offline profiling (knowledge base creation) with online evolutionary search enables more effective, personalized optimization sequences, outperforming baseline compiler settings in various datasets."}}
{"id": "2510.14339", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14339", "abs": "https://arxiv.org/abs/2510.14339", "authors": ["Jialu Zhang", "Jialiang Gu", "Wangmeiyu Zhang", "Jos\u00e9 Pablo Cambronero", "John Kolesar", "Ruzica Piskac", "Daming Li", "Hanyuan Shi"], "title": "A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments", "comment": null, "summary": "Online programming platforms such as Codeforces and LeetCode attract millions\nof users seeking to learn to program or refine their skills for industry\ninterviews. A major challenge for these users is the Time Limit Exceeded (TLE)\nerror, triggered when a program exceeds the execution time bound. Although\ndesigned as a performance safeguard, TLE errors are difficult to resolve: error\nmessages provide no diagnostic insight, platform support is minimal, and\nexisting debugging tools offer little help. As a result, many users abandon\ntheir submissions after repeated TLE failures.\n  This paper presents the first large-scale empirical study of TLE errors in\nonline programming. We manually analyzed 1000 Codeforces submissions with TLE\nerrors, classified their root causes, and traced how users attempted to fix\nthem. Our analysis shows that TLE errors often arise not only from inefficient\nalgorithms but also from infinite loops, improper data structure use, and\ninefficient I/O, challenging the conventional view that TLEs are purely\nperformance issues.\n  Guided by these findings, we introduce Nettle, the first automated repair\ntool specifically designed for TLE errors, and Nettle-Eval, the first framework\nfor evaluating TLE repairs. Integrating LLMs with targeted automated feedback\ngenerated by the compiler and test cases, Nettle produces small, correct code\nedits that eliminate TLEs while preserving functionality. Evaluated on the same\n1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the\nstrongest LLM baseline, and all of its repairs pass both Nettle-Eval and the\nplatform's official checker, confirming the reliability of our framework.", "AI": {"tldr": "This paper studies why Time Limit Exceeded (TLE) errors happen on Codeforces, finds that they\u2019re due to more than just slow algorithms, and introduces Nettle, an automated tool that fixes TLEs with a 98.5% success rate\u2014outperforming previous methods and validated with a new robust evaluation framework.", "motivation": "Many users on online programming platforms face the persistent problem of Time Limit Exceeded (TLE) errors, which are difficult to diagnose and resolve due to a lack of actionable feedback and limited platform support. This often leads to frustration and users abandoning their coding attempts.", "method": "The authors performed a large-scale empirical study by manually analyzing 1000 Codeforces submissions with TLE errors. They classified the root causes of these errors and studied user behaviors in fixing them. Based on these insights, they developed Nettle, an automated repair tool for TLE errors, and Nettle-Eval, an evaluation framework to assess the repairs. Nettle integrates LLMs with compiler feedback and test cases to generate correct code edits.", "result": "The analysis revealed that TLE errors stem from a broader set of issues than just inefficient algorithms, including infinite loops and inefficient I/O. Nettle fixed 98.5% of the 1000 real-world TLE cases, surpassing the strongest LLM baseline. All the fixes passed both the Nettle-Eval framework and Codeforces\u2019 official checker.", "conclusion": "TLE errors in programming platforms are more complex than previously thought. Targeted automated tools like Nettle, coupled with thoughtful evaluation frameworks, can effectively and reliably help users resolve TLE errors, making online programming more accessible and less frustrating."}}
{"id": "2510.14341", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14341", "abs": "https://arxiv.org/abs/2510.14341", "authors": ["Xu He", "Shu Wang", "Kun Sun"], "title": "PathFix: Automated Program Repair with Expected Path", "comment": "This is the author's version of a paper accepted at SecDev 2025\n  (IEEE)", "summary": "Automated program repair (APR) techniques are effective in fixing inevitable\ndefects in software, enhancing development efficiency and software robustness.\nHowever, due to the difficulty of generating precise specifications, existing\nAPR methods face two main challenges: generating too many plausible patch\ncandidates and overfitting them to partial test cases. To tackle these\nchallenges, we introduce a new APR method named PathFix, which leverages\npath-sensitive constraints extracted from correct execution paths to generate\npatches for repairing buggy code. It is based on one observation: if a buggy\nprogram is repairable, at least one expected path is supposed to replace the\nfault path in the patched program. PathFix operates in four main steps. First,\nit traces fault paths reaching the fault output in the buggy program. Second,\nit derives expected paths by analyzing the desired correct output on the\ncontrol flow graph, where an expected path defines how a feasible patch leads\nto the correct execution. Third, PathFix generates and evaluates patches by\nsolving state constraints along the expected path. Fourth, we validate the\ncorrectness of the generated patch. To further enhance repair performance and\nmitigate scalability issues introduced by path-sensitive analysis, we integrate\na large language model (LLM) into our framework. Experimental results show that\nPathFix outperforms existing solutions, particularly in handling complex\nprogram structures such as loops and recursion.", "AI": {"tldr": "PathFix is a novel APR framework that uses path-sensitive analysis and LLMs to generate targeted code repairs. By focusing on correct execution paths, it produces more accurate patches and handles complex code better than existing methods.", "motivation": "Automated program repair (APR) techniques struggle with generating too many plausible patches and overfitting to incomplete test suites because generating precise specifications for buggy software is difficult. There is a need for improved approaches that can generate more accurate repairs and address these key limitations.", "method": "The paper presents PathFix, a new APR method utilizing path-sensitive constraints. PathFix operates in four steps: (1) tracing fault paths in buggy software, (2) extracting expected execution paths leading to correct outputs, (3) generating and evaluating patches by solving state constraints along these expected paths, and (4) validating patch correctness. Additionally, a large language model (LLM) is integrated to improve repair performance and scalability.", "result": "PathFix demonstrates superior performance compared to existing APR solutions, especially in repairing complex program structures, such as those containing loops and recursion.", "conclusion": "PathFix effectively overcomes core APR challenges by leveraging path-sensitive analysis and LLMs. It generates more precise patches and achieves better repair outcomes, especially with complicated code patterns, outperforming prior approaches."}}
{"id": "2510.14465", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14465", "abs": "https://arxiv.org/abs/2510.14465", "authors": ["Adem Ait", "Gwendal Jouneaux", "Javier Luis C\u00e1novas Izquierdo", "Jordi Cabot"], "title": "Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects", "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering, ASE 2025", "summary": "The stakeholders involved in software development are becoming increasingly\ndiverse, with both human contributors from varied backgrounds and AI-powered\nagents collaborating together in the process. This situation presents unique\ngovernance challenges, particularly in Open-Source Software (OSS) projects,\nwhere explicit policies are often lacking or unclear. This paper presents the\nvision and foundational concepts for a novel Domain-Specific Language (DSL)\ndesigned to define and enforce rich governance policies in systems involving\ndiverse stakeholders, including agents. This DSL offers a pathway towards more\nrobust, adaptable, and ultimately automated governance, paving the way for more\neffective collaboration in software projects, especially OSS ones.", "AI": {"tldr": "Proposes a special language (DSL) to create clearer, adaptable governance rules for diverse teams (humans and AI) in open-source software projects.", "motivation": "Increasing diversity in software development (from both humans and AI agents) creates complex governance needs. Current OSS projects often lack clear, explicit governance policies, necessitating new approaches for effective collaboration.", "method": "Conceptual design of a DSL for governance policy specification, addressing the diversity and automation needs of modern software development, illustrated through the context of OSS projects.", "result": "The paper presents foundational concepts and a vision for a new Domain-Specific Language (DSL) aimed at defining and enforcing governance policies in software development projects with diverse human and AI-powered stakeholders, focusing especially on Open-Source Software (OSS) environments.", "conclusion": "Implementing and adopting this DSL could lead to more robust, automated, and adaptable governance frameworks, enhancing collaboration and policy clarity in OSS projects and other multi-stakeholder software environments."}}
{"id": "2510.14509", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14509", "abs": "https://arxiv.org/abs/2510.14509", "authors": ["Jingyao Liu", "Chen Huang", "Zhizhao Guan", "Wenqiang Lei", "Yang Deng"], "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task", "comment": null, "summary": "E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple\nBDD test scenarios with corresponding Python step implementations for each\nrequirement}, and (iii) a fully automated testing pipeline built on the Behave\nframework. To ensure its quality while reducing the annotation effort, E2EDev\nleverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework\n(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with\nE2EDev}, our analysis reveals a persistent struggle to effectively solve these\ntasks, underscoring the critical need for more effective and cost-efficient\nE2ESD solutions. Our codebase and benchmark are publicly available at\nhttps://github.com/SCUNLP/E2EDev.", "AI": {"tldr": "E2EDev is a new benchmark with fine-grained requirements, BDD test scenarios, and an automated Behave-based pipeline, supported by a HITL annotation framework. Current frameworks and LLMs still struggle on these tasks, highlighting the need for better E2ESD solutions.", "motivation": "There is a need for fine-grained, efficient end-to-end software development (E2ESD) tools and benchmarks, as existing frameworks and large language models (LLMs) struggle to solve these tasks effectively and cost-efficiently.", "method": "The paper introduces E2EDev, which includes a detailed set of user requirements, multiple BDD (Behavior-Driven Development) test scenarios with corresponding Python steps, and an automated testing pipeline on the Behave framework, supported by a Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA) to ensure quality with less annotation effort.", "result": "Analysis shows that current E2ESD frameworks and LLMs still face significant challenges in effectively addressing these tasks, pointing to gaps in the field.", "conclusion": "There is an ongoing need for more effective and affordable solutions in end-to-end software development tasks, and E2EDev provides a new benchmark to foster progress in this area."}}
{"id": "2510.14625", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14625", "abs": "https://arxiv.org/abs/2510.14625", "authors": ["Mehrdad Saadatmand", "Abbas Khan", "Beatriz Marin", "Ana C. R Paiva", "Nele Van Asch", "Graham Moran", "Felix Cammaerts", "Monique Snoeck", "Alexandra Mendes"], "title": "Software Testing Education and Industry Needs - Report from the ENACTEST EU Project", "comment": "* The paper is going to appear in the proceedings of the 26th\n  International Conference on Product-Focused Software Process Improvement\n  (PROFES 2025). To cite the paper, please check and refer to the PROFES 2025\n  proceedings", "summary": "The evolving landscape of software development demands that software testers\ncontinuously adapt to new tools, practices, and acquire new skills. This study\ninvestigates software testing competency needs in industry, identifies\nknowledge gaps in current testing education, and highlights competencies and\ngaps not addressed in academic literature. This is done by conducting two focus\ngroup sessions and interviews with professionals across diverse domains,\nincluding railway industry, healthcare, and software consulting and performing\na curated small-scale scoping review. The study instrument, co-designed by\nmembers of the ENACTEST project consortium, was developed collaboratively and\nrefined through multiple iterations to ensure comprehensive coverage of\nindustry needs and educational gaps. In particular, by performing a thematic\nqualitative analysis, we report our findings and observations regarding:\nprofessional training methods, challenges in offering training in industry,\ndifferent ways of evaluating the quality of training, identified knowledge gaps\nwith respect to academic education and industry needs, future needs and trends\nin testing education, and knowledge transfer methods within companies. Finally,\nthe scoping review results confirm knowledge gaps in areas such as AI testing,\nsecurity testing and soft skills.", "AI": {"tldr": "There are serious gaps between what industry needs and what current software testing education provides, particularly in new domains like AI testing, security, and soft skills.", "motivation": "Software testers must keep up with rapid changes in tools and practices, but there's uncertainty about whether current educational offerings and training actually meet evolving industry needs.", "method": "The researchers conducted two focus groups and interviews with software testing professionals from diverse industries, along with a curated, small-scale scoping review. They used a collaboratively designed and iteratively refined study instrument, then analyzed results via thematic qualitative analysis.", "result": "The study found significant knowledge gaps in current software testing education, especially in AI testing, security testing, and soft skills. These gaps are not sufficiently addressed in academic literature or professional training. Industry professionals face challenges in training delivery and evaluating its quality, and have identified areas where training and knowledge transfer methods need improvement.", "conclusion": "The current approach to software testing education and professional training does not fully prepare testers for industry's actual competency needs. Academics and industry practitioners must collaborate to update curricula and training methods, with particular focus on emerging topics and soft skills."}}
{"id": "2510.14635", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14635", "abs": "https://arxiv.org/abs/2510.14635", "authors": ["Qingyao Li", "Xinyi Dai", "Weiwen Liu", "Xiangyang Li", "Yasheng Wang", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "ATGen: Adversarial Reinforcement Learning for Test Case Generation", "comment": null, "summary": "Large Language Models (LLMs) excel at code generation, yet their outputs\noften contain subtle bugs, for which effective test cases are a critical\nbottleneck. Existing test generation methods, whether based on prompting or\nsupervised fine-tuning, rely on static datasets. This imposes a\n``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover\nnovel or more complex bugs beyond their training scope. To overcome this, we\nintroduce ATGen, a framework that trains a test case generator via adversarial\nreinforcement learning. ATGen pits a test generator against an adversarial code\ngenerator that continuously crafts harder bugs to evade the current policy.\nThis dynamic loop creates a curriculum of increasing difficulty challenging\ncurrent policy. The test generator is optimized via Reinforcement Learning (RL)\nto jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to\nlearn a progressively stronger policy that breaks the fixed-difficulty ceiling\nof static training. Extensive experiments demonstrate that ATGen significantly\noutperforms state-of-the-art baselines. We further validate its practical\nutility, showing it serves as both a more effective filter for Best-of-N\ninference and a higher-quality reward source for training code generation\nmodels. Our work establishes a new, dynamic paradigm for improving the\nreliability of LLM-generated code.", "AI": {"tldr": "Static datasets limit current code test generators. ATGen uses adversarial reinforcement learning to dynamically challenge and improve test case generation, outperforming existing methods and increasing the reliability of code from LLMs.", "motivation": "Existing test generation methods for code produced by Large Language Models are limited by static datasets, unable to evolve and challenge models with more complex or novel bugs.", "method": "ATGen, a framework utilizing adversarial reinforcement learning, pits a test case generator against a code generator that crafts increasingly difficult bugs, creating a dynamic, curriculum-based training loop. The test generator is optimized jointly for output accuracy and attack success using RL.", "result": "ATGen outperformed current state-of-the-art baselines in test case generation, serving as a better filter in Best-of-N inference and a more effective reward source for code model training.", "conclusion": "A dynamic adversarial curriculum via ATGen breaks the limits of static training, improving the reliability of LLM-generated code and establishing a novel paradigm for test case generation."}}
{"id": "2510.14653", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14653", "abs": "https://arxiv.org/abs/2510.14653", "authors": ["Sven Tarlowski", "Lutz Eckstein"], "title": "Requirement Identification for Traffic Simulations in Driving Simulators", "comment": "2 Pages, 1 figure", "summary": "This paper addresses the challenge of ensuring realistic traffic conditions\nby proposing a methodology that systematically identifies traffic simulation\nrequirements. Using a structured approach based on sub-goals in each study\nphase, specific technical needs are derived for microscopic levels, agent\nmodels, and visual representation. The methodology aims to maintain a high\ndegree of fidelity, enhancing both the validity of experimental outcomes and\nparticipant engagement. By providing a clear link between study objectives and\ntraffic simulation design, this approach supports robust automotive development\nand testing.", "AI": {"tldr": "The paper presents a stepwise methodology for identifying realistic traffic simulation requirements, improving experimental validity and engagement, which aids automotive development.", "motivation": "There is a challenge in ensuring traffic simulation reflects realistic conditions, which is necessary for reliable automotive development and testing.", "method": "The paper introduces a structured methodology that uses sub-goals in each study phase to derive specific requirements for microscopic traffic levels, agent models, and visual representations.", "result": "This methodology enables systematic identification of technical needs, ensuring high fidelity in traffic simulations and improving validity of experiments and participant engagement.", "conclusion": "The approach clearly connects study objectives to traffic simulation design, supporting more robust automotive development and testing."}}
{"id": "2510.14700", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14700", "abs": "https://arxiv.org/abs/2510.14700", "authors": ["Bin Liu", "Yanjie Zhao", "Guoai Xu", "Haoyu Wang"], "title": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?", "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable capabilities\nin software engineering and cybersecurity tasks, including code generation,\nvulnerability discovery, and automated testing. One critical but underexplored\napplication is automated web vulnerability reproduction, which transforms\nvulnerability reports into working exploits. Although recent advances suggest\npromising potential, challenges remain in applying LLM agents to real-world web\nvulnerability reproduction scenarios. In this paper, we present the first\ncomprehensive evaluation of state-of-the-art LLM agents for automated web\nvulnerability reproduction. We systematically assess 20 agents from software\nengineering, cybersecurity, and general domains across 16 dimensions, including\ntechnical capabilities, environment adaptability, and user experience factors,\non 3 representative web vulnerabilities. Based on the results, we select three\ntop-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation\non our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types\nand 6 web technologies. Our results reveal that while LLM agents achieve\nreasonable success on simple library-based vulnerabilities, they consistently\nfail on complex service-based vulnerabilities requiring multi-component\nenvironments. Complex environment configurations and authentication barriers\ncreate a gap where agents can execute exploit code but fail to trigger actual\nvulnerabilities. We observe high sensitivity to input guidance, with\nperformance degrading by over 33% under incomplete authentication information.\nOur findings highlight the significant gap between current LLM agent\ncapabilities and the demands of reliable automated vulnerability reproduction,\nemphasizing the need for advances in environmental adaptation and autonomous\nproblem-solving capabilities.", "AI": {"tldr": "This paper rigorously evaluates various large language model agents for automated web vulnerability reproduction, finding that while LLMs can handle simple cases, they fail with complex real-world vulnerabilities due to environmental and authentication challenges, indicating the need for more adaptive and autonomous agent capabilities.", "motivation": "Automated web vulnerability reproduction is crucial for translating vulnerability reports into working exploits, aiding security researchers and practitioners. However, leveraging LLM agents for this task remains underexplored, especially in the context of real-world web vulnerabilities.", "method": "The authors conducted the first comprehensive evaluation of 20 state-of-the-art LLM agents from software engineering, cybersecurity, and general domains. They assessed these agents across 16 dimensions (technical, adaptability, user factors) and selected three leading agents for deeper benchmarking on 80 real-world CVEs involving diverse vulnerability types and web technologies.", "result": "LLM agents had reasonable success with simple, library-based vulnerabilities but consistently failed to reproduce complex, service-based vulnerabilities requiring multi-component environments and authentication. Performance suffered significantly when incomplete guidance (e.g., authentication details) was provided, with over 33% degradation observed.", "conclusion": "There exists a significant gap between current LLM agent capabilities and the practical requirements of automated web vulnerability reproduction, especially for complex scenarios. Advances are needed in environmental adaptability and autonomous problem-solving to make LLM agents reliably effective for real-world web security tasks."}}
{"id": "2510.14778", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14778", "abs": "https://arxiv.org/abs/2510.14778", "authors": ["Maor Reuben", "Ido Mendel", "Or Feldman", "Moshe Kravchik", "Mordehai Guri", "Rami Puzis"], "title": "Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks", "comment": null, "summary": "Supply chain attacks significantly threaten software security with malicious\ncode injections within legitimate projects. Such attacks are very rare but may\nhave a devastating impact. Detecting spurious code injections using automated\ntools is further complicated as it often requires deciphering the intention of\nboth the inserted code and its context. In this study, we propose an\nunsupervised approach for highlighting spurious code injections by quantifying\ncohesion disruptions in the source code. Using a name-prediction-based cohesion\n(NPC) metric, we analyze how function cohesion changes when malicious code is\nintroduced compared to natural cohesion fluctuations. An analysis of 54,707\nfunctions over 369 open-source C++ repositories reveals that code injection\nreduces cohesion and shifts naming patterns toward shorter, less descriptive\nnames compared to genuine function updates. Considering the sporadic nature of\nreal supply-chain attacks, we evaluate the proposed method with extreme\ntest-set imbalance and show that monitoring high-cohesion functions with NPC\ncan effectively detect functions with injected code, achieving a Precision@100\nof 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that\nautomated cohesion measurements, in general, and name-prediction-based\ncohesion, in particular, may help identify supply chain attacks, improving\nsource code integrity.", "AI": {"tldr": "The paper presents an unsupervised method for detecting supply chain attacks in software by measuring cohesion disruptions using a name-prediction-based metric. The approach effectively identifies functions with malicious code injections even in highly imbalanced scenarios, illustrating its utility for enhancing software security.", "motivation": "Supply chain attacks, involving malicious code injected into legitimate software, are a serious but rare threat with potentially devastating consequences. Automated detection is challenging because it is hard to determine the intent of injected code within its context. Current tools are limited in detecting such sophisticated attacks.", "method": "The study introduces an unsupervised method to detect malicious code injections by measuring disruptions in source code cohesion. It proposes a name-prediction-based cohesion (NPC) metric that quantifies changes in function cohesion and naming patterns after code injections, contrasting those with normal code evolution.", "result": "Analyzing over 54,000 functions from 369 open-source C++ projects, the method showed that injected code tends to decrease cohesion and leads to shorter, less descriptive function names. Evaluation under highly imbalanced test conditions (to simulate the rarity of supply chain attacks) demonstrated promising detection capability, with a Precision@100 of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000.", "conclusion": "Automated measurement of code cohesion, especially using name-prediction-based metrics, shows potential as an effective means for highlighting possible supply chain attacks. This approach could thus improve code integrity and security by identifying malicious code injections in software repositories."}}
{"id": "2510.14928", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14928", "abs": "https://arxiv.org/abs/2510.14928", "authors": ["Eric Christopher", "Kevin Crossan", "Wolff Dobson", "Chris Kennelly", "Drew Lewis", "Kun Lin", "Martin Maas", "Parthasarathy Ranganathan", "Emma Rapati", "Brian Yang"], "title": "Instruction Set Migration at Warehouse Scale", "comment": null, "summary": "Migrating codebases from one instruction set architecture (ISA) to another is\na major engineering challenge. A recent example is the adoption of Arm (in\naddition to x86) across the major Cloud hyperscalers. Yet, this problem has\nseen limited attention by the academic community. Most work has focused on\nstatic and dynamic binary translation, and the traditional conventional wisdom\nhas been that this is the primary challenge.\n  In this paper, we show that this is no longer the case. Modern ISA migrations\ncan often build on a robust open-source ecosystem, making it possible to\nrecompile all relevant software from scratch. This introduces a new and\nmultifaceted set of challenges, which are different from binary translation.\n  By analyzing a large-scale migration from x86 to Arm at Google, spanning\nalmost 40,000 code commits, we derive a taxonomy of tasks involved in ISA\nmigration. We show how Google automated many of the steps involved, and\ndemonstrate how AI can play a major role in automatically addressing these\ntasks. We identify tasks that remain challenging and highlight research\nchallenges that warrant further attention.", "AI": {"tldr": "Migrating codebases between instruction set architectures (e.g., from x86 to Arm) is now more about tackling a broad set of challenges than just binary translation, thanks to open-source ecosystems. Google's large-scale migration shows much of the process can be automated\u2014especially with AI\u2014though some hard problems remain and need further research.", "motivation": "The motivation stems from the increasing need to migrate codebases between ISAs, as seen in the adoption of Arm by cloud hyperscalers, and the observation that academic focus has lagged behind the evolving challenges of ISA migration.", "method": "The authors analyze a large-scale x86-to-Arm migration at Google, involving around 40,000 code commits, and derive a taxonomy of migration tasks. They study Google's automation strategies and the role of AI in the migration process.", "result": "The analysis reveals a taxonomy of migration tasks, highlights successful automation strategies, and identifies the significant impact of AI. Some migration tasks remain unresolved, pointing to ongoing research needs.", "conclusion": "The paper concludes that the key challenges in migrating codebases between ISAs have shifted from binary translation to broader, multifaceted tasks enabled by open-source ecosystems, and that automation\u2014particularly with AI\u2014can address many steps, although some challenges remain."}}
