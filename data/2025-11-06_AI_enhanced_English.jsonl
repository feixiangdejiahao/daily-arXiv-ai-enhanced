{"id": "2511.02854", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02854", "abs": "https://arxiv.org/abs/2511.02854", "authors": ["Yixiang Chen", "Tianshi Zheng", "Shijue Huang", "Zhitao He", "Yi R. Fung"], "title": "SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation", "comment": "15 pages, 8 figures,2 tables", "summary": "Test-time scaling without interpreter feedback is essential for real-world\ncode generation scenarios where test cases are not readily available. While\nexisting paradigms often rely on either greedy exploitation (i.e., iterative\nrefinement) or stochastic exploration (i.e., relying on sample-based voting or\nreranking mechanisms), the balance between these two dimensions remains\nunderexplored. To investigate the LLM's intrinsic ability to balance\nexploitation and exploration, we introduce SELF-REDRAFT, a framework built upon\nSelf-Refine that encourages the model to propose new drafts for solutions that\nare fundamentally flawed. Our results show that SELF-REDRAFT consistently\nachieves better performance than Self-Refine when converged under the same\nmaximum number of iterations. Still, we observe that significant room for\nimprovement remains, largely due to two core aspects of current self-redraft\ncapabilities: constrained capacity for generating instructive feedback and\nfragile discriminative judgment. We also find that balancing strategies vary\nnotably across different LLMs, reflecting distinct, model-specific behaviors.\nOverall, our study establishes a baseline for intrinsic\nexploration-exploitation balancing in test-time scaling and identifies feedback\nand discrimination as key areas with potential for future advances.", "AI": {"tldr": "SELF-REDRAFT improves LLM code generation by balancing solution refinement and exploration at test time, but further progress depends on better feedback and judgment abilities in LLMs.", "motivation": "Real-world code generation often lacks interpreter feedback, making test-time scaling crucial. Existing methods either exploit known solutions or explore new ones, but rarely balance both.", "method": "The paper proposes SELF-REDRAFT, a framework building on Self-Refine, which encourages LLMs to generate new solutions when current drafts are clearly flawed. This investigates how well LLMs can balance between exploiting known good solutions and exploring new possibilities.", "result": "SELF-REDRAFT outperforms Self-Refine in performance when limited to the same iteration count. However, it is limited by the LLMs\u2019 ability to generate useful feedback and distinguish between good and bad solutions. The strategy for balancing exploration and exploitation differs among various LLM models.", "conclusion": "SELF-REDRAFT establishes a baseline for LLMs' intrinsic balance between exploration and exploitation in test-time scaling, but improving feedback and judgment abilities is needed for further progress."}}
{"id": "2511.02859", "categories": ["cs.SE", "D.2.9"], "pdf": "https://arxiv.org/pdf/2511.02859", "abs": "https://arxiv.org/abs/2511.02859", "authors": ["Bianca Leech", "Ridewaan Hanslo"], "title": "The Evolution of Agile and Hybrid Project Management Methodologies: A Systematic Literature Review", "comment": "10 pages, 5 images, 1 table, 7th World Symposium on Software\n  Engineering (WSSE 2025)", "summary": "The rapid evolution of IT projects has driven the transformation of project\nmanagement methodologies, from traditional waterfall approaches to agile\nframeworks and, more recently, hybrid models. This systematic literature review\ninvestigates the evolution of agile methodologies into hybrid frameworks,\nanalysing their implementation challenges and success factors. We identify key\ntrends through PRISMA-guided analysis of peer-reviewed studies from the last 8\nyears. Hybrid methodologies emerge from agile limitations in large-scale and\nregulated environments, combining iterative flexibility with structured\ngovernance. Agile has several implementation challenges, leading to hybrid\nmethods, and the success hinges on leadership support, tailored process\nintegration, and continuous improvement mechanisms. The study explores the need\nfor contextual adaptation over rigid frameworks, offering practical insights\nfor organisations navigating hybrid transitions.", "AI": {"tldr": "This review analyzes how IT project management has shifted from agile to hybrid models, identifies why hybrids are used, outlines implementation challenges, and gives practical advice for organizations transitioning to hybrid approaches.", "motivation": "With the fast-changing nature of IT projects, there is a need to understand how project management approaches have evolved from traditional to agile and now to hybrid models, and the factors driving these changes.", "method": "A systematic literature review was conducted, using PRISMA guidelines, to analyze recent (last 8 years) peer-reviewed studies regarding the evolution, challenges, and success factors of agile-to-hybrid project management methodologies.", "result": "Hybrid project management methodologies have developed primarily because of the limitations of pure agile approaches in large-scale or regulated settings. Success with hybrids depends on leadership support, context-driven process customization, and mechanisms for continuous improvement.", "conclusion": "Organizations should focus on adapting project methodologies to their specific contexts rather than adhering strictly to a single framework. Practical guidance is provided for managing the transition to hybrid methods, emphasizing flexibility and tailored integration over rigidity."}}
{"id": "2511.02866", "categories": ["cs.SE", "cs.AI", "cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.02866", "abs": "https://arxiv.org/abs/2511.02866", "authors": ["Ahmad Tahmasivand", "Noureldin Zahran", "Saba Al-Sayouri", "Mohammed Fouda", "Khaled N. Khasawneh"], "title": "LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models", "comment": "Accepted at IEEE ICCD 2025. Code: https://github.com/ata990/lm-fix.\n  Detects over 94 percent single-bit flips (near 100 percent multi-bit) with\n  about 1 to 7.7 percent overhead; recovery is over 100x faster than a full\n  reload. Keywords: LLMs, bit-flip, fault injection, reliability, security,\n  Rowhammer, SDC, Jailbreaking, Attack, Defense, GPU DRAM faults", "summary": "This paper presents LM-Fix, a lightweight detection and rapid recovery\nframework for faults in large language models (LLMs). Existing integrity\napproaches are often heavy or slow for modern LLMs. LM-Fix runs a short\ntest-vector pass and uses hash-guided checks to detect bit-flip faults, then\nrepairs them locally without a full reload. Across multiple models, it detects\nover 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with\napproximately 1% to 7.7% runtime overhead; recovery is more than 100x faster\nthan reloading. These results show a practical, low-overhead solution to keep\nLLMs reliable in production", "AI": {"tldr": "LM-Fix is a fast, lightweight method to detect and quickly fix faults in large language models, detecting most bit-flip errors with minimal overhead and vastly accelerating recovery compared to full reloads.", "motivation": "Ensuring the reliability and integrity of large language models (LLMs) in production is a significant challenge due to their size and complexity. Traditional fault detection and recovery methods are often too resource-intensive or slow for modern LLMs, necessitating more efficient solutions.", "method": "The paper introduces LM-Fix, a lightweight framework for fault detection and rapid recovery in LLMs. LM-Fix employs a short test-vector pass and hash-guided checks to efficiently identify bit-flip faults, and offers localized repair strategies without the need for a complete model reload.", "result": "LM-Fix is able to detect more than 94% of single-bit flip faults and nearly 100% of multi-bit flip faults in various models using a test vector length (TVL) of 200. The overhead on runtime is low, ranging from approximately 1% to 7.7%. Recovery through LM-Fix is more than 100 times faster than a full model reload.", "conclusion": "LM-Fix provides a practical, low-overhead solution for maintaining the reliability of LLMs in production by enabling efficient fault detection and rapid recovery, significantly reducing downtime and resource consumption compared to traditional approaches."}}
{"id": "2511.02869", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.02869", "abs": "https://arxiv.org/abs/2511.02869", "authors": ["Amirreza Esmaeili", "Fahd Seddik", "Yongyi Ji", "Fatemeh Fard", "Fuxiang Chen"], "title": "Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models", "comment": null, "summary": "Programming languages can benefit from one another by utilizing a language\nmodel for software engineering tasks. Full fine-tuning and Parameter Efficient\nFine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for\nmultilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims\nto enhance task performance by leveraging information from multiple programming\nlanguages, but primarily focuses on the target programming language.\n  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that\neffectively learns from other programming languages before adapting to the\ntarget task. Though previous experiments showed that AdvFusion outperformed\nAdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited\nto only two tasks, code summarization and method name prediction. In this\nstudy, we expanded our work and investigated AdvFusion on Code Large Language\nModels (Code-LLMs), considering three new tasks: code generation, code\ntranslation, and commit message generation. We observed that different\nCode-LLMs/tasks exhibit different characteristics. In code generation,\nAdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,\nCompacter, and TaskAdapter). In commit message generation, AdapterFusion\nperformed better than AdvFusion, and contrary to code generation, we found that\nthe other PEFT methods do not have better performance. In code translation,\nAdvFusion performed worse than AdapterFusion overall, with the performance gap\nmarginally widening as the model size increases. However, consistent with code\ngeneration, other PEFT methods showed better performance.", "AI": {"tldr": "This paper analyzes the performance of a novel fine-tuning method, AdvFusion, on large code language models for various programming tasks. Results show that its effectiveness depends on the specific task and model, with no one-size-fits-all solution for parameter-efficient fine-tuning in code-related applications.", "motivation": "Programming languages have varying strengths, and language models can transfer knowledge across languages for software engineering tasks. There is a need to further evaluate and improve methods for multilingual adaptation on a wider range of software engineering problems.", "method": "The authors previously proposed AdvFusion, a Parameter Efficient Fine-Tuning (PEFT) method that learns from multiple programming languages before focusing on the target task. In this study, they extend AdvFusion to Code Large Language Models (Code-LLMs) and evaluate its effectiveness on three new tasks: code generation, code translation, and commit message generation, comparing it with AdapterFusion and other PEFT approaches like LoRA, Compacter, and TaskAdapter.", "result": "AdvFusion was superior to AdapterFusion for code generation but not as strong as LoRA, Compacter, and TaskAdapter. For commit message generation, AdapterFusion outperformed AdvFusion, and other PEFT methods did not surpass their performance. In code translation, AdapterFusion did better than AdvFusion, with the gap increasing for larger models, but other PEFT methods again outperformed them.", "conclusion": "The effectiveness of AdvFusion and other PEFT architectures varies by task and model type in Code-LLMs. There is no universally superior method; instead, the optimal approach depends on the specific software engineering task and model."}}
{"id": "2511.02874", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02874", "abs": "https://arxiv.org/abs/2511.02874", "authors": ["Jannatul Shefa", "Taylan G. Topcu"], "title": "An Analysis of Early-Stage Functional Safety Analysis Methods and Their Integration into Model-Based Systems Engineering", "comment": null, "summary": "As systems become increasingly complex, conducting effective safety analysis\nin the earlier phases of a system's lifecycle is essential to identify and\nmitigate risks before they escalate. To that end, this paper investigates the\ncapabilities of key safety analysis techniques, namely: Failure Mode and\nEffects Analysis (FMEA), Functional Hazard Analysis (FHA), and Functional\nFailure Identification and Propagation (FFIP), along with the current state of\nthe literature in terms of their integration into Model-Based Systems\nEngineering (MBSE). A two-phase approach is adopted. The first phase is focused\non contrasting FMEA, FHA, and FFIP techniques, examining their procedures,\nalong with a documentation of their relative strengths and limitations. Our\nanalysis highlights FFIP's capability in identifying emergent system behaviors,\nsecond-order effects, and fault propagation; thus, suggesting it is better\nsuited for the safety needs of modern interconnected systems. Second, we review\nthe existing research on the efforts to integrate each of these methods into\nMBSE. We find that MBSE integration efforts primarily focus on FMEA, and\nintegration of FHA and FFIP is nascent. Additionally, FMEA-MBSE integration\nefforts could be organized into four categories: model-to-model transformation,\nuse of external customized algorithms, built-in MBSE packages, and manual use\nof standard MBSE diagrams. While our findings indicate a variety of MBSE\nintegration approaches, there is no universally established framework or\nstandard. This leaves room for an integration approach that could support the\nongoing Digital Engineering transformation efforts by enabling a more\nsynergistic lifecycle safety management methods and tools.", "AI": {"tldr": "This paper compares major safety analysis techniques (FMEA, FHA, FFIP), highlighting FFIP's strengths for complex systems and reviewing how these techniques are incorporated into MBSE. Integration efforts mainly focus on FMEA and lack standardization, revealing the need for a unified MBSE safety analysis framework.", "motivation": "With system complexity growing, early and effective safety analysis is needed to identify and address risks before they escalate, particularly within the context of Model-Based Systems Engineering (MBSE).", "method": "The paper adopts a two-phase approach: (1) a comparative analysis of FMEA, FHA, and FFIP safety analysis techniques with documentation of their strengths and limitations; (2) a literature review examining efforts to integrate these techniques into MBSE, with categorization of FMEA integration methods.", "result": "FFIP is found to be particularly capable in identifying emergent behaviors, second-order effects, and fault propagation\u2014making it well-suited to modern complex systems. Most MBSE integration efforts focus on FMEA, with FHA and FFIP integrations still nascent. FMEA-MBSE integration is organized into four categories (model transformation, external algorithms, built-in packages, manual diagramming).", "conclusion": "There is no universally accepted framework for integrating safety analysis techniques into MBSE, indicating an opportunity for developing a more unified approach that supports digital engineering and lifecycle safety management."}}
{"id": "2511.02876", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02876", "abs": "https://arxiv.org/abs/2511.02876", "authors": ["Anjali Chouhan", "Sruti Srinivasa Ragavan", "Amey Karkare"], "title": "CS Educator challenges and their solutions : A systematic mapping study", "comment": null, "summary": "Computer Science (CS) education is expanding rapidly, but educators continue\nto face persistent challenges in teaching and learning environments.Despite\ngrowing interest, limited systematic work exists to categorize and synthesize\nthe specific challenges faced by CS educators and the remedies adopted in\nresponse.This is problematic because it remains unclear which areas have been\nthoroughly addressed and which still lack sufficient scholarly attention. In\nthis study, we conducted a structured literature review of peer-reviewed\nresearch papers published over the last five years, focusing on challenges and\nremedies across ten categorized themes, including pedagogical, emotional,\ntechnological, and institutional dimensions.Our analysis revealed recurring\nissues in areas such as assessment practices, teacher training, classroom\nmanagement, and emotional well-being, along with various strategies such as\nprofessional development programs and policy interventions adopted to mitigate\nthem while also revealing several areas that have received insufficient\nattention.This review offers a consolidated understanding of the CS education\nlandscape, providing valuable insights for researchers, curriculum designers,\nand policymakers aiming to improve teaching effectiveness and educator support.", "AI": {"tldr": "A structured review of recent literature reveals ongoing challenges and strategies in CS education across ten themes, pointing out both well-addressed and neglected areas, with practical insights for improving teacher support and effectiveness.", "motivation": "Computer Science (CS) education is growing fast, yet educators still encounter persistent obstacles. There's a lack of systematic research categorizing these challenges and the remedies used, making it difficult to know which issues have been addressed and which remain under-explored.", "method": "The authors conducted a structured literature review, analyzing peer-reviewed research papers from the last five years. They focused on challenges and remedies within ten categories, such as pedagogical, emotional, technological, and institutional aspects.", "result": "The study found recurring challenges in assessment practices, teacher training, classroom management, and emotional well-being. It also identified strategies like professional development programs and policy interventions, while highlighting several under-researched areas.", "conclusion": "This review synthesizes recent research, offering a comprehensive overview of issues and responses in CS education, and serves as a resource for researchers, curriculum designers, and policymakers to enhance teaching and support for educators."}}
{"id": "2511.02885", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02885", "abs": "https://arxiv.org/abs/2511.02885", "authors": ["Gwendal Jouneaux", "Jordi Cabot"], "title": "AgentSLA : Towards a Service Level Agreement for AI Agents", "comment": null, "summary": "AI components are increasingly becoming a key element of all types of\nsoftware systems to enhance their functionality. These AI components are often\nimplemented as AI Agents, offering more autonomy than a plain integration of\nLarge Language Models (LLMs), moving from a Model-as-a-Service paradigm to an\nAgent-as-a-Service one, bringing new challenges to the development of smart\nsoftware systems. Indeed, while support for the design, implementation, and\ndeployment of those agents exist, the specification of Quality of Service (QoS)\nand definition of Service Level Agreements (SLAs) aspects for those agents,\nimportant to ensure the quality of the resulting systems, remains an open\nchallenge. Part of this is due to the difficulty to clearly define quality in\nthe context of AI components, resulting in a lack of consensus on how to best\napproach Quality Assurance (QA) for these types of systems. To address this\nchallenge, this paper proposes both a quality model for AI agents based on the\nISO/IEC 25010 standard, and a domain specific language to support the\ndefinition of SLAs for the services provided by these AI agents.", "AI": {"tldr": "This paper tackles the lack of standardized ways to define quality and SLAs for AI agents. It presents a new quality model based on ISO/IEC 25010 and a domain-specific language to help specify service agreements, aiming to improve quality assurance in systems using AI agents.", "motivation": "AI agents are becoming essential for smart software systems, transitioning from simple LLM integrations to more autonomous agents. However, defining and assuring quality (Quality of Service and Service Level Agreements) for these agents is difficult and lacks standardized approaches, creating challenges for software developers.", "method": "The paper proposes a quality model for AI agents grounded in the ISO/IEC 25010 standard. Additionally, it introduces a domain-specific language (DSL) to help specify SLAs for the services these AI agents provide.", "result": "A structured quality model tailored to AI agents and a newly developed DSL for SLA specification are presented, aiming to improve quality assurance in systems involving AI agents.", "conclusion": "The proposed quality model and DSL provide a foundation for better defining and managing the quality and SLAs of AI agent-based services, addressing a major gap in current software engineering practices for intelligent systems."}}
{"id": "2511.02922", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02922", "abs": "https://arxiv.org/abs/2511.02922", "authors": ["Yunhan Qiao", "Christopher Hundhausen", "Summit Haque", "Md Istiak Hossain Shihab"], "title": "Comprehension-Performance Gap in GenAI-Assisted Brownfield Programming: A Replication and Extension", "comment": "12 pages", "summary": "Code comprehension is essential for brownfield programming tasks, in which\ndevelopers maintain and enhance legacy code bases. Generative AI (GenAI) coding\nassistants such as GitHub Copilot have been shown to improve developer\nproductivity, but their impact on code understanding is less clear. We\nreplicate and extend a previous study by exploring both performance and\ncomprehension in GenAI-assisted brownfield programming tasks. In a\nwithin-subjects experimental study, 18 computer science graduate students\ncompleted feature implementation tasks with and without Copilot. Results show\nthat Copilot significantly reduced task time and increased the number of test\ncases passed. However, comprehension scores did not differ across conditions,\nrevealing a comprehension-performance gap: participants passed more test cases\nwith Copilot, but did not demonstrate greater understanding of the legacy\ncodebase. Moreover, we failed to find a correlation between comprehension and\ntask performance. These findings suggest that while GenAI tools can accelerate\nprogramming progress in a legacy codebase, such progress may come without an\nimproved understanding of that codebase. We consider the implications of these\nfindings for programming education and GenAI tool design.", "AI": {"tldr": "Using AI coding assistants like Copilot helps developers work faster and pass more tests on legacy code, but doesn't actually help them understand the code better. This suggests a gap between doing more tasks and really comprehending the code.", "motivation": "The study is motivated by the need to understand how generative AI coding assistants, like GitHub Copilot, affect code comprehension during brownfield programming tasks, i.e., working with and modifying legacy code.", "method": "A within-subjects experimental study was conducted with 18 computer science graduate students. Each participant completed feature implementation tasks on legacy code with and without the assistance of Copilot. Performance and comprehension were both measured.", "result": "Results showed that using Copilot significantly reduced the time spent on tasks and increased the number of test cases passed. However, comprehension scores did not improve when using Copilot, and there was no correlation found between task performance and comprehension.", "conclusion": "GenAI tools like Copilot can speed up programming in legacy code by increasing productivity and performance. However, this progress does not translate into better comprehension of legacy code. This points to a comprehension-performance gap, which has implications for programming education and tool design."}}
{"id": "2511.02927", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02927", "abs": "https://arxiv.org/abs/2511.02927", "authors": ["Rafael Baez", "Alejandro Olivas", "Nathan K. Diamond", "Marcelo Frias", "Yannic Noller", "Saeid Tizpaz-Niari"], "title": "Risk Estimation in Differential Fuzzing via Extreme Value Theory", "comment": "In Proceedings of the 40th IEEE/ACM International Conference on\n  Automated Software Engineering (ASE 25), 13 Pages, 4 Figures, 5 Tables", "summary": "Differential testing is a highly effective technique for automatically\ndetecting software bugs and vulnerabilities when the specifications involve an\nanalysis over multiple executions simultaneously. Differential fuzzing, in\nparticular, operates as a guided randomized search, aiming to find (similar)\ninputs that lead to a maximum difference in software outputs or their\nbehaviors. However, fuzzing, as a dynamic analysis, lacks any guarantees on the\nabsence of bugs: from a differential fuzzing campaign that has observed no bugs\n(or a minimal difference), what is the risk of observing a bug (or a larger\ndifference) if we run the fuzzer for one or more steps?\n  This paper investigates the application of Extreme Value Theory (EVT) to\naddress the risk of missing or underestimating bugs in differential fuzzing.\nThe key observation is that differential fuzzing as a random process resembles\nthe maximum distribution of observed differences. Hence, EVT, a branch of\nstatistics dealing with extreme values, is an ideal framework to analyze the\ntail of the differential fuzzing campaign to contain the risk. We perform\nexperiments on a set of real-world Java libraries and use differential fuzzing\nto find information leaks via side channels in these libraries. We first\nexplore the feasibility of EVT for this task and the optimal hyperparameters\nfor EVT distributions. We then compare EVT-based extrapolation against baseline\nstatistical methods like Markov's as well as Chebyshev's inequalities, and the\nBayes factor. EVT-based extrapolations outperform the baseline techniques in\n14.3% of cases and tie with the baseline in 64.2% of cases. Finally, we\nevaluate the accuracy and performance gains of EVT-enabled differential fuzzing\nin real-world Java libraries, where we reported an average saving of tens of\nmillions of bytecode executions by an early stop.", "AI": {"tldr": "This paper shows that using Extreme Value Theory (EVT) helps better assess and manage the risk of undetected bugs in differential fuzzing, outperforming traditional statistical methods and leading to significant efficiency improvements in real-world software analysis.", "motivation": "Differential fuzzing is effective in discovering bugs, but traditional dynamic analysis methods offer no assurance about bugs that may go undetected in a fuzzing campaign, creating risk in software assurance.", "method": "The paper applies Extreme Value Theory (EVT), a statistical approach for analyzing extreme values, to model the distribution of maximum observed differences in differential fuzzing. Through experiments on real-world Java libraries, the study tests the feasibility of EVT, tunes hyperparameters, and compares EVT-based extrapolation to other statistical techniques such as Markov's and Chebyshev's inequalities and the Bayes factor.", "result": "EVT-based extrapolation outperforms baseline statistical techniques in 14.3% of cases and ties in 64.2% of cases. The EVT-enabled differential fuzzing approach achieves notable efficiency: saving tens of millions of bytecode executions via earlier stopping.", "conclusion": "EVT provides a promising framework for quantifying and minimizing the risk of missing bugs in differential fuzzing campaigns, while enabling substantial computational savings."}}
{"id": "2511.03026", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03026", "abs": "https://arxiv.org/abs/2511.03026", "authors": ["Logan Murphy", "Torin Viger", "Alessio Di Sandro", "Aren A. Babikian", "Marsha Chechik"], "title": "Assurance Case Development for Evolving Software Product Lines: A Formal Approach", "comment": null, "summary": "In critical software engineering, structured assurance cases (ACs) are used\nto demonstrate how key system properties are supported by evidence (e.g., test\nresults, proofs). Creating rigorous ACs is particularly challenging in the\ncontext of software product lines (SPLs), i.e, sets of software products with\noverlapping but distinct features and behaviours. Since SPLs can encompass very\nlarge numbers of products, developing a rigorous AC for each product\nindividually is infeasible. Moreover, if the SPL evolves, e.g., by the\nmodification or introduction of features, it can be infeasible to assess the\nimpact of this change. Instead, the development and maintenance of ACs ought to\nbe lifted such that a single AC can be developed for the entire SPL\nsimultaneously, and be analyzed for regression in a variability-aware fashion.\nIn this article, we describe a formal approach to lifted AC development and\nregression analysis. We formalize a language of variability-aware ACs for SPLs\nand study the lifting of template-based AC development. We also define a\nregression analysis to determine the effects of SPL evolutions on\nvariability-aware ACs. We describe a model-based assurance management tool\nwhich implements these techniques, and illustrate our contributions by\ndeveloping an AC for a product line of medical devices.", "AI": {"tldr": "The paper introduces a formal, tool-supported approach that enables scalable, efficient development and regression analysis of assurance cases across entire software product lines, demonstrated with a medical device product line example.", "motivation": "Creating rigorous assurance cases (ACs) for critical software is essential but particularly challenging in software product lines (SPLs), which include many products with shared and differing features. Managing evidence for each product individually is infeasible, especially as the product line evolves.", "method": "The paper formalizes a language for variability-aware assurance cases tailored to SPLs and studies 'lifting' template-based assurance case development. It also develops a regression analysis to understand the effects of SPL changes on these assurance cases. A model-based management tool implementing these techniques is also described and demonstrated on a medical device product line.", "result": "The authors successfully formalized an approach and implemented a tool for developing and maintaining ACs for entire SPLs, demonstrated through a medical device example, enabling more efficient and manageable assurance for evolving SPLs.", "conclusion": "Lifting AC development to the product line level and enabling variability-aware regression analysis makes it feasible to assure large and evolving software product lines more rigorously and efficiently."}}
{"id": "2511.03103", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03103", "abs": "https://arxiv.org/abs/2511.03103", "authors": ["Rafael Jos\u00e9 Moura", "Maria Gizele Nascimento", "Fumio Machida", "Ermeson Andrade"], "title": "Adaptive Detection of Software Aging under Workload Shift", "comment": "SIMP\\'OSIO EM SISTEMAS COMPUTACIONAIS DE ALTO DESEMPENHO (SSCAD)", "summary": "Software aging is a phenomenon that affects long-running systems, leading to\nprogressive performance degradation and increasing the risk of failures. To\nmitigate this problem, this work proposes an adaptive approach based on machine\nlearning for software aging detection in environments subject to dynamic\nworkload conditions. We evaluate and compare a static model with adaptive\nmodels that incorporate adaptive detectors, specifically the Drift Detection\nMethod (DDM) and Adaptive Windowing (ADWIN), originally developed for concept\ndrift scenarios and applied in this work to handle workload shifts. Experiments\nwith simulated sudden, gradual, and recurring workload transitions show that\nstatic models suffer a notable performance drop when applied to unseen workload\nprofiles, whereas the adaptive model with ADWIN maintains high accuracy,\nachieving an F1-Score above 0.93 in all analyzed scenarios.", "AI": {"tldr": "Adaptive machine learning models, especially with ADWIN, reliably detect software aging despite varied workloads, surpassing static approaches and keeping performance high (F1 > 0.93).", "motivation": "Software aging leads to performance degradation and failures in long-running systems, especially under dynamic workload conditions. Current static detection models are insufficient to address such variations.", "method": "The paper proposes an adaptive approach using machine learning for detecting software aging. It evaluates static models against adaptive models that use Drift Detection Method (DDM) and Adaptive Windowing (ADWIN), which are designed to handle concept drift and are here applied to shift in workload profiles. Experiments are conducted with simulated workloads exhibiting sudden, gradual, and recurring transitions.", "result": "Static models experience significant performance drops when exposed to new workload profiles. In contrast, the adaptive model using ADWIN consistently maintains high accuracy, achieving an F1-Score above 0.93 across all tested scenarios.", "conclusion": "Adaptive models, particularly those incorporating ADWIN, are highly effective at detecting software aging under changing workload conditions, outperforming static models and maintaining robust performance."}}
{"id": "2511.03136", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03136", "abs": "https://arxiv.org/abs/2511.03136", "authors": ["Kexing Ji", "Shiyun Fu", "Cuiyun Gao", "Yujia Chen", "Zezhou Yang", "Chaozheng Wang", "Yuetang Deng"], "title": "Automated Prompt Generation for Code Intelligence: An Empirical study and Experience in WeChat", "comment": "Accepted by ASE 2025 Industry Track", "summary": "Large Code Models (LCMs) show potential in code intelligence, but their\neffectiveness is greatly influenced by prompt quality. Current prompt design is\nmostly manual, which is time-consuming and highly dependent on specific LCMs\nand tasks. While automated prompt generation (APG) exists in NLP, it is\nunderexplored for code intelligence. This creates a gap, as automating the\nprompt process is essential for developers facing diverse tasks and black-box\nLCMs.\n  To mitigate this, we empirically investigate two important parts of APG:\nInstruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides a\ntask-related description to instruct LCMs, while MSR guides them to produce\nlogical steps before the final answer. We evaluate widely-used APG methods for\neach part on four open-source LCMs and three code intelligence tasks: code\ntranslation (PL-PL), code summarization (PL-NL), and API recommendation\n(NL-PL).Experimental results indicate that both IG and MSR dramatically enhance\nperformance compared to basic prompts. Based on these results, we propose a\nnovel APG approach combining the best methods of the two parts. Experiments\nshow our approach achieves average improvements of 28.38% in CodeBLEU (code\ntranslation), 58.11% in ROUGE-L (code summarization), and 84.53% in\nSuccessRate@1 (API recommendation) over basic prompts. To validate its\neffectiveness in an industrial scenario, we evaluate our approach on\nWeChat-Bench, a proprietary dataset, achieving an average MRR improvement of\n148.89% for API recommendation.", "AI": {"tldr": "Manual prompt engineering for code models is tedious and task-specific. This work empirically evaluates and combines existing automated prompt generation methods\u2014Instruction Generation and Multi-Step Reasoning\u2014for code intelligence tasks. The combined approach dramatically improves performance over basic prompts (up to 148% in industrial API recommendation) for open-source and real-world data.", "motivation": "Large Code Models (LCMs) are powerful but rely heavily on well-crafted prompts to perform effectively in code intelligence tasks. Manual prompt design is labor-intensive and often tailored to specific models or tasks. Automated prompt generation (APG) is established in NLP but insufficiently explored for code intelligence, creating a need for automated solutions to boost LCM performance on diverse and opaque tasks.", "method": "The study empirically investigates APG by focusing on two key components: Instruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides task-specific context to guide LCMs, while MSR encourages logical step-by-step reasoning. They evaluate popular APG methods for IG and MSR across four open-source LCMs and three code intelligence tasks\u2014code translation, summarization, and API recommendation. Then, they propose a combined APG approach using the best-performing IG and MSR methods.", "result": "Both IG and MSR markedly improve LCM performance compared to basic prompting. The new combined APG approach delivers significant average metric gains: 28.38% in CodeBLEU for code translation, 58.11% in ROUGE-L for summarization, and 84.53% in SuccessRate@1 for API recommendation. In a real-world industrial test (WeChat-Bench), there is an impressive 148.89% increase in average MRR for API recommendation.", "conclusion": "Automated prompt generation methods, especially combining Instruction Generation and Multi-Step Reasoning, substantially boost the effectiveness of Large Code Models in varied code intelligence tasks, both in open-source and real-world scenarios. The proposed approach significantly beats manual or basic prompts."}}
{"id": "2511.03153", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03153", "abs": "https://arxiv.org/abs/2511.03153", "authors": ["Khouloud Oueslati", "Maxime Lamothe", "Foutse Khomh"], "title": "RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring", "comment": null, "summary": "Large Language Models (LLMs) have substantially influenced various software\nengineering tasks. Indeed, in the case of software refactoring, traditional\nLLMs have shown the ability to reduce development time and enhance code\nquality. However, these LLMs often rely on static, detailed instructions for\nspecific tasks. In contrast, LLM-based agents can dynamically adapt to evolving\ncontexts and autonomously make decisions by interacting with software tools and\nexecuting workflows. In this paper, we explore the potential of LLM-based\nagents in supporting refactoring activities. Specifically, we introduce\nRefAgent, a multi-agent LLM-based framework for end-to-end software\nrefactoring. RefAgent consists of specialized agents responsible for planning,\nexecuting, testing, and iteratively refining refactorings using self-reflection\nand tool-calling capabilities. We evaluate RefAgent on eight open-source Java\nprojects, comparing its effectiveness against a single-agent approach, a\nsearch-based refactoring tool, and historical developer refactorings. Our\nassessment focuses on: (1) the impact of generated refactorings on software\nquality, (2) the ability to identify refactoring opportunities, and (3) the\ncontribution of each LLM agent through an ablation study. Our results show that\nRefAgent achieves a median unit test pass rate of 90%, reduces code smells by a\nmedian of 52.5%, and improves key quality attributes (e.g., reusability) by a\nmedian of 8.6%. Additionally, it closely aligns with developer refactorings and\nthe search-based tool in identifying refactoring opportunities, attaining a\nmedian F1-score of 79.15% and 72.7%, respectively. Compared to single-agent\napproaches, RefAgent improves the median unit test pass rate by 64.7% and the\nmedian compilation success rate by 40.1%. These findings highlight the promise\nof multi-agent architectures in advancing automated software refactoring.", "AI": {"tldr": "RefAgent, a multi-agent LLM framework for software refactoring, outperforms traditional and single-agent methods in code quality, reliability, and refactoring opportunity identification, showing strong potential for automated code improvement.", "motivation": "Existing LLMs are limited in software refactoring tasks due to reliance on static instructions. LLM-based agents offer dynamic adaptation and autonomy, motivating the investigation of their effectiveness and the proposal of a flexible, multi-agent system.", "method": "The authors designed RefAgent, a multi-agent LLM-based framework with specialized agents (planning, execution, testing, refinement) utilizing self-reflection and tool-calling. Performance was empirically evaluated on eight open-source Java projects and compared to single-agent, search-based, and historical developer approaches, using metrics such as unit test pass rate, code smell reduction, and F1-score.", "result": "RefAgent achieved a median unit test pass rate of 90%, median code smell reduction of 52.5%, and improved key quality attributes by 8.6%. It closely matches developers and tools in identifying refactoring opportunities (median F1-score: 79.15%/72.7%). It outperformed single-agent approaches in test pass rate (by 64.7%) and compilation success (by 40.1%).", "conclusion": "Multi-agent LLM-based frameworks like RefAgent significantly advance automated software refactoring, achieving high code quality, reliability, and alignment with human and tool-based refactoring strategies."}}
{"id": "2511.03182", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03182", "abs": "https://arxiv.org/abs/2511.03182", "authors": ["Vinaik Chhetri", "A. B Siddique", "Umar Farooq"], "title": "Understanding Robustness of Model Editing in Code LLMs: An Empirical Study", "comment": "26 pages, 2 figures, 15 tables", "summary": "Large language models (LLMs) are increasingly used in software development.\nHowever, while LLMs remain static after pretraining, programming languages and\nAPIs continue to evolve, leading to the generation of deprecated or\nincompatible code that undermines reliability. Retraining LLMs from scratch to\nreflect such changes is computationally expensive, making model editing a\npromising lightweight alternative that updates only a small subset of\nparameters. Despite its potential, it remains unclear whether model editing\nyields genuine syntactic and semantic adaptations or merely superficial fixes.\nIn this work, we present a systematic study of five state-of-the-art model\nediting methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We\napply these methods to three leading open-source code LLMs, CodeLlama,\nCodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios.\nOur evaluation covers both instant and sequential editing settings, using three\ndisjoint evaluation sets designed to assess reliability, generalization, and\nspecificity. We measure model correctness at three levels: successful\ncompilation, partial test case pass, and full test pass. Our findings show that\ninstant edits consistently degrade model performance, with syntactic validity\ndropping by up to 86 percentage points and functional correctness declining by\n45 points even in the best-performing setting. Sequential edits further amplify\nthis degradation, and in some cases, model performance collapses entirely.\nAcross all models, most passing generations relied on workarounds rather than\ncorrectly adopting the intended changes, while faulty adoptions that result in\ntest failures or compilation errors were significantly more frequent. Correct\nadoptions, where the model correctly integrates the intended change, occurred\nin only about 6% of cases.", "AI": {"tldr": "Updating code LLMs using model editing methods to reflect API changes often fails: performance drops significantly, most outputs use workarounds instead of correct fixes, and only a small fraction (<6%) of changes are properly adopted. Existing editing approaches are insufficient for reliable code evolution.", "motivation": "LLMs are increasingly used in programming, but since they don't adapt after pretraining, they may produce outdated or incompatible code as languages and APIs change. Retraining is expensive, so lightweight model editing is proposed, but its true effectiveness is unclear.", "method": "A systematic study of five state-of-the-art model editing methods (Constrained FT, GRACE, MEMIT, PMET, ROME) was conducted on three leading code LLMs (CodeLlama, CodeQwen1.5, DeepSeek-Coder) using controlled API deprecation scenarios. Evaluation assessed reliability, generalization, and specificity through three different metrics: compilation success, partial test pass, and full test pass.", "result": "Instant model edits degrade syntactic validity (up to \u221286 points) and functional correctness (\u221245 points at best). Sequential edits worsen these effects; sometimes, models fail completely. Most successful outputs use workarounds rather than correct API changes. Faulty or incorrect adoptions are common, while correct integrations of the intended change occur only ~6% of the time.", "conclusion": "Current model editing methods struggle to effectively update code LLMs for API changes; edits often weaken models and rarely produce genuinely correct and reliable code. Reliance on workarounds highlights the need for better techniques."}}
{"id": "2511.03404", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03404", "abs": "https://arxiv.org/abs/2511.03404", "authors": ["Qianhui Zhao", "Li Zhang", "Fang Liu", "Junhang Cheng", "Chengru Wu", "Junchen Ai", "Qiaoyuanhe Meng", "Lichen Zhang", "Xiaoli Lian", "Shubin Song", "Yuanping Guo"], "title": "Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling", "comment": null, "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nprogress in automated code generation. In real-world software engineering, the\ngrowing demand for rapid iteration and continuous delivery underscores the\nimportance of project-level code generation, where LLMs are expected to\ngenerate complete software projects directly from complex user requirements.\nAlthough existing studies have made initial explorations, they still face key\nlimitations, including unrealistic datasets and unreliable evaluation metrics\nthat fail to reflect real-world complexity, the semantic gap between\nhuman-written requirements and machine-interpretable structures, and\ndifficulties in managing hierarchical dependencies and maintaining quality\nthroughout the generation process. To address these limitations, we first\nintroduce CodeProjectEval, a project-level code generation dataset built from\n18 real-world repositories with 12.7 files and 2,388.6 lines of code per task\non average, supplemented with documentation and executable test cases for\nautomatic evaluation. We further propose ProjectGen, a multi-agent framework\nthat decomposes projects into architecture design, skeleton generation, and\ncode filling stages with iterative refinement and memory-based context\nmanagement. Within this framework, we introduce the Semantic Software\nArchitecture Tree (SSAT), a structured and semantically rich representation\nthat effectively bridges user requirements and source code implementation.\nExperiments show that ProjectGen achieves state-of-the-art performance, passing\n52/124 test cases on the small-scale project-level code generation dataset\nDevBench, a 57% improvement over the baseline approaches, and 310 test cases on\nCodeProjectEval, representing an improvement of roughly tenfold compared to the\nbaselines.", "AI": {"tldr": "This paper introduces CodeProjectEval, a realistic project-level code generation dataset, and ProjectGen, a multi-agent LLM-driven framework featuring semantic architecture mapping and iterative generation. ProjectGen achieves substantial performance gains over baselines, greatly improving real-world automatic software project generation.", "motivation": "Existing project-level code generation methods suffer from unrealistic datasets, unreliable metrics, and difficulty mapping requirements to code and managing complexity. There is a pressing need to enable LLMs to generate complete projects directly from real user requirements for practical software engineering use.", "method": "The study introduces CodeProjectEval, a new dataset for project-level code generation, and proposes ProjectGen\u2014a framework that decomposes the generation task into architecture design, skeleton generation, and code filling, supplemented by iterative refinement and memory-based context management. The framework uses SSAT, a structured semantic representation, to bridge requirements and implementation.", "result": "ProjectGen outperformed baseline methods, passing 52/124 test cases on DevBench (57% improvement) and 310 test cases on CodeProjectEval (about ten times more than baselines), indicating state-of-the-art performance.", "conclusion": "ProjectGen, with its multi-agent framework and SSAT, significantly advances the capability of LLMs for project-level code generation, demonstrating substantial improvements over prior baselines on real-world datasets."}}
{"id": "2511.03421", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03421", "abs": "https://arxiv.org/abs/2511.03421", "authors": ["Shihai Wang", "Tao Chen"], "title": "Light over Heavy: Automated Performance Requirements Quantification with Linguistic Inducement", "comment": "accepted by ICSE 2026", "summary": "Elicited performance requirements need to be quantified for compliance in\ndifferent engineering tasks, e.g., configuration tuning and performance\ntesting. Much existing work has relied on manual quantification, which is\nexpensive and error-prone due to the imprecision. In this paper, we present\nLQPR, a highly efficient automatic approach for performance requirements\nquantification.LQPR relies on a new theoretical framework that converts\nquantification as a classification problem. Despite the prevalent applications\nof Large Language Models (LLMs) for requirement analytics, LQPR takes a\ndifferent perspective to address the classification: we observed that\nperformance requirements can exhibit strong patterns and are often\nshort/concise, therefore we design a lightweight linguistically induced\nmatching mechanism. We compare LQPR against nine state-of-the-art\nlearning-based approaches over diverse datasets, demonstrating that it is\nranked as the sole best for 75% or more cases with two orders less cost. Our\nwork proves that, at least for performance requirement quantification,\nspecialized methods can be more suitable than the general LLM-driven\napproaches.", "AI": {"tldr": "LQPR is a fast and accurate automated method for quantifying performance requirements, outperforming general LLM approaches and existing learning-based solutions while being much less resource intensive.", "motivation": "Quantifying performance requirements is essential for engineering tasks but current methods are manual, costly, and prone to errors. There is a need for a more efficient and accurate automated approach.", "method": "The authors present LQPR, an automatic approach that treats quantification as a classification problem, using a lightweight linguistically induced matching mechanism instead of typical LLM-based methods.", "result": "LQPR outperformed nine state-of-the-art learning-based methods in varied datasets, achieving best ranking in 75% or more cases and operating at significantly lower computational cost.", "conclusion": "Specialized approaches like LQPR can be more effective and efficient than general LLM-driven techniques for quantifying performance requirements."}}
{"id": "2511.03517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03517", "abs": "https://arxiv.org/abs/2511.03517", "authors": ["Wencheng Ye", "Yan Liu"], "title": "U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility", "comment": null, "summary": "Large language models (LLMs) have shown strong capabilities in software\nengineering tasks, yet most existing LLM-based SWE-Agents mainly tackle\nwell-defined problems using conventional methods, often overlooking alternative\nor innovative solutions beyond their predefined frameworks. This limitation is\nevident in open-world software environments, where emerging challenges\ntranscend established paradigms.\n  We propose U2F (Unknown Unknowns to Functional solutions), a\ncognitive-inspired, uncertainty-embracing multi-agent framework that\nsystematically surfaces \"Unknown Unknowns\" - novel solution pathways absent\nfrom initial formulations but holding innovative potential. U2F consists of two\nkey components: (1) a Discovery-Exploration-Integration agent system for\nuncovering and synthesizing potential solutions, and (2) cognitive enhancement\nmechanisms across three dimensions: cross-domain analogical reasoning, reverse\nthinking, and external validation, which strategically reframe and extend\nconventional solution boundaries.\n  Applied to 218 real-world software enabler stories curated from authentic\nengineering tasks, U2F achieved notable improvements: human experts reported a\n14 percent increase in overall novelty, 51 percent improvement in semantic\nnovelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based\nevaluator. These results highlight the potential of embracing uncertainty as a\ncatalyst for innovation in software engineering.", "AI": {"tldr": "U2F is a new cognitive-inspired framework that helps software agents find novel and feasible solutions beyond conventional approaches. By embracing uncertainty and using analogical reasoning, reverse thinking, and external validation, U2F significantly increases the novelty of solutions in real-world software tasks.", "motivation": "Most existing LLM-based software engineering (SWE) agents focus on well-defined problems and use conventional methods, which limits their ability to find innovative or alternative solutions. This is especially problematic in open-world environments where new challenges can't be addressed by established paradigms. Thus, there is a need for approaches that can systematically explore and surface novel solutions.", "method": "The authors propose U2F (Unknown Unknowns to Functional solutions), a cognitive-inspired, multi-agent framework. U2F comprises two main components: (1) a Discovery-Exploration-Integration agent system that uncovers and synthesizes new solution pathways, and (2) cognitive enhancement mechanisms (cross-domain analogical reasoning, reverse thinking, and external validation) to expand conventional solution boundaries.", "result": "When tested on 218 real-world software enabler stories, U2F led to a 14% increase in overall solution novelty, a 51% improvement in semantic novelty, and maintained a high feasibility score (4.02/5.0), as judged by human experts and validated by an LLM-based evaluator.", "conclusion": "The study demonstrates that U2F's approach of embracing and systematically surfacing uncertainty results in significantly more innovative solutions for software engineering tasks, suggesting this method catalyzes creativity and expands solution spaces beyond conventional frameworks."}}
{"id": "2511.03549", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03549", "abs": "https://arxiv.org/abs/2511.03549", "authors": ["Ziv Nevo", "Orna Raz", "Karen Yorav"], "title": "Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code Understanding", "comment": "7 pages, 6 figures, to be published in AISM 2025, see\n  https://aism25.github.io/aism25/", "summary": "Understanding the purpose of source code is a critical task in software\nmaintenance, onboarding, and modernization. While large language models (LLMs)\nhave shown promise in generating code explanations, they often lack grounding\nin the broader software engineering context. We propose a novel approach that\nleverages natural language artifacts from GitHub -- such as pull request\ndescriptions, issue descriptions and discussions, and commit messages -- to\nenhance LLM-based code understanding. Our system consists of three components:\none that extracts and structures relevant GitHub context, another that uses\nthis context to generate high-level explanations of the code's purpose, and a\nthird that validates the explanation. We implemented this as a standalone tool,\nas well as a server within the Model Context Protocol (MCP), enabling\nintegration with other AI-assisted development tools. Our main use case is that\nof enhancing a standard LLM-based code explanation with code insights that our\nsystem generates. To evaluate explanations' quality, we conducted a small scale\nuser study, with developers of several open projects, as well as developers of\nproprietary projects. Our user study indicates that when insights are generated\nthey often are helpful and non trivial, and are free from hallucinations.", "AI": {"tldr": "The paper presents a tool that uses natural language context from GitHub to boost LLMs' code explanations. Evaluated via a user study, it shows improved, meaningful, and reliable insights for developers.", "motivation": "LLMs can generate code explanations but often lack broader software engineering context, limiting their usefulness for tasks like maintenance and onboarding. Leveraging natural language artifacts from software repositories can potentially ground and enhance code understanding.", "method": "The proposed system consists of three modules: extracting relevant GitHub artifacts, generating code purpose explanations using this context, and validating the explanations. The system is implemented as both a standalone tool and a server within the Model Context Protocol (MCP). Evaluation is done via a user study with developers from open and proprietary projects.", "result": "The user study found that the generated insights are generally helpful, non-trivial, and do not exhibit hallucinations, suggesting the system improves upon standard LLM-based code explanations.", "conclusion": "The study concludes that enriching LLM-based code explanations with contextual insights from GitHub artifacts provides developers with more helpful, non-trivial, and hallucination-free explanations."}}
{"id": "2511.03690", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03690", "abs": "https://arxiv.org/abs/2511.03690", "authors": ["Xingyao Wang", "Simon Rosenberg", "Juan Michelini", "Calvin Smith", "Hoang Tran", "Engel Nyst", "Rohit Malhotra", "Xuhui Zhou", "Valerie Chen", "Robert Brennan", "Graham Neubig"], "title": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents", "comment": null, "summary": "Agents are now used widely in the process of software development, but\nbuilding production-ready software engineering agents is a complex task.\nDeploying software agents effectively requires flexibility in implementation\nand experimentation, reliable and secure execution, and interfaces for users to\ninteract with agents. In this paper, we present the OpenHands Software Agent\nSDK, a toolkit for implementing software development agents that satisfy these\ndesiderata. This toolkit is a complete architectural redesign of the agent\ncomponents of the popular OpenHands framework for software development agents,\nwhich has 64k+ GitHub stars. To achieve flexibility, we design a simple\ninterface for implementing agents that requires only a few lines of code in the\ndefault case, but is easily extensible to more complex, full-featured agents\nwith features such as custom tools, memory management, and more. For security\nand reliability, it delivers seamless local-to-remote execution portability,\nintegrated REST/WebSocket services. For interaction with human users, it can\nconnect directly to a variety of interfaces, such as visual workspaces (VS\nCode, VNC, browser), command-line interfaces, and APIs. Compared with existing\nSDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native\nsandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and\nbuilt-in security analysis. Empirical results on SWE-Bench Verified and GAIA\nbenchmarks demonstrate strong performance. Put together, these elements allow\nthe OpenHands Software Agent SDK to provide a practical foundation for\nprototyping, unlocking new classes of custom applications, and reliably\ndeploying agents at scale.", "AI": {"tldr": "OpenHands Software Agent SDK offers a comprehensive, secure, and flexible toolkit for building and deploying software engineering agents, with strong benchmark results and features surpassing leading alternatives.", "motivation": "The motivation is to address the complexity and challenges in building, deploying, and interacting with production-ready software engineering agents, including flexibility, reliability, security, and user interaction.", "method": "The paper introduces the OpenHands Software Agent SDK, which is an architectural redesign of the agent component from OpenHands framework. The SDK features a simple extensible interface, supports custom tools and memory, enables seamless execution locally/remotely, integrates communication protocols (REST/WebSocket), supports multiple user interfaces (visual, CLI, APIs), and includes unique offerings like sandboxed execution, lifecycle control, multi-LLM routing, and built-in security analysis. The performance is evaluated empirically using SWE-Bench Verified and GAIA benchmarks.", "result": "The empirical results on benchmarks demonstrate strong performance for the OpenHands SDK. It is able to flexibly, securely, and reliably enable software engineering agent deployment with features not present in comparable SDKs.", "conclusion": "The OpenHands Software Agent SDK provides a practical and scalable solution for developing, prototyping, and deploying robust software development agents, unlocking new applications and improving deployment reliability."}}
