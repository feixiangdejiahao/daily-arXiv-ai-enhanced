<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.PL](#cs.PL) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis](https://arxiv.org/abs/2510.11722)
*Haruhiko Yoshioka,Kazumasa Shimari,Hidetake Uwano,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: eye2vec streamlines the study of eye movements in code reading by automating fixation analysis using distributed representations, making research easier and results more meaningful.


<details>
  <summary>Details</summary>
Motivation: Current eye-tracking studies in software comprehension require manually selected and defined areas of interest (AOIs), which is time-consuming and subject to interpretation, hampering generalization and automation.

Method: eye2vec models eye movement as continuous transitions between syntactic code elements using distributed representations, thus allowing automated and diverse analyses.

Result: eye2vec can represent eye fixation transitions in a way that supports a range of data analysis approaches and enables richer semantic understanding compared to traditional AOI-based manual methods.

Conclusion: eye2vec provides a new infrastructure that enables more flexible and semantically rich analysis of software developers' eye movements over source code, improving on traditional eye-tracking study limitations.

Abstract: This paper presents eye2vec, an infrastructure for analyzing software
developers' eye movements while reading source code. In common eye-tracking
studies in program comprehension, researchers must preselect analysis targets
such as control flow or syntactic elements, and then develop analysis methods
to extract appropriate metrics from the fixation for source code. Here,
researchers can define various levels of AOIs like words, lines, or code
blocks, and the difference leads to different results. Moreover, the
interpretation of fixation for word/line can vary across the purposes of the
analyses. Hence, the eye-tracking analysis is a difficult task that depends on
the time-consuming manual work of the researchers. eye2vec represents
continuous two fixations as transitions between syntactic elements using
distributed representations. The distributed representation facilitates the
adoption of diverse data analysis methods with rich semantic interpretations.

</details>


### [2] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: Rather than focusing only on model optimizations, this work highlights the importance of reducing noisy, verbose input data for LLMs. By treating token budgets as attention budgets and prioritizing task-relevant information, the paper suggests key future directions for more scalable and sustainable LLM-data workflows.


<details>
  <summary>Details</summary>
Motivation: LLMs are limited by the excessive and noisy nature of real-world, text-rich data when used in data-intensive applications. Current optimization efforts focus mostly on models themselves, neglecting upstream input verbosity, which hampers effectiveness and increases cost.

Method: Conceptual analysis and proposition for treating LLM token budgets as attention budgets. The paper outlines future research directions and system design principles rather than presenting empirical experiments.

Result: The authors propose to treat input verbosity reduction as strategic attention allocation rather than mere compression, identifying it as a crucial research direction. They highlight specific open challenges including benchmarks creation, adaptive reduction pipelines, and preprocessing integration.

Conclusion: Focusing on input-side, task-aware text reduction can improve the scalability, accuracy, and sustainability of integrating LLMs with data-intensive workflows.

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


### [3] [Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?](https://arxiv.org/abs/2510.12702)
*Cedric Richter,Heike Wehrheim*

Main category: cs.SE

TL;DR: LLMs can convert natural language hints in code into formal contracts (preconditions and postconditions), supporting more reliable automatic software verification. This reduces false alarms and helps find real bugs, bridging the gap caused by missing specifications in real-world code.


<details>
  <summary>Details</summary>
Motivation: Automatic software verifiers need formal specifications to work effectively, but such specifications are often missing in real-world code. LLMs show promise in inferring specifications from natural language hints.

Method: The paper introduces NL2Contract, a task using LLMs to translate informal natural language into formal contracts (preconditions and postconditions). It proposes metrics for soundness, bug discrimination, and usability in verification contexts. Experiments compare contract generation with postcondition-only approaches using various LLMs.

Result: LLMs can generate sound functional contracts for all possible inputs, with expressiveness sufficient to distinguish buggy from correct code. Verifiers with inferred contracts report fewer false alarms than those with only postconditions. Inferred preconditions generally match developer intent and help catch real-world bugs.

Conclusion: NL2Contract enables effective inference of formal software contracts from natural language, improving automatic verification by reducing false alarms and catching real-world bugs. LLMs are practical tools for closing the gap between informal code documentation and formal verification needs.

Abstract: Automatic software verifiers have become increasingly effective at the task
of checking software against (formal) specifications. Yet, their adoption in
practice has been hampered by the lack of such specifications in real world
code. Large Language Models (LLMs) have shown promise in inferring formal
postconditions from natural language hints embedded in code such as function
names, comments or documentation. Using the generated postconditions as
specifications in a subsequent verification, however, often leads verifiers to
suggest invalid inputs, hinting at potential issues that ultimately turn out to
be false alarms.
  To address this, we revisit the problem of specification inference from
natural language in the context of automatic software verification. In the
process, we introduce NL2Contract, the task of employing LLMs to translate
informal natural language into formal functional contracts, consisting of
postconditions as well as preconditions. We introduce metrics to validate and
compare different NL2Contract approaches, using soundness, bug discriminative
power of the generated contracts and their usability in the context of
automatic software verification as key metrics. We evaluate NL2Contract with
different LLMs and compare it to the task of postcondition generation
nl2postcond. Our evaluation shows that (1) LLMs are generally effective at
generating functional contracts sound for all possible inputs, (2) the
generated contracts are sufficiently expressive for discriminating buggy from
correct behavior, and (3) verifiers supplied with LLM inferred functional
contracts produce fewer false alarms than when provided with postconditions
alone. Further investigations show that LLM inferred preconditions generally
align well with developers intentions which allows us to use automatic software
verifiers to catch real-world bugs.

</details>


### [4] [Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling](https://arxiv.org/abs/2510.11838)
*Xu Yang,Jiayuan Zhou,Michael Pacheco,Wenhan Zhu,Pengfei He,Shaowei Wang,Kui Liu,Ruiqi Pan*

Main category: cs.SE

TL;DR: Lingxi is an LLM-powered agent framework that solves software repository issues by learning and applying procedural knowledge from past fixes, significantly outperforming existing methods due to its knowledge-driven approach.


<details>
  <summary>Details</summary>
Motivation: Current LLM-powered agents underperform in complex repository-level bug fixing because they lack structured procedural knowledge and depend on computationally expensive blind exploration.

Method: Lingxi builds procedural knowledge using a hierarchical abstraction mechanism from historical issues, and applies this knowledge through a knowledge-driven scaling approach for new issue resolution.

Result: Lingxi resolves 74.6% of bugs on the SWE-bench Verified benchmark (Past@1 setting), outperforming five state-of-the-art methods by 5.4% to 14.9%, with improvements directly attributed to its knowledge-driven methodology.

Conclusion: Lingxi's success in resolving repository-level issues stems from its use of procedural knowledge extracted from historical fixes, rather than brute-force solution exploration.

Abstract: Driven by the advancements of Large Language Models (LLMs), LLM-powered
agents are making significant improvements in software engineering tasks, yet
struggle with complex, repository-level issue resolution. Existing agent-based
methods have two key limitations. First, they lack of procedural knowledge
(i.e., how an issue is fixed step-by-step and rationales behind it) to learn
and leverage for issue resolution. Second, they rely on massive computational
power to blindly explore the solution space. % To address those limitations, we
propose Lingxi, an issue resolution framework that leverages procedural
knowledge extracted from historical issue-fixing data to guide agents in
solving repository-level issues. \ourTool first constructs this knowledge
offline through a hierarchical abstraction mechanism, enabling agents to learn
the how and why behind a fix, not just the final solution. During online
application, it employs a knowledge-driven scaling method that leverages the
procedural knowledge of similar issues to intelligently analyze the target
issue from multiple perspectives, in sharp contrast to undirected, brute-force
exploration. % Lingxi successfully resolves 74.6\% of bugs on the SWE-bench
Verified benchmark in Past@1 setting, outperforming five state-of-the-art
techniques by a significant margin (5.4\% to 14.9\%). Our comprehensive
ablation study confirmed that the success of Lingxi comes directly from its use
of procedural knowledge. Without it, the performance gains from scaling alone
is negligible. Our qualitative study further shows that the ``design patterns
$\&$ coding practices'' is the most critical knowledge aspect, and that the
roles of different knowledge aspects switch across different stages (i.e.,
analysis, planning, and fixing).

</details>


### [5] [DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems](https://arxiv.org/abs/2510.11872)
*Alessandro Cornacchia,Vaastav Anand,Muhammad Bilal,Zafar Qazi,Marco Canini*

Main category: cs.SE

TL;DR: DMAS-Forge is a new framework that makes deploying and testing distributed multi-agent AI systems easier by separating application logic from deployment specifics and generating needed code automatically, reducing manual work.


<details>
  <summary>Details</summary>
Motivation: Deploying and testing multi-agent AI applications as distributed systems is currently a challenging and labor-intensive process, especially given the rapidly evolving programming frameworks and protocols.

Method: The authors introduce DMAS-Forge, a framework that decouples application logic from specific deployment choices and automatically generates necessary code and configurations for distributed multi-agent applications.

Result: A prototype of DMAS-Forge is presented, demonstrating the feasibility of transparent and simplified deployment for multi-agent AI systems. Opportunities and future work for this approach are discussed.

Conclusion: DMAS-Forge addresses deployment challenges in the agentic AI landscape by reducing manual effort and simplifying the creation of distributed multi-agent applications.

Abstract: Agentic AI applications increasingly rely on multiple agents with distinct
roles, specialized tools, and access to memory layers to solve complex tasks --
closely resembling service-oriented architectures. Yet, in the rapid evolving
landscape of programming frameworks and new protocols, deploying and testing AI
agents as distributed systems remains a daunting and labor-intensive task. We
present DMAS-Forge, a framework designed to close this gap. DMAS-Forge
decouples application logic from specific deployment choices, and aims at
transparently generating the necessary glue code and configurations to spawn
distributed multi-agent applications across diverse deployment scenarios with
minimal manual effort. We present our vision, design principles, and a
prototype of DMAS-Forge. Finally, we discuss the opportunities and future work
for our approach.

</details>


### [6] [TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs](https://arxiv.org/abs/2510.12011)
*Bei Zhou,Maximilian Balmus,Cesare Corrado,Ludovica Cicci,Shuang Qian,Steven A. Niederer*

Main category: cs.SE

TL;DR: TorchCor is a free, GPU-accelerated Python library for cardiac electrophysiology simulations, making efficient, accurate, large-scale modeling accessible without costly supercomputers.


<details>
  <summary>Details</summary>
Motivation: Cardiac electrophysiology simulations are important for research and clinical decision-making, but they typically require expensive and inaccessible high-performance computing resources.

Method: The authors developed TorchCor, a high-performance Python library built on PyTorch, which leverages general-purpose GPUs to perform finite element method-based CEP simulations efficiently.

Result: TorchCor accelerates CEP simulations, especially for large 3D meshes, and its accuracy is validated against analytical solutions and a standard benchmark problem.

Conclusion: TorchCor democratizes access to high-performance cardiac electrophysiology simulations by enabling efficient computing on GPUs and making the tool freely available to both academic and commercial users.

Abstract: Cardiac electrophysiology (CEP) simulations are increasingly used for
understanding cardiac arrhythmias and guiding clinical decisions. However,
these simulations typically require high-performance computing resources with
numerous CPU cores, which are often inaccessible to many research groups and
clinicians. To address this, we present TorchCor, a high-performance Python
library for CEP simulations using the finite element method on general-purpose
GPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,
particularly for large 3D meshes. The accuracy of the solver is verified
against manufactured analytical solutions and the $N$-version benchmark
problem. TorchCor is freely available for both academic and commercial use
without restrictions.

</details>


### [7] [Enhancing Neural Code Representation with Additional Context](https://arxiv.org/abs/2510.12082)
*Huy Nguyen,Christoph Treude,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: Adding version history and structural context to code inputs significantly improves neural models for code comprehension tasks like clone detection and code summarisation. Combining different contexts yields the highest gains, and humans judge the resulting outputs as more accurate and adequate. Optimising use of context is a promising path for better program understanding models.


<details>
  <summary>Details</summary>
Motivation: Automated program comprehension is essential for software engineering tasks like code summarisation and clone detection, but current deep learning models largely use only the source code, neglecting important contextual information such as version history and structural relationships, which may limit their effectiveness.

Method: The paper conducts an empirical study by evaluating five recent neural models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) on two datasets (SeSaMe and CodeSearchNet) for code clone detection and summarisation. The models are fine-tuned and tested under both code-only and context-augmented settings. Performance improvements are measured quantitatively, and human evaluation is also performed.

Result: Adding contextual information, especially version history, consistently improves model performance in code clone detection and code summarisation. The extent of improvement varies by type of context and model, but combining multiple contextual sources yields the biggest gains (up to +21.48% macro-F1). Human evaluations confirm that summaries generated with context are preferred for accuracy and content adequacy.

Conclusion: Incorporating contextual signals into neural code comprehension models significantly boosts their effectiveness on important software engineering tasks and shows promise for further improvements by optimising how this context is encoded.

Abstract: Automated program comprehension underpins many software engineering tasks,
from code summarisation to clone detection. Recent deep learning models achieve
strong results but typically rely on source code alone, overlooking contextual
information such as version history or structural relationships. This limits
their ability to capture how code evolves and operates. We conduct an empirical
study on how enriching code representations with such contextual signals
affects neural model performance on key comprehension tasks. Two downstream
tasks, code clone detection and code summarisation, are evaluated using SeSaMe
(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative
models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under
code-only and context-augmented settings. Results show that context generally
improves performance: version history consistently boosts clone detection
(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%
METEOR), while call-graph effects vary by model and task. Combining multiple
contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100
Java snippets confirms that context-augmented summaries are significantly
preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).
These findings highlight the potential of contextual signals to enhance code
comprehension and open new directions for optimising contextual encoding in
neural SE models.

</details>


### [8] [Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach](https://arxiv.org/abs/2510.12120)
*Zhenyu Mao,Jacky Keung,Fengji Zhang,Shuo Liu,Yifei Wang,Jialong Li*

Main category: cs.SE

TL;DR: SEMAP applies established software engineering principles to multi-agent LLM systems to address key failure points, demonstrating substantial reductions in failure rates for code development and vulnerability detection tasks compared to prior systems.


<details>
  <summary>Details</summary>
Motivation: The paper is driven by the increasing need for automation in software engineering (SE) tasks using Large Language Models (LLMs) and recognizes the shortcomings in current multi-agent LLM systems for software development, which suffer from failures due to lack of application of core SE foundational principles.

Method: The authors propose and implement SEMAP, a protocol-layer methodology that instantiates three core software engineering design principles—explicit behavioral contract modeling, structured messaging, and lifecycle-guided execution with verification—for multi-agent LLM systems. SEMAP is integrated on top of Google’s Agent-to-Agent (A2A) infrastructure. The system is empirically evaluated with the Multi-Agent System Failure Taxonomy (MAST) framework across several SE tasks.

Result: Empirical results show SEMAP significantly reduces the failure rates in multi-agent LLM systems: up to 69.6% reduction in failures at the function-level and 56.7% at the deployment-level for code development, and failure reductions of 47.4% on Python vulnerability detection tasks and 28.2% on C/C++ tasks.

Conclusion: The SEMAP protocol improves the robustness and reliability of multi-agent LLM systems in software engineering tasks by embedding foundational SE structuring principles, effectively addressing failure causes prevalent in previous approaches.

Abstract: The increasing demand for software development has driven interest in
automating software engineering (SE) tasks using Large Language Models (LLMs).
Recent efforts extend LLMs into multi-agent systems (MAS) that emulate
collaborative development workflows, but these systems often fail due to three
core deficiencies: under-specification, coordination misalignment, and
inappropriate verification, arising from the absence of foundational SE
structuring principles. This paper introduces Software Engineering Multi-Agent
Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE
design principles for multi-agent LLMs: (1) explicit behavioral contract
modeling, (2) structured messaging, and (3) lifecycle-guided execution with
verification, and is implemented atop Google's Agent-to-Agent (A2A)
infrastructure. Empirical evaluation using the Multi-Agent System Failure
Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures
across different SE tasks. In code development, it achieves up to a 69.6%
reduction in total failures for function-level development and 56.7% for
deployment-level development. For vulnerability detection, SEMAP reduces
failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.

</details>


### [9] [iCodeReviewer: Improving Secure Code Review with Mixture of Prompts](https://arxiv.org/abs/2510.12186)
*Yun Peng,Kisub Kim,Linghan Meng,Kui Liu*

Main category: cs.SE

TL;DR: iCodeReviewer uses LLMs and a mixture-of-prompts system to automate secure code review, achieving higher precision and coverage. It reduces false positives and its comments are highly accepted in practice, with strong experimental results.


<details>
  <summary>Details</summary>
Motivation: Automated secure code review faces challenges like limited precision, coverage, and insufficient evaluation, which are critical given the importance of detecting security issues to prevent crashes and service disruptions.

Method: The paper introduces iCodeReviewer, an automated code review tool leveraging large language models (LLMs). It uses a mixture-of-prompts architecture with dynamic prompt experts that focus on specific security issues, and a routing algorithm activates relevant prompt experts to minimize false positives.

Result: On an internal dataset, iCodeReviewer achieved an F1 score of 63.98% in detecting and localizing security issues. Its review comments reached an acceptance rate of up to 84% in real production environments.

Conclusion: iCodeReviewer effectively addresses the limitations of current automated secure code review techniques by increasing coverage, reducing false positives, and providing practical, accepted review comments in production.

Abstract: Code review is an essential process to ensure the quality of software that
identifies potential software issues at an early stage of software development.
Among all software issues, security issues are the most important to identify,
as they can easily lead to severe software crashes and service disruptions.
Recent research efforts have been devoted to automated approaches to reduce the
manual efforts required in the secure code review process. Despite the
progress, current automated approaches on secure code review, including static
analysis, deep learning models, and prompting approaches, still face the
challenges of limited precision and coverage, and a lack of comprehensive
evaluation.
  To mitigate these challenges, we propose iCodeReviewer, which is an automated
secure code review approach based on large language models (LLMs).
iCodeReviewer leverages a novel mixture-of-prompts architecture that
incorporates many prompt experts to improve the coverage of security issues.
Each prompt expert is a dynamic prompt pipeline to check the existence of a
specific security issue. iCodeReviewer also implements an effective routing
algorithm to activate only necessary prompt experts based on the code features
in the input program, reducing the false positives induced by LLM
hallucination. Experiment results in our internal dataset demonstrate the
effectiveness of iCodeReviewer in security issue identification and
localization with an F1 of 63.98%. The review comments generated by
iCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed
in production environments.

</details>


### [10] [Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening](https://arxiv.org/abs/2510.12294)
*Gergő Balogh,Dávid Kószó,Homayoun Safarpour Motealegh Mahalegi,László Tóth,Bence Szakács,Áron Búcsú*

Main category: cs.SE

TL;DR: The paper reviews research using verbalization methods to study developer cognition, employing GPT to streamline the review of 9,000 titles. LLMs perform well in supporting large reviews, matching human decisions with minor disagreement. Most studies focus on technical SE themes, with less attention to human aspects, and psychological methods are more commonly adopted in SE than the reverse.


<details>
  <summary>Details</summary>
Motivation: Understanding the cognitive processes of software developers is crucial but challenging. To address this, the authors focus on verbalization techniques as a means to explore developer thinking, decision-making, and behavior, aiming to bridge methods from software engineering and psychology.

Method: The paper conducts a scoping review at the intersection of software engineering and psychology, specifically focusing on verbal data. It utilizes a large language model (GPT) to screen and assess the relevance of over 9,000 papers based on their titles, and validates these outputs against human reviewers.

Result: GPT-based screening showed high consistency with human reviewers, with only a 13% disagreement rate. Thematic analysis revealed that research predominantly focuses on SE craft-related topics, with fewer studies addressing human-centered issues. Additionally, SE commonly leverages PSY techniques, while psychology rarely applies SE approaches.

Conclusion: Verbalization techniques provide a practical method for studying developer cognition. LLM-assisted screening is effective for large-scale interdisciplinary reviews, but there is a thematic imbalance towards craft over human-centered topics. Interdisciplinary collaboration flows mainly from psychology to software engineering, but not vice versa.

Abstract: Understanding how software developers think, make decisions, and behave
remains a key challenge in software engineering (SE). Verbalization techniques
(methods that capture spoken or written thought processes) offer a lightweight
and accessible way to study these cognitive aspects. This paper presents a
scoping review of research at the intersection of SE and psychology (PSY),
focusing on the use of verbal data. To make large-scale interdisciplinary
reviews feasible, we employed a large language model (LLM)-assisted screening
pipeline using GPT to assess the relevance of over 9,000 papers based solely on
titles. We addressed two questions: what themes emerge from
verbalization-related work in SE, and how effective are LLMs in supporting
interdisciplinary review processes? We validated GPT's outputs against human
reviewers and found high consistency, with a 13\% disagreement rate. Prominent
themes mainly were tied to the craft of SE, while more human-centered topics
were underrepresented. The data also suggests that SE frequently draws on PSY
methods, whereas the reverse is rare.

</details>


### [11] [(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm](https://arxiv.org/abs/2510.12364)
*Kevin Krings,Nino S. Bohn,Thomas Ludwig*

Main category: cs.SE

TL;DR: This paper explores how generative AI enables a new, intuitive programming paradigm called Vibe Coding (VC), differing from traditional AI-assisted development. Interviews reveal that VC fosters creativity and inclusivity but comes with challenges around reproducibility and scalability. VC represents a cultural shift in programming deserving deeper research.


<details>
  <summary>Details</summary>
Motivation: Generative AI, especially large language models, has transformed software development. The authors aim to investigate how this shift enables a new programming paradigm, Vibe Coding (VC), which is more intuitive and affect-driven compared to conventional methods.

Method: They conducted five semi-structured interview sessions with ten experienced software practitioners. The study used qualitative analysis to identify thematic dimensions around VC.

Result: Five thematic dimensions were identified: creativity, sustainability, the future of programming, collaboration, and criticism. VC was found to blur the boundaries between professional and non-developers, offering new forms of expression but posing challenges in reproducibility, scalability, and inclusivity.

Conclusion: VC is conceptualized as 'co-drifting' with AI (in contrast to 'co-piloting'), representing a meaningful shift in programming culture. The authors suggest that VC warrants further research in HCI and software engineering.

Abstract: Recent advancements in generative artificial intelligence (GenAI),
particularly large language models, have introduced new possibilities for
software development practices. In our paper we investigate the emerging Vibe
Coding (VC) paradigm that emphasizes intuitive, affect-driven, and
improvisational interactions between developers and AI systems. Building upon
the discourse of End-User Development (EUD), we explore how VC diverges from
conventional programming approaches such as those supported by tools like
GitHub Copilot. Through five semi-structured interview sessions with ten
experienced software practitioners, we identify five thematic dimensions:
creativity, sustainability, the future of programming, collaboration, and
criticism. Our analysis conceptualizes VC within the metaphor of co-drifting,
contrasting it with the prevalent co-piloting perspective of AI-assisted
development. We argue that VC reconfigures the developers role, blurring
boundaries between professional and non-developers. While VC enables novel
forms of expression and rapid prototyping, it also introduces challenges
regarding reproducibility, scalability, and inclusivity. We propose that VC
represents a meaningful shift in programming culture, warranting further
investigation within human-computer interaction (HCI) and software engineering
research.

</details>


### [12] [Should I Run My Cloud Benchmark on Black Friday?](https://arxiv.org/abs/2510.12397)
*Sören Henning,Adriano Vogel,Esteban Perez-Wohlfeil,Otmar Ertl,Rick Rabiser*

Main category: cs.SE

TL;DR: Cloud performance variability exists but is not as severe as expected; subtle daily and weekly patterns emerge, and major world events like Black Friday are investigated for their effects on cloud benchmark outcomes.


<details>
  <summary>Details</summary>
Motivation: There is widespread skepticism regarding the reproducibility and credibility of cloud-based performance experiments, due to the perceived high variability in results. The study seeks to empirically examine and quantify this variability and its drivers.

Method: Researchers executed a stream processing application benchmark repeatedly at different times of the day over several months, and analyzed the results for variability and patterns. They extended the analysis to assess performance during significant events like Black Friday.

Result: Performance variability was confirmed at the application level, but its degree is less than often assumed. The study's large scale enabled identification of daily and weekly performance patterns, and extended to evaluate the influence of major events.

Conclusion: The study concludes that performance variability in cloud benchmarking is real but less dramatic than commonly believed, and subtle patterns can be observed over time. It also investigates the impact of major events, such as Black Friday, on these benchmarks.

Abstract: Benchmarks and performance experiments are frequently conducted in cloud
environments. However, their results are often treated with caution, as the
presumed high variability of performance in the cloud raises concerns about
reproducibility and credibility. In a recent study, we empirically quantified
the impact of this variability on benchmarking results by repeatedly executing
a stream processing application benchmark at different times of the day over
several months. Our analysis confirms that performance variability is indeed
observable at the application level, although it is less pronounced than often
assumed. The larger scale of our study compared to related work allowed us to
identify subtle daily and weekly performance patterns. We now extend this
investigation by examining whether a major global event, such as Black Friday,
affects the outcomes of performance benchmarks.

</details>


### [13] [DarTwin made precise by SysMLv2 -- An Experiment](https://arxiv.org/abs/2510.12478)
*Øystein Haugen,Stefan Klikovits,Martin Arthur Andersen,Jonathan Beaulieu,Francis Bordeleau,Joachim Denil,Joost Mertens*

Main category: cs.SE

TL;DR: The paper analyzes SysMLv2's support for domain-specific extensions by developing a DSL for Digital Twin evolution (DarTwin DSL). While demonstrating wide applicability, the paper finds tooling for graphical notation lacking, signaling a need for further SysMLv2 tool improvements.


<details>
  <summary>Details</summary>
Motivation: SysMLv2 introduces features for specifying domain-specific concepts and language extensions, promising easier creation of DSLs and better interfacing with existing system descriptions. The motivation is to leverage these features for advancing Model-Driven Engineering (MDE) in Digital Twin (DT) systems.

Method: The paper reviews SysMLv2’s new mechanisms, evaluates its capabilities through practical use cases, and develops a Domain-Specific Language (DarTwin DSL) using SysMLv2 to formalize the DarTwin notation for DT evolution. The usability and limitations, particularly regarding graphical notation, are demonstrated and analyzed.

Result: A working DarTwin DSL is created via SysMLv2, showing potential for broad application of DarTwin evolution templates across SysMLv2 tools. However, current SysMLv2 tooling shows limitations in graphical notation capabilities.

Conclusion: SysMLv2 enables systematic domain-specific language creation and application for Digital Twin evolution management, although improvements in tooling, especially for graphical representation, are needed for full practical utility.

Abstract: The new SysMLv2 adds mechanisms for the built-in specification of
domain-specific concepts and language extensions. This feature promises to
facilitate the creation of Domain-Specific Languages (DSLs) and interfacing
with existing system descriptions and technical designs. In this paper, we
review these features and evaluate SysMLv2's capabilities using concrete use
cases. We develop DarTwin DSL, a DSL that formalizes the existing DarTwin
notation for Digital Twin (DT) evolution, through SysMLv2, thereby supposedly
enabling the wide application of DarTwin's evolution templates using any
SysMLv2 tool. We demonstrate DarTwin DSL, but also point out limitations in the
currently available tooling of SysMLv2 in terms of graphical notation
capabilities. This work contributes to the growing field of Model-Driven
Engineering (MDE) for DTs and combines it with the release of SysMLv2, thus
integrating a systematic approach with DT evolution management in systems
engineering.

</details>


### [14] [Diff-XYZ: A Benchmark for Evaluating Diff Understanding](https://arxiv.org/abs/2510.12487)
*Evgeniy Glukhov,Michele Conti,Egor Bogomolov,Yaroslav Golubev,Alexander Bezzubov*

Main category: cs.SE

TL;DR: Diff-XYZ introduces a code-diff understanding benchmark supporting three tasks and multiple diff formats. Results show format choice impacts model performance by task and size, making Diff-XYZ valuable for developing better LLM-based code editing tools.


<details>
  <summary>Details</summary>
Motivation: Efficient and reliable manipulation of code diffs is critical for large-scale code editing and refactoring by AI agents. Existing benchmarks may not sufficiently evaluate or support this capability in LLMs.

Method: The authors introduce Diff-XYZ, a benchmark dataset for code-diff understanding, featuring three supervised tasks: applying a diff, anti-applying a diff, and generating a diff. The dataset uses real code commits and provides standardized metrics and protocols. Multiple diff representations are empirically compared through experiments with different model sizes and use cases.

Result: The study shows that the choice of diff format affects performance depending on the scenario and model size. For example, search-replace formats benefit large models during diff generation but are less effective for diff analysis and with smaller models.

Conclusion: Diff-XYZ serves as a practical resource for evaluating and guiding the development of LLMs' handling of code diffs, highlighting the importance of matching diff representation to task and model. The benchmark is publicly available for further research.

Abstract: Reliable handling of code diffs is central to agents that edit and refactor
repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff
understanding with three supervised tasks: apply (old code $+$ diff
$\rightarrow$ new code), anti-apply (new code $-$ diff $\rightarrow$ old code),
and diff generation (new code $-$ old code $\rightarrow$ diff). Instances in
the benchmark are triples $\langle \textit{old code}, \textit{new code},
\textit{diff} \rangle$ drawn from real commits in CommitPackFT, paired with
automatic metrics and a clear evaluation protocol. We use the benchmark to do a
focused empirical study of the unified diff format and run a cross-format
comparison of different diff representations. Our findings reveal that
different formats should be used depending on the use case and model size. For
example, representing diffs in search-replace format is good for larger models
in the diff generation scenario, yet not suited well for diff analysis and
smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing
and improving diff handling in LLMs that can aid future development of diff
formats and models editing code. The dataset is published on HuggingFace Hub:
https://huggingface.co/datasets/JetBrains-Research/diff-xyz.

</details>


### [15] [The EmpathiSEr: Development and Validation of Software Engineering Oriented Empathy Scales](https://arxiv.org/abs/2510.12546)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: The paper identifies the lack of suitable empathy measurement tools in SE, develops and validates SE-specific scales through a rigorous process, and provides the first validated instruments for assessing empathy among practitioners and towards users, enabling better research and interventions in SE contexts.


<details>
  <summary>Details</summary>
Motivation: Empathy is recognized as essential in software engineering (SE), influencing collaboration and user-centered design, but no validated measurement instrument exists specifically for SE's unique social and technical contexts. Generic empathy scales do not adequately reflect the diversity and specificity of empathy required in SE.

Method: The authors developed two domain-specific empathy scales—EmpathiSEr-P (for practitioners) and EmpathiSEr-U (for practitioners towards users)—using a multi-phase approach involving expert evaluation, cognitive interviews, and practitioner surveys. Their framework covers cognitive empathy, affective empathy, and empathic responses.

Result: The study produced the first psychometrically validated empathy scales tailored to SE, providing specific measurement tools that reflect the diverse and context-bound expressions of empathy among software practitioners and towards users.

Conclusion: These new instruments empower SE researchers and practitioners to accurately assess empathy and drive empathy-enhancing interventions in software teams and user interactions, overcoming the limitations of generic empathy scales.

Abstract: Empathy plays a critical role in software engineering (SE), influencing
collaboration, communication, and user-centred design. Although SE research has
increasingly recognised empathy as a key human aspect, there remains no
validated instrument specifically designed to measure it within the unique
socio-technical contexts of SE. Existing generic empathy scales, while
well-established in psychology and healthcare, often rely on language,
scenarios, and assumptions that are not meaningful or interpretable for
software practitioners. These scales fail to account for the diverse,
role-specific, and domain-bound expressions of empathy in SE, such as
understanding a non-technical user's frustrations or another practitioner's
technical constraints, which differ substantially from empathy in clinical or
everyday contexts. To address this gap, we developed and validated two
domain-specific empathy scales: EmpathiSEr-P, assessing empathy among
practitioners, and EmpathiSEr-U, capturing practitioner empathy towards users.
Grounded in a practitioner-informed conceptual framework, the scales encompass
three dimensions of empathy: cognitive empathy, affective empathy, and empathic
responses. We followed a rigorous, multi-phase methodology, including expert
evaluation, cognitive interviews, and two practitioner surveys. The resulting
instruments represent the first psychometrically validated empathy scales
tailored to SE, offering researchers and practitioners a tool for assessing
empathy and designing empathy-enhancing interventions in software teams and
user interactions.

</details>


### [16] [Evaluating End-User Device Energy Models in Sustainability Reporting of Browser-Based Web Services](https://arxiv.org/abs/2510.12566)
*Maja H. Kirkeby,Timmie Lagermann*

Main category: cs.SE

TL;DR: Widely used models for web service sustainability reporting often inaccurately estimate energy use, with systematic errors dependent on website type, device, and task. More precise, context-aware models are needed for trustworthy sustainability assessments.


<details>
  <summary>Details</summary>
Motivation: Sustainability reporting for web-based services relies on simplified energy and carbon models, but the accuracy and precision of these models are not well-studied. Accurate sustainability reporting requires reliable measurement methods.

Method: The authors conducted an empirical study where actual energy use was measured during user interactions with different website categories (shopping, booking, navigation, news). Predefined user flows were executed on four different laptop platforms to compare measured energy use against the predictions of popular sustainability models (e.g., Digst, DIMPACT).

Result: The study found that the constant-power approximation (P * t) used in these models can differ significantly from measured energy consumption. The extent of this discrepancy varies depending on website category, device used, and the nature of user activity. The deviations observed are systematic rather than random.

Conclusion: The findings indicate that current sustainability models can be inaccurate for certain conditions. There is a need for models to use category-aware and device-specific power parameters to improve the reproducibility and accuracy of sustainability reporting for web-based services.

Abstract: Sustainability reporting in web-based services increasingly relies on
simplified energy and carbon models such as the Danish Agency of Digital
Government's Digst framework and the United Kingdom-based DIMPACT model.
Although these models are widely adopted, their accuracy and precision remain
underexplored. This paper presents an empirical study evaluating how well such
models reflect actual energy consumption during realistic user interactions
with common website categories. Energy use was measured across shopping,
booking, navigation, and news services using predefined user flows executed on
four laptop platforms. The results show that the commonly applied
constant-power approximation (P * t) can diverge substantially from measured
energy, depending on website category, device type, and task characteristics.
The findings demonstrate that model deviations are systematic rather than
random and highlight the need for category-aware and device-reflective power
parameters in reproducible sustainability reporting frameworks.

</details>


### [17] [Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods](https://arxiv.org/abs/2510.12616)
*Muhammad Ashfaq,Ahmed R. Sadik,Teerath Das,Muhammad Waseem,Niko Makitalo,Tommi Mikkonen*

Main category: cs.SE

TL;DR: This paper reviews recent progress in runtime composition for dynamic Systems of Systems, outlining challenges, solutions, tool gaps, and evaluation strategies. It stresses the need for better evaluation metrics, scalable architectures, and cross-domain frameworks, offering valuable insights for future research and development in adaptive SoSs.


<details>
  <summary>Details</summary>
Motivation: Modern Systems of Systems (SoSs), such as those found in smart cities or autonomous vehicles, operate in dynamic environments that require flexible and adaptive runtime composition. However, the current literature lacks an integrated understanding of how SoSs achieve runtime composition, presenting a knowledge gap.

Method: The authors performed a Systematic Literature Review (SLR), screening 1,774 studies published between 2019 and 2024 and selecting 80 primary studies for thematic analysis.

Result: The study identifies four major challenge categories in runtime composition: modeling and analysis, resilient operations, system orchestration, and constituent system heterogeneity. Solutions focus on seven areas, including co-simulation/digital twins, semantic ontologies, integration frameworks, adaptive architectures, middleware, formal methods, and AI-driven resilience. There is a dominance of service-oriented frameworks and simulation platforms, but critical gaps persist in tool interoperability, workflow integration, and benchmarking. Evaluation methods are diverse, and the approaches have broad domain applicability.

Conclusion: The review highlights key tensions in SoS runtime composition, such as autonomy versus coordination and the modeling-reality gap. The authors call for standardized evaluation metrics, scalable decentralized architectures, and cross-domain frameworks, providing direction for future research and practice.

Abstract: Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic
environments (e.g., smart cities, autonomous vehicles) where runtime
composition -- the on-the-fly discovery, integration, and coordination of
constituent systems (CSs)--is crucial for adaptability. Despite growing
interest, the literature lacks a cohesive synthesis of runtime composition in
dynamic SoSs. Objective: This study synthesizes research on runtime composition
in dynamic SoSs and identifies core challenges, solution strategies, supporting
tools, and evaluation methods. Methods: We conducted a Systematic Literature
Review (SLR), screening 1,774 studies published between 2019 and 2024 and
selecting 80 primary studies for thematic analysis (TA). Results: Challenges
fall into four categories: modeling and analysis, resilient operations, system
orchestration, and heterogeneity of CSs. Solutions span seven areas:
co-simulation and digital twins, semantic ontologies, integration frameworks,
adaptive architectures, middleware, formal methods, and AI-driven resilience.
Service-oriented frameworks for composition and integration dominate tooling,
while simulation platforms support evaluation. Interoperability across tools,
limited cross-toolchain workflows, and the absence of standardized benchmarks
remain key gaps. Evaluation approaches include simulation-based,
implementation-driven, and human-centered studies, which have been applied in
domains such as smart cities, healthcare, defense, and industrial automation.
Conclusions: The synthesis reveals tensions, including autonomy versus
coordination, the modeling-reality gap, and socio-technical integration. It
calls for standardized evaluation metrics, scalable decentralized
architectures, and cross-domain frameworks. The analysis aims to guide
researchers and practitioners in developing and implementing dynamically
composable SoSs.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [18] [Verifying Correctness of Shared Channels in a Cooperatively Scheduled Process-Oriented Language](https://arxiv.org/abs/2510.11751)
*Jan Pedersen,Kevin Chalmers*

Main category: cs.PL

TL;DR: The paper shows that modeling the runtime environment is crucial for ensuring correct concurrent behavior in systems with shared communicating channels, particularly under cooperative scheduling, as correctness depends on resource availability.


<details>
  <summary>Details</summary>
Motivation: Ensuring correct concurrent behavior is essential for reliable component interactions, especially under cooperative scheduling. Understanding how shared channels operate under different runtime conditions can reveal potential pitfalls and correctness issues.

Method: The authors use FDR, a refinement checking and modeling tool, to create both specifications and implementation models of shared communicating channels in ProcessJ, a cooperatively scheduled language.

Result: The study finds that correctly implementing channel behavior is possible but relies heavily on the availability of resources to run all processes. Inadequate resources can affect the correctness of concurrent component interactions.

Conclusion: To guarantee correct behavior of concurrent components in real-world scenarios, it is imperative to model the runtime environment. This ensures that resource limitations and scheduling strategies are considered in system design.

Abstract: Correct concurrent behaviour is important in understanding how components
will act within certain conditions. In this work. we analyse the behaviour of
shared communicating channels within a coorporatively scheduled runtime. We use
the refinement checking and modelling tool FDR to develop both specifications
of how such shared channels should behave and models of the implementations of
these channels in the cooperatively scheduled language ProcessJ. Our results
demonstrate that although we can certainly implement the correct behaviour of
such channels, the outcome is dependant on having adequate resources available
to execute all processes involved. We conclude that modelling the runtime
environment of concurrent components is necessary to ensure components behave
as specified in the real world.

</details>


### [19] [AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework](https://arxiv.org/abs/2510.11759)
*Hongyu Lin,Haolin Pan,Haoran Luo,Yuchen Li,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.PL

TL;DR: AwareCompiler is a novel agent framework for automated compiler optimization that uses structured knowledge and data-driven training to overcome major challenges, achieving strong performance gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: Automating compiler optimization is important for better program performance but faces issues like semantic misalignment, inefficient agent-environment interaction, and sparse rewards due to complex decision spaces.

Method: The paper introduces AwareCompiler, an agentic framework that tackles these challenges with structured knowledge integration and dataset construction, adaptive pass generation driven by knowledge, and a hybrid data-driven training pipeline.

Result: On standard benchmarks, AwareCompiler significantly outperforms existing baselines for both performance and efficiency.

Conclusion: The knowledge and data synergy in AwareCompiler offers an effective solution for automated compiler optimization. The approach has been validated experimentally and code is publicly available.

Abstract: Compiler optimization is crucial for enhancing program performance by
transforming the sequence of optimization passes while maintaining correctness.
Despite the promising potential of large language models (LLMs)-based agent for
software optimization, automating compiler optimization remains challenging due
to: (1) semantic misalignment between abstract program representations and
concrete optimization passes, (2) inefficient interaction mechanisms between
agents and compiler environments, and (3) reward sparsity from the extensive
decision-making process within large optimization spaces. This paper introduces
\textbf{AwareCompiler}, an agentic framework for compiler optimization that
addresses these challenges through three key innovations: structured knowledge
integration and dataset construction, knowledge-driven adaptive pass
generation, and data-driven hybrid training pipeline. Experimental results on
standard benchmarks demonstrate that AwareCompiler significantly outperforms
existing baselines in both performance and efficiency, highlighting the
effectiveness of our synergistic knowledge-data-driven approach. Our code is
publicly available at https://github.com/LHY-24/AwareCompiler.

</details>


### [20] [Functional Reasoning for Distributed Systems with Failures](https://arxiv.org/abs/2510.12131)
*Haobin Ni,Robbert van Renesse,Greg Morrisett*

Main category: cs.PL

TL;DR: The paper introduces Sync and Async, two complementary languages for compositional formal reasoning in distributed systems, connecting informal Hoare-style reasoning to formal proofs. Safety properties proven in Sync are maintained in Async, as demonstrated by protocol verifications using their Rocq implementation.


<details>
  <summary>Details</summary>
Motivation: Existing correctness arguments for distributed systems are often intuitive, based on Hoare-style reasoning, but lack formal grounding and reliability. There is a need to connect these informal arguments to robust, formal methods, especially in the presence of faults such as Byzantine failures.

Method: The authors introduce twin programming languages—Sync and Async—designed to represent distributed systems formally. Sync models systems synchronously as data-parallel programs, with semantics suited for formal reasoning. Async models systems asynchronously, as collections of monadic programs for each node. Sync programs are compiled to Async, and executable code can be extracted. Formal proofs are provided showing safety properties are preserved from Sync (denotational semantics) to Async (operational semantics). The approach is implemented in the Rocq tool and applied to verifying two consensus protocols.

Result: The paper proves that safety properties established using Hoare-style reasoning in Sync are preserved when programs are compiled and executed in Async. Implementation of Rocq successfully verifies safety in BOSCO and SeqPaxos consensus protocols, demonstrating practical applicability.

Conclusion: This work gives a formal foundation for intuitive, compositional reasoning about distributed systems, bridging informal and formal correctness arguments. The twin language approach strengthens formal verification for systems, including those encountering Byzantine faults, and is supported by real protocol verification.

Abstract: Distributed system theory literature often argues for correctness using an
informal, Hoare-like style of reasoning. While these arguments are intuitive,
they have not all been foolproof, and whether they directly correspond to
formal proofs is in question. We formally ground this kind of reasoning and
connect it to standard formal approaches through language design and
meta-analysis, which leads to a functional style of compositional formal
reasoning for a class of distributed systems, including cases involving
Byzantine faults. The core of our approach is twin languages: Sync and Async,
which formalize the insight from distributed system theory that an asynchronous
system can be reduced to a synchronous system for more straightforward
reasoning under certain conditions. Sync describes a distributed system as a
single, synchronous, data-parallel program. It restricts programs syntactically
and has a functional denotational semantics suitable for Hoare-style formal
reasoning. Async models a distributed system as a collection of interacting
monadic programs, one for each non-faulty node in the system. It has a standard
trace-based operational semantics, modeling asynchrony with interleaving. Sync
compiles to Async and can then be extracted to yield executable code. We prove
that any safety property proven for a Sync program in its denotational
semantics is preserved in the operational semantics of its compiled Async
programs. We implement the twin languages in Rocq and verify the safety
properties of two fault-tolerant consensus protocols: BOSCO and SeqPaxos.

</details>


### [21] [Operational methods in semantics](https://arxiv.org/abs/2510.12295)
*Roberto M. Amadio*

Main category: cs.PL

TL;DR: This work advocates for operational semantics as the preferred method for specifying, implementing, and analyzing programming languages, highlighting its practicality, scalability, and suitability for diverse language features.


<details>
  <summary>Details</summary>
Motivation: There is a need for abstract and effective frameworks to understand and reason about the semantics of programming languages, especially those that can be widely applied and do not require overly advanced mathematics.

Method: The paper presents an operational approach. It starts with abstract descriptions of computation steps, then builds on this foundation to introduce semantic equivalences, specification languages, and static analyses.

Result: Operational semantics provides a practical and scalable way to specify, implement, and reason about programming languages. It supports a variety of features and is commonly used for tasks from property testing to proving correctness of compilers and static analyzers.

Conclusion: Operational semantics balances mathematical rigor with practical usability, making it an effective and popular choice for language specification and analysis across many programming paradigms.

Abstract: The focus of these lecture notes is on abstract models and basic ideas and
results that relate to the operational semantics of programming languages
largely conceived. The approach is to start with an abstract description of the
computation steps of programs and then to build on top semantic equivalences,
specification languages, and static analyses. While other approaches to the
semantics of programming languages are possible, it appears that the
operational one is particularly effective in that it requires a moderate level
of mathematical sophistication and scales reasonably well to a large variety of
programming features. In practice, operational semantics is a suitable
framework to build portable language implementations and to specify and test
program properties. It is also used routinely to tackle more ambitious tasks
such as proving the correctness of a compiler or a static analyzer.

</details>


### [22] [GUPPY: Pythonic Quantum-Classical Programming](https://arxiv.org/abs/2510.12582)
*Mark Koch,Alan Lawrence,Kartik Singhal,Seyon Sivarajah,Ross Duncan*

Main category: cs.PL

TL;DR: The paper introduces Guppy, a Python-embedded language for writing hybrid quantum programs with complex control flow, targeting real quantum hardware.


<details>
  <summary>Details</summary>
Motivation: There is a need for a user-friendly way to write high-level quantum programs with complex control flow, using familiar syntax, and targeting real quantum hardware.

Method: The authors introduce Guppy, a specialized domain-specific language embedded in Python, enabling the development of hybrid quantum programs with complex control structures.

Result: Guppy allows users to write hybrid quantum programs in Pythonic syntax and aims to make them executable on actual quantum hardware. The work is ongoing.

Conclusion: Guppy provides an accessible, high-level programming solution for hybrid quantum systems, bridging the gap between Pythonic control flow and quantum hardware execution.

Abstract: We present ongoing work on Guppy, a domain-specific language embedded in
Python that allows users to write high-level hybrid quantum programs with
complex control flow in Pythonic syntax, aiming to run them on actual quantum
hardware.

</details>
