{"id": "2511.16707", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16707", "abs": "https://arxiv.org/abs/2511.16707", "authors": ["Yuki Kataoka", "Ryuhei So", "Masahiro Banno", "Yasushi Tsujimoto", "Tomohiro Takayama", "Yosuke Yamagishi", "Takahiro Tsuge", "Norio Yamamoto", "Chiaki Suda", "Toshi A. Furukawa"], "title": "Large language models for automated PRISMA 2020 adherence checking", "comment": null, "summary": "Evaluating adherence to PRISMA 2020 guideline remains a burden in the peer review process. To address the lack of shareable benchmarks, we constructed a copyright-aware benchmark of 108 Creative Commons-licensed systematic reviews and evaluated ten large language models (LLMs) across five input formats. In a development cohort, supplying structured PRISMA 2020 checklists (Markdown, JSON, XML, or plain text) yielded 78.7-79.7% accuracy versus 45.21% for manuscript-only input (p less than 0.0001), with no differences between structured formats (p>0.9). Across models, accuracy ranged from 70.6-82.8% with distinct sensitivity-specificity trade-offs, replicated in an independent validation cohort. We then selected Qwen3-Max (a high-sensitivity open-weight model) and extended evaluation to the full dataset (n=120), achieving 95.1% sensitivity and 49.3% specificity. Structured checklist provision substantially improves LLM-based PRISMA assessment, though human expert verification remains essential before editorial decisions.", "AI": {"tldr": "Providing LLMs with structured PRISMA 2020 checklists greatly improves their ability to assess guideline adherence in systematic reviews, but human oversight remains crucial for final decisions.", "motivation": "Evaluating adherence to the PRISMA 2020 guideline is a challenge in peer review, lacking standardized and shareable benchmarks.", "method": "The authors constructed a benchmark of 108 Creative Commons-licensed systematic reviews and assessed ten large language models (LLMs) using five input types, including structured PRISMA checklists in various formats and manuscript-only input.", "result": "Structured PRISMA checklists (Markdown, JSON, XML, plain text) provided significantly higher accuracy (about 79%) for LLMs compared to manuscripts only (45%). No significant differences were found among structured formats. In further testing, the Qwen3-Max model reached 95.1% sensitivity and 49.3% specificity when evaluated on 120 reviews.", "conclusion": "Supplying structured PRISMA checklist formats substantially improves LLM-based assessment accuracy compared to only providing manuscripts. However, human experts are still needed for final editorial decisions."}}
{"id": "2511.16708", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.16708", "abs": "https://arxiv.org/abs/2511.16708", "authors": ["Shreshth Rajan"], "title": "Multi-Agent Code Verification with Compound Vulnerability Detection", "comment": "18 pages, 3 figures, 9 tables", "summary": "LLMs generate buggy code: 29.6% of SWE-bench \"solved\" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.", "AI": {"tldr": "LLM-generated code is highly buggy, and current tools miss many errors. CodeX-Verify, a multi-agent system, combines four specialized detectors and mathematically demonstrates that multi-agent approaches catch more bugs. It runs faster, achieves high accuracy (up to 79.3%), and is practical for real-world deployment, greatly outperforming single-agent methods.", "motivation": "LLMs are increasingly used for coding tasks, but they frequently generate buggy code. Existing bug detection tools miss many bugs and suffer from significant false positives, which creates a need for a more robust and accurate solution for code verification.", "method": "The authors built CodeX-Verify, a multi-agent system employing four specialized agents, each focusing on different types of bugs. They mathematically prove the benefit of combining agents with distinct detection patterns, empirically confirm low correlation among agents, and systematically test agent combinations on verified code samples for bug detection accuracy. They also analyze exponential risk from overlapping vulnerabilities.", "result": "CodeX-Verify detects 76.1% of bugs, matching the best existing tool but running faster and without the need for test execution. Employing multiple agents improves detection accuracy by up to 39.7 percentage points compared to single-agent setups. The best two-agent combination achieves 79.3% accuracy. The system handles 300 real-world patches in under 200ms per sample, demonstrating practical applicability.", "conclusion": "Using multiple specialized agents significantly enhances bug detection accuracy for LLM-generated code compared to single-agent or existing approaches. CodeX-Verify is both efficient and accurate, making it suitable for production environments."}}
{"id": "2511.16858", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16858", "abs": "https://arxiv.org/abs/2511.16858", "authors": ["Toufique Ahmed", "Jatin Ganhotra", "Avraham Shinnar", "Martin Hirzel"], "title": "Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair", "comment": null, "summary": "Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.", "AI": {"tldr": "Despite advancements like large language models, automated program repair still struggles with test overfitting, where fixes pass visible tests but break when evaluated on hidden ones.", "motivation": "To investigate whether the test overfitting problem in automated program repair persists with modern approaches, particularly those involving large language models.", "method": "Experimental analysis of test overfitting in the context of repository-level SWE-bench tasks.", "result": "Empirical evidence shows that repaired code often overfits to seen tests and fails on hidden ones, indicating that overfitting is still prevalent.", "conclusion": "Test overfitting remains an issue in automated program repair, even with recent advances such as large language models."}}
{"id": "2511.16882", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.16882", "abs": "https://arxiv.org/abs/2511.16882", "authors": ["Tim Menzies", "Tao Chen", "Yulong Ye", "Kishan Kumar Ganguly", "Amirali Rayegan", "Srinath Srinivasan", "Andre Lustosa"], "title": "MOOT: a Repository of Many Multi-Objective Optimization Tasks", "comment": null, "summary": "Software engineers must make decisions that trade off competing goals (faster vs. cheaper, secure vs. usable, accurate vs. interpretable, etc.). Despite MSR's proven techniques for exploring such goals, researchers still struggle with these trade-offs. Similarly, industrial practitioners deliver sub-optimal products since they lack the tools needed to explore these trade-offs.\n  To enable more research in this important area, we introduce MOOT, a repository of multi-objective optimization tasks taken from recent SE research papers. MOOT's tasks cover software configuration, cloud tuning, project health, process modeling, hyperparameter optimization, and more. Located at github.com/timm/moot, MOOT's current 120+ tasks are freely available under an MIT license (and we invite community contributions). As shown here, this data enables dozens of novel research questions.", "AI": {"tldr": "MOOT is a new open-source repository offering over 120 multi-objective optimization tasks from SE papers, helping researchers and practitioners address complex engineering trade-offs and enabling fresh research opportunities.", "motivation": "Software engineers and researchers struggle with complex trade-offs but lack comprehensive resources and tools to benchmark or optimize these multi-objective decisions. MOOT aims to bridge this gap.", "method": "The authors compiled multi-objective tasks from recent software engineering research and organized them into a freely available, open-source repository (MOOT).", "result": "MOOT contains over 120 multi-objective tasks covering various domains and is open to community contributions, supporting new research questions and better tool development for practitioners.", "conclusion": "MOOT provides a public repository of multi-objective optimization tasks to facilitate both academic research and practical applications in software engineering decision-making."}}
{"id": "2511.17027", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17027", "abs": "https://arxiv.org/abs/2511.17027", "authors": ["Zhijie Chen", "Xiang Chen", "Ziming Li", "Jiacheng Xue", "Chaoyang Gao"], "title": "ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting", "comment": null, "summary": "Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.", "AI": {"tldr": "ReVul-CoT, a novel LLM framework for software vulnerability assessment, leverages dynamic knowledge retrieval and guided multi-step reasoning to surpass existing methods, marking a notable improvement in automated software security analysis.", "motivation": "LLMs have the potential for software vulnerability assessment but are limited by lack of domain-specific knowledge and shallow reasoning abilities, preventing effective evaluation of complex software vulnerabilities.", "method": "The paper introduces ReVul-CoT, a framework that combines Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT) prompting. RAG retrieves contextually relevant domain knowledge from a local database, while CoT guides the LLM to perform step-by-step reasoning on vulnerabilities.", "result": "ReVul-CoT was tested on a large vulnerability dataset and outperformed current SVA baselines by 16.50%-42.26% in MCC, as well as achieving improvements of 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC compared to the best previous methods. Ablation studies supported the importance of each framework component.", "conclusion": "Integrating RAG and CoT prompt strategies with LLMs significantly boosts the effectiveness of software vulnerability assessments and provides a foundation for future work in this field."}}
{"id": "2511.17131", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17131", "abs": "https://arxiv.org/abs/2511.17131", "authors": ["Horia Cristescu", "Charles Park", "Trong Canh Nguyen", "Sergiu Talmacel", "Alexandru-Gabriel Ilie", "Stefan Adam"], "title": "UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability", "comment": "18 pages, 8 figures, 5 tables. Benchmark comprising 226 tasks across two difficulty tiers. Code and benchmark available at https://github.com/UiPath/uipath_enterprise_benchmark", "summary": "While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes.", "AI": {"tldr": "UI-CUBE is a new benchmark that demonstrates stark limitations in existing CUAs for enterprise workflow automation, revealing architectural flaws and setting a realistic bar for future production-ready systems.", "motivation": "Existing CUA benchmarks mostly focus on task completion and functional correctness, failing to assess readiness for enterprise deployment. This paper aims to address the gap by providing a benchmark that tests operational reliability, a requirement for production environments.", "method": "The authors present UI-CUBE, a new benchmark comprising 226 tasks split into two tiers of difficulty. The tasks test both simple UI interactions and complex workflows, including copy-paste and enterprise application scenarios. The benchmark uses systematic interface variation, multi-resolution testing, and automated validation of task success by checking application state. Five state-of-the-art models and human evaluators are tested against UI-CUBE.", "result": "Current CUAs perform well (67-85% success) on simple UI tasks but drop sharply (9-19%) on complex workflows, showing a capability cliff. Humans also struggle on complex tasks (61.2%), indicating a realistic performance ceiling. The discontinuity in performance suggests fundamental architectural issues in memory management, hierarchical planning, and state coordination.", "conclusion": "UI-CUBE exposes fundamental limits in current CUA architectures and reveals that today\u2019s agents cannot reliably automate complex enterprise workflows. It serves as a diagnostic tool for enterprise-readiness and highlights where architectural changes are needed for production-grade CUAs."}}
{"id": "2511.17262", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17262", "abs": "https://arxiv.org/abs/2511.17262", "authors": ["Jinfeng Wen", "Yuehan Sun"], "title": "SlsReuse: LLM-Powered Serverless Function Reuse", "comment": null, "summary": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.\n  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.", "AI": {"tldr": "This paper introduces SlsReuse, an LLM-based framework that enables efficient reuse of serverless functions via advanced semantic matching. It significantly improves function recommendation effectiveness, proven by high recall rates compared to existing methods.", "motivation": "Serverless computing allows developers to build applications without managing infrastructure, but novice developers struggle due to heterogeneous, platform-specific programming requirements. Creating serverless functions from scratch is time-consuming and error-prone.", "method": "The paper introduces SlsReuse, an LLM-powered framework that facilitates serverless function reuse. SlsReuse constructs a reusable function repository and uses prompt engineering with few-shot learning to generate semantic-enhanced representations of heterogeneous functions. For a given natural language task, SlsReuse performs intent-aware function discovery using a multi-level pruning strategy and similarity matching.", "result": "On a dataset of 110 task queries and using ChatGPT-4o, SlsReuse achieves a Recall@10 of 91.20%, outperforming the state-of-the-art baseline by 24.53 percentage points.", "conclusion": "SlsReuse effectively supports serverless function reuse by bridging the semantic gap between developer task descriptions and function implementations, greatly enhancing serverless development efficiency."}}
{"id": "2511.17271", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17271", "abs": "https://arxiv.org/abs/2511.17271", "authors": ["Sebastian B\u00f6hm", "Florian Sattler", "Norbert Siegmund", "Sven Apel"], "title": "Detecting Performance-Relevant Changes in Configurable Software Systems", "comment": null, "summary": "Performance is a volatile property of a software system and frequent performance profiling is required to keep the knowledge about a software system's performance behavior up to date. Repeating all performance measurements after every revision is a cost-intensive task, especially in the presence of configurability, where one has to measure multiple configurations to obtain a comprehensive picture. Configuration sampling is a common approach to control the measurement cost. However, it cannot guarantee completeness and might miss performance regressions, especially if they only affect few configurations. As an alternative to solve the cost reduction problem, we present ConfFLARE: ConfFLARE estimates whether a change potentially impacts performance by identifying data-flow interactions with performance-relevant code and extracts which software features participate in such interactions. Based on these features, we can select a subset of relevant configurations to focus performance profiling efforts on. In a study conducted on both, synthetic and real-world software systems, ConfFLARE correctly detects performance regressions in almost all cases and identifies relevant features in all but two cases, reducing the number of configurations to be tested on average by $79\\%$ for synthetic and by $70\\%$ for real-world regression scenarios saving hours of performance testing time.", "AI": {"tldr": "Profiling every configuration after software changes is expensive and can miss regressions; ConfFLARE uses data-flow analysis to focus on only the most relevant configurations, cutting profiling effort by over 70% while still reliably catching performance issues.", "motivation": "Current performance profiling of configurable software systems is costly and often incomplete, as traditional configuration sampling may miss regressions that only affect a few configurations. This creates a need for more efficient and comprehensive profiling techniques.", "method": "The authors introduce ConfFLARE\u2014a tool that estimates a change\u2019s potential impact on performance by identifying data-flow interactions with performance-relevant code, extracting the participating software features, and then selecting only the relevant configuration subsets for targeted profiling.", "result": "In studies on both synthetic and real-world systems, ConfFLARE detected almost all performance regressions and identified relevant features in all but two cases. It reduced the average number of configuration tests by 79% for synthetic scenarios and by 70% for real-world cases, significantly saving testing time.", "conclusion": "ConfFLARE offers an effective and efficient alternative for performance profiling in configurable software systems, reducing measurement costs and maintaining high regression detection coverage."}}
{"id": "2511.17303", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17303", "abs": "https://arxiv.org/abs/2511.17303", "authors": ["Timmie M. R. Lagermann", "Kristina Sophia Carter", "Su Mei Gwen Ho", "Lu\u00eds Cruz", "Kerstin Eder", "Maja H. Kirkeby"], "title": "Framework Matters: Energy Efficiency of UI Automation Testing Frameworks", "comment": "10 pages, 6 figures, submitted to The 41st ACM/SIGAPP Symposium On Applied Computing (SAC2026)", "summary": "We examine per action energy consumption across four web user interface (UI) automation testing frameworks to determine whether consistent tendencies can guide energy-aware test design. Using a controlled client-server setup with external power metering, we repeat each UI action (refresh, click variants, checkbox, drag&drop, input-text, scroll) 35 times. Across each of the actions, energy costs vary by both framework and action. Puppeteer is the most efficient for left-click, right-click, double-click, checkbox, and input-text; Selenium is the most efficient for refresh and scroll; Nightwatch is generally the least energy efficient. The energy cost of performing the same action varied by up to a factor of six depending on the framework. This indicates that providing transparency of energy consumption for UI automation testing frameworks allows developers to make informed, energy-aware decisions when testing a specific UI action.", "AI": {"tldr": "Different web UI automation frameworks have dramatically varying energy costs for the same actions. Puppeteer and Selenium are typically more efficient depending on the action, while Nightwatch tends to be least efficient. Making energy usage data available helps developers design more sustainable test processes.", "motivation": "The motivation is to understand whether different web UI automation testing frameworks exhibit consistent energy consumption tendencies, so that developers can design energy-aware tests.", "method": "A controlled client-server experiment was set up with external power metering. Six UI actions (refresh, left-click, right-click, double-click, checkbox, drag&drop, input-text, scroll) were each repeated 35 times across four automation frameworks (Puppeteer, Selenium, Nightwatch, and another). Energy consumption was measured and compared.", "result": "Energy consumption significantly varied by both action type and framework, with differences of up to a factor of six for the same action. Puppeteer was generally most energy efficient for most actions, Selenium for refresh and scroll, and Nightwatch the least efficient overall.", "conclusion": "Transparency in energy consumption across frameworks enables developers to make informed, energy-aware choices when designing UI automation tests for specific actions."}}
{"id": "2511.17330", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17330", "abs": "https://arxiv.org/abs/2511.17330", "authors": ["Haoxin Tu", "Huan Zhao", "Yahui Song", "Mehtab Zafar", "Ruijie Meng", "Abhik Roychoudhury"], "title": "Agentic Program Verification", "comment": "21 pages, 8 figures", "summary": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.\n  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.\n  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.", "AI": {"tldr": "AutoRocq introduces an autonomous LLM-driven agent for program verification, refining proofs in collaboration with the Rocq theorem prover. Unlike previous methods needing extensive training, AutoRocq learns on-the-fly, showing strong results in benchmarks like SV-COMP and Linux modules. This approach advances automated, trusted code generation by integrating verification with AI coding workflows.", "motivation": "Automated code generation via LLMs is growing, raising the need for reliable, scalable automated verification methods. Program verification is essential for trusted software, but manually verifying large volumes of AI-generated code is infeasible.", "method": "AutoRocq operates by iteratively refining proofs through an autonomous loop. The agent communicates with the Rocq theorem prover to receive feedback and context, autonomously deciding the structure of proof trees without extensive pre-training, contrasting with previous approaches dependent on proof example datasets.", "result": "Experiments on SV-COMP benchmarks and Linux kernel modules show strong performance for AutoRocq in automating program verification, indicating it could enable reliable 'generate and validate' workflows with AI coding agents.", "conclusion": "AutoRocq, an LLM-based agent for program verification, effectively leverages autonomous proof search and collaborative interaction with the Rocq theorem prover, demonstrating promising automation in verifying real-world code."}}
{"id": "2511.17368", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17368", "abs": "https://arxiv.org/abs/2511.17368", "authors": ["Eric L. Melin", "Ahmed Musa Awon", "Nasir U. Eisty", "Neil A. Ernst", "Shurui Zhou"], "title": "Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software", "comment": "11 pages, 2 figures, 6 tables", "summary": "Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.", "AI": {"tldr": "Scientific Software harbors far more technical debt than general-purpose software, potentially compromising research quality and reproducibility. The study fine-tuned transformer-based models for SATD detection, finding substantial differences and recommend strategies for better debt management in scientific code.", "motivation": "The study is motivated by the lack of understanding of the relationship between Self-Admitted Technical Debt (SATD) and Scientific Software (SSW), despite the critical importance of software quality for reproducible and valid scientific results.", "method": "The researchers analyzed SATD in 27 repositories\u2014both scientific and general-purpose\u2014across multiple domains and languages. They also fine-tuned and compared 10 transformer-based models (ranging from 100M to 7B parameters) on 67,066 labeled code comments, evaluating their effectiveness in identifying SATD.", "result": "The analysis revealed that SSW contains 9.25 times more Scientific Debt and 4.93 times more SATD than general-purpose software, primarily due to domain-specific complexities and evolving research requirements. Their best-performing transformer-based model surpasses previous approaches in identifying SATD.", "conclusion": "SATD in Scientific Software is significantly more prevalent and impactful than in general-purpose software, posing unique challenges to software quality and scientific reproducibility. By understanding SATD patterns and deploying advanced detection models, developers can better manage technical debt and enhance the integrity of scientific research."}}
{"id": "2511.17417", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17417", "abs": "https://arxiv.org/abs/2511.17417", "authors": ["Soroush Javdan", "Pragash Krishnamoorthy", "Olga Baysal"], "title": "CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval", "comment": null, "summary": "The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \\textbf{CREST} (\\textbf{C}riteria-specific \\textbf{R}etrieval via \\textbf{E}nsemble of \\textbf{S}pecialized \\textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.", "AI": {"tldr": "CREST, a retrieval method using specialized models for different criteria in Trouble Reports, outperforms single-model approaches by improving retrieval accuracy, score calibration, and interpretability, thus helping troubleshoot software faults more efficiently in telecom systems.", "motivation": "The telecommunication industry relies on efficient troubleshooting to ensure network reliability and service quality. However, the complexity and volume of Trouble Reports (TRs), which document software faults, make retrieving relevant information difficult, especially given the multiple criteria that describe each fault. Current retrieval systems struggle to handle this diversity effectively.", "method": "The study introduces CREST, a retrieval approach that uses an ensemble of specialized models, each trained on a specific Trouble Report criterion. Instead of relying on a single model, CREST aggregates outputs from these models to capture the unique aspects described by various TR fields, thereby improving retrieval performance and interpretability.", "result": "Experiments on a set of Ericsson's internal TRs showed that CREST's criterion-specific ensemble approach significantly outperformed traditional single model systems. CREST provided more accurate retrieval, better score calibration, and clearer explanations for why specific reports were retrieved.", "conclusion": "Using specialized retrieval models for different TR criteria improves both the effectiveness and transparency of TR retrieval systems, facilitating quicker software fault resolution and better software maintainability in telecom settings."}}
