<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating Software Process Models for Multi-Agent Class-Level Code Generation](https://arxiv.org/abs/2511.09794)
*Wasique Islam Shafin,Md Nakhla Rafi,Zhenhao Li,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: Waterfall-style multi-agent collaboration with LLMs leads to cleaner, more maintainable code but can reduce functional correctness and change error patterns. Effects vary across models and workflow stages, revealing trade-offs between rigid process discipline and flexible problem-solving in automated code generation.


<details>
  <summary>Details</summary>
Motivation: Recent advances in Large Language Models (LLMs) have enabled their use in automating software development tasks. Despite their potential, research has mainly focused on single-agent, function-level tasks rather than collaborative, multi-agent approaches at the class level. The authors aim to explore how process discipline and specialized roles in multi-agent workflows affect code quality and correctness.

Method: The study simulates a Waterfall-style development process (Requirement, Design, Implementation, Testing) using three different LLMs—GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku—over 100 Python tasks from the ClassEval benchmark. Each LLM is assigned specific roles in the workflow to mimic a structured multi-agent team.

Result: Waterfall-style, multi-agent collaboration yields cleaner and more maintainable code but at the cost of reduced functional correctness for most models (e.g., a drop of approximately 38% for GPT-4o-mini and DeepSeek-Chat). However, Claude-3.5-Haiku is an exception, demonstrating a 9.5% improvement in correctness. Process constraints change error patterns: there are fewer structural issues but increased semantic and validation errors. The Testing phase significantly boosts verification coverage, but also introduces more reasoning failures. Requirement and Design stages have lesser impact.

Conclusion: Multi-agent LLM workflows, structured in a Waterfall style, fundamentally shift the way LLMs collaborate and fail, redistributing rather than uniformly enhancing performance. They make code more maintainable, but can lower correctness and shift error types, highlighting trade-offs between workflow discipline and flexible reasoning in automated software development.

Abstract: Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\% for GPT-4o-mini and -39.8\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation.

</details>


### [2] [EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines](https://arxiv.org/abs/2511.09964)
*Noah van der Vleuten,Anthony Flores,Shray Mathur,Max Rakitin,Thomas Hopkins,Kevin G. Yager,Esther H. R. Tsai*

Main category: cs.SE

TL;DR: The paper introduces EnvTrace, a simulation-based method to evaluate LLMs for instrument control by checking execution traces in a digital twin. Over 30 LLMs are assessed, with results showing that top models can generate control code at near-human levels, supporting a future where LLMs and digital twins enable safer, more autonomous AI control systems.


<details>
  <summary>Details</summary>
Motivation: Standard algorithmic benchmarks are insufficient for evaluating large language models (LLMs) intended for instrument control, as they don't reflect the complexities of physical systems' real-world behavior.

Method: This paper introduces EnvTrace, a simulation-based evaluation framework that uses execution traces to assess the semantic equivalence of code for instrument control tasks. EnvTrace is demonstrated using a beamline control digital twin, and over 30 LLMs are evaluated for their performance in generating control code via trace alignment.

Result: Many leading LLMs can perform rapid control-code generation close to a human level of functional correctness, as measured by multi-faceted trace alignment scores across key behavioral dimensions.

Conclusion: EnvTrace offers a novel and effective method for evaluating LLMs in context-rich, simulation-based scenarios relevant to instrument control. This approach enables pre-execution validation and paves the way for autonomous AI systems where LLMs and digital twins work together for safe and effective control.

Abstract: Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI.

</details>


### [3] [Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents](https://arxiv.org/abs/2511.10049)
*Divyanshu Saxena,Rishikesh Maurya,Xiaoxuan Ou,Gagan Somashekar,Shachee Mishra Gupta,Arun Iyer,Yu Kang,Chetan Bansal,Aditya Akella,Saravan Rajmohan*

Main category: cs.SE

TL;DR: Fixed benchmarks fall short for ever-evolving enterprise AI agents. The paper proposes dynamically generating evaluation benchmarks using developer intent and LLMs, resulting in a maintainable, feedback-driven evaluation system demonstrated in a real-world migration case.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation methods for AI agents—based on fixed benchmarks and metrics—are inadequate for enterprise-scale scenarios where requirements and services are constantly changing and well-defined ground-truth data is limited.

Method: The authors propose a dynamic process for generating evaluation benchmarks that adapt as requirements evolve. This involves using semi-structured documents authored by developers to capture high-level intent, and employs state-of-the-art large language models (LLMs) to automatically generate benchmarks from a small set of these documents. The process is demonstrated through a case study involving service migration at a large enterprise.

Result: The proposed approach produces a maintainable and adaptable evaluation framework. This framework supports quick feedback on agent performance, allowing for timely and targeted improvements.

Conclusion: Dynamic, intent-driven benchmark generation using LLMs enhances the robustness and relevance of AI agent evaluation in evolving enterprise settings. This supports continuous improvement and more effective deployment of AI agents in production environments.

Abstract: The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements.

</details>


### [4] [Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics](https://arxiv.org/abs/2511.10271)
*Xin Sun,Daniel Ståhl,Kristian Sandahl,Christoph Kessler*

Main category: cs.SE

TL;DR: This paper highlights that LLM-generated code is mostly evaluated for functional correctness in research, while industry cares more about maintainability and readability. The study finds trade-offs between non-functional qualities and considerable variance across LLMs, urging better quality assurance for code generation beyond just passing tests.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of systematic evaluation of non-functional qualities (like security, maintainability, and performance efficiency) of code generated by LLMs, since current research mostly focuses only on functional correctness (i.e., whether the code passes tests).

Method: The study uses three complementary methods: (1) a systematic review of 108 papers; (2) two industry workshops with practitioners from various organizations; (3) empirical analysis of LLM-generated code patches for real-world software issues, focusing on non-functional qualities.

Result: The results show that academic research predominantly emphasizes security and performance efficiency, leaving qualities like maintainability understudied. Industry experts, however, prioritize maintainability and readability and warn of technical debt risks from LLM-generated code. Empirical results demonstrate that improvements in one code quality often come at the expense of others, and variation exists across models and optimization strategies.

Conclusion: There is a significant mismatch between academic focus, industry priorities, and LLM model behaviour regarding non-functional code quality. The study calls for robust quality assurance mechanisms in LLM-driven code generation workflows to ensure generated code meets industry-relevant quality standards, not just functional correctness.

Abstract: In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.

</details>


### [5] [A Large-Scale Collection Of (Non-)Actionable Static Code Analysis Reports](https://arxiv.org/abs/2511.10323)
*Dávid Kószó,Tamás Aladics,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: Alert fatigue from Static Code Analysis tools is worsened by too many non-actionable warnings and a lack of suitable datasets. This paper presents a methodology and a new large-scale dataset (NASCAR) for Java SCA warnings to address these issues; both are now publicly available for researchers.


<details>
  <summary>Details</summary>
Motivation: Static Code Analysis (SCA) tools produce too many warnings, many of which are non-actionable, resulting in alert fatigue and decreased developer productivity. There is a lack of large, labeled datasets—especially for Java—that researchers need to train models to better filter and categorize these warnings.

Method: The paper introduces a novel methodology for collecting and categorizing SCA warnings, allowing effective distinction between actionable and non-actionable alerts. The methodology is applied to generate a large dataset specifically for Java code.

Result: A dataset named NASCAR, consisting of over 1 million entries of categorized Java SCA warnings, is created. Both the dataset and the tools for its creation are made publicly available to support further research.

Conclusion: The work fills the crucial gap in available datasets for SCA warning categorization in Java, enabling advancements in filtering alerts and mitigating alert fatigue. Open-sourcing both the dataset and tools facilitates further research and development in improving SCA tools.

Abstract: Static Code Analysis (SCA) tools, while invaluable for identifying potential coding problems, functional bugs, or vulnerabilities, often generate an overwhelming number of warnings, many of which are non-actionable. This overload of alerts leads to ``alert fatigue'', a phenomenon where developers become desensitized to warnings, potentially overlooking critical issues and ultimately hindering productivity and code quality. Analyzing these warnings and training machine learning models to identify and filter them requires substantial datasets, which are currently scarce, particularly for Java. This scarcity impedes efforts to improve the accuracy and usability of SCA tools and mitigate the effects of alert fatigue. In this paper, we address this gap by introducing a novel methodology for collecting and categorizing SCA warnings, effectively distinguishing actionable from non-actionable ones. We further leverage this methodology to generate a large-scale dataset of over 1 million entries of Java source code warnings, named NASCAR: (Non-)Actionable Static Code Analysis Reports. To facilitate follow-up research in this domain, we make both the dataset and the tools used to generate it publicly available.

</details>


### [6] [Towards Comprehensive Sampling of SMT Solutions](https://arxiv.org/abs/2511.10326)
*Shuangyu Lyu,Chuan Luo,Ruizhi Shi,Wei Wu,Chanjuan Liu,Chunming Hu*

Main category: cs.SE

TL;DR: PanSampler is a new, efficient SMT sampler for software and hardware testing. It ensures high coverage and fault detection with fewer test cases, reducing testing costs and improving overall efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Generating diverse solutions for SMT formulas is vital for software and hardware testing, enabling effective fault detection and coverage of system behaviors. However, existing methods often require too many solutions, which increases resource consumption and reduces efficiency.

Method: The paper presents PanSampler, which uses three novel techniques: a diversity-aware SMT algorithm, AST-guided scoring functions, and post-sampling optimization. It iteratively samples solutions, evaluates them, and uses local search to enhance coverage while minimizing the number of samples.

Result: Experimental results show PanSampler achieves higher target coverage and fault detection capability compared to previous samplers, while requiring significantly fewer solutions. It reduces the number of test cases needed by 32.6% to 76.4% to reach comparable fault detection effectiveness.

Conclusion: PanSampler is an effective SMT sampler that enhances testing efficiency, reduces resource usage, and improves fault detection by generating diverse solutions with high coverage using fewer samples.

Abstract: This work focuses on effectively generating diverse solutions for satisfiability modulo theories (SMT) formulas, targeting the theories of bit-vectors, arrays, and uninterpreted functions, which is a critical task in software and hardware testing. Generating diverse SMT solutions helps uncover faults and detect safety violations during the verification and testing process, resulting in the SMT sampling problem, i.e., constructing a small number of solutions while achieving comprehensive coverage of the constraint space. While high coverage is crucial for exploring system behaviors, reducing the number of solutions is of great importance, as excessive solutions increase testing time and resource usage, undermining efficiency. In this work, we introduce PanSampler, a novel SMT sampler that achieves high coverage with a small number of solutions. It incorporates three novel techniques, i.e., diversity-aware SMT algorithm, abstract syntax tree (AST)-guided scoring function and post-sampling optimization technology, enhancing its practical performance. It iteratively samples solutions, evaluates candidates, and employs local search to refine solutions, ensuring high coverage with a small number of samples. Extensive experiments on practical benchmarks demonstrate that PanSampler exhibits a significantly stronger capability to reach high target coverage, while requiring fewer solutions than current samplers to achieve the same coverage level. Furthermore, our empirical evaluation on practical subjects, which are collected from real-world software systems, shows that PanSampler achieves higher fault detection capability and reduces the number of required test cases from 32.6\% to 76.4\% to reach the same fault detection effectiveness, leading to a substantial improvement in testing efficiency. PanSampler advances SMT sampling, reducing the cost of software testing and hardware verification.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [Cyclotron: Compilation of Recurrences to Distributed and Systolic Architectures](https://arxiv.org/abs/2511.09987)
*Shiv Sundram,Akhilesh Balasingam,Nathan Zhang,Kunle Olukotun,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: Cyclotron is a framework and compiler that translates high-level recurrence equations into efficient, portable distributed algorithms for different hardware, optimizing memory locality and communication. It achieves performance competitive with established libraries in distributed environments.


<details>
  <summary>Details</summary>
Motivation: Expressing complex streaming dataflow algorithms for distributed hardware is challenging due to the need for portable, efficient compilation and handling of memory locality and communication. Existing solutions often lack portability or require significant manual optimization.

Method: Cyclotron introduces a new framework and compiler that uses recurrence equations over logical tensors to describe streaming dataflows. The framework provides a multi-stage compilation: high-level input language, an intermediate language for iteration spaces, and eventually a target-specific language for distributed processors. Optimizations in the IR localize memory accesses, and a novel scheduling language allows flexible control over data streaming and pipelining.

Result: The authors successfully port their IR to a reconfigurable simulator of systolic arrays, chiplet-style distributed hardware, and distributed-memory CPU clusters. In experiments, Cyclotron generates distributed implementations for matrix multiplication and solvers that are competitive with established solutions like ScaLAPACK.

Conclusion: Cyclotron offers a portable and efficient approach to expressing and compiling streaming dataflow algorithms for various distributed hardware, reducing the burden on programmers and enabling competitive, optimized implementations.

Abstract: We present Cyclotron, a framework and compiler for using recurrence equations to express streaming dataflow algorithms, which then get portably compiled to distributed topologies of interlinked processors. Our framework provides an input language of recurrences over logical tensors, which then gets lowered into an intermediate language of recurrences over logical iteration spaces, and finally into programs of send, receive, and computation operations specific to each individual processor. In Cyclotron's IR, programs are optimized such that external memory interactions are confined to the boundaries of the iteration space. Within inner iteration spaces, all data accesses become local: data accesses target values residing in local fast memory or on neighboring processing units, avoiding costly memory movement. We provide a scheduling language allowing users to define how data gets streamed and broadcasted between processors, enabling pipelined execution of computation kernels over distributed topologies of processing elements. We demonstrate the portability of our approach by compiling our IR to a reconfigurable simulator of systolic arrays and chiplet style distributed hardware, as well as to distributed-memory CPU clusters. In the simulated reconfigurable setting, we use our compiler for hardware design space exploration in which link costs and latencies can be specified. In the distributed CPU setting, we show how to use recurrences and our scheduling language to express various matrix multiplication routines (Cannon, SUMMA, PUMMA, weight stationary) and solvers (Triangular solve and Cholesky). For matrix multiplication and the triangular solve, we generate distributed implementations competitive with ScaLAPACK.

</details>


### [8] [Omnidirectional type inference for ML: principality any way](https://arxiv.org/abs/2511.10343)
*Alistair O'Brien,Didier Rémy,Gabriel Scherer*

Main category: cs.PL

TL;DR: Omnidirectional type inference enables dynamic resolution of type constraints, overcoming the rigidity and rejection of well-typed programs found in ML extensions, and restores principality for advanced language features in OCaml.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations caused by the loss of principality in ML type system extensions (such as GADTs, higher-rank polymorphism, and static overloading), which makes type inference less predictable and efficient, and leads to well-typed programs being unnecessarily rejected by rigid, statically-ordered inference algorithms.

Method: They propose 'omnidirectional type inference', which allows type constraints to be solved in any order, using suspended match constraints and incremental instantiation to handle dynamic flows of type information, particularly addressing the challenges of let-generalization.

Result: This approach restores principality for fragile extensions by enabling principal type inference for features such as static overloading and semi-explicit first-class polymorphism in OCaml, making the inference more expressive than the current OCaml typechecker.

Conclusion: Omnidirectional type inference forms a general and versatile framework for principal type inference in ML-like languages, recovering predictability and expressiveness even in the presence of fragile language features.

Abstract: The Damas-Hindley-Milner (ML) type system owes its success to principality, the property that every well-typed expression has a unique most general type. This makes inference predictable and efficient. Unfortunately, many extensions of ML (GADTs, higher-rank polymorphism, and static overloading) endanger princpality by introducing _fragile_ constructs that resist principal inference. Existing approaches recover principality through directional inference algorithms, which propagate _known_ type information in a fixed (or static) order (e.g. as in bidirectional typing) to disambiguate such constructs. However, the rigidity of a static inference order often causes otherwise well-typed programs to be rejected.
  We propose _omnidirectional_ type inference, where type information flows in a dynamic order. Typing constraints may be solved in any order, suspending when progress requires known type information and resuming once it becomes available, using _suspended match constraints_. This approach is straightforward for simply typed systems, but extending it to ML is challenging due to let-generalization. Existing ML inference algorithms type let-bindings (let x = e1 in e2) in a fixed order: type e1, generalize its type, and then type e2. To overcome this, we introduce _incremental instantiation_, allowing partially solved type schemes containing suspended constraints to be instantiated, with a mechanism to incrementally update instances as the scheme is refined.
  Omnidirectionality provides a general framework for restoring principality in the presence of fragile features. We demonstrate its versatility on two fundamentally different features of OCaml: static overloading of record labels and datatype constructors and semi-explicit first-class polymorphism. In both cases, we obtain a principal type inference algorithm that is more expressive than OCaml's current typechecker.

</details>
