{"id": "2507.11671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11671", "abs": "https://arxiv.org/abs/2507.11671", "authors": ["Mst Shamima Aktar", "Peng Liang", "Muhammad Waseem", "Amjed Tahir", "Mojtaba Shahin", "Muhammad Azeem Akbar", "Arif Ali Khan", "Aakash Ahmad", "Musengamana Jean de Dieu", "Ruiyin Li"], "title": "Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems", "comment": "49 pages, 10 images, 16 tables, Manuscript submitted to a journal\n  (2025)", "summary": "Quantum software represents disruptive technologies in terms of\nquantum-specific software systems, services, and applications - leverage the\nprinciples of quantum mechanics via programmable quantum bits (Qubits) that\nmanipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.\nQuantum software architecture enables quantum software developers to abstract\naway implementation-specific details (i.e., mapping of Qubits and QuGates to\nhigh-level architectural components and connectors). Architectural patterns and\nstrategies can provide reusable knowledge and best practices to engineer\nquantum software systems effectively and efficiently. However, quantum software\npractitioners face significant challenges in selecting and implementing\nappropriate patterns and strategies due to the complexity of quantum software\nsystems and the lack of guidelines. To address these challenges, this study\nproposes decision models for selecting patterns and strategies in six critical\ndesign areas in quantum software systems: Communication, Decomposition, Data\nProcessing, Fault Tolerance, Integration and Optimization, and Algorithm\nImplementation. These decision models are constructed based on data collected\nfrom both a mining study (i.e., GitHub and Stack Exchange) and a Systematic\nLiterature Review, which were used to identify relevant patterns and strategies\nwith their involved Quality Attributes (QAs). We then conducted semi-structured\ninterviews with 16 quantum software practitioners to evaluate the familiarity,\nunderstandability, completeness, and usefulness of the proposed decision\nmodels. The results show that the proposed decision models can aid\npractitioners in selecting suitable patterns and strategies to address the\nchallenges related to the architecture design of quantum software systems. The\ndataset is available at [6], allowing the community to reproduce and build upon\nour findings."}
{"id": "2507.11687", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11687", "abs": "https://arxiv.org/abs/2507.11687", "authors": ["Atharva Naik", "Lawanya Baghel", "Dhakshin Govindarajan", "Darsh Agrawal", "Daniel Fried", "Carolyn Rose"], "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization", "comment": null, "summary": "Large Language Models, though successful in code generation, struggle with\ncode quality analysis because they are limited by static training data and\ncan't easily adapt to evolving best practices. We introduce MetaLint, a new\ninstruction-following framework that formulates code quality analysis as the\ntask of detecting and fixing problematic semantic code fragments or code idioms\nbased on high-level specifications. Unlike conventional approaches that train\nmodels on static, rule-based data, MetaLint employs instruction tuning on\nsynthetic linter-generated data to support easy-to-hard generalization,\nenabling models to adapt to novel or complex code patterns without retraining.\nTo evaluate this, we construct a benchmark of challenging idioms inspired by\nreal-world coding standards such as Python Enhancement Proposals (PEPs) and\nassess whether MetaLint-trained models reason adaptively or simply memorize.\nOur results show that MetaLint improves generalization to unseen PEP idioms,\nachieving a 70.37% F-score on idiom detection with the highest recall (70.43%)\namong all evaluated models. It also achieves 26.73% on localization,\ncompetitive for its 4B parameter size and comparable to larger state-of-the-art\nmodels like o3-mini, highlighting its potential for future-proof code quality\nanalysis."}
{"id": "2507.11689", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11689", "abs": "https://arxiv.org/abs/2507.11689", "authors": ["Sergio Di Meglio", "Valeria Pontillo", "Luigi Libero Lucio Starace"], "title": "REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps", "comment": "Manuscript accepted for the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA)", "summary": "In Computer Science Bachelor's programs, software quality is often\nunderemphasized due to limited time and a focus on foundational skills, leaving\nmany students unprepared for industry expectations. To better understand the\ntypical quality of student code and inform both education and hiring practices,\nwe analyze 40 full-stack web applications developed in a third-year Web\nTechnologies course. Using an automated static analysis pipeline, we assess\nadherence to REST API design rules. Results reveal frequent violations of\nfoundational conventions, such as missing hyphens in endpoint paths (98%),\nincorrect pluralization (88%), and misuse of HTTP methods (83%). These findings\nhighlight the need for more focused instruction on API design and support the\nadoption of automated tools to improve code quality in student projects."}
{"id": "2507.11898", "categories": ["cs.SE", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11898", "abs": "https://arxiv.org/abs/2507.11898", "authors": ["Rathin Singha", "Harry Qian", "Srinath Saikrishnan", "Tracy Zhao", "Ryan Beckett", "Siva Kesava Reddy Kakarla", "George Varghese"], "title": "Extremal Testing for Network Software using LLMs", "comment": null, "summary": "Physicists often manually consider extreme cases when testing a theory. In\nthis paper, we show how to automate extremal testing of network software using\nLLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS\nname length limits); then ask the LLM to generate tests that violate the\nconstraints. We demonstrate how easy this process is by generating extremal\ntests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.\nWe show how this methodology extends to centralized network software such as\nshortest path algorithms, and how LLMs can generate filtering code to reject\nextremal input. We propose using agentic AI to further automate extremal\ntesting. LLM-generated extremal testing goes beyond an old technique in\nsoftware testing called Boundary Value Analysis."}
{"id": "2507.11676", "categories": ["cs.PL", "cs.LO", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11676", "abs": "https://arxiv.org/abs/2507.11676", "authors": ["Chris Heunen", "Louis Lemonnier", "Christopher McNally", "Alex Rice"], "title": "Quantum circuits are just a phase", "comment": "43 pages, 5 figures", "summary": "Quantum programs today are written at a low level of abstraction - quantum\ncircuits akin to assembly languages - and even advanced quantum programming\nlanguages essentially function as circuit description languages. This state of\naffairs impedes scalability, clarity, and support for higher-level reasoning.\nMore abstract and expressive quantum programming constructs are needed.\n  To this end, we introduce a novel yet simple quantum programming language for\ngenerating unitaries from \"just a phase\"; we combine a (global) phase operation\nthat captures phase shifts with a quantum analogue of the \"if let\" construct\nthat captures subspace selection via pattern matching. This minimal language\nlifts the focus from quantum gates to eigendecomposition, conjugation, and\ncontrolled unitaries; common building blocks in quantum algorithm design.\n  We demonstrate several aspects of the expressive power of our language in\nseveral ways. Firstly, we establish that our representation is universal by\nderiving a universal quantum gate set. Secondly, we show that important quantum\nalgorithms can be expressed naturally and concisely, including Grover's search\nalgorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal\nProcessing, and the Quantum Eigenvalue Transformation. Furthermore, we give\nclean denotational semantics grounded in categorical quantum mechanics.\nFinally, we implement a prototype compiler that efficiently translates terms of\nour language to quantum circuits, and prove that it is sound with respect to\nthese semantics. Collectively, these contributions show that this construct\noffers a principled and practical step toward more abstract and structured\nquantum programming."}
{"id": "2507.11976", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11976", "abs": "https://arxiv.org/abs/2507.11976", "authors": ["Jana-Rebecca Rehse", "Michael Grohs", "Finn Klessascheck", "Lisa-Marie Klein", "Tatiana von Landesberger", "Luise Pufahl"], "title": "A Task Taxonomy for Conformance Checking", "comment": "Preprint submitted to Information Systems", "summary": "Conformance checking is a sub-discipline of process mining, which compares\nobserved process traces with a process model to analyze whether the process\nexecution conforms with or deviates from the process design. Organizations can\nleverage this analysis, for example to check whether their processes comply\nwith internal or external regulations or to identify potential improvements.\nGaining these insights requires suitable visualizations, which make complex\nresults accessible and actionable. So far, however, the development of\nconformance checking visualizations has largely been left to tool vendors. As a\nresult, current tools offer a wide variety of visual representations for\nconformance checking, but the analytical purposes they serve often remain\nunclear. However, without a systematic understanding of these purposes, it is\ndifficult to evaluate the visualizations' usefulness. Such an evaluation hence\nrequires a deeper understanding of conformance checking as an analysis domain.\nTo this end, we propose a task taxonomy, which categorizes the tasks that can\noccur when conducting conformance checking analyses. This taxonomy supports\nresearchers in determining the purpose of visualizations, specifying relevant\nconformance checking tasks in terms of their goal, means, constraint type, data\ncharacteristics, data target, and data cardinality. Combining concepts from\nprocess mining and visual analytics, we address researchers from both\ndisciplines to enable and support closer collaborations."}
{"id": "2507.11731", "categories": ["cs.PL", "68N15", "D.3.2"], "pdf": "https://arxiv.org/pdf/2507.11731", "abs": "https://arxiv.org/abs/2507.11731", "authors": ["Neng-Fa Zhou", "Cristian Grozea", "Håkan Kjellerstrand", "Oisín Mac Fhearaí"], "title": "Picat Through the Lens of Advent of Code", "comment": "14 pages", "summary": "Picat is a logic-based, multi-paradigm programming language that integrates\nfeatures from logic, functional, constraint, and imperative programming\nparadigms. This paper presents solutions to several problems from the 2024\nAdvent of Code (AoC). While AoC problems are not designed for any specific\nprogramming language, certain problem types, such as reverse engineering and\npath-finding, are particularly well-suited to Picat due to its built-in\nconstraint solving, pattern matching, backtracking, and dynamic programming\nwith tabling. This paper demonstrates that Picat's features, especially its\nSAT-based constraint solving and tabling, enable concise, declarative, and\nhighly efficient implementations of problems that would require significantly\nmore effort in imperative languages."}
{"id": "2507.12084", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12084", "abs": "https://arxiv.org/abs/2507.12084", "authors": ["Keke Gai", "Haochen Liang", "Jing Yu", "Liehuang Zhu", "Dusit Niyato"], "title": "LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation", "comment": null, "summary": "Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing\nremains an important approach to securing smart contracts. Even though mutation\nscheduling is a key factor influencing fuzzing effectiveness, existing fuzzers\nhave primarily explored seed scheduling and generation, while mutation\nscheduling has been rarely addressed by prior work. In this work, we propose a\nLarge Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing\nframework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and\nhybrid testing techniques. Key components of the proposed LLAMA include: (i) a\nhierarchical prompting strategy that guides LLMs to generate semantically valid\ninitial seeds, coupled with a lightweight pre-fuzzing phase to select\nhigh-potential inputs; (ii) a multi-feedback optimization mechanism that\nsimultaneously improves seed generation, seed selection, and mutation\nscheduling by leveraging runtime coverage and dependency feedback; and (iii) an\nevolutionary fuzzing engine that dynamically adjusts mutation operator\nprobabilities based on effectiveness, while incorporating symbolic execution to\nescape stagnation and uncover deeper vulnerabilities. Our experiments\ndemonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage\nand vulnerability detection. Specifically, it achieves 91% instruction coverage\nand 90% branch coverage, while detecting 132 out of 148 known vulnerabilities\nacross diverse categories. These results highlight LLAMA's effectiveness,\nadaptability, and practicality in real-world smart contract security testing\nscenarios."}
{"id": "2507.11827", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.11827", "abs": "https://arxiv.org/abs/2507.11827", "authors": ["Shaurya Gomber", "Debangshu Banerjee", "Gagandeep Singh"], "title": "Universal Synthesis of Differentiably Tunable Numerical Abstract Transformers", "comment": "42 pages, 8 figures", "summary": "Numerical abstract interpretation is a widely used framework for the static\nanalysis of numerical programs. However, existing numerical abstract\ninterpreters rely on hand-crafted, instruction-specific transformers tailored\nto each domain, with no general algorithm for handling common operations across\ndomains. This limits extensibility, prevents precise compositional reasoning\nover instruction sequences, and forces all downstream tasks to use the same\nfixed transformer regardless of their precision, efficiency, or task-specific\nrequirements. To address these limitations, we propose a universal transformer\nsynthesis algorithm that constructs a parametric family of sound abstract\ntransformers for any given polyhedral numerical domain and a concrete operator\nfrom the class of Quadratic-Bounded Guarded Operators (QGO), which includes\nboth individual instructions and structured sequences. Each instantiation in\nthis family is sound by construction, enabling downstream analyses to adapt the\ntransformer to their particular needs. The space of transformers is\ndifferentiable but complex. To efficiently explore this space of transformers,\nwe introduce the Adaptive Gradient Guidance (AGG) procedure, a gradient-guided\nsearch strategy that steers the search process based on downstream analysis\nobjectives and runtime constraints. We implement these ideas in the USTAD\nframework and evaluate their effectiveness across three numerical abstract\ndomains: Zones, Octagons, and Polyhedra. Our results demonstrate that the\nuniversal synthesis algorithm successfully constructs sound families of\ntransformers across domains, and that USTAD achieves significant, tunable\nprecision gains over baselines by leveraging compositional reasoning and\nefficient gradient-guided traversal of the transformer space."}
{"id": "2507.12104", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12104", "abs": "https://arxiv.org/abs/2507.12104", "authors": ["Francisco Javier Cavero", "Juan C. Alonso", "Antonio Ruiz-Cortés"], "title": "From Static to Intelligent: Evolving SaaS Pricing with LLMs", "comment": "12 pages. Accepted at the SOC4AI Workshop (Service-Oriented Computing\n  for AI Applications), held in conjunction with the 22nd International\n  Conference on Service-Oriented Computing (ICSOC 2024)", "summary": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites."}
{"id": "2507.11897", "categories": ["cs.PL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.11897", "abs": "https://arxiv.org/abs/2507.11897", "authors": ["Tyler Hou", "Shadaj Laddad", "Joseph M. Hellerstein"], "title": "Towards Relational Contextual Equality Saturation", "comment": "Appeared at EGRAPHS 2024", "summary": "Equality saturation is a powerful technique for program optimization.\nContextual equality saturation extends this to support rewrite rules that are\nconditioned on where a term appears in an expression. Existing work has brought\ncontextual reasoning to egg; in this paper, we share our ongoing work to extend\nthis to relational equality saturation in egglog. We summarize the existing\napproaches to contextual equality saturation, outline its main applications,\nand identify key challenges in combining this approach with relational models."}
{"id": "2507.12118", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12118", "abs": "https://arxiv.org/abs/2507.12118", "authors": ["Noe Zermeño", "Cristina Zuheros", "Lucas Daniel Del Rosso Calache", "Francisco Herrera", "Rosana Montes"], "title": "An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment", "comment": null, "summary": "In recent years, attention has increasingly focused on enhancing user\nsatisfaction with user interfaces, spanning both mobile applications and\nwebsites. One fundamental aspect of human-machine interaction is the concept of\nweb usability. In order to assess web usability, the A/B testing technique\nenables the comparison of data between two designs. Expanding the scope of\ntests to include the designs being evaluated, in conjunction with the\ninvolvement of both real and fictional users, presents a challenge for which\nfew online tools offer support. We propose a methodology for web usability\nevaluation based on user-centered approaches such as design thinking and\nlinguistic decision-making, named Linguistic Decision-Making for Web Usability\nEvaluation. This engages people in role-playing scenarios and conducts a number\nof usability tests, including the widely recognized System Usability Scale. We\nincorporate the methodology into a decision support system based on A/B\ntesting. We use real users in a case study to assess three Moodle platforms at\nthe University of Guadalajara, Mexico."}
{"id": "2507.12367", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.12367", "abs": "https://arxiv.org/abs/2507.12367", "authors": ["Diganta Misra", "Nizar Islah", "Victor May", "Brice Rauby", "Zihan Wang", "Justine Gehring", "Antonio Orvieto", "Muawiz Chaudhary", "Eilif B. Muller", "Irina Rish", "Samira Ebrahimi Kahou", "Massimo Caccia"], "title": "GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities", "comment": "Version 2 of the dataset from: arXiv:2411.05830", "summary": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark."}
{"id": "2507.12284", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12284", "abs": "https://arxiv.org/abs/2507.12284", "authors": ["Artem Chervyakov", "Alexander Kharitonov", "Pavel Zadorozhny", "Adamenko Pavel", "Rodion Levichev", "Dmitrii Vorobev", "Dmitrii Salikhov", "Aidar Valeev", "Alena Pestova", "Maria Dziuba", "Ilseyar Alimova", "Artem Zavgorodnev", "Aleksandr Medvedev", "Stanislav Moiseev", "Elena Bruches", "Daniil Grebenkin", "Roman Derunets", "Vikulov Vladimir", "Anton Emelyanov", "Dmitrii Babaev", "Vladimir V. Ivanov", "Valentin Malykh", "Alena Fenogenova"], "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks", "comment": null, "summary": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures."}
{"id": "2507.12367", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.12367", "abs": "https://arxiv.org/abs/2507.12367", "authors": ["Diganta Misra", "Nizar Islah", "Victor May", "Brice Rauby", "Zihan Wang", "Justine Gehring", "Antonio Orvieto", "Muawiz Chaudhary", "Eilif B. Muller", "Irina Rish", "Samira Ebrahimi Kahou", "Massimo Caccia"], "title": "GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities", "comment": "Version 2 of the dataset from: arXiv:2411.05830", "summary": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark."}
{"id": "2507.12415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.12415", "abs": "https://arxiv.org/abs/2507.12415", "authors": ["Xinyi He", "Qian Liu", "Mingzhe Du", "Lin Yan", "Zhijie Fan", "Yiming Huang", "Zejian Yuan", "Zejun Ma"], "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?", "comment": null, "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field."}
