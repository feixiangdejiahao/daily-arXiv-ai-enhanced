{"id": "2601.03364", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03364", "abs": "https://arxiv.org/abs/2601.03364", "authors": ["Nadia Damianova", "Santiago Berrezueta-Guzman"], "title": "The Anatomy of a Successful Student Scrum Team: Motivation, Personalities, and Academic Adaptation", "comment": "Preprint submitted to Elsevier", "summary": "Agile methods, and Scrum in particular, are widely taught in software engineering education; however, there is limited empirical evidence on how these practices function in long-running, student-led projects under academic and hybrid work constraints. This paper presents a year-long case study of an eight-person student development team tasked with designing and implementing a virtual reality game that simulates a university campus and provides program-related educational content. We analyze how the team adapted Scrum practices (sprint structure, roles, backlog management) to fit semester rhythms, exams, travel, and part-time availability, and how communication and coordination were maintained in a hybrid on-site/remote environment. Using qualitative observations and artifacts from Discord, Notion, and GitHub, as well as contribution metrics and a custom communication effectiveness index (score: 0.76/1.00), we evaluate three dimensions: (1) the effectiveness of collaboration tools, (2) the impact of hybrid work on communication and productivity, and (3) the feasibility of aligning Scrum with academic timelines. Our findings show that (i) lightweight, tool-mediated coordination enabled stable progress even during remote periods; (ii) one-week sprints and flexible ceremonies helped reconcile Scrum with academic obligations; and (iii) shared motivation, role clarity, and compatible working styles were as critical as process mechanics. We propose practical recommendations for instructors and student teams adopting agile methods in hybrid, project-based learning settings."}
{"id": "2601.03378", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03378", "abs": "https://arxiv.org/abs/2601.03378", "authors": ["Yu Huo", "Siyu Zhang", "Kun Zeng", "Yuquan Lu", "Cheng Yang", "Yifu Guo", "Xiaoying Tang"], "title": "RepoShapley: Shapley-Enhanced Context Filtering for Repository-Level Code Completion", "comment": "22pages, 9 figures, conference", "summary": "Repository-level code completion benefits from retrieval-augmented generation (RAG). However, controlling cross-file evidence is difficult because chunk utility is often interaction-dependent: some snippets help only when paired with complementary context, while others harm decoding when they conflict. We propose RepoShapley, a coalition-aware context filtering framework supervised by Shapley-style marginal contributions. Our module ChunkShapley constructs offline labels by (i) single-chunk probing with teacher-forced likelihood to estimate signed, weighted effects, (ii) a surrogate game that captures saturation and interference, (iii) exact Shapley computation for small retrieval sets, and (iv) bounded post-verification that selects a decoding-optimal coalition using the frozen generator. We distill verified $KEEP$ or $DROP$ decisions and retrieval triggering into a single model via discrete control tokens. Experiments across benchmarks and backbones show that RepoShapley improves completion quality while reducing harmful context and unnecessary retrieval. Code: https://anonymous.4open.science/r/a7f3c9."}
{"id": "2601.03430", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.03430", "abs": "https://arxiv.org/abs/2601.03430", "authors": ["Mohamed Ouf", "Shayan Noei", "Zeph Van Iterson", "Mariam Guizani", "Ying Zou"], "title": "An Empirical Analysis of Community and Coding Patterns in OSS4SG vs. Conventional OSS", "comment": null, "summary": "Open Source Software for Social Good (OSS4SG) projects aim to address critical societal challenges, such as healthcare access and community safety. Understanding the community dynamics and contributor patterns in these projects is essential for ensuring their sustainability and long-term impact. However, while extensive research has focused on conventional Open Source Software (OSS), little is known about how the mission-driven nature of OSS4SG influences its development practices. To address this gap, we conduct a large-scale empirical study of 1,039 GitHub repositories, comprising 422 OSS4SG and 617 conventional OSS projects, to compare community structure, contributor engagement, and coding practices. Our findings reveal that OSS4SG projects foster significantly more stable and \"sticky\" (63.4%) communities, whereas conventional OSS projects are more \"magnetic\" (75.4%), attracting a high turnover of contributors. OSS4SG projects also demonstrate consistent engagement throughout the year, while conventional OSS communities exhibit seasonal fluctuations. Additionally, OSS4SG projects rely heavily on core contributors for both code quality and issue resolution, while conventional OSS projects leverage casual contributors for issue resolution, with core contributors focusing primarily on code quality."}
{"id": "2601.03432", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03432", "abs": "https://arxiv.org/abs/2601.03432", "authors": ["Danny Brahman", "Mohammad Mahoor"], "title": "CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models", "comment": "Accepted at the International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics, 2025. Will be published at ACL anthology", "summary": "Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have demonstrated remarkable advancements in logical reasoning, there remains a significant gap in evaluating their code generation capabilities. Existing benchmark datasets fall short in pinpointing specific strengths and weaknesses, impeding targeted enhancements in models' reasoning abilities to synthesize code. To bridge this gap, our paper introduces an innovative, pedagogical benchmarking method that mirrors the evaluation processes encountered in academic programming courses. We introduce CodeEval, a multi-dimensional benchmark dataset designed to rigorously evaluate LLMs across 24 distinct aspects of Python programming. The dataset covers three proficiency levels - beginner, intermediate, and advanced - and includes both class-based and function-based problem types with detailed problem specifications and comprehensive test suites. To facilitate widespread adoption, we also developed RunCodeEval, an open-source execution framework that provides researchers with a ready-to-use evaluation pipeline for CodeEval. RunCodeEval handles test execution, context setup, and metrics generation, enabling researchers to quickly obtain detailed insights into model strengths and weaknesses across complexity levels, problem types, and programming categories. This combination enables targeted evaluation and guides improvements in LLMs' programming proficiencies."}
{"id": "2601.03708", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03708", "abs": "https://arxiv.org/abs/2601.03708", "authors": ["Qingyun Zou", "Jiahao Cui", "Nuo Chen", "Bingsheng He", "Weng-Fai Wong"], "title": "MHRC-Bench: A Multilingual Hardware Repository-Level Code Completion benchmark", "comment": null, "summary": "Large language models (LLMs) have achieved strong performance on code completion tasks in general-purpose programming languages. However, existing repository-level code completion benchmarks focus almost exclusively on software code and largely overlook hardware description languages. In this work, we present \\textbf{MHRC-Bench}, consisting of \\textbf{MHRC-Bench-Train} and \\textbf{MHRC-Bench-Eval}, the first benchmark designed for multilingual hardware code completion at the repository level. Our benchmark targets completion tasks and covers three major hardware design coding styles. Each completion target is annotated with code-structure-level and hardware-oriented semantic labels derived from concrete syntax tree analysis. We conduct a comprehensive evaluation of models on MHRC-Bench-Eval. Comprehensive evaluation results and analysis demonstrate the effectiveness of MHRC-Bench."}
{"id": "2601.03512", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03512", "abs": "https://arxiv.org/abs/2601.03512", "authors": ["Yuhan Wu", "Huan Zhang", "Wei Cheng", "Chen Shen", "Jingyue Yang", "Wei Hu"], "title": "Bootstrapping Code Translation with Weighted Multilanguage Exploration", "comment": null, "summary": "Code translation across multiple programming languages is essential yet challenging due to two vital obstacles: scarcity of parallel data paired with executable test oracles, and optimization imbalance when handling diverse language pairs. We propose BootTrans, a bootstrapping method that resolves both obstacles. Its key idea is to leverage the functional invariance and cross-lingual portability of test suites, adapting abundant pivot-language unit tests to serve as universal verification oracles for multilingual RL training. Our method introduces a dual-pool architecture with seed and exploration pools to progressively expand training data via execution-guided experience collection. Furthermore, we design a language-aware weighting mechanism that dynamically prioritizes harder translation directions based on relative performance across sibling languages, mitigating optimization imbalance. Extensive experiments on the HumanEval-X and TransCoder-Test benchmarks demonstrate substantial improvements over baseline LLMs across all translation directions, with ablations validating the effectiveness of both bootstrapping and weighting components."}
{"id": "2601.03768", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.03768", "abs": "https://arxiv.org/abs/2601.03768", "authors": ["Yichen Xu", "Martin Odersky"], "title": "Agentic Proof Automation: A Case Study", "comment": null, "summary": "Proof engineering is notoriously labor-intensive: proofs that are straightforward on paper often require lengthy scripts in theorem provers. Recent advances in large language models (LLMs) create new opportunities for proof automation: modern LLMs not only generate proof scripts, but also support agentic behavior, exploring codebases and iteratively refining their outputs against prover feedback. These advances enable an emerging scheme where LLM-based agents undertake most proof engineering under human guidance. Humans provide mathematical insight (definitions, theorems, proof strategies); agents handle the mechanical work of proof development. We call this scheme agentic proof automation. We present this scheme through a case study: mechanizing the semantic type soundness of a sophisticated formal system, System Capless, in Lean 4, comprising over 14,000 lines of code. Using off-the-shelf LLM agents with a single lightweight proof-checking tool, the agents completed 189 proof engineering tasks with an 87% success rate, only 16% requiring human intervention. The case study demonstrates that agents are capable proof engineers that substantially boost productivity, though they fall short in creative reasoning and still require human guidance in certain cases. We release an interactive explorer where readers can examine all agent interactions; the mechanization is open-sourced for experiments and extensions."}
{"id": "2601.03513", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03513", "abs": "https://arxiv.org/abs/2601.03513", "authors": ["Yi Wang", "Zhenting Huang", "Zhaohan Ding", "Ruoxue Liao", "Yuan Huang", "Xinzijian Liu", "Jiajun Xie", "Siheng Chen", "Linfeng Zhang"], "title": "Deploy-Master: Automating the Deployment of 50,000+ Agent-Ready Scientific Tools in One Day", "comment": null, "summary": "Open-source scientific software is abundant, yet most tools remain difficult to compile, configure, and reuse, sustaining a small-workshop mode of scientific computing. This deployment bottleneck limits reproducibility, large-scale evaluation, and the practical integration of scientific tools into modern AI-for-Science (AI4S) and agentic workflows.\n  We present Deploy-Master, a one-stop agentic workflow for large-scale tool discovery, build specification inference, execution-based validation, and publication. Guided by a taxonomy spanning 90+ scientific and engineering domains, our discovery stage starts from a recall-oriented pool of over 500,000 public repositories and progressively filters it to 52,550 executable tool candidates under license- and quality-aware criteria. Deploy-Master transforms heterogeneous open-source repositories into runnable, containerized capabilities grounded in execution rather than documentation claims. In a single day, we performed 52,550 build attempts and constructed reproducible runtime environments for 50,112 scientific tools. Each successful tool is validated by a minimal executable command and registered in SciencePedia for search and reuse, enabling direct human use and optional agent-based invocation.\n  Beyond delivering runnable tools, we report a deployment trace at the scale of 50,000 tools, characterizing throughput, cost profiles, failure surfaces, and specification uncertainty that become visible only at scale. These results explain why scientific software remains difficult to operationalize and motivate shared, observable execution substrates as a foundation for scalable AI4S and agentic science."}
{"id": "2601.03836", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.03836", "abs": "https://arxiv.org/abs/2601.03836", "authors": ["Ivan Perez", "Angel Herranz"], "title": "Logic Programming with Extensible Types", "comment": "In Proceedings ICLP 2025, arXiv:2601.00047", "summary": "Logic programming languages present clear advantages in terms of declarativeness and conciseness. However, the ideas of logic programming have been met with resistance in other programming communities, and have not generally been adopted by other paradigms and languages. This paper proposes a novel way to incorporate logic programming in an existing codebase in a typed functional programming language. Our approach integrates with the host language without sacrificing static typing, and leverages strengths of typed functional programming such as polymorphism and higher-order. We do so by combining three ideas. First, we use the extensible types technique to allow values of the host language to contain logic variables. Second, we implement a unification algorithm that works for any data structure that supports certain operations.Third, we introduce a domain-specific language to define and query predicates. We demonstrate our proposal via a series of examples, and provide aids to make the notation convenient for users, showing that the proposed approach is not just technically possible but also practical. Our ideas have been implemented in the language Haskell with very good results."}
{"id": "2601.03556", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03556", "abs": "https://arxiv.org/abs/2601.03556", "authors": ["Sabrina Haque", "Sarvesh Ingale", "Christoph Csallner"], "title": "Do Autonomous Agents Contribute Test Code? A Study of Tests in Agentic Pull Requests", "comment": null, "summary": "Testing is a critical practice for ensuring software correctness and long-term maintainability. As agentic coding tools increasingly submit pull requests (PRs), it becomes essential to understand how testing appears in these agent-driven workflows. Using the AIDev dataset, we present an empirical study of test inclusion in agentic pull requests. We examine how often tests are included, when they are introduced during the PR lifecycle and how test-containing PRs differ from non-test PRs in terms of size, turnaround time, and merge outcomes. Across agents, test-containing PRs are more common over time and tend to be larger and take longer to complete, while merge rates remain largely similar. We also observe variation across agents in both test adoption and the balance between test and production code within test PRs. Our findings provide a descriptive view of testing behavior in agentic pull requests and offer empirical grounding for future studies of autonomous software development."}
{"id": "2601.03854", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.03854", "abs": "https://arxiv.org/abs/2601.03854", "authors": ["Ziyi Yang", "George Pîrlea", "Ilya Sergey"], "title": "Inductive First-Order Formula Synthesis by ASP: A Case Study in Invariant Inference", "comment": "In Proceedings ICLP 2025, arXiv:2601.00047", "summary": "We present a framework for synthesising formulas in first-order logic (FOL) from examples, which unifies and advances state-of-the-art approaches for inference of transition system invariants. To do so, we study and categorise the existing methodologies, encoding techniques in their formula synthesis via answer set programming (ASP). Based on the derived categorisation, we propose orthogonal slices, a new technique for formula enumeration that partitions the search space into manageable chunks, enabling two approaches for incremental candidate pruning. Using a combination of existing techniques for first-order (FO) invariant synthesis and the orthogonal slices implemented in our framework FORCE, we significantly accelerate a state-of-the-art algorithm for distributed system invariant inference. We also show that our approach facilitates composition of different invariant inference frameworks, allowing for novel optimisations."}
{"id": "2601.03574", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03574", "abs": "https://arxiv.org/abs/2601.03574", "authors": ["Mamdouh Alenezi"], "title": "Auditable DevOps Automation via VSM and GQM", "comment": null, "summary": "DevOps automation can accelerate software delivery, yet many organizations still struggle to justify and prioritize automation work in terms of strategic project-management outcomes such as waste reduction, delivery predictability, cross-team coordination, and customer-facing quality. This paper presents \\textit{VSM--GQM--DevOps}, a unified, traceable framework that integrates (i) Value Stream Mapping (VSM) to visualize the end-to-end delivery system and quantify delays, rework, and handoffs, (ii) the Goal--Question--Metric (GQM) paradigm to translate stakeholder objectives into a minimal, decision-relevant measurement model (combining DORA with project and team outcomes), and (iii) maturity-aligned DevOps automation to remediate empirically observed bottlenecks through small, reversible interventions. The framework operationalizes traceability from observed waste to goal-aligned questions, metrics, and automation candidates, and provides a defensible prioritization approach that balances expected impact, confidence, and cost. We also define a multi-site, longitudinal mixed-method validation protocol that combines telemetry-based quasi-experimental analysis (interrupted time series and, where feasible, controlled rollouts) with qualitative triangulation from interviews and retrospectives. The expected contribution is a validated pathway and a set of practical instruments that enables organizations to select automation investments that demonstrably improve both delivery performance and project-management outcomes."}
{"id": "2601.03897", "categories": ["cs.PL", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.03897", "abs": "https://arxiv.org/abs/2601.03897", "authors": ["Ziad Ismaili Alaoui", "Detlef Plump"], "title": "Implementing Binary Search Trees in GP 2 (Extended Abstract)", "comment": "In Proceedings GCM 2025, arXiv:2601.03249", "summary": "We present an approach to implement binary search trees in the rule-based graph programming language GP 2. Our implementation uses GP 2's rooted graph transformation rules to be fast and supports insertion, deletion and query operations. We argue that the worst-case runtime for each of the operations is O(n) for a tree with n nodes. In addition, we expect that, on average, the operations run in time O(log(n)). Hence the implementation would match the time complexity of binary search trees implementations in imperative languages."}
{"id": "2601.03621", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03621", "abs": "https://arxiv.org/abs/2601.03621", "authors": ["Verya Monjezi", "Ashish Kumar", "Ashutosh Trivedi", "Gang Tan", "Saeid Tizpaz-Niari"], "title": "On the Robustness of Fairness Practices: A Causal Framework for Systematic Evaluation", "comment": null, "summary": "Machine learning (ML) algorithms are increasingly deployed to make critical decisions in socioeconomic applications such as finance, criminal justice, and autonomous driving. However, due to their data-driven and pattern-seeking nature, ML algorithms may develop decision logic that disproportionately distributes opportunities, benefits, resources, or information among different population groups, potentially harming marginalized communities. In response to such fairness concerns, the software engineering and ML communities have made significant efforts to establish the best practices for creating fair ML software. These include fairness interventions for training ML models, such as including sensitive features, selecting non-sensitive attributes, and applying bias mitigators. But how reliably can software professionals tasked with developing data-driven systems depend on these recommendations? And how well do these practices generalize in the presence of faulty labels, missing data, or distribution shifts? These questions form the core theme of this paper."}
{"id": "2601.04085", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04085", "abs": "https://arxiv.org/abs/2601.04085", "authors": ["Jingwen Xu", "Yiyang Lu", "Changze Lv", "Zisu Huang", "Zhengkang Guo", "Zhengyuan Wang", "Muzhao Tian", "Xuanjing Huang", "Xiaoqing Zheng"], "title": "CSSG: Measuring Code Similarity with Semantic Graphs", "comment": null, "summary": "Existing code similarity metrics, such as BLEU, CodeBLEU, and TSED, largely rely on surface-level string overlap or abstract syntax tree structures, and often fail to capture deeper semantic relationships between programs.We propose CSSG (Code Similarity using Semantic Graphs), a novel metric that leverages program dependence graphs to explicitly model control dependencies and variable interactions, providing a semantics-aware representation of code.Experiments on the CodeContests+ dataset show that CSSG consistently outperforms existing metrics in distinguishing more similar code from less similar code under both monolingual and cross-lingual settings, demonstrating that dependency-aware graph representations offer a more effective alternative to surface-level or syntax-based similarity measures."}
{"id": "2601.03640", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.03640", "abs": "https://arxiv.org/abs/2601.03640", "authors": ["Mohd Ariful Haque", "Kishor Datta Gupta", "Mohammad Ashiqur Rahman", "Roy George"], "title": "Verbatim Data Transcription Failures in LLM Code Generation: A State-Tracking Stress Test", "comment": null, "summary": "Many real-world software tasks require exact transcription of provided data into code, such as cryptographic constants, protocol test vectors, allowlists, and calibration tables. These tasks are operationally sensitive because small omissions or alterations can remain silent while producing syntactically valid programs. This paper introduces a deliberately minimal transcription-to-code benchmark to isolate this reliability concern in LLM-based code generation. Given a list of high-precision decimal constants, a model must generate Python code that embeds the constants verbatim and performs a simple aggregate computation. We describe the prompting variants, evaluation protocol based on exact-string inclusion, and analysis framework used to characterize state-tracking and long-horizon generation failures. The benchmark is intended as a compact stress test that complements existing code-generation evaluations by focusing on data integrity rather than algorithmic reasoning."}
{"id": "2601.03731", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03731", "abs": "https://arxiv.org/abs/2601.03731", "authors": ["Jia Li", "Yuxin Su", "Michael R. Lyu"], "title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level", "comment": null, "summary": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering."}
{"id": "2601.03780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03780", "abs": "https://arxiv.org/abs/2601.03780", "authors": ["Md Ahasanuzzaman", "Bram Adams", "Emad Fallahzadeh", "Gustavo A. Oliva", "Ahmed E. Hassan"], "title": "Assessing and Improving the Representativeness of Code Generation Benchmarks Using Knowledge Units (KUs) of Programming Languages -- An Empirical Study", "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4, Claude and LLaMA have shown impressive performance in code generation, typically evaluated using benchmarks (e.g., HumanEval). However, effective code generation requires models to understand and apply a wide range of language concepts. If the concepts exercised in benchmarks are not representative of those used in real-world projects, evaluations may yield incomplete. Despite this concern, the representativeness of code concepts in benchmarks has not been systematically examined.\n  To address this gap, we present the first empirical study that analyzes the representativeness of code generation benchmarks through the lens of Knowledge Units (KUs) - cohesive sets of programming language capabilities provided by language constructs and APIs. We analyze KU coverage in two widely used Python benchmarks, HumanEval and MBPP, and compare them with 30 real-world Python projects. Our results show that each benchmark covers only half of the identified 20 KUs, whereas projects exercise all KUs with relatively balanced distributions. In contrast, benchmark tasks exhibit highly skewed KU distributions.\n  To mitigate this misalignment, we propose a prompt-based LLM framework that synthesizes KU-based tasks to rebalance benchmark KU distributions and better align them with real-world usage. Using this framework, we generate 440 new tasks and augment existing benchmarks. The augmented benchmarks substantially improve KU coverage and achieve over a 60% improvement in distributional alignment. Evaluations of state-of-the-art LLMs on these augmented benchmarks reveal consistent and statistically significant performance drops (12.54-44.82%), indicating that existing benchmarks overestimate LLM performance due to their limited KU coverage. Our findings provide actionable guidance for building more realistic evaluations of LLM code-generation capabilities."}
{"id": "2601.03857", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03857", "abs": "https://arxiv.org/abs/2601.03857", "authors": ["Alessandra Parziale", "Gianmario Voria", "Valeria Pontillo", "Amleto Di Salle", "Patrizio Pelliccione", "Gemma Catolino", "Fabio Palomba"], "title": "Once Upon a Team: Investigating Bias in LLM-Driven Software Team Composition and Task Allocation", "comment": null, "summary": "LLMs are increasingly used to boost productivity and support software engineering tasks. However, when applied to socially sensitive decisions such as team composition and task allocation, they raise concerns of fairness. Prior studies have revealed that LLMs may reproduce stereotypes; however, these analyses remain exploratory and examine sensitive attributes in isolation. This study investigates whether LLMs exhibit bias in team composition and task assignment by analyzing the combined effects of candidates' country and pronouns. Using three LLMs and 3,000 simulated decisions, we find systematic disparities: demographic attributes significantly shaped both selection likelihood and task allocation, even when accounting for expertise-related factors. Task distributions further reflected stereotypes, with technical and leadership roles unevenly assigned across groups. Our findings indicate that LLMs exacerbate demographic inequities in software engineering contexts, underscoring the need for fairness-aware assessment."}
{"id": "2601.03878", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03878", "abs": "https://arxiv.org/abs/2601.03878", "authors": ["Giovanni Rosa", "David Moreno-Lumbreras", "Gregorio Robles", "Jesús M. González-Barahona"], "title": "Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design", "comment": "This paper is a Stage 1 Registered Report. The study protocol and analysis plan were peer reviewed and accepted at SANER 2026 with a Continuity Acceptance (CA) score for Stage 2", "summary": "Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation."}
{"id": "2601.03988", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03988", "abs": "https://arxiv.org/abs/2601.03988", "authors": ["Nicolas Lacroix", "Mireille Blay-Fornarino", "Sébastien Mosser", "Frederic Precioso"], "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures", "comment": "SANER 2026 Registered Report", "summary": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.\n  Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.\n  Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies."}
{"id": "2601.04010", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.04010", "abs": "https://arxiv.org/abs/2601.04010", "authors": ["Yannick Landeck", "Dian Balta", "Martin Wimmer", "Christian Knierim"], "title": "An Ontology-Based Approach to Security Risk Identification of Container Deployments in OT Contexts", "comment": "Accepted for publication on the Software Engineering in Practice (SEIP) track of the Internation Conference on Software Engineering (ICSE'26)", "summary": "In operational technology (OT) contexts, containerised applications often require elevated privileges to access low-level network interfaces or perform administrative tasks such as application monitoring. These privileges reduce the default isolation provided by containers and introduce significant security risks. Security risk identification for OT container deployments is challenged by hybrid IT/OT architectures, fragmented stakeholder knowledge, and continuous system changes. Existing approaches lack reproducibility, interpretability across contexts, and technical integration with deployment artefacts. We propose a model-based approach, implemented as the Container Security Risk Ontology (CSRO), which integrates five key domains: adversarial behaviour, contextual assumptions, attack scenarios, risk assessment rules, and container security artefacts. Our evaluation of CSRO in a case study demonstrates that the end-to-end formalisation of risk calculation, from artefact to risk level, enables automated and reproducible risk identification. While CSRO currently focuses on technical, container-level treatment measures, its modular and flexible design provides a solid foundation for extending the approach to host-level and organisational risk factors."}
{"id": "2601.04124", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.04124", "abs": "https://arxiv.org/abs/2601.04124", "authors": ["Lloyd Montgomery", "Clara Lüders", "Christian Rahe", "Walid Maalej"], "title": "Smells Depend on the Context: An Interview Study of Issue Tracking Problems and Smells in Practice", "comment": "30 pages, 1 figure, accepted at the ACM TOSEM journal", "summary": "Issue Tracking Systems (ITSs) enable software developers and managers to collect and resolve issues collaboratively. While researchers have extensively analysed ITS data to automate or assist specific activities such as issue assignments, duplicate detection, or priority prediction, developer studies on ITSs remain rare. Particularly, little is known about the challenges Software Engineering (SE) teams encounter in ITSs and when certain practices and workarounds (such as leaving issue fields like \"priority\" empty) are considered problematic. To fill this gap, we conducted an in-depth interview study with 26 experienced SE practitioners from different organisations and industries. We asked them about general problems encountered, as well as the relevance of 31 ITS smells (aka potentially problematic practices) discussed in the literature. By applying Thematic Analysis to the interview notes, we identified 14 common problems including issue findability, zombie issues, workflow bloat, and lack of workflow enforcement. Participants also stated that many of the ITS smells do not occur or are not problematic. Our results suggest that ITS problems and smells are highly dependent on context factors such as ITS configuration, workflow stage, and team size. We also discuss potential tooling solutions to configure, monitor, and visualise ITS smells to cope with these challenges."}
