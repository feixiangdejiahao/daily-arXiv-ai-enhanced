{"id": "2512.10748", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.10748", "abs": "https://arxiv.org/abs/2512.10748", "authors": ["Cass Alexandru", "Henning Urbat", "Thorsten Wi√ümann"], "title": "Intrinsically Correct Algorithms and Recursive Coalgebras", "comment": null, "summary": "Recursive coalgebras provide an elegant categorical tool for modelling recursive algorithms and analysing their termination and correctness. By considering coalgebras over categories of suitably indexed families, the correctness of the corresponding algorithms follows intrinsically just from the type of the computed maps. However, proving recursivity of the underlying coalgebras is non-trivial, and proofs are typically ad hoc. This layer of complexity impedes the formalization of coalgebraically defined recursive algorithms in proof assistants. We introduce a framework for constructing coalgebras which are intrinsically recursive in the sense that the type of the coalgebra guarantees recursivity from the outset. Our approach is based on the novel concept of a well-founded functor on a category of families indexed by a well-founded relation. We show as our main result that every coalgebra for a well-founded functor is recursive, and demonstrate that well-known techniques for proving recursivity and termination such as ranking functions are subsumed by this abstract setup. We present a number of case studies, including Quicksort, the Euclidian algorithm, and CYK parsing. Both the main theoretical result and selected case studies have been formalized in Cubical Agda."}
{"id": "2512.10861", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.10861", "abs": "https://arxiv.org/abs/2512.10861", "authors": ["Cade Lueker", "Andrew Fox", "Bor-Yuh Evan Chang"], "title": "Towards Cumulative Abstract Semantics via Handlers", "comment": null, "summary": "We consider the problem of modularizing control flow in a generic abstract interpretation framework. A generic abstract interpretation framework is not truly flexible if it does not allow interpreting with different path- and flow-sensitivities, by going forwards or backwards, and over- or under-approximately. Most interpreters inherently intertwine syntax and semantics, making the implementation antagonistic to modularity. Current approaches to modular designs require the use of complex data structures (e.g., monad transformers), providing modularity but often proving unwieldy (e.g., lifts). We observe that leveraging scoped effects within an interpreter facilitates the accumulation of semantic fragments against a fixed syntax. In this paper, we define cumulative abstract semantics, illustrating the potential for creating multiple dynamic evaluators and static analyses from one interpreter. This modularity is achieved by grouping effects into two categories: syntax elimination and domain-semantic introduction handlers. Our contribution shows the benefits of using effects as an instrument for designing a clean, elegant, and modular abstract interpretation framework."}
{"id": "2512.10079", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10079", "abs": "https://arxiv.org/abs/2512.10079", "authors": ["Federico Formica", "Mark Lawford", "Claudio Menghi"], "title": "Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives", "comment": null, "summary": "Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research."}
{"id": "2512.10173", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10173", "abs": "https://arxiv.org/abs/2512.10173", "authors": ["Mantas Baksys", "Stefan Zetzsche", "Olivier Bouissou", "Remi Delmas", "Soonho Kong"], "title": "ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis", "comment": null, "summary": "Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification."}
{"id": "2512.10218", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10218", "abs": "https://arxiv.org/abs/2512.10218", "authors": ["Thanosan Prathifkumar", "Noble Saji Mathews", "Meiyappan Nagappan"], "title": "Does SWE-Bench-Verified Test Agent Ability or Model Memory?", "comment": null, "summary": "SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind."}
{"id": "2512.10238", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10238", "abs": "https://arxiv.org/abs/2512.10238", "authors": ["Antu Saha"], "title": "Studying and Automating Issue Resolution for Software Quality", "comment": "3 pages", "summary": "Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems."}
{"id": "2512.10393", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10393", "abs": "https://arxiv.org/abs/2512.10393", "authors": ["Guoqiang Chen", "Lingyun Ying", "Ziyang Song", "Daguang Liu", "Qiang Wang", "Zhiqi Wang", "Li Hu", "Shaoyin Cheng", "Weiming Zhang", "Nenghai Yu"], "title": "Cross-modal Retrieval Models for Stripped Binary Analysis", "comment": null, "summary": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters."}
{"id": "2512.10415", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10415", "abs": "https://arxiv.org/abs/2512.10415", "authors": ["Devanshu Sahoo", "Vasudev Majhi", "Arjun Neekhra", "Yash Sinha", "Murari Mandal", "Dhruv Kumar"], "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation", "comment": "Under Review", "summary": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment."}
{"id": "2512.10452", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10452", "abs": "https://arxiv.org/abs/2512.10452", "authors": ["Yang Yang", "Li Kuang", "Jiakun Liu", "Zhongxin Liu", "Yingjie Xia", "David Lo"], "title": "UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval", "comment": "Accepted by the 48th IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "summary": "Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios."}
{"id": "2512.10493", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10493", "abs": "https://arxiv.org/abs/2512.10493", "authors": ["Binquan Zhang", "Li Zhang", "Haoyuan Zhang", "Fang Liu", "Song Wang", "Bo Shen", "An Fu", "Lin Shi"], "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild", "comment": null, "summary": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development."}
{"id": "2512.10618", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10618", "abs": "https://arxiv.org/abs/2512.10618", "authors": ["Georgia M. Kapitsaki", "Maria Papoutsoglou", "Christoph Treude", "Ioanna Theophilou"], "title": "Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories", "comment": "40 pages", "summary": "Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance."}
{"id": "2512.10713", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10713", "abs": "https://arxiv.org/abs/2512.10713", "authors": ["Itay Dreyfuss", "Antonio Abu Nassar", "Samuel Ackerman", "Axel Ben David", "Rami Katan", "Orna Raz", "Marcel Zalmanovici"], "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code", "comment": null, "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks."}
{"id": "2512.10799", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.10799", "abs": "https://arxiv.org/abs/2512.10799", "authors": ["Karolina Gorna", "Nicolas Iooss", "Yannick Seurin", "Rida Khatoun"], "title": "Zorya: Automated Concolic Execution of Single-Threaded Go Binaries", "comment": null, "summary": "Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks."}
