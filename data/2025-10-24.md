<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezzè*

Main category: cs.SE

TL;DR: E-Test leverages LLMs to discover and generate test cases for production scenarios not covered by existing test suites. It achieves superior results over current methods, enhancing software reliability and reducing testers' manual workload.


<details>
  <summary>Details</summary>
Motivation: Test suites often fail to fully cover all possible execution scenarios, especially as software evolves and new scenarios emerge in production. Identifying and creating new test cases to cover these undiscovered scenarios is highly labor-intensive, particularly for large and long-lived projects.

Method: The authors propose E-Test, an approach that uses Large Language Models (LLMs) to identify execution scenarios not yet covered by the current test suite, by analyzing scenarios gathered from production usage. E-Test then generates new test cases specifically targeting these uncovered scenarios to augment the test suite.

Result: E-Test was evaluated on 1,975 scenarios from popular open-source Java projects and the Defects4J dataset. It significantly outperformed previous state-of-the-art methods and vanilla LLMs, achieving an F1-score of 0.55 compared to 0.34 (state-of-the-art) and 0.39 (vanilla LLMs) in retrieving not-yet-tested scenarios.

Conclusion: E-Test efficiently augments test suites by automatically targeting and testing execution scenarios not covered by existing tests, outperforming current approaches and reducing manual effort required for comprehensive testing.

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [2] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: The paper introduces an AI task for generating natural language documentation from spreadsheet operations, evaluates five LLMs on a new benchmark, and finds that they can accurately explain spreadsheet code, paving the way for better collaboration and knowledge management, albeit with some remaining challenges.


<details>
  <summary>Details</summary>
Motivation: Spreadsheets are widely used by knowledge workers, but their lack of systematic documentation impedes automation, knowledge transfer, and collaboration. This creates a risk of losing institutional knowledge, motivating the need for automated and comprehensible spreadsheet documentation methods.

Method: The paper defines Spreadsheet Operations Documentation (SOD) as an AI task that generates human-readable explanations for spreadsheet operations. It introduces a new benchmark dataset of 111 spreadsheet code snippets paired with natural language summaries. The study evaluates five large language models (LLMs)—GPT-4o, GPT-4o-mini, LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B—using BLEU, GLEU, ROUGE-L, and METEOR for performance assessment.

Result: The results indicate that LLMs can effectively generate accurate and useful documentation for spreadsheet operations. However, the study also highlights certain challenges that need further attention for widespread adoption and robust performance.

Conclusion: AI-driven natural language documentation for spreadsheets is feasible and has potential to improve reproducibility, maintainability, and collaborative workflows, although additional work is needed to tackle existing challenges.

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [3] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: KGACG is a new multi-agent framework designed to automate the generation of complex, large-scale application-level software. It uses cooperative agents for planning, coding, and testing, demonstrating its effectiveness with a Java game, and aims to resolve the organizational and process maintenance issues seen in previous LLM-driven code generation.


<details>
  <summary>Details</summary>
Motivation: Although Large Language Models have improved code generation, creating large, complex application-level software still faces major hurdles, such as maintaining organizational code structure and managing the generation process.

Method: The authors propose KGACG, a Knowledge-Guided Application-Level Code Generation framework, involving three agents (Code Organization & Planning Agent, Coding Agent, Testing Agent) working collaboratively with a feedback mechanism to transform software requirements and architecture into executable code.

Result: They showcase the framework’s collaborative process using a Java Tank Battle game as a case study, highlighting how agents interact and address development challenges.

Conclusion: KGACG represents progress toward fully automated application-level code generation, especially for large projects, by improving code organization, planning, and maintainability through agent collaboration and knowledge guidance.

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [4] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: A novel synthetic bug generation method produces more realistic bugs by instructing agents to add features that accidentally break tests. This yields better SWE agent models with less training data, setting new performance benchmarks.


<details>
  <summary>Details</summary>
Motivation: Training large language models as software engineering agents requires high-quality, realistic bugs in datasets, but previous synthetic bug generation methods often fail to accurately simulate real-world development mistakes.

Method: The authors propose a new synthetic bug generation method where SWE Agents are instructed to add new features that may unintentionally break tests, resulting in more realistic, diverse, and challenging bugs that mirror human editing patterns.

Result: Qualitative analysis confirms that generated bugs align more closely with human-authored errors. Experiments show that these bugs yield superior training efficiency, enabling better model performance with fewer training samples. Training with this dataset helped produce state-of-the-art results on SWE-bench Verified benchmarks for both FrogBoss (32B model) with a pass@1 of 54.6% and FrogMini (14B model) with pass@1 of 45.3%.

Conclusion: The proposed bug generation approach creates more realistic and challenging bugs, leading to more efficient language model training and state-of-the-art results on SWE-bench Verified.

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [5] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel Böhme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: This paper shows that the order of mutator application in greybox fuzzers matters. Introducing MuoFuzz, a fuzzer that learns effective mutator sequences, the authors demonstrate superior bug finding and code coverage compared to existing tools.


<details>
  <summary>Details</summary>
Motivation: Greybox fuzzers typically apply random mutators in random order, but the order may affect fuzzing effectiveness. The paper seeks to investigate whether choosing mutator sequences more intelligently improves results.

Method: Fit a linear model to analyze interaction effects between mutator pairs; develop MuoFuzz, a greybox fuzzer that learns optimal mutator sequences via conditional probability and random walk sampling.

Result: MuoFuzz achieves the highest code coverage on FuzzBench and MAGMA benchmarks, discovering four bugs missed by AFL++ and one missed by both AFL++ and MOPT.

Conclusion: Strategic selection and learning of mutator sequences can greatly enhance the performance of greybox fuzzers over conventional random or isolated mutator selection approaches.

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [6] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: This paper introduces FAIGMOE, a four-phase framework to guide midsize and enterprise organizations in adopting Generative AI. FAIGMOE addresses unique challenges and GenAI-specific requirements, offering practical protocols and templates, but its effectiveness awaits empirical validation.


<details>
  <summary>Details</summary>
Motivation: Existing technology adoption frameworks lack specificity for successful GenAI implementation in midsize and enterprise organizations. Distinct challenges—such as resource constraints for midsize organizations and organizational complexity for enterprises—necessitate a tailored approach to GenAI adoption.

Method: The paper introduces FAIGMOE, a new conceptual framework specifically designed for the adoption and integration of GenAI in midsize and enterprise organizations. FAIGMOE synthesizes technology adoption theories, change management, and innovation diffusion into four implementation phases. It also integrates GenAI-specific concerns like prompt engineering and hallucination management.

Result: The FAIGMOE framework offers scalable, actionable protocols for GenAI adoption, including readiness assessment, strategic alignment, risk governance, technical architecture, and change management. It provides detailed implementation guidance, assessment instruments, and governance templates, but requires empirical validation in future research.

Conclusion: FAIGMOE represents the first comprehensive conceptual framework tailored for GenAI adoption in midsize and enterprise organizations. It addresses previously unmet needs, filling a critical gap in existing literature and offering practitioners structured support for GenAI integration.

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [7] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: Switching from Bazel (fast, high-resource artifact-based build tool) to Go Build (slower, simpler) in Kubernetes and other projects leads to longer builds and higher resource costs, despite improved maintainability. The study warns that abandoning artifact-based tools can bring notable performance penalties.


<details>
  <summary>Details</summary>
Motivation: Developers frequently use build systems, so their performance is critical. Although modern artifact-based build tools speed up builds, teams sometimes revert to simpler, more maintainable alternatives, but the consequences of these downgrades are not well understood.

Method: The paper presents a case study of the Kubernetes project, specifically focusing on its downgrade from Bazel (artifact-based) to Go Build (language-specific). The authors analyzed full and incremental build times, memory usage, and CPU load during the transition period. They also replicated the study on four other projects that downgraded from Bazel, to check if findings generalize.

Result: Bazel provides faster build times compared to Go Build, but it requires significantly more memory and higher CPU loads, especially under certain parallelism settings. Downgrading to Go Build leads to increased build times and potentially higher CI resource costs. Replication in other projects confirms Bazel's higher memory consumption but suggests reduced build time penalties over time.

Conclusion: While downgrading from artifact-based build tools like Bazel may improve maintainability, it often results in significant performance costs for large projects, particularly in terms of build time, memory usage, and CI resource expenses. Stakeholders should carefully consider these trade-offs when choosing build systems.

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [8] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: The paper describes a model-driven process to migrate legacy PL/SQL code in Oracle Forms to multi-tier Java applications. Using KDM models and a TDD-like transformation approach with multiple validations, the process is demonstrated to be effective and well-structured for legacy system modernization.


<details>
  <summary>Details</summary>
Motivation: Many enterprises use legacy RAD applications like Oracle Forms, and there is a strong need to migrate their monolithic PL/SQL code to modern, multi-tier Java architectures to leverage recent technologies and practices.

Method: The paper proposes a model-driven re-engineering process for migrating legacy PL/SQL code to Java, using KDM (Knowledge-Discovery Metamodel) models for code representation. It also integrates a TDD-like method to incrementally develop model transformations and employs three types of validations to ensure the quality of the converted code.

Result: The paper presents a detailed implementation and validation of the proposed migration process, demonstrating its effectiveness for converting legacy code and discussing challenges encountered during the application of model-driven engineering techniques.

Conclusion: Model-driven re-engineering, supported by incremental transformation development and thorough code validation, is a feasible and effective approach for migrating legacy RAD applications. The described process offers a structured way to transition from PL/SQL to Java, facilitating modernization efforts.

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>


### [9] [Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents](https://arxiv.org/abs/2510.20211)
*Zhenning Yang,Hui Guan,Victor Nicolet,Brandon Paulsen,Joey Dodds,Daniel Kroening,Ang Chen*

Main category: cs.SE

TL;DR: IaC frameworks often lose track of manual or external infrastructure changes, leading to errors and drift. NSync is an automated system that monitors cloud APIs to detect these changes and updates the IaC config accordingly, using LLMs and specialized tools. Experiments show NSync is more accurate and efficient than previous solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem of infrastructure drift when Infrastructure-as-Code (IaC) frameworks are used alongside traditional cloud management interfaces like consoles, CLIs, or SDKs. Standard tools can't keep track of changes made outside the IaC configuration, leading to inconsistencies, errors, and outdated configurations.

Method: The paper introduces NSync, an automated system that monitors cloud API calls to detect out-of-band changes that cause drift. It uses LLMs to infer intent from API logs and synthesizes targeted updates to the IaC configuration, using a self-improving knowledge base and a pipeline for realistic drift injection and evaluation.

Result: NSync was tested on five real-world Terraform projects and 372 drift scenarios. It significantly improved reconciliation accuracy (pass@3 metric rose from 0.71 to 0.97) and was 1.47 times more token efficient compared to the baseline.

Conclusion: NSync effectively reconciles IaC configurations with real-world infrastructure by tracking API-level changes and using automated reasoning for IaC updates, mitigating infrastructure drift and outperforming existing methods in both accuracy and efficiency.

Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally,
cloud consoles, command-line interfaces (CLI), and SDKs are the tools of
choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have
quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the
infrastructure in a "source-of-truth" configuration. They are capable of
automatically carrying out modifications to the cloud -- deploying, updating,
or destroying resources -- to bring the actual infrastructure into alignment
with the IaC configuration. However, when IaC is used alongside consoles, CLIs,
or SDKs, it loses visibility into external changes, causing infrastructure
drift, where the configuration becomes outdated, and later IaC operations may
undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates
out-of-band changes back into the IaC program. Our key insight is that
infrastructure changes eventually all occur via cloud API invocations -- the
lowest layer for cloud management operations. NSync gleans insights from API
traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update
the IaC configuration to capture the changes). It employs an agentic
architecture that leverages LLMs to infer high-level intents from noisy API
sequences, synthesize targeted IaC updates using specialized tools, and
continually improve through a self-evolving knowledge base of past
reconciliations. We further introduce a novel evaluation pipeline for injecting
realistic drifts into cloud infrastructure and assessing reconciliation
performance. Experiments across five real-world Terraform projects and 372
drift scenarios show that NSync outperforms the baseline both in terms of
accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$
improvement).

</details>


### [10] [Classport: Designing Runtime Dependency Introspection for Java](https://arxiv.org/abs/2510.20340)
*Serena Cofano,Daniel Williams,Aman Sharma,Martin Monperrus*

Main category: cs.SE

TL;DR: Java lacks runtime support for tracking dependencies, which is crucial for software security. The authors introduce Classport, a system that embeds dependency data in Java class files for runtime access, and show its effectiveness across real-world projects. This innovation could improve runtime integrity and supply chain security.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the lack of support in Java for runtime introspection of dependencies, which is essential for Software Supply Chain security. Being able to observe dependencies during program execution can help detect integrity issues and mitigate security risks.

Method: The proposed solution is Classport, a system that embeds dependency information directly into Java class files, allowing programs to retrieve and observe their dependencies at runtime.

Result: Classport was evaluated on six real-world projects, demonstrating its capability to successfully identify dependencies during program execution.

Conclusion: Classport enables runtime dependency introspection in Java, facilitating new approaches for runtime integrity checking and enhancing Software Supply Chain security.

Abstract: Runtime introspection of dependencies, i.e., the ability to observe which
dependencies are currently used during program execution, is fundamental for
Software Supply Chain security. Yet, Java has no support for it. We solve this
problem with Classport, a system that embeds dependency information into Java
class files, enabling the retrieval of dependency information at runtime. We
evaluate Classport on six real-world projects, demonstrating the feasibility in
identifying dependencies at runtime. Runtime dependency introspection with
Classport opens important avenues for runtime integrity checking.

</details>


### [11] [Symmetry in Software Platforms as an Architectural Principle](https://arxiv.org/abs/2510.20389)
*Bjorn Remseth*

Main category: cs.SE

TL;DR: Robustness in software platform architectures results from enforcing regular structures that remain stable under certain transformations, referred to as symmetries.


<details>
  <summary>Details</summary>
Motivation: Software platforms need architectural robustness to maintain consistent behavior despite changes or transformations. The paper is motivated by the desire to understand how such robustness can be systematically achieved.

Method: The paper investigates software platforms by framing their stable behaviors as invariance under certain transformations, defined as symmetries. The approach is conceptual and draws on structural regularity principles.

Result: It finds that architectural robustness in software platforms can emerge when structural regularities, preserved by symmetries, are enforced.

Conclusion: Imposing structure-preserving symmetries is key to achieving robust architectures in software platforms, making their interfaces and behaviors stable.

Abstract: Software platforms often act as structure preserving systems. They provide
consistent interfaces and behaviors that remain stable under specific
transformations that we denote as symmetries. This paper explores the idea that
architectural robustness emerges from enforcing such structural regularities

</details>


### [12] [FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards](https://arxiv.org/abs/2510.20403)
*Santiago Gil,Ecem E. Baş,Christian D. Jensen,Sebastian Engelsgaard,Giuseppe Abbiati,Cláudio Gomes*

Main category: cs.SE

TL;DR: The paper presents a secure, IP-protected distributed co-simulation method based on UniFMU. By client-initiated connections and hosting models on trusted platforms, it increases cybersecurity. Tested in various network settings, it demonstrates good protection and performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Ensuring intellectual property (IP) protection in distributed co-simulation is crucial, but current systems lack secure guidelines, especially against hacking attacks.

Method: An approach is proposed for distributed co-simulation utilizing UniFMU with enhanced cybersecurity and IP protection. The connection is client-initiated, and models/binaries reside on trusted platforms. Functionality is demonstrated via two co-simulation demos across four network settings.

Result: The paper analyzes the trade-offs between IP-protected distribution and performance in different network settings, showing the effectiveness of their approach in maintaining security and efficiency.

Conclusion: The approach successfully strengthens IP and cybersecurity for distributed co-simulation, balancing security concerns without substantially sacrificing performance.

Abstract: Distributed co-simulation plays a key role in enabling collaborative modeling
and simulation by different stakeholders while protecting their Intellectual
Property (IP). Although IP protection is provided implicitly by co-simulation,
there is no consensus in the guidelines to conduct distributed co-simulation of
continuous-time or hybrid systems with no exposure to potential hacking
attacks. We propose an approach for distributed co-simulation on top of UniFMU
with enhanced cybersecurity and IP protection mechanisms, ensuring that the
connection is initiated by the client and the models and binaries live on
trusted platforms. We showcase the functionality of this approach using two
co-simulation demos in four different network settings and analyze the
trade-off between IP-protected distribution and performance efficiency in these
settings.

</details>


### [13] [Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia](https://arxiv.org/abs/2510.20514)
*Lea Salome Brugger,Xavier Denis,Peter Müller*

Main category: cs.SE

TL;DR: Deductive verification is powerful but not widely used. Through interviews and thematic analysis, the authors confirmed established challenges and discovered new ones, such as difficulty in proof maintenance and usability. They offer key recommendations to make deductive verification more accessible and practical for a broader audience.


<details>
  <summary>Details</summary>
Motivation: Deductive verification ensures systems behave as intended, but despite its effectiveness, it is not widely adopted in practice. The authors seek to understand what enables or blocks broader use of this technique.

Method: The authors conducted semi-structured interviews with 30 deductive verification practitioners from both industry and academia. They applied thematic analysis to systematically assess the challenges and opportunities revealed in these interviews.

Result: The study confirmed known challenges, such as the high expertise required for formal proofs. It also uncovered less-discussed obstacles, including issues with proof maintenance, limited control over automation, and usability problems. The analysis identified both enablers and barriers to adoption and led to recommendations for improving usability, automation, and integration.

Conclusion: To encourage wider adoption of deductive verification, stakeholders should address not only familiar difficulties but also underexplored barriers like maintenance and usability. The paper provides actionable recommendations for practitioners, tool developers, and researchers.

Abstract: Deductive verification is an effective method to ensure that a given system
exposes the intended behavior. In spite of its proven usefulness and
feasibility in selected projects, deductive verification is still not a
mainstream technique. To pave the way to widespread use, we present a study
investigating the factors enabling successful applications of deductive
verification and the underlying issues preventing broader adoption. We
conducted semi-structured interviews with 30 practitioners of verification from
both industry and academia and systematically analyzed the collected data
employing a thematic analysis approach. Beside empirically confirming familiar
challenges, e.g., the high level of expertise needed for conducting formal
proofs, our data reveal several underexplored obstacles, such as proof
maintenance, insufficient control over automation, and usability concerns. We
further use the results from our data analysis to extract enablers and barriers
for deductive verification and formulate concrete recommendations for
practitioners, tool builders, and researchers, including principles for
usability, automation, and integration with existing workflows.

</details>


### [14] [Large Language Models for Fault Localization: An Empirical Study](https://arxiv.org/abs/2510.20521)
*YingJian Xiao,RongQun Hu,WeiWei Gong,HongWei Li,AnQuan Jie*

Main category: cs.SE

TL;DR: LLMs' ability to localize code faults hinges on context and prompt strategy—bug reports help most, few-shot learning yields limited gains, and reasoning-heavy prompts only work with strong models. These findings inform practical ways to improve automated code debugging with LLMs.


<details>
  <summary>Details</summary>
Motivation: Although LLMs are highly effective at automated program repair, their success largely depends on the quality of upstream fault localization, which has not been comprehensively evaluated.

Method: The authors conduct a systematic empirical study of various LLMs (including both open-source and closed-source) on the task of statement-level code fault localization, using HumanEval-Java and Defects4J datasets. They test different prompting strategies, such as standard prompting, few-shot learning, and chain-of-reasoning, evaluating models based on accuracy, time efficiency, and economic cost.

Result: Including bug report context leads to significant performance improvements. Few-shot learning can help but shows diminishing returns, and the effectiveness of chain-of-thought reasoning depends on the model's reasoning abilities.

Conclusion: The study reveals key performance characteristics and trade-offs of different LLMs for fault localization, providing actionable insights for leveraging LLMs and optimizing prompting strategies to improve automated software debugging.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, particularly in automated program repair. However, the
effectiveness of such repairs is highly dependent on the performance of
upstream fault localization, for which comprehensive evaluations are currently
lacking. This paper presents a systematic empirical study on LLMs in the
statement-level code fault localization task. We evaluate representative
open-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source
models (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization
capabilities on the HumanEval-Java and Defects4J datasets. The study
investigates the impact of different prompting strategies--including standard
prompts, few-shot examples, and chain-of-reasoning--on model performance, with
a focus on analysis across accuracy, time efficiency, and economic cost
dimensions. Our experimental results show that incorporating bug report context
significantly enhances model performance. Few-shot learning shows potential for
improvement but exhibits noticeable diminishing marginal returns, while
chain-of-thought reasoning's effectiveness is highly contingent on the model's
inherent reasoning capabilities. This study not only highlights the performance
characteristics and trade-offs of different models in fault localization tasks,
but also offers valuable insights into the strengths of current LLMs and
strategies for improving fault localization effectiveness.

</details>


### [15] [A Soundness and Precision Benchmark for Java Debloating Tools](https://arxiv.org/abs/2510.20679)
*Jonas Klauke,Tom Ohlmer,Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Eric Bodden*

Main category: cs.SE

TL;DR: Deblometer, a Java micro-benchmark, reveals that popular debloating tools often remove necessary code elements, causing crashes or altered program behavior. Dynamic class loading and annotations are notably problematic. These tools require enhancement to achieve safer and more effective debloating.


<details>
  <summary>Details</summary>
Motivation: Modern software development heavily relies on third-party libraries, resulting in a large number of dependencies—many of which are unused and contribute to software bloat. Addressing this inefficiency is critical for improving performance, maintainability, and security.

Method: The authors developed Deblometer, a micro-benchmark composed of 59 carefully curated test cases that cover various Java language features. Each case specifies ground truth for necessary and bloated code elements (classes, methods, fields). Deblometer is used to systematically evaluate the soundness (preserving necessary code) and precision (removing unnecessary code) of debloating tools.

Result: All evaluated Java debloating tools (Deptrim, JShrink, ProGuard) removed some required program constructs, leading to semantic changes or execution failures. Dynamic class loading proved especially challenging. Deptrim was more conservative, retaining more bloated code, while ProGuard was more aggressive, risking removal of necessary constructs. JShrink exhibited poor soundness due to insufficient annotation handling, resulting in corrupted outputs.

Conclusion: Current debloating tools for Java fail to fully balance precision and soundness, often removing required constructs and causing instability. Significant improvements are needed to support stable and reliable debloated software.

Abstract: Modern software development reuses code by importing libraries as
dependencies. Software projects typically include an average of 36
dependencies, with 80% being transitive, meaning they are dependencies of
dependencies. Recent research indicates that only 24.9% of these dependencies
are required at runtime, and even within those, many program constructs remain
unused, adding unnecessary code to the project. This has led to the development
of debloating tools that remove unnecessary dependencies and program constructs
while balancing precision by eliminating unused constructs and soundness by
preserving all required constructs. To systematically evaluate this trade-off,
we developed Deblometer, a micro-benchmark consisting of 59 test cases designed
to assess support for various Java language features in debloating tools. Each
test case includes a manually curated ground truth specifying necessary and
bloated classes, methods, and fields, enabling precise measurement of soundness
and precision. Using Deblometer, we evaluated three popular Java debloating
tools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools
remove required program constructs, which results in changed semantics or
execution crashes. In particular, the dynamic class loading feature introduces
unsoundness in all evaluated tools. Our comparison shows that Deptrim retains
more bloated constructs, while ProGuard removes more required constructs.
JShrink's soundness is significantly affected by limited support for
annotations, which leads to corrupted debloated artifacts. These soundness
issues highlight the need to improve debloating tools to ensure stable and
reliable debloated software.

</details>


### [16] [Exploring Large Language Models for Access Control Policy Synthesis and Summarization](https://arxiv.org/abs/2510.20692)
*Adarsh Vatsa,Bethel Hall,William Eiers*

Main category: cs.SE

TL;DR: This paper shows that while LLMs struggle to generate correct access control policies on their own, especially non-reasoning models, they perform much better with reasoning tasks and in combination with symbolic methods for analyzing and summarizing existing policies.


<details>
  <summary>Details</summary>
Motivation: Access control policies in cloud systems are complex and error-prone, requiring manual writing and analysis. There is a growing need for automation and better tools to synthesize and understand these policies.

Method: The paper investigates the use of various Large Language Models (LLMs) for both access control policy synthesis and summarization. It evaluates LLM effectiveness in generating correct policies and introduces a novel semantic-based request summarization approach using LLMs.

Result: LLMs can generate syntactically correct policies, but suffer from permissiveness issues: only 45.8% equivalence for non-reasoning LLMs and 93.7% for reasoning LLMs. For policy analysis, LLMs combined with symbolic methods provide promising results in summarizing policy behavior.

Conclusion: LLMs are not fully reliable for automated policy generation due to permissiveness errors, but are useful when paired with symbolic techniques for policy analysis and summarization.

Abstract: Cloud computing is ubiquitous, with a growing number of services being hosted
on the cloud every day. Typical cloud compute systems allow administrators to
write policies implementing access control rules which specify how access to
private data is governed. These policies must be manually written, and due to
their complexity can often be error prone. Moreover, existing policies often
implement complex access control specifications and thus can be difficult to
precisely analyze in determining their behavior works exactly as intended.
Recently, Large Language Models (LLMs) have shown great success in automated
code synthesis and summarization. Given this success, they could potentially be
used for automatically generating access control policies or aid in
understanding existing policies. In this paper, we explore the effectiveness of
LLMs for access control policy synthesis and summarization. Specifically, we
first investigate diverse LLMs for access control policy synthesis, finding
that: although LLMs can effectively generate syntactically correct policies,
they have permissiveness issues, generating policies equivalent to the given
specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time
for reasoning LLMs. We then investigate how LLMs can be used to analyze
policies by introducing a novel semantic-based request summarization approach
which leverages LLMs to generate a precise characterization of the requests
allowed by a policy. Our results show that while there are significant hurdles
in leveraging LLMs for automated policy generation, LLMs show promising results
when combined with symbolic approaches in analyzing existing policies.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs](https://arxiv.org/abs/2510.19850)
*Mostapha Kalami Heris*

Main category: cs.PL

TL;DR: This paper proposes Prompt Decorators—a concise, modular way to control LLM reasoning and expression using structured tokens—demonstrating improved transparency, reproducibility, and standardized outputs compared to traditional prompt engineering.


<details>
  <summary>Details</summary>
Motivation: Users of Large Language Models (LLMs) currently lack consistent, interpretable means to control model reasoning and output style. Conventional prompt engineering is verbose and limits reproducibility, modularity, and interpretability.

Method: The paper introduces Prompt Decorators: a declarative, composable syntax using control tokens to govern LLM behavior (such as reasoning style, tone, and structure). The framework formalizes 20 decorators across two functional families, defines a unified syntax, scoping, and processing pipeline for predictable behavior composition.

Result: Prompt Decorators enable auditable, interpretable, and reusable prompt designs. Use cases illustrate improved reasoning transparency, reduced prompt complexity, and standardized behavior across various domains.

Conclusion: Prompt Decorators allow decoupling of task intent from execution behavior, promoting reproducibility, transparency, and standardized control over LLM outputs. They provide a scalable, declarative interface for designing AI interactions, with implications for interoperability and consistency.

Abstract: Large Language Models (LLMs) are central to reasoning, writing, and
decision-support workflows, yet users lack consistent control over how they
reason and express outputs. Conventional prompt engineering relies on verbose
natural-language instructions, limiting reproducibility, modularity, and
interpretability. This paper introduces Prompt Decorators, a declarative,
composable syntax that governs LLM behavior through compact control tokens such
as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems
Thinking"). Each decorator modifies a behavioral dimension, such as reasoning
style, structure, or tone, without changing task content. The framework
formalizes twenty core decorators organized into two functional families
(Cognitive & Generative and Expressive & Systemic), each further decomposed
into subcategories that govern reasoning, interaction, expression, and
session-control. It defines a unified syntax, scoping model, and deterministic
processing pipeline enabling predictable and auditable behavior composition. By
decoupling task intent from execution behavior, Prompt Decorators create a
reusable and interpretable interface for prompt design. Illustrative use cases
demonstrate improved reasoning transparency, reduced prompt complexity, and
standardized model behavior across domains. The paper concludes with
implications for interoperability, behavioral consistency, and the development
of declarative interfaces for scalable AI systems.

</details>


### [18] [A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification](https://arxiv.org/abs/2510.19853)
*Assaf Marron,David Harel*

Main category: cs.PL

TL;DR: This paper introduces the concept of an "algorithm realm": a document cataloging all background knowledge necessary for executing algorithms written in natural language or pseudocode. It discusses how the realm can be systematically created (partly automated by AI), aiding implementation, verification, and the assessment of faithful execution by agents.


<details>
  <summary>Details</summary>
Motivation: The motivation is to clarify what knowledge an executing agent needs to possess in order to reliably and independently execute algorithm specifications written in natural language or pseudocode.

Method: The paper presents a conceptual framework, introducing the idea of the "realm" of an algorithm specification. This involves cataloging necessary knowledge, such as language syntax, semantics, domain entities, relationships, and operational instructions. It further discusses how generating this realm can be partially automated using large language models and reuse of existing documentation.

Result: The paper proposes that the realm concept helps systematically characterize and document the background knowledge needed for correct execution of algorithm specifications. This approach facilitates algorithm implementation in various systems and shapes the process for mechanical verification. It also distinguishes execution faithfulness from correctness, raising methods for their assessment.

Conclusion: The realm concept provides a practical means to document, formalize, and verify the knowledge required for executing algorithm specifications, supporting both methodological implementations and faithfulness assessment independent of specific system implementations.

Abstract: An algorithm specification in natural language or pseudocode is expected to
be clear and explicit enough to enable mechanical execution. In this position
paper we contribute an initial characterization of the knowledge that an
executing agent, human or machine, should possess in order to be able to carry
out the instructions of a given algorithm specification as a stand-alone
entity, independent of any system implementation. We argue that, for that
algorithm specification, such prerequisite knowledge, whether unique or shared
with other specifications, can be summarized in a document of practical size.
We term this document the realm of the algorithm specification. The generation
of such a realm is itself a systematic analytical process, significant parts of
which can be automated with the help of large language models and the reuse of
existing documents. The algorithm-specification's realm would consist of
specification language syntax and semantics, domain knowledge restricted to the
referenced entities, inter-entity relationships, relevant underlying
cause-and-effect rules, and detailed instructions and means for carrying out
certain operations. Such characterization of the realm can contribute to
methodological implementation of the algorithm specification in diverse systems
and to its formalization for mechanical verification. The paper also touches
upon the question of assessing execution faithfulness, which is distinct from
correctness: in the absence of a reference interpretation of natural language
or pseudocode specification with a given vocabulary, how can we determine if an
observed agent's execution indeed complies with the input specification.

</details>


### [19] [Deconstructed Proto-Quipper: A Rational Reconstruction](https://arxiv.org/abs/2510.20018)
*Ryan Kavanagh,Chuta Sano,Brigitte Pientka*

Main category: cs.PL

TL;DR: Proto-Quipper-A simplifies the semantics of Proto-Quipper quantum languages, making them easier to reason about and formalize by introducing a normalizing, call-by-value language grounded in linear lambda calculus and standard logical relations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the complexity and mechanization challenges in the operational semantics of Proto-Quipper quantum programming languages, which are made difficult by their effectful nature and reliance on set-theoretic operations and name generation when handling quantum circuits.

Method: The paper introduces Proto-Quipper-A, a reconstructed language based on linear lambda calculus. It incorporates adjoint-logical foundations to integrate circuit languages with functional languages, aiming for a simpler and more tractable semantics. The paper also employs standard logical relations for proving normalization, as opposed to more complex existing methods.

Result: Proto-Quipper-A achieves a simple call-by-value reduction semantics and is proven to be normalizing using standard logical relations. This simplicity and the normalizing property demonstrate Proto-Quipper-A's tractability as a foundational language for the Proto-Quipper family.

Conclusion: Proto-Quipper-A provides a more accessible and formally manageable semantic foundation for Proto-Quipper languages, enabling easier reasoning and mechanization for quantum circuit programming via normalization and simpler semantic structures.

Abstract: The Proto-Quipper family of programming languages aims to provide a formal
foundation for the Quipper quantum programming language. Unfortunately,
Proto-Quipper languages have complex operational semantics: they are inherently
effectful, and they rely on set-theoretic operations and fresh name generation
to manipulate quantum circuits. This makes them difficult to reason about using
standard programming language techniques and, ultimately, to mechanize. We
introduce Proto-Quipper-A, a rational reconstruction of Proto-Quipper languages
for static circuit generation. It uses a linear $\lambda$-calculus to describe
quantum circuits with normal forms that closely correspond to box-and-wire
circuit diagrams. Adjoint-logical foundations integrate this circuit language
with a linear/non-linear functional language and let us reconstruct
Proto-Quipper's circuit programming abstractions using more primitive
adjoint-logical operations. Proto-Quipper-A enjoys a simple call-by-value
reduction semantics, and to illustrate its tractability as a foundation for
Proto-Quipper languages, we show that it is normalizing. We show how to use
standard logical relations to prove normalization of linear and substructural
systems, thereby avoiding the inherent complexity of existing linear logical
relations.

</details>


### [20] [Deciding not to Decide: Sound and Complete Effect Inference in the Presence of Higher-Rank Polymorphism](https://arxiv.org/abs/2510.20532)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: The paper introduces a sound and complete effect inference algorithm for advanced type-and-effect systems with subtyping and higher-rank polymorphism. By transforming effect constraints to logical formulae, the approach resolves scoping challenges and is both formally verified and practically implemented.


<details>
  <summary>Details</summary>
Motivation: Type-and-effect systems manage data and computational effects, but unlike traditional type systems, they lack widespread use due to complexity and limited inference algorithms. The challenge is to balance expressiveness, intuitiveness, and decidability in inference algorithms.

Method: The authors present a novel effect inference algorithm for a type-and-effect system featuring subtyping, higher-rank polymorphism, and intuitive set-like semantics for effects. They address the scoping issues of higher-rank polymorphism by delaying the solving of effect constraints and converting these constraints into propositional logic formulae.

Result: The algorithm is proven sound and complete relative to a declarative type-and-effect system. The results are fully formalized using the Rocq proof assistant and the algorithm is implemented in a realistic programming language.

Conclusion: The proposed algorithm effectively enables expressive, intuitive, and decidable effect inference for advanced type-and-effect systems, demonstrating practical feasibility and formal correctness.

Abstract: Type-and-effect systems help the programmer to organize data and
computational effects in a program. While for traditional type systems
expressive variants with sophisticated inference algorithms have been developed
and widely used in programming languages, type-and-effect systems did not yet
gain widespread adoption. One reason for this is that type-and-effect systems
are more complex and the existing inference algorithms make compromises between
expressiveness, intuitiveness, and decidability. In this work, we present an
effect inference algorithm for a type-and-effect system with subtyping,
expressive higher-rank polymorphism, and intuitive set-like semantics of
effects. In order to deal with scoping issues of higher-rank polymorphism, we
delay solving of effect constraints by transforming them into formulae of
propositional logic. We prove soundness and completeness of our algorithm with
respect to a declarative type-and-effect system. All the presented results have
been formalized in the Rocq proof assistant, and the algorithm has been
successfully implemented in a realistic programming language.

</details>


### [21] [Compiling the Mimosa programming language to RTOS tasks](https://arxiv.org/abs/2510.20547)
*Nikolaus Huber,Susanne Graf,Philipp Rümmer,Wang Yi*

Main category: cs.PL

TL;DR: This paper adapts a known compilation method (from Lustre) for the Mimosa language, which targets time-triggered embedded software, and demonstrates how such programs can run on real-time operating systems.


<details>
  <summary>Details</summary>
Motivation: To provide a reliable compilation strategy for Mimosa, a language for describing time-triggered embedded systems, making it suitable for practical implementation on real-time operating systems.

Method: Formal description of adapting the Lustre compilation scheme to Mimosa's semantics; mapping the coordination layer to RTOS primitives.

Result: The authors show that Mimosa programs can be compiled using the adapted Lustre scheme, allowing their execution on platforms supporting real-time operating system primitives.

Conclusion: The paper successfully adapts the Lustre compilation scheme for Mimosa, enabling its semantics to be implemented on real-time operating systems.

Abstract: This paper introduces a compilation scheme for programs written in the Mimosa
programming language, which builds upon the MIMOS model of computation. Mimosa
describes embedded systems software as a collection of time-triggered processes
which communicate through FIFO queues. We formally describe an adaptation of
the Lustre compilation scheme to the semantics of Mimosa and show how the
coordination layer can be mapped to real-time operating system primitives.

</details>


### [22] [SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe Code in Rust and Mixed-Language Applications](https://arxiv.org/abs/2510.20688)
*Oliver Braunsdorf,Tim Lange,Konrad Hohentanner,Julian Horsch,Johannes Kinder*

Main category: cs.PL

TL;DR: SafeFFI optimizes memory safety in Rust programs using FFI by reducing unnecessary checks and overhead, performing as well as or better than state-of-the-art systems while reliably catching memory errors.


<details>
  <summary>Details</summary>
Motivation: Modern Rust programs require 'unsafe' code to interface with C/C++ and for low-level implementations, but such code can compromise memory safety in primarily safe Rust applications. Existing sanitization methods for detecting memory errors apply checks even where the Rust type system guarantees safety, leading to unnecessary overhead.

Method: SafeFFI strategically places memory safety checks only at the interface between unsafe and safe code, leveraging Rust's type system for enforcement within safe code regions. This approach bypasses whole-program analysis and focuses instrumentation at FFI boundaries.

Result: SafeFFI significantly reduces the number of runtime sanitizer checks (by up to 98%), and imposes far less compile-time overhead (2.64x versus over 8.83x in previous systems), while still detecting all types of memory safety violations in Rust binaries.

Conclusion: SafeFFI presents a more efficient mechanism for ensuring memory safety in Rust, especially in applications using FFI, maintaining error detection capabilities with lower performance impact than existing solutions.

Abstract: Unsafe Rust code is necessary for interoperability with C/C++ libraries and
implementing low-level data structures, but it can cause memory safety
violations in otherwise memory-safe Rust programs. Sanitizers can catch such
memory errors at runtime, but introduce many unnecessary checks even for memory
accesses guaranteed safe by the Rust type system. We introduce SafeFFI, a
system for optimizing memory safety instrumentation in Rust binaries such that
checks occur at the boundary between unsafe and safe code, handing over the
enforcement of memory safety from the sanitizer to the Rust type system. Unlike
previous approaches, our design avoids expensive whole-program analysis and
adds much less compile-time overhead (2.64x compared to over 8.83x). On a
collection of popular Rust crates and known vulnerable Rust code, SafeFFI
achieves superior performance compared to state-of-the-art systems, reducing
sanitizer checks by up to 98%, while maintaining correctness and flagging all
spatial and temporal memory safety violations.

</details>
