<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca automates the mining of command specifications using LLMs and system-level observation, removing the need for manual specs and enabling improved security, reliability, and analysis for opaque command-line tools.


<details>
  <summary>Details</summary>
Motivation: Manual creation of partial specifications for opaque commands is tedious, error-prone, and restricts the adoption of advanced program analysis systems; automation is needed to scale these systems effectively.

Method: Caruca uses a large language model to parse user-facing documentation into structured invocation syntax, systematically explores invocations and environments, and extracts properties using system-call and filesystem-level interposition.

Result: Caruca accurately generated specifications for all but one out of 60 commands from GNU Coreutils, POSIX, and third-party utilities, proving its robustness and utility for existing specification-dependent systems.

Conclusion: Caruca successfully automates the generation of specifications for opaque commands, eliminating manual specification effort and enabling integration with state-of-the-art static analysis tools.

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>


### [2] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: Traditional software practices don't align well with the unpredictable nature of LLM agents. The paper introduces ArbiterOS, a new architecture to provide structured governance and reliability for autonomous LLM-based systems.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents are prone to brittleness and unpredictability in high-stakes scenarios due to misalignment between probabilistic model behavior and expectations set by traditional software engineering.

Method: The authors present a new agent engineering framework, ArbiterOS, grounded in governance-first principles to better align control and predictability in LLM-driven autonomous systems.

Result: A new formal architecture—ArbiterOS—is introduced as a solution to systematically govern and manage the operations of LLM agents for improved trustworthiness and reliability.

Conclusion: The paper proposes that the unreliability of current LLM-based agents is due to a fundamental paradigm mismatch and introduces ArbiterOS, a formal architecture based on a governance-first paradigm to address these issues.

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [3] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: Current AI coding assistants perform much worse on multi-turn coding tasks than single-turn tasks, with significantly lower correctness and security. The MT-Sec benchmark shows the need to evaluate and improve LLMs in realistic, iterative coding settings, not just easy one-step problems.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for AI coding assistants focus mostly on single-turn tasks and do not represent how coding is done iteratively in real-world situations. There is a lack of systematic evaluation of both correctness and security in multi-turn coding scenarios, which is needed to reflect actual development practices.

Method: The authors build MT-Sec, a new benchmark for multi-turn coding, by using a synthetic data pipeline that converts single-turn coding tasks into multi-turn interaction sequences, while keeping them semantically consistent and making it possible to use existing test suites. They evaluate 32 different LLMs (open and closed source), as well as three agent scaffoldings, on this benchmark and analyze their outputs for correctness and security in both full-program and code-diff generation settings.

Result: The study finds a consistent 20-27% decrease in 'correct and secure' outputs when shifting from single-turn to multi-turn scenarios, even for top models. Performance drops even further in multi-turn code-diff tasks, with more functionally incorrect and insecure code. Agent scaffoldings improve single-turn tasks but do not help as much in multi-turn settings.

Conclusion: Multi-turn coding settings remain challenging for current LLMs, with notable declines in correctness and security compared to single-turn benchmarks. There is a critical need for novel benchmarks like MT-Sec that better measure LLMs' practical performance in real-world, iterative coding. Improvements in model and evaluation design are needed for safer, more reliable AI-driven development tools.

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [4] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: The paper presents A11yn, a method for training LLMs to generate accessible web UIs. By penalizing WCAG violations, A11yn reduces accessibility issues in generated interfaces by 60% without compromising quality, demonstrating that LLMs can be aligned for inclusivity.


<details>
  <summary>Details</summary>
Motivation: Large language models can generate web interfaces from instructions, but these generated interfaces often inherit accessibility flaws from their training data. There is a need for methods that ensure LLM-generated web UIs comply with accessibility standards to serve users with diverse needs.

Method: The authors propose A11yn, a method that aligns LLMs to produce accessibility-compliant web UIs by optimizing a reward function that penalizes Web Content Accessibility Guidelines (WCAG) violations. Penalties are scaled according to the severity of each violation as determined by an accessibility testing engine. They support training with a new dataset, UIReq-6.8K (6,800 diverse web UI generation instructions), and evaluate results with RealUIReq-300 (300 real-world web UI requests).

Result: A11yn significantly reduces the Inaccessibility Rate by 60% compared to the base model, while maintaining the semantic and visual integrity of the generated UIs. It outperforms strong baseline models on standard benchmarks.

Conclusion: A11yn demonstrates that it is feasible and effective to systematically optimize LLMs toward generating accessibility-compliant web UIs. Aligning code generation for accessibility is achievable, providing more inclusive web experiences.

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [5] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: Spectral Signature defense for backdoor attacks on LLM code models is often less effective than believed. The paper shows that common configurations can be improved and introduces a proxy metric that better estimates performance, making such defenses more practical and reliable.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks are a major concern in the integration of Large Language Models (LLMs) into software development, particularly since these attacks can manipulate output via hidden triggers. Detecting such attacks—especially in code models—is difficult, and existing Spectral Signature defense methods have shown limited effectiveness.

Method: The paper revisits Spectral Signature-based defense methods for backdoor detection in code models. It systematically evaluates these methods under diverse attack scenarios and defense settings, and analyzes key factors influencing their performance. Additionally, the paper investigates alternative configurations and introduces a new proxy metric to estimate effectiveness.

Result: The study finds that the commonly used settings for Spectral Signature are often suboptimal when applied to code models. Through experimentation with different key factors, the authors identify a new proxy metric that more accurately reflects defense performance without requiring model retraining.

Conclusion: Spectral Signature defenses, as traditionally implemented, may not provide optimal backdoor detection for LLM code models. The newly proposed proxy metric offers a more practical and accurate way to evaluate defense performance, potentially improving real-world security applications.

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [6] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: Finding and patching recurring bugs in large codebases is hard and crucial for security. This paper introduces BugStone, which uses LLVM and LLMs to detect and automate fixing bugs that follow the same pattern. It proves highly effective and precise on real, challenging datasets, uncovering thousands of vulnerabilities that were previously missed.


<details>
  <summary>Details</summary>
Motivation: Debugging large codebases is time-consuming, especially when similar bugs recur in different code segments but are often missed after the first is fixed. Such recurring bugs can pose security risks, especially since bug reports may expose patterns for exploitation. Automating the discovery and patching of such Recurring Pattern Bugs (RPBs) is crucial for improving security and efficiency.

Method: The paper presents BugStone, an automated program analysis tool that uses LLVM (for static analysis) and a Large Language Model (LLM) to detect recurring bug patterns. By learning how one instance of a bug was fixed, BugStone identifies similar code patterns across a codebase (like the Linux kernel) where the same bug may appear.

Result: BugStone, starting from 135 unique RPBs, uncovered over 22,000 new potential issues in the Linux kernel, with manual checking confirming the validity of 246 out of 400 sample findings. Additionally, on a dataset of over 1.9k security bugs (from 23 recent papers), BugStone achieved 92.2% precision and 79.1% pairwise accuracy in finding and matching recurring bug patterns and fixes.

Conclusion: Recurring Pattern Bugs are widespread and dangerous. The BugStone system effectively leverages patched bug patterns and LLM-driven analysis to find and help patch similar bugs across large programs, significantly improving security coverage. The high precision and solid accuracy of BugStone demonstrate the efficacy of combining program analysis tools with LLMs for automated bug detection and repair.

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [7] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: The paper introduces NL2Scenic, an open dataset and evaluation framework for transforming natural language scenarios into Scenic code for autonomous driving simulation. It proposes robust metrics for benchmarking LLMs, finds that retrieval-augmented prompting boosts smaller models, and shows that mid-size open-source models are a practical alternative to proprietary solutions.


<details>
  <summary>Details</summary>
Motivation: Testing autonomous driving systems requires generating precise and reproducible simulation scenarios. While Scenic (a domain-specific language) facilitates such scenario creation for CARLA, converting natural language (NL) specifications to Scenic code using large language models (LLMs) faces challenges due to scarce training data, limited reproducibility, and inconsistent evaluation metrics.

Method: The paper presents NL2Scenic, which includes a curated open dataset of 146 NL/Scenic code pairs, a difficulty-stratified test set, an Example Retriever, and various prompting strategies (14 variants such as Zero-Shot, Few-Shot, Chain-of-Thought, Self-Play, and Mixture of Tools). The framework evaluates 13 LLMs (proprietary and open-source) using both text-based (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution-based metrics, as well as human expert assessments. A new combined metric, EDIT-COMP, is also proposed.

Result: The results highlight that the EDIT-SIM metric aligns best with human expert judgments. The newly proposed EDIT-COMP serves as a robust dataset-level proxy metric. Among the models, GPT-4o performs best, while Qwen2.5Coder-14B achieves about 88% of the expert score on local hardware, indicating strong performance. Smaller models benefit significantly from retrieval-augmented prompting, such as the Few-Shot with Example Retriever (FSER) method. Scaling model size reveals diminishing returns past mid-size, with Qwen2.5Coder outperforming CodeLlama at similar scales.

Conclusion: NL2Scenic and the EDIT-COMP metric provide a reproducible and standardized benchmark for assessing Scenic code generation from natural language, supporting cost-effective, practical scenario programming for autonomous driving. Mid-size open-source models offer a promising balance of cost and performance, especially when combined with retrieval-augmented prompting strategies.

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>


### [8] [A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning](https://arxiv.org/abs/2510.14292)
*Haolin Pan,Hongbin Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: This paper proposes and evaluates an evolutionary framework that personalizes compiler optimization pass sequences using knowledge from large-scale analysis, achieving significant improvements over standard optimization levels like -Oz.


<details>
  <summary>Details</summary>
Motivation: Standard compiler optimization passes (like -O3, -Oz) treat all programs similarly, often failing to maximize performance for specific cases. Finding the best sequence of optimization passes per program is very hard (NP-hard), so there's a need for smarter automatic tuning.

Method: The paper proposes a Hybrid Knowledge-Guided Evolutionary Framework. This involves two phases: (1) Offline, where a knowledge base is constructed from analyzing pass behaviors, clustering passes, building synergy graphs, and evolving prototype sequences; (2) Online, where a specialized genetic algorithm leverages this knowledge for personalized compiler pass optimization using knowledge-infused genetic operators.

Result: On seven public datasets, the proposed framework produces an average 11.0% further reduction in LLVM IR instructions compared to the already strongly optimized opt -Oz baseline.

Conclusion: By combining large-scale offline compilation analysis with intelligent, knowledge-guided genetic algorithms, the framework effectively finds better, personalized compiler optimization pass sequences, outperforming standard approaches.

Abstract: Compiler pass auto-tuning is critical for enhancing software performance, yet
finding the optimal pass sequence for a specific program is an NP-hard problem.
Traditional, general-purpose optimization flags like -O3 and -Oz adopt a
one-size-fits-all approach, often failing to unlock a program's full
performance potential. To address this challenge, we propose a novel Hybrid,
Knowledge-Guided Evolutionary Framework. This framework intelligently guides
online, personalized optimization using knowledge extracted from a large-scale
offline analysis phase. During the offline stage, we construct a comprehensive
compilation knowledge base composed of four key components: (1) Pass Behavioral
Vectors to quantitatively capture the effectiveness of each optimization; (2)
Pass Groups derived from clustering these vectors based on behavior similarity;
(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a
library of Prototype Pass Sequences evolved for distinct program types. In the
online stage, a bespoke genetic algorithm leverages this rich knowledge base
through specially designed, knowledge-infused genetic operators. These
operators transform the search by performing semantically-aware recombination
and targeted, restorative mutations. On a suite of seven public datasets, our
framework achieves an average of 11.0% additional LLVM IR instruction reduction
over the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art
capability in discovering personalized, high-performance optimization
sequences.

</details>


### [9] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: The paper reveals that Time Limit Exceeded (TLE) errors in online programming have diverse causes, not just slow algorithms. The authors analyze real code to categorize these errors and demonstrate that Nettle, their automated tool, fixes TLEs much better than current language models, providing a strong new solution for learners and coders.


<details>
  <summary>Details</summary>
Motivation: Online programming learners and candidates struggle to fix Time Limit Exceeded (TLE) errors, a frequent but obscure feedback in platforms like Codeforces and LeetCode. Current support is insufficient, and the nature of TLE issues is misunderstood.

Method: The authors conducted a large-scale empirical study by manually analyzing 1,000 Codeforces submissions with TLE errors to classify root causes and user fixes. They designed and evaluated Nettle, an automated repair tool combined with a new evaluation framework (Nettle-Eval) to fix TLEs using LLMs, compiler, and test case feedback.

Result: The study identified multiple root causes of TLEs beyond inefficient algorithms, including infinite loops, poor data structures, and I/O bottlenecks. Nettle, the introduced repair tool, achieved a 98.5% fix rate for TLE errors, outperforming existing LLM baselines. All repairs were validated by Nettle-Eval and the platform's checker.

Conclusion: TLE errors stem from a range of issues beyond just algorithmic inefficiency. Automated tools like Nettle, leveraging LLMs and dynamic feedback, can effectively resolve these diverse TLE errors at a high success rate, suggesting new approaches for platform support.

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [10] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: PathFix is a novel APR method using execution path constraints and LLMs to generate more accurate patches, surpassing prior methods, particularly for complex code like loops and recursion.


<details>
  <summary>Details</summary>
Motivation: Existing automated program repair (APR) methods struggle with generating too many plausible patch candidates and overfitting to incomplete test cases, primarily due to the challenge of producing precise program specifications.

Method: PathFix is introduced as a new APR technique that leverages path-sensitive constraints from correct execution paths to guide patch generation. The process includes tracing faulty paths, deriving expected paths on the control flow graph, solving state constraints to generate patches, and validating their correctness. Additionally, a large language model (LLM) is integrated for improved repair performance and scalability.

Result: Experimental results show that PathFix outperforms existing APR methods, especially in effectively repairing complex program structures involving loops and recursion.

Conclusion: PathFix presents a more precise and efficient method for automated program repair by utilizing path-sensitive analysis and LLMs, reducing overfitting and improving patch quality over prior approaches.

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [11] [Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects](https://arxiv.org/abs/2510.14465)
*Adem Ait,Gwendal Jouneaux,Javier Luis Cánovas Izquierdo,Jordi Cabot*

Main category: cs.SE

TL;DR: This paper proposes a new Domain-Specific Language to define and automate governance in open-source software projects, improving management of diverse stakeholders, including both humans and AI agents.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the increasing diversity of software development stakeholders, both human and AI-powered, especially in open-source contexts where governance policies are often undefined or ambiguous, leading to collaboration and management challenges.

Method: The paper proposes the vision and foundational concepts for a Domain-Specific Language (DSL) specifically crafted to define and enforce governance policies in software systems with varied stakeholders, including AI agents.

Result: The DSL enables definition and automated enforcement of rich, adaptable governance policies to manage interactions between diverse stakeholders in OSS projects.

Conclusion: The introduction of this DSL sets the stage for more robust, adaptable, and automated governance models, enhancing effective collaboration in OSS projects involving a mix of human and AI contributors.

Abstract: The stakeholders involved in software development are becoming increasingly
diverse, with both human contributors from varied backgrounds and AI-powered
agents collaborating together in the process. This situation presents unique
governance challenges, particularly in Open-Source Software (OSS) projects,
where explicit policies are often lacking or unclear. This paper presents the
vision and foundational concepts for a novel Domain-Specific Language (DSL)
designed to define and enforce rich governance policies in systems involving
diverse stakeholders, including agents. This DSL offers a pathway towards more
robust, adaptable, and ultimately automated governance, paving the way for more
effective collaboration in software projects, especially OSS ones.

</details>


### [12] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: E2EDev is a new publicly available benchmark and pipeline for end-to-end software development tasks. It exposes the limitations of current frameworks and models, highlighting the ongoing need for better, more efficient solutions.


<details>
  <summary>Details</summary>
Motivation: There is a persistent challenge in effectively solving end-to-end software development (E2ESD) tasks, especially regarding creating cost-efficient and high-quality automated solutions. Current frameworks struggle to meet these requirements.

Method: E2EDev is proposed as a benchmark comprising detailed user requirements, multiple BDD test scenarios with Python step implementations, and a fully automated pipeline built on the Behave framework. It also integrates a Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA) to enhance data quality while reducing annotation effort.

Result: Analysis using E2EDev on various E2ESD frameworks and large language model backbones shows that existing solutions still struggle to effectively solve E2ESD tasks.

Conclusion: There is still a critical need for more effective and efficient E2ESD solutions, as current models and frameworks do not yet adequately address the complexity of these tasks. E2EDev is presented as a public resource to encourage progress.

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [13] [Software Testing Education and Industry Needs - Report from the ENACTEST EU Project](https://arxiv.org/abs/2510.14625)
*Mehrdad Saadatmand,Abbas Khan,Beatriz Marin,Ana C. R Paiva,Nele Van Asch,Graham Moran,Felix Cammaerts,Monique Snoeck,Alexandra Mendes*

Main category: cs.SE

TL;DR: This paper reveals significant gaps between what software testing education provides and what industry currently demands, especially concerning AI, security testing, and soft skills. Using interviews, focus groups, and a scoping review, the study emphasizes the urgent need to update both academic and industrial training to keep pace with industry advancements.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of software development requires testers to constantly update their skills and adapt to new methodologies, driving the need to assess current software testing competencies, educational gaps, and industry requirements.

Method: The study utilized two focus group sessions and interviews with professionals from various domains, along with a small-scale scoping literature review. A collaboratively designed and iteratively refined study instrument guided the investigation, and thematic qualitative analysis was applied to extract key findings.

Result: Key results include identification of competencies required in industry, recognition of knowledge gaps in academic education, and exposure of unmet needs such as AI testing, security testing, and soft skills. The study also sheds light on preferred professional training methods, the challenges experienced in industry training, evaluation of training quality, and approaches to knowledge transfer within companies.

Conclusion: There are persisting gaps between educational offerings and industry needs—particularly in areas relating to modern technologies and soft skills. Training methods and knowledge transfer approaches within companies are varied but face common challenges, underscoring the need for continuous evolution of both academic curricula and industrial training programs.

Abstract: The evolving landscape of software development demands that software testers
continuously adapt to new tools, practices, and acquire new skills. This study
investigates software testing competency needs in industry, identifies
knowledge gaps in current testing education, and highlights competencies and
gaps not addressed in academic literature. This is done by conducting two focus
group sessions and interviews with professionals across diverse domains,
including railway industry, healthcare, and software consulting and performing
a curated small-scale scoping review. The study instrument, co-designed by
members of the ENACTEST project consortium, was developed collaboratively and
refined through multiple iterations to ensure comprehensive coverage of
industry needs and educational gaps. In particular, by performing a thematic
qualitative analysis, we report our findings and observations regarding:
professional training methods, challenges in offering training in industry,
different ways of evaluating the quality of training, identified knowledge gaps
with respect to academic education and industry needs, future needs and trends
in testing education, and knowledge transfer methods within companies. Finally,
the scoping review results confirm knowledge gaps in areas such as AI testing,
security testing and soft skills.

</details>


### [14] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: ATGen is an adversarial reinforcement learning-based framework that trains test generators to uncover increasingly challenging bugs in LLM-generated code, outperforming previous methods and boosting code reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing test case generation methods for LLM-generated code, which rely on static datasets and thus can only identify bugs of fixed or limited complexity. There is a need to develop a technique capable of uncovering increasingly complex bugs as LLMs become more sophisticated at code generation.

Method: The proposed method introduces ATGen, a test case generator that uses adversarial reinforcement learning. In this framework, a test generator and an adversarial code generator are trained together. The adversarial code generator creates increasingly difficult bugs to evade the test generator, thereby establishing a dynamic and progressively harder curriculum for test case generation. The test generator is trained with reinforcement learning to maximize both the accuracy of its outputs and its success in attacking the code generated by the adversarial generator.

Result: Experiments show that ATGen significantly outperforms state-of-the-art test generation baselines. It also proves to be a superior filter for selecting the best outputs in code generation (Best-of-N inference) and provides higher-quality rewards for training code generation models, demonstrating improvement in reliability and robustness of LLM-generated code.

Conclusion: ATGen overcomes the fixed-difficulty ceiling of static dataset-dependent approaches, establishing a new and effective dynamic paradigm for generating high-quality test cases and improving code reliability in LLM outputs.

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [15] [Requirement Identification for Traffic Simulations in Driving Simulators](https://arxiv.org/abs/2510.14653)
*Sven Tarlowski,Lutz Eckstein*

Main category: cs.SE

TL;DR: A structured method systematically ties study objectives to technical simulation needs, resulting in more realistic and valid traffic simulations for automotive development and testing.


<details>
  <summary>Details</summary>
Motivation: Current traffic simulation methods often lack realistic conditions and a clear connection between study objectives and simulation requirements, risking the validity of experimental outcomes.

Method: A structured methodology based on sub-goals in each study phase to derive technical needs for microscopic simulation, agent models, and visuals.

Result: The methodology links study goals to simulation design systematically, improving both participant engagement and the quality of experimental results.

Conclusion: The proposed methodology enhances fidelity and experimental validity in traffic simulations, strengthening automotive development and testing.

Abstract: This paper addresses the challenge of ensuring realistic traffic conditions
by proposing a methodology that systematically identifies traffic simulation
requirements. Using a structured approach based on sub-goals in each study
phase, specific technical needs are derived for microscopic levels, agent
models, and visual representation. The methodology aims to maintain a high
degree of fidelity, enhancing both the validity of experimental outcomes and
participant engagement. By providing a clear link between study objectives and
traffic simulation design, this approach supports robust automotive development
and testing.

</details>


### [16] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: LLM agents can automate parts of web vulnerability exploitation but struggle with complex, real-world scenarios due to environment and authentication challenges, highlighting the need for significant improvements before they can reliably reproduce web vulnerabilities end-to-end.


<details>
  <summary>Details</summary>
Motivation: Although large language model (LLM) agents show promise in software engineering and cybersecurity, their potential for automatically reproducing web vulnerabilities from vulnerability reports remains underexplored. This paper aims to comprehensively evaluate LLM agents' effectiveness in this critical but challenging application area.

Method: The authors conduct a comprehensive evaluation of 20 state-of-the-art LLM agents from diverse domains. They assess these agents across 16 dimensions, including technical skills, adaptability to different environments, and user experience, using three representative web vulnerabilities. The top three agents are then tested in-depth using a benchmark dataset of 80 real-world CVEs covering 7 vulnerability types and 6 web technologies.

Result: LLM agents perform reasonably well on simple, library-based vulnerabilities but consistently fail on complex service-based vulnerabilities that require multi-component environments. Challenges include complex environment configurations and authentication barriers, resulting in a gap where agents can execute code but cannot fully trigger vulnerabilities. Performance drops significantly (over 33%) when input guidance is incomplete, especially regarding authentication.

Conclusion: Current LLM agents have a significant gap in fulfilling the demands of reliable, fully automated web vulnerability reproduction. Advances are needed in agents' environmental adaptation and autonomous problem-solving abilities to bridge this gap.

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


### [17] [Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks](https://arxiv.org/abs/2510.14778)
*Maor Reuben,Ido Mendel,Or Feldman,Moshe Kravchik,Mordehai Guri,Rami Puzis*

Main category: cs.SE

TL;DR: The paper proposes an unsupervised method using name-prediction-based cohesion to detect rare but damaging code injections in software supply chain attacks. Experiments on large-scale open-source C++ projects show the method effectively spots spurious injections by identifying reduced cohesion and less descriptive naming, showing promise for improving automated code security checks.


<details>
  <summary>Details</summary>
Motivation: Supply chain attacks, where malicious code is injected into legitimate software projects, are rare but can be highly destructive. Detecting such attacks is challenging for automated tools because it requires understanding both the inserted code and its surrounding context.

Method: The study introduces an unsupervised method to identify spurious code injections. It quantifies cohesion disruptions in source code using a novel name-prediction-based cohesion (NPC) metric, analyzing how the cohesion of functions changes after code injection versus normal updates. The approach relies on monitoring shifts in naming conventions and cohesion levels in source code, specifically in C++ repositories.

Result: Analysis of 54,707 functions from 369 open-source C++ repositories showed that functions with code injections experienced lower cohesion and less descriptive naming patterns than those with natural updates. The method achieved a Precision@100 of 36.41% in highly imbalanced datasets (1:1,000 ratio) and 12.47% at even rarer ratios (1:10,000), indicating strong detection performance in difficult real-world scenarios.

Conclusion: The NPC metric and cohesion monitoring approach are effective in identifying malicious code injections in open-source projects. Automated cohesion measurement, especially via naming analysis, can provide useful signals for detecting and mitigating supply chain attacks, thus enhancing source code security.

Abstract: Supply chain attacks significantly threaten software security with malicious
code injections within legitimate projects. Such attacks are very rare but may
have a devastating impact. Detecting spurious code injections using automated
tools is further complicated as it often requires deciphering the intention of
both the inserted code and its context. In this study, we propose an
unsupervised approach for highlighting spurious code injections by quantifying
cohesion disruptions in the source code. Using a name-prediction-based cohesion
(NPC) metric, we analyze how function cohesion changes when malicious code is
introduced compared to natural cohesion fluctuations. An analysis of 54,707
functions over 369 open-source C++ repositories reveals that code injection
reduces cohesion and shifts naming patterns toward shorter, less descriptive
names compared to genuine function updates. Considering the sporadic nature of
real supply-chain attacks, we evaluate the proposed method with extreme
test-set imbalance and show that monitoring high-cohesion functions with NPC
can effectively detect functions with injected code, achieving a Precision@100
of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that
automated cohesion measurements, in general, and name-prediction-based
cohesion, in particular, may help identify supply chain attacks, improving
source code integrity.

</details>


### [18] [Instruction Set Migration at Warehouse Scale](https://arxiv.org/abs/2510.14928)
*Eric Christopher,Kevin Crossan,Wolff Dobson,Chris Kennelly,Drew Lewis,Kun Lin,Martin Maas,Parthasarathy Ranganathan,Emma Rapati,Brian Yang*

Main category: cs.SE

TL;DR: Migrating codebases between ISAs (like x86 to Arm) is now less about binary translation and more about managing a broad set of tasks enabled by open-source and recompilation. Google's large migration showcased the role of automation and AI in this process, but also revealed persistent challenges needing further study.


<details>
  <summary>Details</summary>
Motivation: Despite significant engineering efforts being spent on migrating codebases between Instruction Set Architectures (ISAs), such as from x86 to Arm, the academic community has given limited attention to this problem and has mostly focused on binary translation as the main challenge.

Method: The paper conducts a detailed analysis of a large-scale real-world ISA migration at Google, involving nearly 40,000 code commits. The authors categorize the various tasks involved in the ISA migration process and illustrate how automation and AI have been used to address many of them.

Result: A taxonomy of ISA migration tasks was established. The analysis demonstrates Google's successful automation of many parts of the migration, highlights the substantial potential for AI-based automation, and points out migration tasks that are still challenging and require further research.

Conclusion: The paper concludes that ISA migration today often leverages open-source ecosystems and can be conducted by recompiling software rather than relying on binary translation. However, new challenges arise, many of which can be partially addressed by automation and AI, but some still present open research questions.

Abstract: Migrating codebases from one instruction set architecture (ISA) to another is
a major engineering challenge. A recent example is the adoption of Arm (in
addition to x86) across the major Cloud hyperscalers. Yet, this problem has
seen limited attention by the academic community. Most work has focused on
static and dynamic binary translation, and the traditional conventional wisdom
has been that this is the primary challenge.
  In this paper, we show that this is no longer the case. Modern ISA migrations
can often build on a robust open-source ecosystem, making it possible to
recompile all relevant software from scratch. This introduces a new and
multifaceted set of challenges, which are different from binary translation.
  By analyzing a large-scale migration from x86 to Arm at Google, spanning
almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA
migration. We show how Google automated many of the steps involved, and
demonstrate how AI can play a major role in automatically addressing these
tasks. We identify tasks that remain challenging and highlight research
challenges that warrant further attention.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [HITrees: Higher-Order Interaction Trees](https://arxiv.org/abs/2510.14558)
*Amir Mohammad Fadaei Ayyam,Michael Sammler*

Main category: cs.PL

TL;DR: HITrees generalize interaction trees to support higher-order effects, crucial for modeling advanced semantic constructs. Implemented in Lean with a rich effect library, HITrees enable formal semantics for complex language features such as parallel composition and call/cc.


<details>
  <summary>Details</summary>
Motivation: Compositional semantics has become important for verifying complex systems, with interaction trees being a common approach. However, traditional interaction trees lack support for higher-order effects, which are crucial for modeling advanced semantic features such as parallel composition and call/cc.

Method: The authors propose Higher-Order Interaction Trees (HITrees), extending interaction trees to support higher-order effects in non-guarded type theory. Their approach uses specifically designed effects so that fixpoints involving higher-order inputs are expressible as inductive types, and they utilize defunctionalization to encode higher-order outputs into a first-order form. HITrees are implemented in Lean, together with a library of standard effects.

Result: HITrees allow expressing higher-order effects, such as concurrency, recursion, and call/cc. The authors provide two formal interpretations: as state transition systems and as monadic programs. Their system is demonstrated by defining the semantics for a language featuring parallel composition and call/cc.

Conclusion: HITrees successfully extend interaction trees for higher-order effects, solving a key limitation and enabling formal modeling of more complex system semantics in non-guarded type theory.

Abstract: Recent years have witnessed the rise of compositional semantics as a
foundation for formal verification of complex systems. In particular,
interaction trees have emerged as a popular denotational semantics. Interaction
trees achieve compositionality by providing a reusable library of effects.
However, their notion of effects does not support higher-order effects, i.e.,
effects that take or return monadic computations. Such effects are essential to
model complex semantic features like parallel composition and call/cc.
  We introduce Higher-Order Interaction Trees (HITrees), the first variant of
interaction trees to support higher-order effects in a non-guarded type theory.
HITrees accomplish this through two key techniques: first, by designing the
notion of effects such that the fixpoints of effects with higher-order input
can be expressed as inductive types inside the type theory; and second, using
defunctionalization to encode higher-order outputs into a first-order
representation. We implement HITrees in the Lean proof assistant, accompanied
by a comprehensive library of effects including concurrency, recursion, and
call/cc. Furthermore, we provide two interpretations of HITrees, as state
transition systems and as monadic programs. To demonstrate the expressiveness
of HITrees, we apply them to define the semantics of a language with parallel
composition and call/cc.

</details>
