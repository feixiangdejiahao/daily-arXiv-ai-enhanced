{"id": "2511.13972", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13972", "abs": "https://arxiv.org/abs/2511.13972", "authors": ["Jeremiah Bohr"], "title": "Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation", "comment": "23 pages, 2 figures, 3 tables. Under review", "summary": "Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.", "AI": {"tldr": "Studying language models' code generation, the paper finds that prompting with both instructions and examples offers the best stylistic control, producing concise code that maintains its style even after code improvements, whereas instructions or examples alone are less effective.", "motivation": "Language models often generate overly verbose code compared to human-written code. There is a need to understand how to achieve better stylistic control\u2014making code more concise and human-like\u2014particularly when code needs to be revised or enhanced beyond an initial implementation.", "method": "The paper conducts an experimental study using large language models to generate Python code under different prompt conditions. Four prompt conditions were manipulated: instruction-based, example-based, combined (instruction + example), and a baseline. A paired two-turn protocol was used: first, the model generated a solution to a problem, then revised it under improvement instructions, with 160 paired code samples evaluated for stylistic control and expansion discipline.", "result": "Combined prompts (instruction + example) led to the most concise initial code and the greatest adherence to stylistic constraints when code was expanded. Instruction-based prompts created concise code initially and maintained moderate discipline on expansion. Example-based prompts had a small initial effect and did not maintain stylistic discipline on code expansion.", "conclusion": "Stylistic control and expansion discipline are distinct aspects in prompt design for code generation. Combined prompt approaches are most effective for achieving and maintaining desirable coding style across initial generation and subsequent enhancements."}}
{"id": "2511.13996", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13996", "abs": "https://arxiv.org/abs/2511.13996", "authors": ["Daihan Xu", "Diana Martin"], "title": "Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education", "comment": "Full paper oral presentation at the European Society for Engineering Education (SEFI) 2025 Annual Conference (September 2025)", "summary": "ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional \"independent thinking-manual coding-iterative debugging\" to \"AI-assisted ideation-interactive programming-collaborative optimization.\" Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.", "AI": {"tldr": "This study reveals that computer science students are strategically and ethically integrating ChatGPT into their software development process, balancing its assistance while reserving key creative decisions for themselves. However, limited critical evaluation of AI output poses risks, highlighting a demand for clear guidelines to support responsible use.", "motivation": "ChatGPT is widely used by computer science students in software development, but concerns about ethics, overreliance, and academic integrity persist. Previous research has focused on broad trends rather than students' actual strategies and ethical considerations.", "method": "Qualitative study based on semi-structured interviews with computer science students at a UK institution, examining how they use ChatGPT strategically and ethically in coursework and professional projects.", "result": "Students shift from traditional approaches towards collaborative, AI-assisted processes in coding. Most students balance ChatGPT involvement, typically limiting AI's contribution to about 30%, and actively review its outputs to avoid dependency. Few deeply analyze AI-generated code, which may reduce critical thinking. Students value transparency, reject uncredited use, and express concerns about privacy and skill loss, requesting clearer institutional guidelines.", "conclusion": "Students are adapting to AI integration by redefining their learning and coding processes, but responsible use relies on explicit guidance to maintain ethics and skill development. There is a need for clear policies to ensure pedagogically robust and ethical AI use."}}
{"id": "2511.13998", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13998", "abs": "https://arxiv.org/abs/2511.13998", "authors": ["Jielin Qiu", "Zuxin Liu", "Zhiwei Liu", "Rithesh Murthy", "Jianguo Zhang", "Haolin Chen", "Shiyu Wang", "Ming Zhu", "Liangwei Yang", "Juntao Tan", "Roshan Ram", "Akshara Prabhakar", "Tulika Awalgaonkar", "Zixiang Chen", "Zhepeng Cen", "Cheng Qian", "Shelby Heinecke", "Weiran Yao", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering", "comment": "54-pages", "summary": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.", "AI": {"tldr": "LoCoBench-Agent is a new benchmark for evaluating language model agents in real-world coding scenarios. It measures multi-turn, tool-assisted performance on long-context software tasks, revealing trade-offs and differences among models. This benchmark is a significant step toward robust, real-world evaluation of coding agents.", "motivation": "The motivation is to address the limitations of current benchmarks for large language model (LLM) agents in software engineering. Existing benchmarks only focus on single-turn evaluation and fail to capture the complex, interactive, multi-turn, and tool-based workflows required by real-world coding agents.", "method": "The paper introduces LoCoBench-Agent, an evaluation framework that turns 8,000 LoCoBench scenarios into interactive environments for agents. It evaluates agents through multi-turn dialog, tool use, error recovery, and architectural consistency, using 9 metrics that analyze comprehension and efficiency. The framework tests agents with 8 specialized tools across context lengths from 10K to 1M tokens.", "result": "Results show that agents are robust over long contexts, but there is a negative correlation between comprehension and efficiency. Thorough exploration increases comprehension but comes at the cost of efficiency. Also, conversation efficiency and tool usage strategies vary significantly among models, with stronger performers demonstrating more strategic tool usage.", "conclusion": "LoCoBench-Agent provides the first comprehensive, long-context benchmark for evaluating LLM agent performance in realistic software engineering workflows. It sets a rigorous standard for measuring agent capabilities and reveals key performance differences and trade-offs, supporting future improvements in autonomous software development agents."}}
{"id": "2511.14002", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.14002", "abs": "https://arxiv.org/abs/2511.14002", "authors": ["Chengpeng Li", "Farnaz Behrang", "August Shi", "Peng Liu"], "title": "FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale", "comment": "To appear in ASE 2025", "summary": "Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.", "AI": {"tldr": "FlakyGuard leverages graph-based selective context extraction for repairing flaky tests with LLMs, resulting in substantially improved success and acceptance rates compared to existing methods, and delivering useful explanations to developers.", "motivation": "Flaky tests waste developer time and slow down release cycles. Existing automatic repair approaches using LLMs are ineffective in real-world industrial settings due to the context problem: either missing relevant production code or overwhelming the LLM with too much irrelevant information.", "method": "FlakyGuard represents code as graphs and uses selective graph exploration to find only the most relevant context, thereby enabling more effective use of LLMs for repairing flaky tests.", "result": "FlakyGuard repairs 47.6% of reproducible flaky tests, with 51.8% of the fixes being accepted by developers. It outperforms the previous best approaches by at least 22% in repair success rate. Additionally, 100% of surveyed developers found FlakyGuard's root cause explanations useful.", "conclusion": "FlakyGuard is a significant improvement over existing LLM-based flaky test repair tools, effectively addressing the context problem and achieving higher repair and acceptance rates in industrial settings."}}
{"id": "2511.14022", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14022", "abs": "https://arxiv.org/abs/2511.14022", "authors": ["Pradeep Kumar Sharma", "Ishaan Puri", "Mantinder Jit Singh", "Swapnil Shivaprasad", "Hritvik Shrivastava"], "title": "Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning", "comment": null, "summary": "Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.", "AI": {"tldr": "Keeping code-to-filepath models accurate over time is hard because codebases change. The study tests three update strategies: full retraining, injecting recent changes at inference, and incremental fine-tuning. Mixing old and new data during fine-tuning gives the best general accuracy, but which method is best depends on the kind of changes: retraining is best for newness, in-context learning is fast for small updates, and delta-based fine-tuning is ideal for frequent renames/deletes.", "motivation": "The motivation is to address the challenge of keeping machine learning models mapping developer questions to relevant code file paths fresh and accurate as codebases evolve, including file renames, deletions, and API changes.", "method": "The authors frame model freshness as domain drift and compare three update strategies: (A) Full Refresh (retraining on the latest snapshot), (B) In-Context Learning (injecting git diffs or English summaries at inference), and (C) Incremental Fine-Tuning (on delta-derived sets with careful old/new mixing to prevent forgetting). They also use an alias-aware protocol and a Forgetting Probe for evaluation.", "result": "Inc-FT with old-aware mixing achieves the best overall balance between retaining old code and activating new code. ICL with English summaries gives fast improvements when retraining is not possible. Full Refresh delivers maximal new-code accuracy. Git-diff-based Inc-FT excels when there are many renames/deletions, while full-file Inc-FT performs better when code behavior changes dominate.", "conclusion": "Incremental fine-tuning with careful mixing is optimal for balancing retention and adaptation in evolving codebases. In-context learning with English deltas is useful for fast lift when retraining is infeasible, and full refresh is best for maximizing accuracy on new code. Update strategies should be chosen based on the type of code evolution occurring."}}
{"id": "2511.14062", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14062", "abs": "https://arxiv.org/abs/2511.14062", "authors": ["Shenglin Zhang", "Ziang Chen", "Zijing Que", "Yilun Liu", "Yongqian Sun", "Sicheng Wei", "Dan Pei", "Hailin Li"], "title": "LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering", "comment": null, "summary": "Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.", "AI": {"tldr": "LogPurge is a new, automated log purification framework that leverages LLMs and system rules to efficiently select clean log data for anomaly detection model training. It dramatically improves anomaly removal rates and F-1 scores compared to previous unsupervised approaches, offering cost and accuracy advantages for log anomaly detection.", "motivation": "Log anomaly detection is crucial for ensuring system reliability and security but commonly depends on expensive, manually labeled, clean log data, which creates scalability and efficiency issues.", "method": "The proposed method, LogPurge, is a cost-aware, rule-enhanced purification framework that uses a two-stage filtering algorithm. In the first stage, a large language model (LLM) removes anomalous patterns and applies enhanced system rules for better log interpretation. In the second stage, a divide-and-conquer strategy breaks down contaminated regions for further purification using the first stage's approach.", "result": "Experiments on two public and one industrial dataset showed that LogPurge removed an average of 98.74% of anomalies and retained 82.39% of normal samples. It also substantially improved F-1 scores by 35.7% and 84.11% on public datasets, and 149.72% on a private dataset compared to existing unsupervised methods.", "conclusion": "LogPurge effectively reduces the need for costly manual labeling by automating log purification and demonstrates superior anomaly removal and retention of normal samples over current methods."}}
{"id": "2511.14215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14215", "abs": "https://arxiv.org/abs/2511.14215", "authors": ["Malik Muhammad Umer"], "title": "A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints", "comment": null, "summary": "The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.", "AI": {"tldr": "This study shows a customized Scrum-based Agile framework can meet strict aerospace safety standards (DO-178C) while drastically improving development efficiency and defect metrics compared to traditional methods. Key elements include tailored roles, independent validation, and dual acceptance criteria. While Agile increases some verification effort, the overall results demonstrate strong benefits and point to further improvement via automation and broader validation.", "motivation": "Aerospace systems are becoming more complex, necessitating development processes that combine agility with rigorous safety and certification requirements. The motivation is to resolve the tension between adopting Agile methods and meeting DO-178C compliance for safety-critical aerospace software.", "method": "This study develops and empirically validates a Scrum-based Agile framework specifically tailored for DO-178C compliant software. The framework modifies conventional Scrum roles, artifacts, and events to support certification objectives, and introduces multi-disciplinary product ownership, dual compliance-functionality acceptance criteria, independent testing/documentation teams, and certification liaisons. The framework was evaluated using two aerospace projects, comparing outcomes of the Agile process to a traditional Waterfall approach.", "result": "The Agile framework resulted in a 76% reduction in effort per requirement, a 75% faster defect detection rate, a 78% improvement in defect resolution speed, and over 50% lower defect density, while maintaining DO-178C Level A compliance. However, there was increased V&V effort due to iterative development.", "conclusion": "Agile methods can be effectively combined with regulatory compliance for safety-critical aerospace software if the process is rigorously tailored and there is proactive engagement with certification authorities. Future work should validate the framework further and explore process automation opportunities."}}
{"id": "2511.14224", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14224", "abs": "https://arxiv.org/abs/2511.14224", "authors": ["Anji Li", "Mingwei Liu", "Zhenxi Chen", "Zheng Pei", "Zike Li", "Dekun Dai", "Yanlin Wang", "Zibin Zheng"], "title": "KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation", "comment": "13 pages, 11 figures", "summary": "Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.", "AI": {"tldr": "KTester is a framework that uses knowledge-driven strategies to guide LLMs for better and more maintainable unit test generation, outperforming prior methods in both automated and human evaluations.", "motivation": "Automated unit test generation with large language models promises efficiency but often fails to create correct and maintainable tests in practical projects. The need to solve these shortcomings motivates the research.", "method": "KTester integrates project-specific and testing-domain knowledge through static analysis to extract context, separates test case design from test method generation, and uses multi-perspective prompting to improve LLM guidance. It applies structured templates to enhance clarity and maintainability, evaluated via automatic metrics and human studies.", "result": "KTester outperforms state-of-the-art LLM-based baselines in six key metrics, with a 5.69% higher execution pass rate and 8.83% better line coverage, produces fewer test cases in less time, and is rated higher in correctness, readability, and maintainability by human judges.", "conclusion": "Knowledge-driven approaches that incorporate project and domain expertise can substantially improve the quality, correctness, and maintainability of LLM-generated unit tests, proven by KTester's superior performance and practical advantages."}}
{"id": "2511.14367", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14367", "abs": "https://arxiv.org/abs/2511.14367", "authors": ["Dulaji Hidellaarachchi", "Sebastian Baltes", "John Grundy"], "title": "How Does Cognitive Capability and Personality Influence Problem-Solving in Coding Interview Puzzles?", "comment": "11 pages, 8 figures, 7 tables", "summary": "Software engineering is a deeply cognitive activity shaped by individual differences that extend beyond technical skill. This study investigates how cognitive capability and personality traits jointly relate to software problem solving among 80 participants (40 software practitioners, 40 software engineering students). Cognitive capability was measured using Baddeleys three minute grammatical reasoning test, while personality was assessed using the IPIP NEO 50 test. Participants further completed nine interview style problem solving questions. Six questions were related to coding and three were related to logical reasoning. Descriptive and correlational analyses show that practitioners achieved slightly higher grammatical reasoning accuracy and overall task performance than students. Grammatical-reasoning accuracy correlated positively with problem solving performance, indicating that stronger cognitive capability is associated with better performance in coding and logical tasks. Personality performance links were systematic. We identified that the conscientiousness trait correlated most strongly with problem solving and with reasoning accuracy, while the openness to experience trait was positively related to both outcomes. Neuroticism showed small, negative associations with accuracy and performance. Taken together, our results suggest that conscientiousness and openness to experience characteristics complement reasoning accuracy to support software problem solving, whereas elevated negative affect may hinder precision under time pressure. Our findings suggest practical implications for education and industry such as integrating structured reasoning tasks in curricula, and considering personality cognition in recruitment and role allocation. We highlight directions for future research such as longitudinal and task diverse replications with larger samples.", "AI": {"tldr": "This study finds that stronger cognitive capability and certain personality traits (conscientiousness, openness) improve software problem solving, suggesting practical and educational changes, while neuroticism detracts from performance.", "motivation": "To understand how cognitive capability and personality traits interact and influence software problem solving, addressing gaps in our knowledge about non-technical factors affecting performance.", "method": "80 participants (40 practitioners, 40 students) were assessed for cognitive capability via grammatical reasoning tests and for personality via the IPIP NEO 50 test; they completed nine problem solving questions (coding and logical reasoning). Analyses included descriptive and correlational methods.", "result": "Practitioners scored slightly higher than students in reasoning accuracy and overall performance. Cognitive capability (grammatical reasoning) positively correlated with problem-solving ability. Conscientiousness and openness traits had the strongest positive associations with performance; neuroticism correlated negatively.", "conclusion": "Conscientiousness and openness to experience bolster reasoning accuracy and problem-solving in software engineering, while neuroticism impairs performance under pressure. Insights have practical implications for education and industry, such as curriculum design and recruitment strategies."}}
{"id": "2511.14435", "categories": ["cs.SE", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.14435", "abs": "https://arxiv.org/abs/2511.14435", "authors": ["Angelo Ferrando"], "title": "Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems", "comment": "In Proceedings FMAS 2025, arXiv:2511.13245", "summary": "Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.", "AI": {"tldr": "This paper advocates combining runtime verification (RV) and large language models (LLMs) to improve the safety and reliability of autonomous systems, presenting a vision for mutual support where RV safeguards LLM decisions and LLMs help extend RV capabilities. It distinguishes this approach from previous surveys, discusses certification challenges, and suggests future research directions.", "motivation": "The motivation of this paper is to address the challenges in ensuring the safety and trustworthiness of autonomous systems, particularly when they include learning-enabled components and operate in open environments. Traditional formal methods are limited by their reliance on complete models and static assumptions, creating a need for complementary approaches.", "method": "The paper proposes a vision for the symbiotic integration of Runtime Verification (RV) and Large Language Models (LLMs). It describes how RV can act as a safeguard for autonomous systems driven by LLMs and how LLMs can enhance RV through natural language specification capture, pattern recognition, anticipatory reasoning, and managing uncertainty.", "result": "The paper outlines the mutual reinforcement between RV and LLMs, emphasizing how this integration is distinct from previous works and surveys. It discusses specific challenges, certification implications, and future research directions necessary for achieving dependable autonomous systems.", "conclusion": "Integrating RV with LLMs offers a promising path toward safer and more trustworthy autonomy. RV provides runtime guardrails, while LLMs can expand the capabilities of RV in handling dynamic, uncertain environments. The paper highlights the need for further research to address open challenges and realize the vision of dependable autonomy."}}
{"id": "2511.14528", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14528", "abs": "https://arxiv.org/abs/2511.14528", "authors": ["Tatiane Ornelas", "Allysson Allex Ara\u00fajo", "J\u00falia Ara\u00fajo", "Marina Ara\u00fajo", "Bianca Trinkenreich", "Marcos Kalinowski"], "title": "LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations", "comment": null, "summary": "[Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.", "AI": {"tldr": "This study explores experienced SE researchers' views on using LLMs in thematic analysis. While LLMs can enhance efficiency and scalability, concerns about bias, context loss, and reproducibility remain. Human guidance and critical prompting are essential; LLMs should augment, not substitute, interpretive analysis.", "motivation": "Large Language Models (LLMs) are increasingly being integrated into qualitative research in Software Engineering (SE), yet their methodological impact, especially regarding processes like thematic analysis, remains unclear. This motivates a deeper investigation into both the benefits and risks of using LLMs in such research contexts.", "method": "A reflective workshop was conducted with 25 experienced ISERN researchers. Structured discussions focused on various stages of thematic analysis\u2014open coding, theme generation, and theme review\u2014with insights and evaluations documented using color-coded canvases.", "result": "Researchers identified efficiency and scalability benefits from using LLMs, but also pointed out risks such as bias, loss of contextual detail, challenges with reproducibility, and the fast-changing nature of LLM technologies. They stressed the importance of developing prompting skills and maintaining continuous human oversight throughout the process.", "conclusion": "LLMs have potential to support, but not replace, the interpretive processes in qualitative SE research. Responsible integration requires attention to methodological rigor, transparency, and ongoing community reflection."}}
{"id": "2511.14618", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14618", "abs": "https://arxiv.org/abs/2511.14618", "authors": ["Severin Kohler", "Jordi Piera Jim\u00e9nez", "Michael Anywar", "Lars Fuhrmann", "Heather Leslie", "Maximilian Meixner", "Julian Sa\u00df", "Florian K\u00e4rcher", "Diego Bosc\u00e1", "Birger Haarbrandt", "Michael Marschollek", "Roland Eils"], "title": "FHIRconnect: Towards a seamless integration of openEHR and FHIR", "comment": "27 pages, 4 figures", "summary": "Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.", "AI": {"tldr": "This paper introduces FHIRconnect, an open-source language and engine to enable standardized, reusable, and bidirectional data exchange between openEHR and HL7 FHIR, overcoming major interoperability barriers in health IT systems.", "motivation": "Interoperability between openEHR and HL7 FHIR is challenging due to different data modeling approaches and lack of standardized mechanisms for data transformation, which limits integration between healthcare IT systems.", "method": "The paper introduces FHIRconnect, a domain-specific language (DSL) and open-source transformation engine. The approach uses a triple-layered architecture that leverages international archetype-based foundations while allowing local customizations. FHIRconnect enables standardized, bidirectional data exchange and includes an execution engine (openFHIR) and a mapping library.", "result": "The framework achieved 65% mapping reuse across projects and successfully mapped 24 international archetypes to 15 FHIR profiles over seven clinical domains. It produced the first formal DSL specification for openEHR-FHIR transformation and made the engine and mapping library open source.", "conclusion": "FHIRconnect provides the technical foundation for standardized, community-driven mapping between openEHR and FHIR, which can reduce reliance on custom ETL solutions and improve syntactic and semantic interoperability in healthcare IT."}}
{"id": "2511.14711", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14711", "abs": "https://arxiv.org/abs/2511.14711", "authors": ["Aaliyah Chang", "Mariam Guizani", "Brittany Johnson"], "title": "Why Do We Code? A Theory on Motivations and Challenges in Software Engineering from Education to Practice", "comment": null, "summary": "Motivations and challenges jointly shape how individuals enter, persist, and evolve within software engineering (SE), yet their interplay remains underexplored across the transition from education to professional practice. We conducted 15 semi-structured interviews and employed the Gioia Methodology, an adapted grounded theory methodology from organizational behavior, to inductively derive taxonomies of motivations and challenges, and build the Exposure-Pursuit-Evaluation (EPE) Process Model. Our findings reveal that impactful early exposure triggers intrinsic motivations, while non-impactful exposure requires an extrinsic push (e.g., career/ personal goals, external validation). We identify curiosity and avoiding alternatives as a distinct educational drivers, and barriers to belonging as the only challenge persisting across education and career. Our findings show that career progression challenges (e.g., navigating the corporate world) constrain extrinsic fulfillment while technical training challenges, barriers to belonging and threats to motivation constrain intrinsic fulfillment. The theory shows how unmet motivations and recurring challenges influence persistence, career shifts, or departure from the field. Our results provide a grounded model for designing interventions that strengthen intrinsic fulfillment and reduce systemic barriers in SE education and practice.", "AI": {"tldr": "This paper examines how motivations and challenges shape entry and persistence in software engineering, using interviews and qualitative analysis to develop a theory that can inform interventions to help individuals thrive from education through professional practice.", "motivation": "The paper is motivated by the need to better understand how motivations and challenges interact to affect individuals' entry, persistence, and evolution in software engineering, especially during the transition from education to professional practice, a topic not extensively studied.", "method": "The authors conducted 15 semi-structured interviews and applied the Gioia Methodology, an adapted grounded theory methodology from organizational behavior, to inductively build taxonomies of motivations and challenges and to create the Exposure-Pursuit-Evaluation (EPE) Process Model.", "result": "The study finds that impactful early exposure promotes intrinsic motivations, while non-impactful exposure requires extrinsic motivators. Curiosity and avoiding alternative careers are identified as specific educational drivers. Barriers to belonging are the only challenge that persists throughout education and career. Career-related challenges restrict extrinsic fulfillment, while technical, motivational, and belonging challenges limit intrinsic fulfillment. Unmet motivations and recurring challenges affect whether individuals persist in, shift within, or leave the software engineering field.", "conclusion": "The findings offer a grounded theoretical model for understanding and improving motivations and addressing challenges in software engineering, guiding the design of interventions to support intrinsic fulfillment and reduce systemic barriers in SE education and practice."}}
