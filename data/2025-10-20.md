<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Automated Snippet-Alignment Data Augmentation for Code Translation](https://arxiv.org/abs/2510.15004)
*Zhiming Zhang,Qingfu Zhu,Xianzhen Luo,Yixuan Wang,Bohan Li,Wanxiang Che*

Main category: cs.SE

TL;DR: The paper introduces an LLM-based method for generating snippet-alignment training data for code translation, and demonstrates that a two-stage training method using both program- and snippet-level data significantly improves translation accuracy, achieving up to 3.78% improvement on the TransCoder-test benchmark.


<details>
  <summary>Details</summary>
Motivation: Code translation is essential for various software development purposes, and recent Large Language Models (LLMs) show promising abilities in this area. Effective training for code translation depends heavily on the availability and use of parallel corpora, but there is a limitation in both the quantity and fine-grained alignment of existing data. Conventional research primarily augments program-alignment (PA) data; snippet-alignment (SA) data, which may help with finer-grained alignment, is underexplored.

Method: The authors propose a data augmentation approach that leverages LLMs to automatically create snippet-alignment (SA) data. They then introduce a two-stage training strategy: models are first trained on program-alignment (PA) data and subsequently fine-tuned on the enriched SA data to maximize knowledge transfer and alignment quality.

Result: The experiments, conducted on the TransCoder-test dataset, show that augmenting the available data with automatically generated SA data and utilizing the two-stage training process consistently improves code translation model performance. The best result is a 3.78% gain in pass@k compared to the baseline.

Conclusion: Automatically expanding snippet-alignment data using LLMs, combined with a two-stage training approach that leverages both program-level and snippet-level supervision, provides a substantial and consistent boost to code translation models.

Abstract: Code translation aims to translate the code from its source language to the
target language and is used in various software development scenarios. Recent
developments in Large Language Models (LLMs) have showcased their capabilities
in code translation, and parallel corpora play a crucial role in training
models for code translation. Parallel corpora can be categorized into
program-alignment (PA) and snippet-alignment (SA) data. Although PA data has
complete context and is suitable for semantic alignment learning, it may not
provide adequate fine-grained training signals due to its extended length,
while the brevity of SA data enables more fine-grained alignment learning. Due
to limited parallel corpora, researchers explore several augmentation methods
for code translation. Previous studies mainly focus on augmenting PA data. In
this paper, we propose a data augmentation method that leverages LLMs to
generate SA data automatically. To fully leverage both PA data and SA data, we
explore a simple yet effective two-stage training strategy, which consistently
enhances model performance compared to fine-tuning solely on PA data.
Experiments on TransCoder-test demonstrate that our augmented SA data combined
with the two-stage training approach yields consistent improvements over the
baseline, achieving a maximum gain of 3.78% on pass@k.

</details>


### [2] [Assessing Coherency and Consistency of Code Execution Reasoning by Large Language Models](https://arxiv.org/abs/2510.15079)
*Changshu Liu,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: The paper introduces CES, a new framework to assess LLMs' reasoning in program execution and programming tasks, emphasizing coherence over mere output correctness. Experimental results reveal that LLMs often rely on shortcuts, displaying incoherent reasoning and inconsistent performance, especially in bug-related tasks. CES can uncover and address these limitations, improving evaluation and development of reasoning-capable LLMs.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for LLMs in programming tasks often overestimate their reasoning ability by focusing solely on correct output predictions, which can be achieved through shortcuts or data leakage rather than genuine program understanding. There is a need for a more robust evaluation framework that ensures LLMs truly reason about the execution logic.

Method: The paper introduces CES, a framework that evaluates LLMs' ability to simulate program execution and apply this reasoning to programming tasks. CES measures both the correctness of simulation and a new notion called coherence, which reflects adherence to commonsense execution logic, irrespective of correct outputs. It also presents a metric for reasoning consistency across test cases using path coverage. CES is used to benchmark 16 LLMs on tasks like HumanEval and bug analysis.

Result: The evaluation using CES shows that while LLMs deliver coherent execution simulation (81.42% on HumanEval), their output correctness is split (46.92% correct, 53.08% incorrect). Notably, top-performing LLMs like GPT-4 and DeepSeek-R1 sometimes rely on shortcuts, leading to incoherent reasoning. LLMs' reasoning consistency is often weak or random, revealing limitations in handling path-sensitive analyses. For bug-related tasks, LLMs rely on pattern matching or language shortcuts rather than true execution reasoning, threatening their generalizability.

Conclusion: CES provides a robust framework for diagnosing reasoning shortcuts and inconsistencies in LLMs when simulating program execution. LLMs show reasonable coherence but lack consistent, path-sensitive reasoning, undermining their reliability for unseen programming or bug localization tasks. CES can systematically vet suspiciously successful results and encourage the development of LLMs that truly understand program logic.

Abstract: This paper proposes CES, a task to evaluate the abilities of LLMs in
simulating program execution and using that reasoning in programming tasks.
Besides measuring the correctness of variable predictions during execution
simulation, CES introduces the notion of coherence to determine whether the
simulation complies with commonsense execution logic, even if the predicted
values along the simulations are incorrect. This enables CES to rule out
suspiciously correct output predictions due to reasoning shortcuts,
hallucinations, or potential data leakage. CES also introduces a novel metric
to measure reasoning consistency across tests with the same or different prime
path coverage in a spectrum: strong, weak, and random. Evaluating 16 LLMs
(including three reasoning LLMs) using CES indicates 81.42% coherent execution
simulation on HumanEval, 46.92% and 53.08% of which result in correct and
incorrect output predictions. Frontier LLMs such as GPT-4 and DeepSeek-R1 have
the most incoherent execution reasoning, mostly due to natural language
shortcuts. Despite relatively coherent execution simulation, LLMs' reasoning
performance across different tests is inconsistent, mostly random (48.87%) or
weak (45.37%), potentially explaining their weakness in programming tasks that
require path-sensitive program analysis to succeed. We also compare CES with
bug prediction/localization/repair, which intuitively requires control- and
data-flow awareness. We observe that LLMs barely incorporate execution
reasoning into their analysis for bug-related tasks, and their success is
primarily due to inherent abilities in pattern matching or natural language
shortcuts, if not data leakage. Without reasoning, there is a threat to the
generalizability of LLMs in dealing with unseen bugs or patterns in different
contexts. CES can be used to vet the suspicious success of LLMs in these tasks
systematically.

</details>


### [3] [Community Engagement and the Lifespan of Open-Source Software Projects](https://arxiv.org/abs/2510.15408)
*Mohit,Kuljit Kaur Chahal*

Main category: cs.SE

TL;DR: Analyzing nearly 34,000 GitHub OSS projects, this paper finds that early and sustained community engagement—especially through active involvement—strongly drives project longevity and development. The research establishes clear CE metrics, revealing nuanced patterns in how community activity affects project lifespans.


<details>
  <summary>Details</summary>
Motivation: The motivation is to quantitatively understand how community engagement (CE) influences the lifespan and development dynamics of open-source software projects, which is a crucial but under-studied aspect of OSS sustainability.

Method: The study analyzes 33,946 GitHub repositories, defining CE through validated monthly metrics like issues, comments, watchers, and stargazers. Non-parametric statistical tests and correlation analyses are used to examine the relationship between CE metrics and various project dynamics (releases, commits, branches) and lifespan across engagement quartiles.

Result: CE metrics are significantly correlated with project dynamics, more strongly in highly engaged projects. Younger projects have higher per-month CE rates, which typically decline with age except for a subset of long-lived projects that sustain high engagement. Early bursts of CE are important for establishing projects, while ongoing high engagement tends to predict extreme longevity. Active engagement (issues/comments) grows more influential over time, while passive attention (watchers/stargazers) wanes.

Conclusion: Community engagement is a dynamic driver of both OSS project longevity and development. The research validates CE measurement approaches and reveals that specific patterns of community activity, especially sustained and active engagement, are vital for ongoing project success and lifespan.

Abstract: Open-source software (OSS) projects depend on community engagement (CE) for
longevity. However, CE's quantifiable impact on project dynamics and lifespan
is underexplored. Objectives: This study defines CE in OSS, identifies key
metrics, and evaluates their influence on project dynamics (releases, commits,
branches) and lifespan. Methods: We analyzed 33,946 GitHub repositories,
defining and operationalizing CE with validated per-month metrics (issues,
comments, watchers, stargazers). Non-parametric tests and correlations assessed
relationships with project dynamics and lifespan across quartiles. Results: CE
metrics significantly associate with project dynamics, with stronger
correlations in highly engaged projects. For lifespan, a complex pattern
emerged: per-month CE rates are highest in younger projects, declining with
age. Yet, a subset of long-lived projects maintains exceptionally high
activity. Initial CE bursts appear crucial for establishment, while sustained
high engagement drives extreme longevity. Active issue engagement's influence
intensifies with age, but passive attention's declines. Conclusion: CE
dynamically drives OSS project longevity and development. Our findings
establish validated CE metrics and offer deeper insights into how diverse
community activity patterns contribute to project longevity.

</details>


### [4] [Selecting and Combining Large Language Models for Scalable Code Clone Detection](https://arxiv.org/abs/2510.15480)
*Muslim Chochlov,Gul Aftab Ahmed,James Vincent Patten,Yuanhua Han,Guoxian Lu,David Gregg,Jim Buckley*

Main category: cs.SE

TL;DR: The paper benchmarks 76 large language models for code clone detection, finding that no single model is always best, but a select few perform strongly. Ensemble methods (especially using maximum or sum) can significantly boost precision above any single model, with clear gains on large commercial datasets.


<details>
  <summary>Details</summary>
Motivation: Source code clones can lead to significant risks such as intellectual property violations and the creation of vulnerabilities. Detecting these code clones, especially those that have diverged, is challenging at scale. The rapid development and diversity of large language models (LLMs) create uncertainty about which models perform best, and whether model ensembling could further improve detection.

Method: The study surveyed 76 LLMs, filtered them down to suitable candidates, and evaluated them on two large datasets: BigCloneBench (public) and a commercial large-scale code clone dataset. Performance metrics, especially precision, were compared. The study also experimented with different ensemble methods for combining LLM outputs, assessing the impact of score normalization and ensembling strategies (maximum, sum, average).

Result: No single LLM performed best across all evaluations, but CodeT5+110M, CuBERT, and SPTCode emerged as top candidates. Factors like smaller embedding sizes, smaller token vocabularies, and tailored datasets were found to be advantageous. On the commercial dataset, CodeT5+110M doubled the precision of CodeBERT (39.71%). Ensemble methods, especially those using maximum or sum, outperformed simple averaging and, on the commercial dataset, achieved up to 46.91% precision—statistically significant improvements over individual models.

Conclusion: Optimal LLM selection for code clone detection depends on context, as no universally best model was found. Model ensemble approaches, when properly normalized and executed, yield further improvements in detection precision, particularly at scale. Consideration of model architecture and dataset tailoring is also critical for best results.

Abstract: Source code clones pose risks ranging from intellectual property violations
to unintended vulnerabilities. Effective and efficient scalable clone
detection, especially for diverged clones, remains challenging. Large language
models (LLMs) have recently been applied to clone detection tasks. However, the
rapid emergence of LLMs raises questions about optimal model selection and
potential LLM-ensemble efficacy.
  This paper addresses the first question by identifying 76 LLMs and filtering
them down to suitable candidates for large-scale clone detection. The
candidates were evaluated on two public industrial datasets, BigCloneBench, and
a commercial large-scale dataset. No uniformly 'best-LLM' emerged, though
CodeT5+110M, CuBERT and SPTCode were top-performers. Analysis of LLM-candidates
suggested that smaller embedding sizes, smaller tokenizer vocabularies and
tailored datasets are advantageous. On commercial large-scale dataset a
top-performing CodeT5+110M achieved 39.71\% precision: twice the precision of
previously used CodeBERT.
  To address the second question, this paper explores ensembling of the
selected LLMs: effort-effective approach to improving effectiveness. Results
suggest the importance of score normalization and favoring ensembling methods
like maximum or sum over averaging. Also, findings indicate that ensembling
approach can be statistically significant and effective on larger datasets: the
best-performing ensemble achieved even higher precision of 46.91\% over
individual LLM on the commercial large-scale code.

</details>


### [5] [An Experimental Study of Real-Life LLM-Proposed Performance Improvements](https://arxiv.org/abs/2510.15494)
*Lirong Yi,Gregory Gay,Philipp Leitner*

Main category: cs.SE

TL;DR: LLMs can make code faster and often suggest optimizations similar to developers, but their improvements don't usually match human-crafted solutions. Original ideas from LLMs rarely provide major speed boosts, so human expertise still leads in code optimization.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to evaluate whether large language models (LLMs) can not only generate code but also produce code optimizations that improve performance, specifically focusing on real-world Java tasks where developers have already achieved significant speedups.

Method: The authors construct a dataset of 65 real-world optimization tasks from open-source Java programs where developers have made substantial performance improvements. They use an automated pipeline to generate optimization patches for these tasks using two leading LLMs with four different prompt designs. The resulting code is benchmarked and compared with both the pre-optimization baseline and the human-authored, optimized solutions.

Result: The study finds that LLM-generated code typically improves performance compared to the baseline but does not match the effectiveness of human-developed optimizations. Human patches consistently outperform LLM-generated fixes by a statistically significant margin. Semantically, LLM solutions are similar or identical to the developer's approach in about two-thirds of cases and are more original in the remaining one-third, though these original solutions seldom result in significant performance gains.

Conclusion: LLMs can successfully improve code performance over baseline implementations in many cases, but their optimizations are not as effective as those crafted by human developers. While LLMs occasionally offer novel optimization ideas, these do not often lead to superior performance, underscoring a gap between automated and human code optimization capabilities.

Abstract: Large Language Models (LLMs) can generate code, but can they generate fast
code? In this paper, we study this question using a dataset of 65 real-world
tasks mined from open-source Java programs. We specifically select tasks where
developers achieved significant speedups, and employ an automated pipeline to
generate patches for these issues using two leading LLMs under four prompt
variations. By rigorously benchmarking the results against the baseline and
human-authored solutions, we demonstrate that LLM-generated code indeed
improves performance over the baseline in most cases. However, patches proposed
by human developers outperform LLM fixes by a statistically significant margin,
indicating that LLMs often fall short of finding truly optimal solutions. We
further find that LLM solutions are semantically identical or similar to the
developer optimization idea in approximately two-thirds of cases, whereas they
propose a more original idea in the remaining one-third. However, these
original ideas only occasionally yield substantial performance gains.

</details>


### [6] [Enhancing Code Review through Fuzzing and Likely Invariants](https://arxiv.org/abs/2510.15512)
*Wachiraphan Charoenwet,Patanamon Thongtanunam,Van-Thuan Pham,Christoph Treude*

Main category: cs.SE

TL;DR: Manual code reviews miss dynamic bugs. FuzzSight uses fuzzing and invariant analysis to find behavioral differences early, flagging more vulnerabilities and bugs than traditional static methods. It helps code reviewers spot issues that static analysis would overlook.


<details>
  <summary>Details</summary>
Motivation: Manual code reviews often miss dynamic bugs and vulnerabilities because they typically rely on static inspection under tight time constraints. Dynamic analysis methods such as fuzzing can expose unexpected behaviors, but the lack of mechanisms to analyze and interpret their data means that their practical value in code review is limited.

Method: The authors introduce FuzzSight, a framework that uses dynamic analysis to capture likely invariants—dynamically observed properties at various code points—from non-crashing fuzzing inputs. By tracking and highlighting unexpected behavioral differences between program versions, FuzzSight helps reviewers identify which code blocks might warrant closer attention.

Result: FuzzSight flagged 75% of regression bugs and up to 80% of vulnerabilities in their evaluation using 24-hour fuzzing sessions. It surpassed static application security testing (SAST) tools by achieving ten times higher detection rates and fewer false positives in identifying problematic code blocks.

Conclusion: Leveraging fuzzing combined with invariant analysis enhances early-stage code review by providing dynamic behavioral insights, bridging the gap between static code inspection and runtime behavior. FuzzSight demonstrates substantial promise in detecting vulnerabilities and behavioral anomalies that static review could miss.

Abstract: Many software projects employ manual code review to gatekeep defects and
vulnerabilities in the code before integration. However, reviewers often work
under time pressure and rely primarily on static inspection, leaving the
dynamic aspects of the program unexplored. Dynamic analyses could reveal such
behaviors, but they are rarely integrated into reviews. Among them, fuzzing is
typically applied later to uncover crashing bugs. Yet its ability to exercise
code with diverse inputs makes it promising for exposing non-crashing, but
unexpected, behaviors earlier. Still, without suitable mechanisms to analyze
program behaviors, the rich data produced during fuzzing remains inaccessible
to reviewers, limiting its practical value in this context.
  We hypothesize that unexpected variations in program behaviors could signify
potential bugs. The impact of code changes can be automatically captured at
runtime. Representing program behavior as likely invariants, dynamic properties
consistently observed at specific program points, can provide practical signals
of behavioral changes. Such signals offer a way to distinguish between intended
changes and unexpected behavioral shifts from code changes.
  We present FuzzSight, a framework that leverages likely invariants from
non-crashing fuzzing inputs to highlight behavioral differences across program
versions. By surfacing such differences, it provides insights into which code
blocks may need closer attention. In our evaluation, FuzzSight flagged 75% of
regression bugs and up to 80% of vulnerabilities uncovered by 24-hour fuzzing.
It also outperformed SAST in identifying buggy code blocks, achieving ten times
higher detection rates with fewer false alarms. In summary, FuzzSight
demonstrates the potential and value of leveraging fuzzing and invariant
analysis for early-stage code review, bridging static inspection with dynamic
behavioral insights.

</details>


### [7] [Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework](https://arxiv.org/abs/2510.15585)
*Dr Simon Thorne,Dr Advait Sarkar*

Main category: cs.SE

TL;DR: LLMs are powerful but error-prone in generating code and spreadsheet logic. This paper proposes combining Test-Driven Development (TDD) with LLM usage to address these reliability issues. Their framework guides LLMs to produce more accurate, verifiable outputs and supports users lacking programming expertise, especially in critical domains like finance. The approach is designed for empirical testing and collaborative refinement.


<details>
  <summary>Details</summary>
Motivation: Current LLMs, while powerful in code and spreadsheet generation, frequently suffer from issues like hallucinations and logical/syntactic errors, posing major risks in high-stakes areas requiring high reliability. There's a need for approaches that can systematically improve correctness and user confidence.

Method: The authors propose integrating Test-Driven Development (TDD) practices with LLM-generated code and logic. This involves using 'test-first' prompts to guide LLMs, structured experimental design for evaluating the approach, and adaptation across different programming languages and contexts.

Result: The proposed framework aims to improve the accuracy, reliability, and comprehensibility of LLM outputs. It also enhances computational thinking and prompt engineering skills, with particular benefits for non-programmer spreadsheet users. The paper includes an experimental plan and calls for collaborative, empirical validation.

Conclusion: Integrating TDD with LLM code generation could mitigate key reliability issues and increase user confidence, especially for users lacking formal programming backgrounds. The authors advocate responsible LLM deployment in both education and professional settings via test-driven, empirically validated approaches.

Abstract: Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for
generating both traditional software code and spreadsheet logic. Despite their
impressive generative capabilities, these models frequently exhibit critical
issues such as hallucinations, subtle logical inconsistencies, and syntactic
errors, risks particularly acute in high stakes domains like financial
modelling and scientific computations, where accuracy and reliability are
paramount. This position paper proposes a structured research framework that
integrates the proven software engineering practice of Test-Driven Development
(TDD) with Large Language Model (LLM) driven generation to enhance the
correctness of, reliability of, and user confidence in generated outputs. We
hypothesise that a "test first" methodology provides both technical constraints
and cognitive scaffolding, guiding LLM outputs towards more accurate,
verifiable, and comprehensible solutions. Our framework, applicable across
diverse programming contexts, from spreadsheet formula generation to scripting
languages such as Python and strongly typed languages like Rust, includes an
explicitly outlined experimental design with clearly defined participant
groups, evaluation metrics, and illustrative TDD based prompting examples. By
emphasising test driven thinking, we aim to improve computational thinking,
prompt engineering skills, and user engagement, particularly benefiting
spreadsheet users who often lack formal programming training yet face serious
consequences from logical errors. We invite collaboration to refine and
empirically evaluate this approach, ultimately aiming to establish responsible
and reliable LLM integration in both educational and professional development
practices.

</details>


### [8] [Colepp: uma ferramenta multiplataforma para coleta de dados de dispositivos vestiveis](https://arxiv.org/abs/2510.15565)
*Vinicius Moraes de Jesus,Andre Georghton Cardoso Pacheco*

Main category: cs.SE

TL;DR: Colepp is a new open-source tool that synchronizes data from multiple wearable devices, solving issues with dataset quality and consistency. It helps researchers gather reliable physiological and movement data for developing improved algorithms.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for more reliable physiological and movement data collection tools, especially given the challenges with current publicly available datasets and limited control over data collection conditions.

Method: The authors developed Colepp, an open-source, cross-platform tool that collects and synchronizes data (ECG, PPG, accelerometer, gyroscope) from multiple wearable devices using a smartphone as a central hub. It uses a custom synchronization protocol to ensure data from the devices is consistent, and exports datasets in CSV format.

Result: Colepp enables the generation of customizable, real-world datasets with synchronized physiological and movement signals. A specific use case demonstrates its effectiveness in producing consistent and synchronized data.

Conclusion: Colepp addresses major challenges in wearable data collection by providing a flexible, user-friendly tool for obtaining high-quality, synchronized datasets, which can benefit applications like human activity recognition and heart rate estimation.

Abstract: The widespread adoption of wearable devices such as smartwatches and fitness
trackers has fueled the demand for reliable physiological and movement data
collection tools. However, challenges such as limited access to large,
high-quality public datasets and a lack of control over data collection
conditions hinder the development of robust algorithms. This work presents
Colepp, an open-source, cross-platform tool designed to collect and synchronize
data from multiple wearable devices, including heart rate (via ECG and PPG) and
motion signals (accelerometer and gyroscope). The system integrates a
smartphone as a central hub, receiving data from a Polar H10 chest strap and a
Wear OS smartwatch, and exporting synchronized datasets in CSV format. Through
a custom synchronization protocol and user-friendly interface, Colepp
facilitates the generation of customizable, real-world datasets suitable for
applications such as human activity recognition and heart rate estimation. A
use case shows the effectiveness of the tool in producing consistent and
synchronized signals.

</details>


### [9] [Interact and React: Exploring Gender Patterns in Development and the Impact on Innovation and Robustness of a User Interface Tool](https://arxiv.org/abs/2510.15642)
*Sian Brooke*

Main category: cs.SE

TL;DR: More women in open-source projects like React make the software more innovative and robust; their exclusion is detrimental, especially for features and dependencies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to move beyond merely acknowledging women's existence in open-source software and to critically examine how increased gender diversity—specifically, women's participation—might fundamentally change development processes and outcomes.

Method: The paper investigates gender differences among contributors to React—a major JavaScript UI library—by examining metrics of robustness, innovation, and shifting contribution patterns, especially around major version releases, over an 11-year period.

Result: The results show that excluding women adversely affects software, as women make considerable contributions to feature enhancements and dependency management compared to their male counterparts.

Conclusion: Gender inclusion positively affects open-source software innovation and robustness; encouraging greater participation of women leads to more effective, innovative, and resilient software projects like React.

Abstract: In open-source software design, the inclusion of women is often highlighted
simply to remind programmers that women exist. Yet, little attention is given
to how greater gender diversity, specifically women's participation, could
fundamentally alter development patterns. To understand the potential impact of
gender inclusion, this study investigates React, a widely used JavaScript
library for building user interfaces with an active contributor community. I
examine gender differences in metrics of robustness and innovation, as well as
shifts in contribution patterns leading up to major version releases over 11
years of the React project. My results show that the exclusion of women is
detrimental to software as women contribute significantly more to feature
enhancement and dependency management. By exploring how gender influences
innovation and robustness in the development of React, the study offers
critical insights into how increasing gender diversity could lead to more
inclusive, innovative, and robust software.

</details>


### [10] [MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework APIs Fuzzing](https://arxiv.org/abs/2510.15690)
*Shiwen Ou,Yuwei Li,Lu Yu,Chengkun Wei,Tingke Wen,Qiangpu Chen,Yu Chen,Haizhi Tang,Zulie Pan*

Main category: cs.SE

TL;DR: MirrorFuzz introduces a novel, automated API fuzzing approach to detect shared bugs in deep learning frameworks. Tested on four popular frameworks, it greatly increases code coverage and discovers hundreds of new bugs, demonstrating its utility in improving framework security and reliability.


<details>
  <summary>Details</summary>
Motivation: Deep learning frameworks are foundational for AI applications, but bugs in these frameworks can lead to severe reliability and security problems. Many frameworks share similar APIs, making them susceptible to shared vulnerabilities and bugs. Existing research has not sufficiently explored API commonalities and the risks they pose.

Method: The authors introduce MirrorFuzz, an automated tool for fuzzing APIs in deep learning frameworks. MirrorFuzz works in three stages: (1) it gathers historical bug data to identify buggy APIs in a framework; (2) it matches these APIs with similar ones across different frameworks; (3) it uses large language models to generate targeted test code aimed at reproducing and identifying analogous bugs across the matched APIs.

Result: MirrorFuzz was implemented and tested on TensorFlow, PyTorch, OneFlow, and Jittor. It improved code coverage by 39.92% on TensorFlow and 98.20% on PyTorch compared to the state-of-the-art. It discovered 315 bugs, 262 of which were new discoveries. Of the discovered bugs, 80 have been fixed so far and 52 have been assigned official CNVD IDs.

Conclusion: MirrorFuzz is highly effective in uncovering shared bugs across APIs in multiple deep learning frameworks, significantly outperforms existing methods in both coverage and new bug discovery, and contributes to improving the reliability of DL frameworks.

Abstract: Deep learning (DL) frameworks serve as the backbone for a wide range of
artificial intelligence applications. However, bugs within DL frameworks can
cascade into critical issues in higher-level applications, jeopardizing
reliability and security. While numerous techniques have been proposed to
detect bugs in DL frameworks, research exploring common API patterns across
frameworks and the potential risks they entail remains limited. Notably, many
DL frameworks expose similar APIs with overlapping input parameters and
functionalities, rendering them vulnerable to shared bugs, where a flaw in one
API may extend to analogous APIs in other frameworks. To address this
challenge, we propose MirrorFuzz, an automated API fuzzing solution to discover
shared bugs in DL frameworks. MirrorFuzz operates in three stages: First,
MirrorFuzz collects historical bug data for each API within a DL framework to
identify potentially buggy APIs. Second, it matches each buggy API in a
specific framework with similar APIs within and across other DL frameworks.
Third, it employs large language models (LLMs) to synthesize code for the API
under test, leveraging the historical bug data of similar APIs to trigger
analogous bugs across APIs. We implement MirrorFuzz and evaluate it on four
popular DL frameworks (TensorFlow, PyTorch, OneFlow, and Jittor). Extensive
evaluation demonstrates that MirrorFuzz improves code coverage by 39.92\% and
98.20\% compared to state-of-the-art methods on TensorFlow and PyTorch,
respectively. Moreover, MirrorFuzz discovers 315 bugs, 262 of which are newly
found, and 80 bugs are fixed, with 52 of these bugs assigned CNVD IDs.

</details>


### [11] [EASELAN: An Open-Source Framework for Multimodal Biosignal Annotation and Data Management](https://arxiv.org/abs/2510.15767)
*Rathi Adarshi Rammohan,Moritz Meier,Dennis Küster,Tanja Schultz*

Main category: cs.SE

TL;DR: EASELAN is a new annotation framework for complex multimodal biosignal datasets, enhancing ELAN with features like version control and streamlined annotation. It was applied successfully to collect and annotate biosignals from human activities (table setting) for cognitive robotics research, and both the code and annotated database are publicly released.


<details>
  <summary>Details</summary>
Motivation: There is a growing demand for large, richly annotated multimodal data driven by advances in machine learning and adaptive cognitive systems. Fusion models increasingly require handling multiple biosignals alongside audiovisual channels, necessitating sophisticated annotation workflows.

Method: This paper introduces the EASELAN annotation framework, which extends the existing ELAN tool with components for all stages of the annotation pipeline. EASELAN streamlines annotation file preparation, supports adding new channels, integrates version control, and simplifies post-processing. The framework was applied to a biosignals collection on table setting activities for cognitive robots.

Result: EASELAN was successfully utilized in annotating high-dimensional biosignal data for the EASE initiative on human everyday activities. The workflow facilitated seamless integration of multiple modalities and export of rich annotations for further analysis and machine learning. Both the framework's code and an annotated dataset are made publicly available.

Conclusion: EASELAN improves workflows for multimodal and biosignal data annotation, enabling efficient, integrated processing. Its use in an actual biosignal collection project demonstrates its applicability, and the publicly released code and data aim to support broader research.

Abstract: Recent advancements in machine learning and adaptive cognitive systems are
driving a growing demand for large and richly annotated multimodal data. A
prominent example of this trend are fusion models, which increasingly
incorporate multiple biosignals in addition to traditional audiovisual
channels. This paper introduces the EASELAN annotation framework to improve
annotation workflows designed to address the resulting rising complexity of
multimodal and biosignals datasets. It builds on the robust ELAN tool by adding
new components tailored to support all stages of the annotation pipeline: From
streamlining the preparation of annotation files to setting up additional
channels, integrated version control with GitHub, and simplified
post-processing. EASELAN delivers a seamless workflow designed to integrate
biosignals and facilitate rich annotations to be readily exported for further
analyses and machine learning-supported model training. The EASELAN framework
is successfully applied to a high-dimensional biosignals collection initiative
on human everyday activities (here, table setting) for cognitive robots within
the DFG-funded Collaborative Research Center 1320 Everyday Activity Science and
Engineering (EASE). In this paper we discuss the opportunities, limitations,
and lessons learned when using EASELAN for this initiative. To foster research
on biosignal collection, annotation, and processing, the code of EASELAN is
publicly available(https://github.com/cognitive-systems-lab/easelan), along
with the EASELAN-supported fully annotated Table Setting Database.

</details>


### [12] [Towards Supporting Open Source Library Maintainers with Community-Based Analytics](https://arxiv.org/abs/2510.15794)
*Rachna Raj,Diego Elias Costa*

Main category: cs.SE

TL;DR: OSS maintainers often lack feedback on how their libraries are used. An empirical study shows that most API methods are unused, and not all used methods are fully tested. The paper introduces metrics and survey results supporting analytics-based decisions for library testing and evolution.


<details>
  <summary>Details</summary>
Motivation: Open-source software libraries are crucial, but maintainers lack feedback on how their APIs are actually used by dependent projects. Understanding this usage can inform better decisions in testing and library evolution.

Method: The paper uses community-based analytics to examine OSS library usage across dependent ecosystems. An empirical study is performed on 10 popular Java libraries and 50 dependent projects for each library. Additionally, two new metrics are proposed to evaluate test coverage, and a survey of open-source practitioners is conducted.

Result: The study finds that only 16% of API methods are actively used by dependent projects, and merely 74% of those used methods are (at least partially) covered by library test suites. Open-source practitioners found the insights valuable for maintenance and decision making.

Conclusion: Providing maintainers with usage analytics helps them optimize testing strategies and guides the evolution of OSS libraries. The proposed metrics allow for evaluating test coverage relevant to actual API usage, and feedback from practitioners supports the value of these insights.

Abstract: Open-source software (OSS) is a pillar of modern software development. Its
success depends on the dedication of maintainers who work constantly to keep
their libraries stable, adapt to changing needs, and support a growing
community. Yet, they receive little to no continuous feedback on how the
projects that rely on their libraries actually use their APIs. We believe that
gaining these insights can help maintainers make better decisions, such as
refining testing strategies, understanding the impact of changes, and guiding
the evolution of their libraries more effectively. We propose the use of
community-based analytics to analyze how an OSS library is used across its
dependent ecosystem. We conduct an empirical study of 10 popular Java libraries
and each with their respective dependent ecosystem of 50 projects. Our results
reveal that while library developers offer a wide range of API methods, only
16% on average are actively used by their dependent ecosystem. Moreover, only
74% of the used API methods are partially or fully covered by their library
test suite. We propose two metrics to help developers evaluate their test suite
according to the APIs used by their community, and we conduct a survey on
open-source practitioners to assess the practical value of these insights in
guiding maintenance decisions.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Visualizing miniKanren Search with a Fine-Grained Small-Step Semantics](https://arxiv.org/abs/2510.15178)
*Brysen Pfingsten,Jason Hemann*

Main category: cs.PL

TL;DR: The paper introduces a detailed operational semantics and interactive visualizer for miniKanren, making its search process and operational effects more understandable for users through step-by-step visualization and validation.


<details>
  <summary>Details</summary>
Motivation: There is a need for a precise and granular understanding of miniKanren's operational behavior, particularly its interleaving search and goal scheduling, to aid users in reasoning about surprising behaviors and answer orders.

Method: The authors define a deterministic small-step operational semantics for miniKanren that explicitly models the evolving search tree during program execution, capturing each step like activation, suspension, resumption, and success. They build an interactive visualizer based on this semantics.

Result: A working visualizer is developed, allowing users to step through and visualize the execution of miniKanren programs. The tool and abstract semantics are validated via property-based testing and demonstrated with various examples.

Conclusion: Their approach provides an effective pedagogical tool (notional machine) that enhances understanding and reasoning about the fair search behavior and operational characteristics of miniKanren, validated through both testing and illustrative examples.

Abstract: We present a deterministic small-step operational semantics for miniKanren
that explicitly represents the evolving search tree during execution. This
semantics models interleaving and goal scheduling at fine granularity, allowing
each evaluation step-goal activation, suspension, resumption, and success -- to
be visualized precisely. Building on this model, we implement an interactive
visualizer that renders the search tree as it develops and lets users step
through execution. The tool acts as a pedagogical notional machine for
reasoning about miniKanren's fair search behavior, helping users understand
surprising answer orders and operational effects. Our semantics and tool are
validated through property-based testing and illustrated with several examples.

</details>


### [14] [Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language](https://arxiv.org/abs/2510.15747)
*Ehud Shapiro*

Main category: cs.PL

TL;DR: The paper introduces Grassroots Logic Programs (GLP), a new secure logic programming language designed for implementing distributed grassroots platforms (like social networks and cryptocurrencies) on personal devices. It solves key security challenges via cryptographic identities, secure messaging, and proofs of safety. They provide protocols and demo applications showing secure, authenticated connections and blockchain-like properties, making GLP a robust foundation for future grassroots platforms.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enable the creation of secure, distributed grassroots platforms (such as social networks and cryptocurrencies) that are run on individuals' personal devices by cryptographically-identified participants. Existing challenges include malicious or faulty participants and the need for secure programming support to ensure secure communication, reliable identity verification, and code integrity.

Method: The paper introduces Grassroots Logic Programs (GLP), a secure, multiagent, concurrent logic programming language. GLP uses paired single-reader/single-writer (SRSW) logic variables for secure communication between participants, enabling encrypted, signed, and attested messaging for identity and code integrity verification. The language is presented in stages (concurrent GLP, multiagent GLP, security augmentation), with implementation specifications suitable for smartphones. Formal proofs of important safety properties are provided.

Result: The paper proves safety properties of GLP including that its computations are deductions, SRSW preservation, acyclicity, and monotonicity. It is shown that multiagent GLP maintains the grassroots property, and GLP streams have blockchain-like security characteristics. The authors also present a protocol to establish authenticated peer-to-peer connections and successfully demonstrate secure grassroots social networking applications.

Conclusion: GLP is a novel language that enables secure implementation of grassroots platforms with strong guarantees of identity, code integrity, and secure communication. It addresses prior security challenges and provides a foundation for implementing decentralized, secure applications on personal devices.

Abstract: Grassroots platforms are distributed applications run by\linebreak
cryptographically-identified people on their networked personal devices, where
multiple disjoint platform instances emerge independently and coalesce when
they interoperate. Their foundation is the grassroots social graph, upon which
grassroots social networks, grassroots cryptocurrencies, and grassroots
democratic federations can be built.
  Grassroots platforms have yet to be implemented, the key challenge being
faulty and malicious participants: without secure programming support, correct
participants cannot reliably identify each other, establish secure
communication, or verify each other's code integrity.
  We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent,
logic programming language for implementing grassroots platforms. GLP extends
logic programs with paired single-reader/single-writer (SRSW) logic variables,
providing secure communication channels among cryptographically-identified
people through encrypted, signed and attested messages, which enable identity
and code integrity verification. We present GLP progressively: logic programs,
concurrent GLP, multiagent GLP, augmenting it with cryptographic security, and
providing smartphone implementation-ready specifications. We prove safety
properties including that GLP computations are deductions, SRSW preservation,
acyclicity, and monotonicity. We prove multiagent GLP is grassroots and that
GLP streams achieve blockchain security properties. We present a grassroots
social graph protocol establishing authenticated peer-to-peer connections and
demonstrate secure grassroots social networking applications.

</details>
