<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Technique to Baseline QE Artefact Generation Aligned to Quality Metrics](https://arxiv.org/abs/2511.15733)
*Eitan Farchi,Kiran Nayak,Papia Ghosh Majumdar,Saritha Route*

Main category: cs.SE

TL;DR: LLMs can automate Quality Engineering artefact generation, but quality assurance is challenging. This paper presents a systematic method combining LLM generation, reverse generation, and rubric-based refinement to evaluate and improve artefacts using quantitative metrics. Results across 12 projects show the approach can enhance artefact quality, supporting scalable yet reliable QE automation.


<details>
  <summary>Details</summary>
Motivation: Quality Engineering (QE) is increasingly adopting Large Language Models (LLMs) to automate the creation of critical artefacts like requirements and test cases. Despite these advancements, assuring the quality of LLM-generated artefacts is still problematic.

Method: The paper introduces a systematic technique for baselining and evaluating QE artefacts, utilizing LLM-driven generation, reverse generation, and iterative refinement. Evaluations are guided by rubrics focusing on clarity, completeness, consistency, and testability. The approach includes quantitative metrics to assess quality.

Result: Experimental analysis on 12 projects demonstrates that reverse-generated artefacts can surpass low-quality originals and maintain high standards when the original inputs are strong.

Conclusion: The proposed framework enables scalable, automated, yet accountable validation of QE artefacts. It effectively bridges automation and quality assurance, offering reliable evaluation and improvement for artefact quality.

Abstract: Large Language Models (LLMs) are transforming Quality Engineering (QE) by automating the generation of artefacts such as requirements, test cases, and Behavior Driven Development (BDD) scenarios. However, ensuring the quality of these outputs remains a challenge. This paper presents a systematic technique to baseline and evaluate QE artefacts using quantifiable metrics. The approach combines LLM-driven generation, reverse generation , and iterative refinement guided by rubrics technique for clarity, completeness, consistency, and testability. Experimental results across 12 projects show that reverse-generated artefacts can outperform low-quality inputs and maintain high standards when inputs are strong. The framework enables scalable, reliable QE artefact validation, bridging automation with accountability.

</details>


### [2] [Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym](https://arxiv.org/abs/2511.15757)
*Kareem Shehada,Yifan Wu,Wyatt D. Feng,Adithya Iyer,Gryphon Kumfert,Yangruibo Ding,Zhiyun Qian*

Main category: cs.SE

TL;DR: This paper presents RGym, a lightweight framework for automated Linux kernel bug repair using local hardware and LLMs. Their approach achieves strong repair rates at low cost, outperforming previous solutions thanks to improved localization and retry techniques.


<details>
  <summary>Details</summary>
Motivation: Existing automated program repair (APR) tools and benchmarks focus primarily on userspace applications, neglecting the more challenging kernel-space. The Linux kernel's complexity, including its monolithic design, concurrency, and hardware interactions, makes APR difficult. Previous solutions have low success rates and require expensive infrastructure.

Method: The authors introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel. RGym works on local commodity hardware. The proposed APR pipeline uses specialized localization techniques (such as call stacks and blamed commits) to replace unrealistic oracles. They test their method on a carefully filtered dataset of 143 verified kernel bugs, and conduct ablation studies to isolate contributions from different components and retry strategies.

Result: Their method achieved up to a 43.36% bug pass rate using GPT-5 Thinking, with repair costs kept under $0.20 per bug. The ablation study shows that localization strategy, prompt structure, and model choice materially affect success, and that feedback-based retries can further improve outcomes.

Conclusion: The paper demonstrates that a simple, cost-effective APR pipeline using RGym and LLMs can substantially improve kernel bug repair success rates compared to prior approaches, and that careful localization and feedback mechanisms are key.

Abstract: Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.

</details>


### [3] [A Causal Perspective on Measuring, Explaining and Mitigating Smells in \llm-Generated Code](https://arxiv.org/abs/2511.15817)
*Alejandro Velasco,Daniel Rodriguez-Cardenas,Dipin Khati,David N. Palacio,Luftar Rahman Alif,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: The paper analyzes why and how code smells occur in LLM-generated code by using a probabilistic metric (PSC), finding that model architecture and prompts play key roles. PSC helps both measure and reduce code smells, offering a foundation for quality-aware assessments in code-generating LLMs.


<details>
  <summary>Details</summary>
Motivation: There is growing adoption of large language models (LLMs) in software engineering, but concerns remain regarding the quality of code they generate. Specifically, LLMs often introduce 'code smells'—undesirable patterns that compromise code readability, maintainability, or design. Existing research has not yet clearly elucidated when and why these issues arise in generated code.

Method: The paper uses the Propensity Smelly Score (PSC), a probabilistic metric for estimating code smell likelihood, as a basis for systematic analysis. The authors conduct causal analysis to examine the impact of generation strategies, model size, architecture, and prompt formulation on code smell occurrence. User studies are also performed to assess PSC's utility in supporting human judgement of code quality.

Result: Prompt design and model architecture significantly influence the likelihood of code smells in generated content. The use of the PSC metric enables identification of these factors and supports practical mitigation strategies. The study also finds that PSC helps users interpret model behavior and evaluate code quality effectively.

Conclusion: PSC is a robust indicator of structural code quality in LLM-generated code. Its use reveals key drivers of code smell propensity and supports actionable strategies to improve code output. Incorporating PSC and similar metrics into LLM evaluation pipelines can strengthen code quality assessment and support developer decision-making.

Abstract: Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.
  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.

</details>


### [4] [AI-Enabled Orchestration of Event-Driven Business Processes in Workday ERP for Healthcare Enterprises](https://arxiv.org/abs/2511.15852)
*Monu Sharma*

Main category: cs.SE

TL;DR: By integrating AI-driven orchestration into Workday ERP, healthcare organizations can automate responses to operational events, resulting in improved efficiency, visibility, and decision-making. The proposed framework demonstrates that AI-enhanced ERP systems provide increased adaptability and scalability for complex healthcare environments.


<details>
  <summary>Details</summary>
Motivation: Cloud-based ERP platforms like Workday have improved healthcare operations by unifying processes, but traditional ERP workflows lack the flexibility needed to handle the complex, event-driven, and data-heavy environment of healthcare.

Method: The paper proposes an AI-enabled, event-driven orchestration framework embedded in Workday ERP. The framework uses machine learning triggers, anomaly detection, and process mining analytics to automate and optimize responses to operational events. The approach's effectiveness is illustrated through a multi-organization case analysis.

Result: The case analysis demonstrated improvements in process efficiency, cost visibility, and decision accuracy when using the proposed AI-driven framework. Integrating AI with ERP systems led to better operational resilience, governance, and scalability.

Conclusion: Embedding AI into event-driven architectures within Workday ERP enhances intelligent automation for healthcare enterprises, offering a reference model for future ERP integration and automation strategies.

Abstract: The adoption of cloud-based Enterprise Resource Planning (ERP) platforms such as Workday has transformed healthcare operations by integrating financial, supply-chain, and workforce processes into a unified ecosystem. However, traditional workflow logic in ERP systems often lacks the adaptability required to manage event-driven and data-intensive healthcare environments.
  This study proposes an AI-enabled event-driven orchestration framework within Workday ERP that intelligently synchronizes financial and supply-chain workflows across distributed healthcare entities. The framework employs machine-learning triggers, anomaly detection, and process mining analytics to anticipate and automate responses to operational events such as inventory depletion, payment delays, or patient demand fluctuations. A multi-organization case analysis demonstrates measurable gains in process efficiency, cost visibility, and decision accuracy.
  Results confirm that embedding AI capabilities into Workday's event-based architecture enhances operational resilience, governance, and scalability. The proposed model contributes to the broader understanding of intelligent ERP integration and establishes a reference for next-generation automation strategies in healthcare enterprises.

</details>


### [5] [RE for AI in Practice: Managing Data Annotation Requirements for AI Autonomous Driving Systems](https://arxiv.org/abs/2511.15859)
*Hina Saeeda,Mazen Mohamad,Eric Knauss,Jennifer Horkoff,Ali Nouri*

Main category: cs.SE

TL;DR: This paper empirically investigates data annotation requirements for AI-enabled perception systems in autonomous driving by analyzing interviews with industry and research experts. It highlights key challenges and best practices, showing how poor requirements negatively affect system performance and safety, and provides actionable guidelines to improve annotation quality and reliability in the emerging fields of SE for AI and RE for AI.


<details>
  <summary>Details</summary>
Motivation: High-quality data annotation is critical for safe and reliable AI perception systems in autonomous driving. However, the formulation and management of annotation requirements are not well-explored, causing inconsistencies, safety issues, and regulatory concerns.

Method: The authors conducted 19 semi-structured interviews with practitioners from six international companies and four research organizations. They used thematic analysis to identify key challenges and best practices in data annotation requirements.

Result: The analysis revealed five key challenges: ambiguity, edge case complexity, evolving requirements, inconsistencies, and resource constraints. Best practices include compliance with ethical standards, improved guidelines, and embedded quality assurance. The study shows how annotation requirement flaws impact the entire AI development pipeline.

Conclusion: This is the first empirically grounded study to provide actionable insights for improving annotation requirements in AI-enabled perception systems. The findings help enhance annotation quality, system reliability, and regulatory compliance, also bridging gaps between Software Engineering and Requirements Engineering for AI.

Abstract: High-quality data annotation requirements are crucial for the development of safe and reliable AI-enabled perception systems (AIePS) in autonomous driving. Although these requirements play a vital role in reducing bias and enhancing performance, their formulation and management remain underexplored, leading to inconsistencies, safety risks, and regulatory concerns. Our study investigates how annotation requirements are defined and used in practice, the challenges in ensuring their quality, practitioner-recommended improvements, and their impact on AIePS development and performance. We conducted $19$ semi-structured interviews with participants from six international companies and four research organisations. Our thematic analysis reveals five main key challenges: ambiguity, edge case complexity, evolving requirements, inconsistencies, and resource constraints and three main categories of best practices, including ensuring compliance with ethical standards, improving data annotation requirements guidelines, and embedded quality assurance for data annotation requirements. We also uncover critical interrelationships between annotation requirements, annotation practices, annotated data quality, and AIePS performance and development, showing how requirement flaws propagate through the AIePS development pipeline. To the best of our knowledge, this study is the first to offer empirically grounded guidance on improving annotation requirements, offering actionable insights to enhance annotation quality, regulatory compliance, and system reliability. It also contributes to the emerging fields of Software Engineering (SE for AI) and Requirements Engineering (RE for AI) by bridging the gap between RE and AI in a timely and much-needed manner.

</details>


### [6] [InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution](https://arxiv.org/abs/2511.16004)
*KeFan Li,Mengfei Wang,Hengzhi Zhang,Zhichao Li,Yuan Yuan,Mu Li,Xiang Gao,Hailong Sun,Chunming Hu,Weifeng Lv*

Main category: cs.SE

TL;DR: InfCode is a new adversarial multi-agent framework that improves automated software issue resolution by making test and code patch generation compete, resulting in more trustworthy fixes. It achieves a new state-of-the-art on SWE-bench Verified and is publicly available open source.


<details>
  <summary>Details</summary>
Motivation: Current methods for automating software issue resolution often result in patches that pass tests but do not truly resolve the underlying problems, due to inadequate verification and repository-level reasoning.

Method: The authors introduce InfCode, an adversarial multi-agent system comprising a Test Patch Generator and a Code Patch Generator that iteratively challenge each other to improve both tests and patches. A Selector agent identifies the best fix. The framework operates in a containerized environment, enabling realistic inspection, modification, and validation of software repositories.

Result: InfCode outperforms strong baseline methods for automated issue resolution, achieving 79.4% on the SWE-bench Verified benchmark, which sets a new state-of-the-art standard.

Conclusion: Adversarial interaction between testing and patching agents, combined with robust repository-level verification, leads to more reliable automated software issue resolution. InfCode is released as an open-source tool.

Abstract: Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.

</details>


### [7] [InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution](https://arxiv.org/abs/2511.16005)
*Qingao Dong,Mengfei Wang,Hengzhi Zhang,Zhichao Li,Yuan Yuan,Mu Li,Xiang Gao,Hailong Sun,Chunming Hu,Weifeng Lv*

Main category: cs.SE

TL;DR: INFCODE-C++ introduces a novel C++-specific LLM agent combining semantic and structured retrieval to solve issues in C++ code repositories, significantly outperforming existing Python-oriented approaches and highlighting the need for language-aware AI agents.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents perform well on Python code repositories but struggle with C++ projects due to complex language constructs like overloaded identifiers, nested namespaces, template instantiations, and deep control-flow, making context retrieval and fault localization difficult.

Method: The authors propose INFCODE-C++, an autonomous C++-aware system for repository-level issue resolution. It employs two complementary retrieval mechanisms: semantic code-intent retrieval and deterministic AST-structured querying, allowing accurate and contextual understanding specific to C++.

Result: INFCODE-C++ achieves a 25.58% resolution rate on MultiSWE-bench-CPP, outperforming the previous best agent by 10.85 percentage points and more than doubling the performance of the MSWE-agent. Ablation studies highlight the importance of semantic retrieval and structural analysis.

Conclusion: INFCODE-C++ demonstrates the necessity of language-aware reasoning for effective LLM-driven code repair in complex, statically typed languages like C++, setting the stage for future research.

Abstract: Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.

</details>


### [8] [The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report](https://arxiv.org/abs/2511.16092)
*Xing Hu,Raula Gaikovina Kula,Christoph Treude*

Main category: cs.SE

TL;DR: Experts convened to discuss how generative AI will revolutionize coding through IDEs, identifying both opportunities for smarter workflows and challenges around human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: Generative AI models are rapidly improving and can significantly change how humans interact with software development environments. The motivation is to understand and explore the implications of these advancements for developers' workflows within IDEs.

Method: A workshop-style gathering (Shonan Meeting 222) brought together 33 experts from software engineering, AI, and HCI to collaboratively discuss, identify, and analyze the challenges and opportunities resulting from integrating GenAI into IDEs.

Result: The discussions identified key challenges and opportunities regarding how GenAI could reshape the way developers interact with IDEs, focusing on abstraction, automation, and human-AI collaboration.

Conclusion: Generative AI is set to transform IDE interactions by raising abstraction levels and redefining developer workflows. Addressing emerging challenges and seizing new opportunities will require interdisciplinary collaboration and careful consideration of human factors.

Abstract: Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report

</details>


### [9] [Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions](https://arxiv.org/abs/2511.16123)
*Linyi Han,Shidong Pan,Zhenchang Xing,Sofonias Yitagesu,Xiaowang Zhang,Zhiyong Feng,Jiamou Sun,Qing Huang*

Main category: cs.SE

TL;DR: A new three-stage LLM-based framework unifies inconsistent vulnerability descriptions from various sources, improving detail extraction and synthesis. It boosts F1 scores and analyst efficiency, and a visualization tool further aids usability.


<details>
  <summary>Details</summary>
Motivation: Textual vulnerability descriptions (TVDs) from various sources often differ in key aspects, causing difficulty for security analysts trying to achieve a comprehensive understanding of software vulnerabilities. Existing solutions that align TVDs with external knowledge tend to discard useful data and don't synthesize complete representations.

Method: The authors propose a three-stage domain-constrained LLM-based synthesis framework: (1) Extraction guided by rule-based templates to capture all critical details; (2) Self-evaluation using domain-specific anchor words to measure semantic variability across sources; and (3) Fusion employing information entropy to reconcile inconsistencies and prioritize relevant information.

Result: The framework improves key aspect augmentation, raising the F1 score from 0.82 to 0.87, and boosts comprehension and efficiency by over 30%. Additionally, the Digest Labels tool is developed for visualizing TVDs, and human evaluations confirm it increases usability significantly.

Conclusion: The proposed LLM-based synthesis framework effectively unifies key aspects of TVDs, enhances comprehension, and supports more efficient analysis for security professionals. The Digest Labels visualization tool further augments usability of TVDs in practice.

Abstract: Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.

</details>


### [10] [Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts](https://arxiv.org/abs/2511.16224)
*Francesco Salzano,Simone Scalabrino,Rocco Oliveto,Simone Scalabrino*

Main category: cs.SE

TL;DR: LLMs can generate Solidity code that looks similar to real Smart Contracts, but often lacks correct logic and validation—only a minority of zero-shot generations are functionally correct. Retrieval-augmented approaches help, but expert review remains necessary for production use.


<details>
  <summary>Details</summary>
Motivation: Smart Contracts require high reliability and have distinct constraints (e.g., gas efficiency, security) which aren't fully addressed by generic code generation with Large Language Models (LLMs). Previous studies haven't comprehensively evaluated LLMs on these critical properties within Solidity code.

Method: The authors benchmarked four leading LLMs on 500 real-world Solidity functions, using both zero-shot and retrieval-augmented (RAG) approaches. Evaluation was multi-dimensional: code similarity, semantic embeddings, automated functional tests, gas usage, and cognitive/cyclomatic complexity analysis.

Result: LLMs generate Solidity code with high semantic similarity to real functions, but only 20%-26% of zero-shot outputs are functionally correct under automated tests. The generated code is typically simpler and more gas-efficient due to omitted validation. RAG notably improves functional correctness to up to 45% and produces more concise code, but significant reliability gaps remain.

Conclusion: RAG substantially enhances LLM-generated Solidity code reliability, but a large gulf exists between semantic similarity and true functional correctness. Careful expert validation is still essential before considering LLM-generated contracts production-ready.

Abstract: Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.

</details>


### [11] [Data Annotation Quality Problems in AI-Enabled Perception System Development](https://arxiv.org/abs/2511.16410)
*Hina Saeeda,Tommy Johansson,Mazen Mohamad,Eric Knauss*

Main category: cs.SE

TL;DR: Annotation errors in AI for automated driving are widespread and poorly understood across supply chains. This paper conducts a multi-organization study, interviews experts, and creates a validated taxonomy of 18 error types. The taxonomy helps organizations improve annotation practices, diagnostics, and guidelines, promoting safer, more reliable perception systems.


<details>
  <summary>Details</summary>
Motivation: Data annotation errors significantly impact the performance, safety, and reliability of AI-enabled perception systems for automated driving, yet there is little empirical understanding in the industry of how these errors originate and propagate across the automotive supply chain.

Method: The study conducted a multi-organisation case study involving six companies and four research institutes across Europe and the UK. It involved 19 semi-structured interviews with 20 experts, generating 50 hours of transcripts, followed by a six-phase thematic analysis.

Result: The authors developed a taxonomy of 18 recurring annotation error types spanning completeness, accuracy, and consistency. This taxonomy was validated with industry practitioners, who found it useful for root-cause analysis, supplier quality reviews, onboarding, and refining annotation guidelines.

Conclusion: Annotation quality should be conceptualized as a lifecycle and supply-chain issue. The provided taxonomy serves as a shared vocabulary and diagnostic toolset for stakeholder organizations, facilitating actionable improvements in trustworthy AI-enabled perception systems.

Abstract: Data annotation is essential but highly error-prone in the development of AI-enabled perception systems (AIePS) for automated driving, and its quality directly influences model performance, safety, and reliability. However, the industry lacks empirical insights into how annotation errors emerge and spread across the multi-organisational automotive supply chain. This study addresses this gap through a multi-organisation case study involving six companies and four research institutes across Europe and the UK. Based on 19 semi-structured interviews with 20 experts (50 hours of transcripts) and a six-phase thematic analysis, we develop a taxonomy of 18 recurring annotation error types across three data-quality dimensions: completeness (e.g., attribute omission, missing feedback loops, edge-case omissions, selection bias), accuracy (e.g., mislabelling, bounding-box inaccuracies, granularity mismatches, bias-driven errors), and consistency (e.g., inter-annotator disagreement, ambiguous instructions, misaligned hand-offs, cross-modality inconsistencies). The taxonomy was validated with industry practitioners, who reported its usefulness for root-cause analysis, supplier quality reviews, onboarding, and improving annotation guidelines. They described it as a failure-mode catalogue similar to FMEA. By conceptualising annotation quality as a lifecycle and supply-chain issue, this study contributes to SE4AI by offering a shared vocabulary, diagnostic toolset, and actionable guidance for building trustworthy AI-enabled perception systems.

</details>


### [12] [Green Resilience of Cyber-Physical Systems: Doctoral Dissertation](https://arxiv.org/abs/2511.16593)
*Diaeddin Rimawi*

Main category: cs.SE

TL;DR: This paper tackles how to balance resilience (quick recovery) and greenness (low CO2 emissions) in Online Collaborative AI Systems using the GResilience framework and agent-based recovery policies. Experiments show the strategies speed up recovery, stabilize performance, and reduce human involvement, but RL policies slightly raise emissions. Containerization halves emissions. Provided solutions ensure OL-CAIS recover reliably and efficiently during disruptions.


<details>
  <summary>Details</summary>
Motivation: Online Collaborative AI Systems (OL-CAIS), as cyber-physical systems, regularly collaborate with humans to achieve goals but are susceptible to performance degradation from disruptive events. There is a need to recover system performance effectively, balancing resilience (quick recovery) and greenness (energy efficiency/low CO2 emissions). Ensuring this balance is crucial for sustaining long-term collaborative AI operations.

Method: The research models OL-CAIS through three operational states (steady, disruptive, final) and introduces the GResilience framework for green and resilient recovery. The framework deploys multi-objective optimization (for one agent), game-theoretic decision-making (for two agents), and reinforcement learning (for RL agents) to optimize recovery strategies. The paper also presents a measurement system for resilience and greenness and validates the approaches with experiments using collaborative robots learning from humans.

Result: The resilience model yields effective detection of performance transitions, while GResilience framework policies (especially RL-agent based) significantly improve recovery time, stabilize performance, and decrease human dependency, improving resilience. However, RL-agent policies slightly increase CO2 emissions. Catastrophic forgetting is noted after repeated disruptions, but the designed policies moderate its negative impact. Containerized execution reduces CO2 emissions by 50% compared to standard setup.

Conclusion: The study provides new models, metrics, and agent-driven policies that facilitate the green and resilient recovery of OL-CAIS. While RL agents optimize recovery, they marginally trade-off increased CO2 emissions. Containerization is recommended for greener execution. The developed frameworks help balance resilience and energy efficiency, with empirical validation supporting their effectiveness.

Abstract: Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Filling the Gaps of Polarity: Implementing Dependent Data and Codata Types with Implicit Arguments](https://arxiv.org/abs/2511.15819)
*Bohdan Liesnikov,David Binder,Tim Süberkrüb*

Main category: cs.PL

TL;DR: The paper addresses the expression problem's dual extensibility in type systems, proposing an implicit argument inference algorithm for Polarity, a language treating inductive and coinductive types equally. It provides full type system algorithms, implements unification for both type modes, and aims to guide future language designs.


<details>
  <summary>Details</summary>
Motivation: The motivation is rooted in the expression problem, which involves the tradeoff between extensibility in type systems: either by adding new operations (common in functional programming) or new data constructors (common in object-oriented programming). Most dependently typed languages favor extensibility via operations (inductive types) but lack solid support for constructors (coinductive types).

Method: The paper develops a new algorithmic type system and an inference algorithm for implicit arguments in Polarity, a language that treats inductive and coinductive types symmetrically. The work details reduction semantics, conversion checking, and unification rules for pattern matching across both kinds of types.

Result: The paper delivers two major outcomes: 1) a full algorithmic description of the Polarity type system, and 2) a comprehensive unification algorithm that covers both inductive and coinductive types. These contributions are implemented as a work-in-progress language, Polarity, with source available for inspection.

Conclusion: This work helps bridge the gap in dependently typed languages by providing a practical blueprint for handling implicit arguments and type unification across both inductive and coinductive types. It could inform future language designs seeking balanced support for these extensibility modes.

Abstract: The expression problem describes a fundamental tradeoff between two types of extensibility: extending a type with new operations, such as by pattern matching on an algebraic data type in functional programming, and extending a type with new constructors, such as by adding a new object implementing an interface in object-oriented programming. Most dependently typed languages have good support for the former style through inductive types, but support for the latter style through coinductive types is usually much poorer. Polarity is a language that treats both kinds of types symmetrically and allows the developer to switch between type representations.However, it currently lacks several features expected of a state-of-the-art dependently typed language, such as implicit arguments. The central aim of this paper is to provide an algorithmic type system and inference algorithm for implicit arguments that respect the core symmetry of the language. Our work provides two key contributions: a complete algorithmic description of the type system backing Polarity, and a comprehensive description of a unification algorithm that covers arbitrary inductive and coinductive types. We give rules for reduction semantics, conversion checking, and a unification algorithm for pattern-matching, which are essential for a usable implementation. A work-in-progress implementation of the algorithms in this paper is available at https://polarity-lang.github.io/. We expect that the comprehensive account of the unification algorithm and our design decisions can serve as a blueprint for other dependently typed languages that support inductive and coinductive types symmetrically.

</details>


### [14] [Chorex: Restartable, Language-Integrated Choreographies](https://arxiv.org/abs/2511.15820)
*Ashton Wiersdorf,Ben Greenman*

Main category: cs.PL

TL;DR: Chorex is a new language extension for Elixir that enables fault-tolerant distributed programming through choreographic techniques. It handles actor crashes by restoring state and rejoining the network, integrates tightly with Elixir, reports errors clearly, and has a projection strategy that could be ported to other languages.


<details>
  <summary>Details</summary>
Motivation: Building robust distributed applications in languages like Elixir remains challenging, especially with respect to handling actor failures and ensuring tight integration between choreography and the host language. Existing approaches are limited when dealing with failures and often lack seamless error reporting or integration with modern programming environments.

Method: The authors designed and implemented a new language called Chorex that brings choreographic programming to Elixir through metaprogramming. They handle actor failures using a mechanism that spawns replacement processes, restores their states from checkpoints, and updates network configurations dynamically. The system checks for discrepancies between choreography and actor implementations statically, and error feedback is integrated into the developer's source code context. They demonstrate Chorex's features on several use case examples, and measure the performance overhead of its checkpointing approach.

Result: Chorex tolerates actor failures by automatically respawning crashed actors and restoring their states, which helps create robust distributed systems. The metaprogramming-based implementation allows seamless interaction with Elixir and gives clear, source-based error reporting. Chorex is demonstrated on a range of distributed system examples, and the overhead for state checkpointing is quantified. The projection strategy, generating stateless function sets, is argued to be applicable in other host languages.

Conclusion: Chorex demonstrates that robust, failure-tolerant choreographic programming can be achieved in Elixir using metaprogramming. Its error handling, state management, and language integration mechanisms are effective, and its projection strategy could inspire similar solutions in other programming languages.

Abstract: We built Chorex, a language that brings choreographic programming to Elixir as a path toward robust distributed applications. Chorex is unique among choreographic languages because it tolerates failure among actors: when an actor crashes, Chorex spawns a new process, restores state using a checkpoint, and updates the network configuration for all actors. Chorex also proves that full-featured choreographies can be implemented via metaprogramming, and that doing so achieves tight integration with the host language. For example, mismatches between choreography requirements and an actor implementation are reported statically and in terms of source code rather than macro-expanded code. This paper illustrates Chorex on several examples, ranging from a higher-order bookseller to a secure remote password protocol, details its implementation, and measures the overhead of checkpointing. We conjecture that Chorex's projection strategy, which outputs sets of stateless functions, is a viable approach for other languages to support restartable actors.

</details>


### [15] [BlueScript: A Disaggregated Virtual Machine for Microcontrollers](https://arxiv.org/abs/2511.15821)
*Fumika Mochizuki,Tetsuro Yamazaki,Shigeru Chiba*

Main category: cs.PL

TL;DR: This paper presents BlueScript, a VM for microcontrollers that offloads most components to a host machine, using a 'shadow machine' to maintain low communication overhead. Experiments show faster execution than MicroPython and Espruino, with good interactivity, proving that rich VM features are feasible for devices with limited memory.


<details>
  <summary>Details</summary>
Motivation: Microcontroller development using virtual machines is challenging due to limited memory, resulting in restricted features, poor interactivity, and sluggish execution speed. Existing approaches to offloading components from VMs are limited and don’t fully exploit host resources.

Method: The paper proposes a disaggregated VM architecture, specifically designing and implementing the BlueScript VM. This VM offloads most components to a host machine, leveraging its abundant memory and processing power. To minimize communication overhead, a 'shadow machine' data structure on the host mirrors the microcontroller's execution state.

Result: Experiments showed that offloading VM components does not significantly reduce expected benefits. The offloaded incremental compiler yields faster execution speed than MicroPython and Espruino. The interactive capability remains comparable to MicroPython, and the offloaded dynamic compiler further improves performance. Overall, rich features are feasible even for memory-limited microcontrollers.

Conclusion: Disaggregated VM architectures, like the BlueScript VM, can provide both high performance and interactive features for microcontroller development, overcoming the memory limitations typical of microcontroller environments.

Abstract: Virtual machines (VMs) are highly beneficial for microcontroller development. 
In particular, interactive programming environments greatly facilitate iterative development processes, 
and higher execution speeds expand the range of applications that can be developed. 
However, due to their limited memory size, microcontroller VMs provide a limited set of features. 
Widely used VMs for microcontrollers often lack interactive responsiveness and/or high execution speed. 
While researchers have investigated offloading certain VM components to other machines,the types of components that can be offloaded are still restricted. 
In this paper, we propose a disaggregated VM that offloads as many components as possible to a host machine. 
This makes it possible to exploit the abundant memory of the host machine and its powerful processing capability to provide rich features through the VM. 
As an instance of a disaggregated VM, we design and implement a BlueScript VM. 
The BlueScript VM is a virtual machine for microcontrollers that provides an interactive development environment. 
We offload most of the components of the BlueScript VM to a host machine. 
To reduce communication overhead between the host machine and the microcontroller,  
we employed a data structure called a shadow machine on the host machine, 
which mirrors the execution state of the microcontroller. 
Through our experiments, we confirmed that offloading components does not seriously compromise their expected benefits.  
We assess that an offloaded incremental compiler results in faster execution speed than MicroPython and Espruino,  
while keeping interactivity comparable with MicroPython.  
In addition, our experiments observe that the offloaded dynamic compiler improves VM performance. 
Through this investigation, we demonstrate the feasibility of providing rich features even on VMs for memory-limited microcontrollers.

</details>


### [16] [Operon: Incremental Construction of Ragged Data via Named Dimensions](https://arxiv.org/abs/2511.16080)
*Sungbin Moon,Jiho Park,Suyoung Hwang,Donghyun Koh,Seunghyun Moon,Minhyeong Lee*

Main category: cs.PL

TL;DR: Operon is a Rust-based workflow engine designed for workflows with ragged, variable-shaped data. It introduces novel concepts for managing data shapes and dependencies, provides static correctness guarantees, efficient parallel execution, and robust persistence. Empirical results show substantial performance improvements, making it ideal for complex machine learning pipelines.


<details>
  <summary>Details</summary>
Motivation: Modern data processing workflows often deal with ragged data—collections where elements can vary in length. Such data are common in areas like NLP, scientific measurements, and autonomous AI, but current workflow engines do not natively manage the complexity and dependencies of ragged data, requiring manual intervention from users.

Method: The authors introduce Operon, a workflow engine written in Rust. Operon uses a new formalism involving named dimensions and explicit dependency relations. It provides a domain-specific language for pipeline specification with statically verified dimension annotations. The runtime supports dynamic scheduling based on the discovery of data shapes and guarantees deterministic, confluent execution in parallel through a formally proven algorithm. The system also features robust persistence and recovery through explicit state modeling, and efficient parallelism via a multi-queue architecture.

Result: Empirical results show Operon reduces baseline overhead by 14.94 times compared to an existing workflow engine and achieves near-linear scalability in output rates as task load increases.

Conclusion: Operon’s explicit support for ragged data, correctness verification, and parallel efficiency makes it highly suitable for large-scale, data-driven machine learning pipelines. It addresses the limitations of existing engines and is robust in both performance and reliability.

Abstract: Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements, and autonomous AI agents. Existing workflow engines lack native support for tracking the shapes and dependencies inherent to ragged data, forcing users to manage complex indexing and dependency bookkeeping manually. We present Operon, a Rust-based workflow engine that addresses these challenges through a novel formalism of named dimensions with explicit dependency relations. Operon provides a domain-specific language where users declare pipelines with dimension annotations that are statically verified for correctness, while the runtime system dynamically schedules tasks as data shapes are incrementally discovered during execution. We formalize the mathematical foundation for reasoning about partial shapes and prove that Operon's incremental construction algorithm guarantees deterministic and confluent execution in parallel settings. The system's explicit modeling of partially-known states enables robust persistence and recovery mechanisms, while its per-task multi-queue architecture achieves efficient parallelism across heterogeneous task types. Empirical evaluation demonstrates that Operon outperforms an existing workflow engine with 14.94x baseline overhead reduction while maintaining near-linear end-to-end output rates as workloads scale, making it particularly suitable for large-scale data generation pipelines in machine learning applications.

</details>
