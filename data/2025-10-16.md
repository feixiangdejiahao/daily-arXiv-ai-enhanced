<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AutoCode: LLMs as Problem Setters for Competitive Programming](https://arxiv.org/abs/2510.12803)
*Shang Zhou,Zihan Zheng,Kaiyuan Liu,Zeyu Shen,Zerui Cheng,Zexing Chen,Hansen He,Jianzhu Yao,Huanzhi Mao,Qiuyang Mang,Tianfu Fu,Beichen Li,Dongruixuan Li,Wenhao Chai,Zhuang Liu,Aleksandra Korolova,Peter Henderson,Natasha Jaques,Pramod Viswanath,Saining Xie,Jingbo Shang*

Main category: cs.SE

TL;DR: AutoCode is a new system for automatically creating and validating competitive programming problems, achieving near-perfect consistency with official judgments and producing contest-quality problems as confirmed by top programmers. It significantly outperforms previous methods.


<details>
  <summary>Details</summary>
Motivation: Competitive programming problem creation is demanding, requiring careful calibration and validation, and previous automated solutions have had limited reliability. The authors aim to see if general large language models can reliably generate competition-grade problems.

Method: They introduce AutoCode, a system that uses multiple rounds of validation and cross-verification (including brute-force solution checks) to generate, test, and filter competitive programming problems, ensuring high correctness and quality.

Result: AutoCode’s output is nearly as consistent with official judgments as human-produced problems (99% consistency vs. less than 81% for previous methods). Grandmaster-level programmers have judged the system’s novel problems to be of contest quality.

Conclusion: AutoCode effectively generates high-quality competitive programming problems, with test suites approaching 99% consistency with official judgments, outperforming existing methods. The system is confirmed by human experts and top competitive programmers to produce contest-level problems.

Abstract: Writing competitive programming problems is exacting. Authors must: set
constraints, input distributions, and edge cases that rule out shortcuts;
target specific algorithms (e.g., max-flow, dynamic programming, data
structures); and calibrate complexity beyond the reach of most competitors. We
argue that this makes for an ideal test of general large language model
capabilities and study whether they can do this reliably. We introduce
AutoCode, which uses multiple rounds of validation to yield competition-grade
problem statements and test cases. On held-out problems, AutoCode test suites
approach 99% consistency with official judgments, a significant improvement
over current state-of-the-art methods like HardTests, which achieve less than
81%. Furthermore, starting with a random seed problem, AutoCode can create
novel variants with reference and brute-force solutions. By cross-verifying
these generated solutions against test cases, we can further filter out
malformed problems. Our system ensures high correctness, as verified by human
experts. AutoCode successfully produces novel problems judged by
Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.

</details>


### [2] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: This paper shows that keyword-search can replace semantic search in code context retrieval for RAG frameworks, delivering strong performance and significantly lowering computational costs, making it suitable for in-IDE code completion.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the computational resource demands of semantic search in Retrieval-Augmented Generation (RAG) setups for Code Language Models (CLMs), which limits their deployability in lightweight scenarios like in-IDE code completion.

Method: The paper proposes replacing semantic search-based retrieval modules with a simpler keyword-search technique for retrieving relevant code context from large codebases.

Result: The keyword-search solution achieved competitive completion results on the Code Context Competition benchmark, with chRF scores of 0.748 for Kotlin and 0.725 for Python.

Conclusion: Keyword-search is an effective and resource-efficient alternative to semantic search for retrieving code context in RAG systems, enabling practical integration in lightweight applications without the need for extensive GPU resources.

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [3] [ADPerf: Investigating and Testing Performance in Autonomous Driving Systems](https://arxiv.org/abs/2510.13078)
*Tri Minh-Triet Pham,Diego Elias Costa,Weiyi Shang,Jinqiu Yang*

Main category: cs.SE

TL;DR: The paper investigates how latency in obstacle detection modules, especially with 3D data, affects autonomous driving systems. Using their tool ADPerf, the authors stress-test industry-grade systems and show that increased latency causes reliability issues throughout the system, emphasizing the need for dedicated performance testing.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems rely on fast and accurate obstacle detection using sensors like cameras and LiDARs, making detection latency a critical safety and reliability factor. Current understanding of latency and resilience to LiDAR data variability in obstacle detection modules is lacking.

Method: Comprehensive measurement and modeling of the obstacle detection performance in two industry-grade systems (Apollo and Autoware). Introduction of ADPerf, a tool to generate realistic point cloud test cases aimed at revealing potential latency increases.

Result: ADPerf was used to stress-test 3D obstacle detection modules and analyze the impact of increased latency on trajectory prediction modules. The performance bottleneck caused by latency in 3D obstacle detection is highlighted and shown to negatively affect downstream modules.

Conclusion: Performance testing, especially for 3D obstacle detection modules, is essential as detection latency can significantly reduce the reliability and availability of autonomous driving systems by impacting other crucial modules.

Abstract: Obstacle detection is crucial to the operation of autonomous driving systems,
which rely on multiple sensors, such as cameras and LiDARs, combined with code
logic and deep learning models to detect obstacles for time-sensitive
decisions. Consequently, obstacle detection latency is critical to the safety
and effectiveness of autonomous driving systems. However, the latency of the
obstacle detection module and its resilience to various changes in the LiDAR
point cloud data are not yet fully understood. In this work, we present the
first comprehensive investigation on measuring and modeling the performance of
the obstacle detection modules in two industry-grade autonomous driving
systems, i.e., Apollo and Autoware. Learning from this investigation, we
introduce ADPerf, a tool that aims to generate realistic point cloud data test
cases that can expose increased detection latency. Increasing latency decreases
the availability of the detected obstacles and stresses the capabilities of
subsequent modules in autonomous driving systems, i.e., the modules may be
negatively impacted by the increased latency in obstacle detection.
  We applied ADPerf to stress-test the performance of widely used 3D obstacle
detection modules in autonomous driving systems, as well as the propagation of
such tests on trajectory prediction modules. Our evaluation highlights the need
to conduct performance testing of obstacle detection components, especially 3D
obstacle detection, as they can be a major bottleneck to increased latency of
the autonomous driving system. Such an adverse outcome will also further
propagate to other modules, reducing the overall reliability of autonomous
driving systems.

</details>


### [4] [TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models](https://arxiv.org/abs/2510.13106)
*Ruoyu Sun,Da Song,Jiayang Song,Yuheng Huang,Lei Ma*

Main category: cs.SE

TL;DR: TRUSTVIS is an automated framework for evaluating large language model trustworthiness, using perturbation methods and intuitive visualizations. It enables users to identify and address safety issues in models like Vicuna-7b, Llama2-7b, and GPT-3.5, making evaluation more reliable and actionable.


<details>
  <summary>Details</summary>
Motivation: Concerns about the trustworthiness, safety, and robustness of Large Language Models (LLMs) persist as these models become integral to NLP applications. There is a need for comprehensive and accessible tools to evaluate and visualize LLM trustworthiness.

Method: The paper introduces TRUSTVIS, an automated evaluation framework equipped with an interactive user interface for intuitive visualization of trustworthiness metrics. It integrates established perturbation methods (e.g., AutoDAN) and applies majority voting across various evaluation approaches to reliably assess LLMs.

Result: Preliminary case studies on Vicuna-7b, Llama2-7b, and GPT-3.5 show that TRUSTVIS effectively identifies safety and robustness vulnerabilities. The interactive interface enables users to examine results in detail, facilitating targeted improvements in model design and deployment.

Conclusion: TRUSTVIS streamlines and enhances the evaluation of LLM trustworthiness, making sophisticated assessments more accessible and actionable for users, and supports the development of safer, more robust NLP systems.

Abstract: As Large Language Models (LLMs) continue to revolutionize Natural Language
Processing (NLP) applications, critical concerns about their trustworthiness
persist, particularly in safety and robustness. To address these challenges, we
introduce TRUSTVIS, an automated evaluation framework that provides a
comprehensive assessment of LLM trustworthiness. A key feature of our framework
is its interactive user interface, designed to offer intuitive visualizations
of trustworthiness metrics. By integrating well-known perturbation methods like
AutoDAN and employing majority voting across various evaluation methods,
TRUSTVIS not only provides reliable results but also makes complex evaluation
processes accessible to users. Preliminary case studies on models like
Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our
framework in identifying safety and robustness vulnerabilities, while the
interactive interface allows users to explore results in detail, empowering
targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g

</details>


### [5] [Isolating Compiler Bugs through Compilation Steps Analysis](https://arxiv.org/abs/2510.13128)
*Yujie Liu,Mingxuan Zhu,Shengyu Cheng,Dan Hao*

Main category: cs.SE

TL;DR: CompSCAN is a new technique for isolating compiler bugs that analyzes internal compilation steps, outperforming existing methods in accuracy and speed when tested on LLVM and GCC bugs.


<details>
  <summary>Details</summary>
Motivation: Compiler bugs have significant downstream impacts, but isolating them is hard due to compiler complexity and lack of causal analysis in prior work.

Method: CompSCAN uses a three-stage process: extracting failing compilation steps, identifying bug-causing steps and code elements, and ranking suspicious elements for isolation.

Result: CompSCAN isolated more bugs within top rank positions than state-of-the-art techniques ETEM and ODFL (e.g., isolating 123 bugs in top-10), and showed 24.49%-50.18% relative improvements. It was also faster than the baselines.

Conclusion: CompSCAN effectively improves compiler bug isolation both in accuracy and speed compared to existing techniques.

Abstract: Compilers are essential to software systems, and their bugs can propagate to
dependent software. Ensuring compiler correctness is critical. However,
isolating compiler bugs remains challenging due to the internal complexity of
compiler execution. Existing techniques primarily mutate compilation inputs to
generate passing and failing tests, but often lack causal analysis of internal
steps, limiting their effectiveness.
  To address this limitation, we propose CompSCAN, a novel compiler bug
isolation technique that applies analysis over the sequence of compilation
steps. CompSCAN follows a three-stage process: (1) extracting the array of
compilation steps that leads to the original failure, (2) identifying
bug-causing steps and collecting corresponding compiler code elements, and (3)
calculating suspicious scores for each code element and outputting a suspicious
ranking list as the bug isolation result.
  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that
CompSCAN outperforms state-of-the-art techniques in both effectiveness and
efficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the
Top-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two
state-of-the-art compiler bug isolation techniques, CompSCAN achieves relative
improvements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /
49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs
faster on average per bug than both baselines.

</details>


### [6] [GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning](https://arxiv.org/abs/2510.13176)
*Haolin Pan,Chao Zha,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: GRACE is a new compiler auto-tuning method that robustly cuts code size and tuning time by mixing pass synergy analysis, machine learning clustering, and evolutionary search. It outperforms typical optimization (opt -Oz) on LLVM, generalizes well, and is practically fast.


<details>
  <summary>Details</summary>
Motivation: Optimizing compiler passes and phase ordering is crucial for improving program performance, particularly in reducing code size. Existing solutions, such as standard heuristics and iterative compilation, are either too generic or too costly. Machine learning methods have promise but struggle to generalize well to new programs.

Method: The paper introduces GRACE, a compiler auto-tuning framework aiming to optimize LLVM IR instruction count. GRACE narrows the search space by analyzing pass synergies and uses a weighted scoring system to pick promising initial sequences and pass pools. It then utilizes contrastive learning with pass-sequence-based data augmentation to build program embeddings, enabling similarity-aware clustering. Within each cluster, evolutionary search identifies $k$ specialized pass sequences. At deployment, GRACE selects and refines the most suitable sequence for each new program efficiently.

Result: GRACE achieves a reduction in LLVM IR instruction count by an average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to the standard opt -Oz, with an average tuning time under 1 second per program.

Conclusion: GRACE delivers fast and robust compiler optimization, yielding state-of-the-art code size reductions efficiently. Its innovative blend of synergy analysis, machine learning-driven clustering, and evolutionary search enables practical, generalizable, and high-quality auto-tuning for unseen programs.

Abstract: Compiler pass selection and phase ordering present a significant challenge in
achieving optimal program performance, particularly for objectives like code
size reduction. Standard compiler heuristics offer general applicability but
often yield suboptimal, program-specific results due to their one-size-fits-all
nature. While iterative compilation can find tailored solutions, its
prohibitive search cost limits practical use. Machine learning approaches
promise faster inference but frequently struggle with generalization to unseen
programs. This paper introduces GRACE, a novel framework for compiler
auto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE
effectively curtails the search space by leveraging pass synergies and a
weighted scoring method to generate initial high-quality candidate sequences
and a pass pool. It then employs contrastive learning, using pass
sequence-based data augmentation, to create program embeddings that facilitate
similarity-aware clustering. Evolutionary search within these clusters yields a
coreset of $k$ specialized pass sequences designed for robust generalization to
unseen programs. At test time, GRACE efficiently selects the best coreset
sequence and refines it using lightweight techniques. Experimental results on
seven diverse datasets show that GRACE reduces LLVM IR instruction count by an
average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,
while incurring an average tuning time of less than 1s per program,
demonstrating its state-of-the-art performance and practical effectiveness.

</details>


### [7] [Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines](https://arxiv.org/abs/2510.13184)
*Haolin Pan,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: This paper presents a novel auto-tuning framework for LLVM's New Pass Manager, leveraging a structure-aware Genetic Algorithm and a formal grammar to generate valid hierarchical optimization pipelines. The method outperforms traditional approaches, reducing instruction count by 13.62% compared to standard optimization on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current auto-tuning approaches for compiler optimization assume linear pass sequences, which do not align with the hierarchical structure of the LLVM New Pass Manager, resulting in syntactically invalid pipelines and suboptimal optimization.

Method: A formal grammar and forest-based data structure are used to represent valid pipelines, with a structure-aware Genetic Algorithm manipulating pipeline structures directly. The algorithm mines synergistic relationships among compiler passes and includes an optional refinement phase.

Result: The method achieved an average 13.62% reduction in instruction count over the standard opt -Oz optimization level across seven benchmark datasets, demonstrating its effectiveness in finding valid and high-performing optimization pipelines.

Conclusion: The proposed auto-tuning framework for the LLVM New Pass Manager can generate syntactically valid and effective optimization pipelines, achieving better performance than the standard optimization level.

Abstract: Compiler optimization relies on sequences of passes to improve program
performance. Selecting and ordering these passes automatically, known as
compiler auto-tuning, is challenging due to the large and complex search space.
Existing approaches generally assume a linear sequence of passes, a model
compatible with legacy compilers but fundamentally misaligned with the
hierarchical design of the LLVM New Pass Manager. This misalignment prevents
them from guaranteeing the generation of syntactically valid optimization
pipelines. In this work, we present a new auto-tuning framework built from the
ground up for the New Pass Manager. We introduce a formal grammar to define the
space of valid nested pipelines and a forest-based data structure for their
native representation. Upon this foundation, we develop a structure-aware
Genetic Algorithm whose operators manipulate these forests directly, ensuring
that all candidate solutions are valid by construction. The framework first
mines synergistic pass relationships to guide the search. An optional
refinement stage further explores subtle performance variations arising from
different valid structural arrangements.
  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The
discovered pipelines achieve an average of 13.62% additional instruction count
reduction compared to the standard opt -Oz optimization level, showing that our
framework is capable of navigating this complex, constrained search space to
identify valid and effective pass pipelines.

</details>


### [8] [Towards Richer Challenge Problems for Scientific Computing Correctness](https://arxiv.org/abs/2510.13423)
*Matthew Sottile,Mohit Tekriwal,John Sarracino*

Main category: cs.SE

TL;DR: The paper identifies a disconnect between scientific computing and formal methods communities regarding correctness verification. It proposes creating specialized challenge problems and guidelines to better evaluate and improve verification techniques for scientific applications.


<details>
  <summary>Details</summary>
Motivation: There is a growing concern about ensuring correctness in scientific computing, but current formal methods and programming languages verification techniques are not well-suited for the unique challenges of scientific computing applications. This is due in part to a lack of shared understanding between the scientific computing and formal methods/programming languages communities.

Method: The paper suggests developing specialized challenge problems tailored for scientific computing. These problems would help inform the creation and evaluation of verification techniques sensitive to the needs of scientific computing. The authors propose several key dimensions of correctness and provide guidelines for designing such challenge problems.

Result: The outcome is a proposal for targeted challenge problems and design guidelines that can guide future work in creating effective verification tools for scientific computing. This aims to bridge the current gap between the communities and deepen understanding of correctness in scientific applications.

Conclusion: A set of specialized challenge problems and evaluation criteria is needed to address the unique correctness challenges of scientific computing, improving communication between relevant research communities and enabling better FM/PL tool development.

Abstract: Correctness in scientific computing (SC) is gaining increasing attention in
the formal methods (FM) and programming languages (PL) community. Existing
PL/FM verification techniques struggle with the complexities of realistic SC
applications. Part of the problem is a lack of a common understanding between
the SC and PL/FM communities of machine-verifiable correctness challenges and
dimensions of correctness in SC applications.
  To address this gap, we call for specialized challenge problems to inform the
development and evaluation of FM/PL verification techniques for correctness in
SC. These specialized challenges are intended to augment existing problems
studied by FM/PL researchers for general programs to ensure the needs of SC
applications can be met. We propose several dimensions of correctness relevant
to scientific computing, and discuss some guidelines and criteria for designing
challenge problems to evaluate correctness in scientific computing.

</details>


### [9] [Verifying a Sparse Matrix Algorithm Using Symbolic Execution](https://arxiv.org/abs/2510.13424)
*Alexander C. Wilton*

Main category: cs.SE

TL;DR: Symbolic execution improves testing for scientific software, enabling stronger verification and better bug detection than traditional testing, as shown in a sparse matrix algorithm application.


<details>
  <summary>Details</summary>
Motivation: Scientific software faces challenges due to its complexity and optimization, which often result in subtle bugs that are hard to catch with regular testing methods.

Method: The authors propose using symbolic execution as a testing methodology. This approach enables the writing of tests similar to unit tests, but with enhanced verification guarantees compared to traditional testing.

Result: The methodology is applied to a sparse matrix algorithm, demonstrating its practical utility in detecting bugs and verifying correctness.

Conclusion: Symbolic execution offers a more powerful verification mechanism for scientific software, helping to uncover bugs that standard unit tests might miss.

Abstract: Scientific software is, by its very nature, complex. It is mathematical and
highly optimized which makes it prone to subtle bugs not as easily detected by
traditional testing. We outline how symbolic execution can be used to write
tests similar to traditional unit tests while providing stronger verification
guarantees and apply this methodology to a sparse matrix algorithm.

</details>


### [10] [OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies](https://arxiv.org/abs/2510.13561)
*Peng Di,Faqiang Chen,Xiao Bai,Hongjun Yang,Qingfeng Li,Ganglin Wei,Jian Mou,Feng Shi,Keting Chen,Peng Tang,Zhitao Shen,Zheng Li,Wenhui Shi,Junwei Guo,Hang Yu*

Main category: cs.SE

TL;DR: OpenDerisk is an open-source, multi-agent framework tailored to SRE teams, offering advanced diagnostic reasoning and collaboration. It surpasses current solutions in accuracy and efficiency and is proven effective in large-scale, real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Modern software complexity places excessive operational demand on SRE teams. There's a need for AI-driven tools that can mimic expert diagnostic reasoning, as current solutions lack deep causal reasoning and do not fit specialized SRE workflows.

Method: The authors introduce OpenDerisk, an open-source multi-agent framework designed specifically for SRE. It features a diagnostic-native collaboration model, pluggable reasoning engine, knowledge engine, and a standardized protocol for agent communication, enabling specialist agents to collaborate on complex problems.

Result: OpenDerisk outperforms current state-of-the-art solutions in both accuracy and efficiency. It has been successfully deployed at Ant Group, supporting over 3,000 daily users in various production scenarios, demonstrating its scalability and real-world effectiveness.

Conclusion: OpenDerisk fills a critical gap in SRE AI tooling, offering superior diagnostic reasoning, collaboration, and scalability compared to existing approaches. Its open-source nature and production validation signify its practical impact and accessibility for industry adoption.

Abstract: The escalating complexity of modern software imposes an unsustainable
operational burden on Site Reliability Engineering (SRE) teams, demanding
AI-driven automation that can emulate expert diagnostic reasoning. Existing
solutions, from traditional AI methods to general-purpose multi-agent systems,
fall short: they either lack deep causal reasoning or are not tailored for the
specialized, investigative workflows unique to SRE. To address this gap, we
present OpenDerisk, a specialized, open-source multi-agent framework
architected for SRE. OpenDerisk integrates a diagnostic-native collaboration
model, a pluggable reasoning engine, a knowledge engine, and a standardized
protocol (MCP) to enable specialist agents to collectively solve complex,
multi-domain problems. Our comprehensive evaluation demonstrates that
OpenDerisk significantly outperforms state-of-the-art baselines in both
accuracy and efficiency. This effectiveness is validated by its large-scale
production deployment at Ant Group, where it serves over 3,000 daily users
across diverse scenarios, confirming its industrial-grade scalability and
practical impact. OpenDerisk is open source and available at
https://github.com/derisk-ai/OpenDerisk/

</details>


### [11] [Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code](https://arxiv.org/abs/2510.13575)
*Han Fu,Sigrid Eldh,Kristian Wiklund,Andreas Ermedahl,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: This paper shows that large language models can automatically fix most compilation errors in embedded system CI pipelines, outperforming manual fixes in speed and efficiency, even without test cases.


<details>
  <summary>Details</summary>
Motivation: Compilation errors in CI pipelines, caused by co-development of hardware and software, slow down development and require manual intervention. Traditional automated repair methods depend on test cases, which aren't available for non-compilable code, prompting exploration of LLM-driven solutions.

Method: The approach uses large language models to automatically repair compilation errors without test cases. The researchers collected over 40,000 commits from industrial products and compared the performance of CI systems enhanced with four different LLMs to manual human corrections.

Result: LLM-enhanced CI systems resolved up to 63% of compilation errors. Of the CI builds fixed by LLMs, 83% were judged reasonable. Successful cases also completed debugging much faster (under 8 minutes) than manual debugging (hours).

Conclusion: LLMs can effectively automate the repair of compilation errors in industrial embedded systems' CI pipelines, resolving a majority of issues and significantly reducing debugging time.

Abstract: The co-development of hardware and software in industrial embedded systems
frequently leads to compilation errors during continuous integration (CI).
Automated repair of such failures is promising, but existing techniques rely on
test cases, which are not available for non-compilable code.
  We employ an automated repair approach for compilation errors driven by large
language models (LLMs). Our study encompasses the collection of more than 40000
commits from the product's source code. We assess the performance of an
industrial CI system enhanced by four state-of-the-art LLMs, comparing their
outcomes with manual corrections provided by human programmers. LLM-equipped CI
systems can resolve up to 63 % of the compilation errors in our baseline
dataset. Among the fixes associated with successful CI builds, 83 % are deemed
reasonable. Moreover, LLMs significantly reduce debugging time, with the
majority of successful cases completed within 8 minutes, compared to hours
typically required for manual debugging.

</details>


### [12] [Property Testing for Ocean Models. Can We Specify It? (Invited Talk)](https://arxiv.org/abs/2510.13692)
*Deepak A. Cherian*

Main category: cs.SE

TL;DR: The paper suggests using property-based testing methods from computer science, specifically applying geophysical fluid dynamics theory as property tests, to improve the validation of ocean models. Initial examples show promise, but further research is needed to determine practicality and utility.


<details>
  <summary>Details</summary>
Motivation: Testing the correctness of ocean models is challenging due to the oracle problem, where it is difficult to determine whether outputs are 'correct.' The author seeks novel ways to address this challenge.

Method: The author proposes applying the concept of property-based testing, inspired by computer science literature, specifically transforming geophysical fluid dynamics (GFD) theory into property tests for model validation.

Result: The author demonstrates that simple, idealized GFD problems can indeed be reframed as property tests, and that physics principles align well with this approach. The paper identifies several candidate tests but leaves the determination of their feasibility and utility as an open question.

Conclusion: Physics-based property tests may offer a natural and potentially effective approach for addressing the oracle problem in ocean model validation, though further work is needed to evaluate which specific tests are most valuable.

Abstract: I take inspiration from the property-testing literature, particularly the
work of Prof. John Hughes, and explore how such ideas might be applied to
numerical models of the ocean. Specifically, I ask whether geophysical fluid
dynamics (GFD) theory, expressed as property tests, might be used to address
the oracle problem of testing the correctness of ocean models. I propose that a
number of simple idealized GFD problems can be framed as property tests. These
examples clearly illustrate how physics naturally lends itself to specifying
property tests. Which of these proposed tests might be most feasible and
useful, remains to be seen.

</details>


### [13] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: Expanding context window and trying different data processing strategies for OpenCoder, a code LLM, shows competitive results even with less data. The main gain results from an adapted RoPE scaling. Simple file-level training also remains effective, helping lower data and compute barriers for code completion research.


<details>
  <summary>Details</summary>
Motivation: Enable large language models for code to better leverage codebase-wide context for more accurate and context-aware code completion.

Method: Investigated different repository-processing strategies on OpenCoder (1.5B parameters), extended context window from 4,096 to 16,384 tokens via training on an extra 1B tokens of curated repository-level data. Compared repository-level with file-level training and adapted rotary positional embedding (RoPE) scaling.

Result: Despite using much less data than competitors, their model performed comparably on the Long Code Arena benchmark. All repository-processing techniques tested produced strong results, with the main improvement coming from adapting the RoPE scaling. Simpler file-level training at original sequence lengths also remained very effective.

Conclusion: Repository-level training is efficient and less dependent on massive amounts of data/computing resources than previously thought, especially when carefully adjusted for position embeddings. File-level training is also surprisingly competitive.

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [14] [Imperative Quantum Programming with Ownership and Borrowing in Guppy](https://arxiv.org/abs/2510.13082)
*Mark Koch,Agustín Borgna,Craig Roy,Alan Lawrence,Kartik Singhal,Seyon Sivarajah,Ross Duncan*

Main category: cs.PL

TL;DR: The paper introduces a quantum type system that integrates linear type safety into imperative programming, making it user-friendly and reliable. All concepts have been implemented in the Guppy quantum language.


<details>
  <summary>Details</summary>
Motivation: Linear types are essential for enforcing quantum-specific constraints (no-cloning, no-deleting), but have not been widely adopted in imperative quantum programming due to ergonomic or semantic limitations. The paper seeks to bridge this gap.

Method: The method involves designing and developing a type system that blends linear type theory (traditionally for functional languages) with imperative programming paradigms, and practical implementation within the Guppy quantum programming language.

Result: The developed quantum type system combines the benefits of linear typing (enforcing quantum constraints) with the usability and semantics of imperative programming. This system has been fully implemented in Guppy.

Conclusion: The paper concludes by presenting a quantum type system that integrates linear typing with imperative semantics, achieving safety guarantees and practical ergonomics, and notes successful implementation in the Guppy language.

Abstract: Linear types enforce no-cloning and no-deleting theorems in functional
quantum programming. However, in imperative quantum programming, they have not
gained widespread adoption. This work aims to develop a quantum type system
that combines ergonomic linear typing with imperative semantics and maintains
safety guarantees. All ideas presented here have been implemented in
Quantinuum's Guppy programming language.

</details>


### [15] [Extensibility in Programming Languages: An overview](https://arxiv.org/abs/2510.13236)
*Sebastian mateos Nicolajsen*

Main category: cs.PL

TL;DR: This paper reviews and summarizes key aspects and strategies of programming language extensibility, aiming to encourage designers to pay more attention to making languages flexible and customizable.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the lack of accessible overviews on programming language extensibility and seeks to present an introductory exploration for designers and learners.

Method: The author conducts a literature review to identify and analyze key themes and strategies related to extensibility in programming languages, focusing on areas like macros, modules, types, and reflection.

Result: The analysis highlights how various extensibility mechanisms (macros, modules, types, and reflection), along with cross-cutting properties such as parametricism and first-class citizen behavior, contribute to customizability and flexibility in language design.

Conclusion: The author concludes by urging future language designers to critically evaluate and prioritize extensibility in their designs, using the overview provided as inspiration and guidance.

Abstract: I here conduct an exploration of programming language extensibility, making
an argument for an often overlooked component of conventional language design.
Now, this is not a technical detailing of these components, rather, I attempt
to provide an overview as I myself have lacked during my time investigating
programming languages. Thus, read this as an introduction to the magical world
of extensibility. Through a literature review, I identify key extensibility
themes - Macros, Modules, Types, and Reflection - highlighting diverse
strategies for fostering extensibility. The analysis extends to cross-theme
properties such as Parametricism and First-class citizen behaviour, introducing
layers of complexity by highlighting the importance of customizability and
flexibility in programming language constructs. By outlining these facets of
existing programming languages and research, I aim to inspire future language
designers to assess and consider the extensibility of their creations
critically.

</details>


### [16] [Fast Trigonometric Functions using the RLIBM Approach](https://arxiv.org/abs/2510.13426)
*Sehyeok Park,Santosh Nagarakatte*

Main category: cs.PL

TL;DR: This paper presents efficient polynomial approximations for trigonometric functions using advanced range reduction techniques that maintain high precision for 'pi,' enabling fast and correctly rounded results for multiple floating-point formats and modes with a single implementation.


<details>
  <summary>Details</summary>
Motivation: Accurately computing trigonometric functions in floating-point arithmetic is challenging due to errors magnified during range reduction with the irrational number 'pi.' Correct rounding across various representations and modes is difficult.

Method: The paper uses the RLIBM approach to develop polynomial approximations for trigonometric functions and implements fast range reduction techniques that preserve a large number of bits of 'pi,' employing both floating-point and integer computations.

Result: The developed implementations for trigonometric functions are fast and produce correctly rounded results for all inputs and for multiple floating-point representations up to 32 bits in one implementation.

Conclusion: The authors successfully created efficient and broadly applicable polynomial approximations for trigonometric functions with correct rounding, overcoming the critical challenge of range reduction with high-precision 'pi.'

Abstract: This paper describes our experience developing polynomial approximations for
trigonometric functions that produce correctly rounded results for multiple
representations and rounding modes using the RLIBM approach. A key challenge
with trigonometric functions concerns range reduction with "pi", which reduces
a given input in the domain of a 32-bit float to a small domain. Any rounding
error in the value of "pi" is amplified during range reduction, which can
result in wrong results. We describe our experience implementing fast range
reduction techniques that maintain a large number of bits of "pi" both with
floating-point and integer computations. The resulting implementations for
trigonometric functions are fast and produce correctly rounded results for all
inputs for multiple representations up to 32-bits with a single implementation.

</details>


### [17] [A Complementary Approach to Incorrectness Typing](https://arxiv.org/abs/2510.13725)
*Celia Mengyue Li,Sophie Pull,Steven Ramsay*

Main category: cs.PL

TL;DR: A new type system is described that covers both program correctness and incorrectness by introducing a negation-like complement operator, enabling thorough verification and soundness for functional programs that use atoms and pattern matching.


<details>
  <summary>Details</summary>
Motivation: Traditional type systems for functional programming struggle with expressing both correctness and incorrectness, especially in the presence of atoms and pattern matching. A new approach is needed to better certify when programs fail.

Method: The paper introduces a two-sided type system where types range over sets of normal forms (not just values). They define a complement operator on types that acts as a negation, giving rise to refutation principles. The complement's axiomatisation via subtyping is formally shown to be decidable.

Result: Their system can express refutations (incorrectness) as well as correctness, and they demonstrate it on Erlang-like programs, showing cases where programs can be certified to 'go wrong.' The whole system is proven to be sound and complete for normal forms.

Conclusion: They present a two-sided type system with a decidable and expressive complement operator, enabling verification of both correctness and refutation (incorrectness) in functional programs with atoms and pattern matching.

Abstract: We introduce a new two-sided type system for verifying the correctness and
incorrectness of functional programs with atoms and pattern matching. A key
idea in the work is that types should range over sets of normal forms, rather
than sets of values, and this allows us to define a complement operator on
types that acts as a negation on typing formulas. We show that the complement
allows us to derive a wide range of refutation principles within the system,
including the type-theoretic analogue of co-implication, and we use them to
certify that a number of Erlang-like programs go wrong. An expressive
axiomatisation of the complement operator via subtyping is shown decidable, and
the type system as a whole is shown to be not only sound, but also complete for
normal forms.

</details>
