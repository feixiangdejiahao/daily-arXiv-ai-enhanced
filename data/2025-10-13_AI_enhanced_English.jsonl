{"id": "2510.08726", "categories": ["cs.PL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08726", "abs": "https://arxiv.org/abs/2510.08726", "authors": ["Yifan Zhao", "Egan Johnson", "Prasanth Chatarasi", "Vikram Adve", "Sasa Misailovic"], "title": "Neptune: Advanced ML Operator Fusion for Locality and Parallelism on GPUs", "comment": null, "summary": "Operator fusion has become a key optimization for deep learning, which\ncombines multiple deep learning operators to improve data reuse and reduce\nglobal memory transfers. However, existing tensor compilers struggle to fuse\ncomplex reduction computations involving loop-carried dependencies, such as\nattention mechanisms.\n  The paper introduces Neptune, a tensor compiler for advanced operator fusion\nfor sequences of reduction operators. Neptune presents a new approach for\nadvanced operator fusion, which intentionally breaks some existing dependencies\nand compensates by constructing algebraic correction expressions that allow the\nkernel to produce the correct result.\n  On ten attention-based benchmarks, Neptune, starting from simple attention\ncode and a high-level scheduling template, outperforms existing compilers like\nTriton, TVM, and FlexAttention, including Triton-based implementations of\nFlashAttention. Across four different GPU architectures from NVIDIA and AMD,\nNeptune-generated kernels have average speedup of $1.35\\times$ over the next\nbest alternative, demonstrating its effectiveness for deep learning workloads.", "AI": {"tldr": "Neptune is a new tensor compiler that improves operator fusion for attention mechanisms by breaking dependencies and adding algebraic corrections, achieving up to 1.35x speedup over other compilers on multiple GPUs.", "motivation": "Existing tensor compilers have difficulties fusing complex reduction computations with loop-carried dependencies, which are common in attention mechanisms in deep learning. This limitation hinders the optimization potential of operator fusion.", "method": "The paper introduces Neptune, a tensor compiler designed specifically for advanced operator fusion of sequences of reduction operators. Neptune uses a new approach: it intentionally breaks certain dependencies and constructs algebraic correction expressions to ensure that the output remains correct despite the fusion.", "result": "On ten benchmarks focused on attention mechanisms, Neptune-generated kernels, using simple attention code and a high-level scheduling template, outperform other compilers such as Triton, TVM, and FlexAttention (including Triton-based FlashAttention). Neptune shows an average speedup of 1.35x over the best alternative across four GPU architectures from NVIDIA and AMD.", "conclusion": "Neptune presents an effective method for optimizing complex reduction operator fusion in deep learning, particularly in attention workloads, providing consistent performance improvements over existing tensor compilers."}}
{"id": "2510.08889", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.08889", "abs": "https://arxiv.org/abs/2510.08889", "authors": ["Songlin Jia", "Craig Liu", "Siyuan He", "Haotian Deng", "Yuyan Bao", "Tiark Rompf"], "title": "Typestate via Revocable Capabilities", "comment": null, "summary": "Managing stateful resources safely and expressively is a longstanding\nchallenge in programming languages, especially in the presence of aliasing.\nWhile scope-based constructs such as Java's synchronized blocks offer ease of\nreasoning, they restrict expressiveness and parallelism. Conversely,\nimperative, flow-sensitive management enables fine-grained control but demands\nsophisticated typestate analyses and often burdens programmers with explicit\nstate tracking.\n  In this work, we present a novel approach that unifies the strengths of both\nparadigms by extending flow-insensitive capability mechanisms into\nflow-sensitive typestate tracking. Our system decouples capability lifetimes\nfrom lexical scopes, allowing functions to provide, revoke, and return\ncapabilities in a flow-sensitive manner, based on the existing mechanisms\nexplored for the safety and ergonomics of scoped capability programming.\n  We implement our approach as an extension to the Scala 3 compiler, leveraging\npath-dependent types and implicit resolution to enable concise, statically\nsafe, and expressive typestate programming. Our prototype generically supports\na wide range of stateful patterns, including file operations, advanced locking\nprotocols, DOM construction, and session types. This work demonstrates that\nexpressive and safe typestate management can be achieved with minimal\nextensions to existing capability-based languages, paving the way for more\nrobust and ergonomic stateful programming.", "AI": {"tldr": "The paper presents a scalable solution for safe and expressive stateful resource management by extending existing capability mechanisms into flow-sensitive typestate tracking, implemented in Scala 3. It supports a variety of patterns and improves both safety and ergonomics with minimal language changes.", "motivation": "Managing stateful resources safely is difficult, particularly when aliasing is present. Existing solutions either restrict expressiveness and parallelism (e.g., scope-based constructs) or require complex and explicit state management by programmers (e.g., imperative, flow-sensitive management with typestate). The motivation is to find an approach that combines safety with expressiveness without increasing complexity.", "method": "The authors extend flow-insensitive capability mechanisms into flow-sensitive typestate tracking. Their system decouples capability lifetimes from lexical scopes, making capabilities manageable in a flow-sensitive style. The approach is implemented as an extension to the Scala 3 compiler, using path-dependent types and implicit resolution for safety and conciseness.", "result": "The prototype supports various stateful usage patterns such as file operations, locking protocols, DOM construction, and session types. It achieves expressive and safe typestate management with minimal changes to existing capability-based languages.", "conclusion": "Expressive and safe typestate management is possible with minimal language extension, providing both robustness and ergonomics for stateful programming."}}
{"id": "2510.08939", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.08939", "abs": "https://arxiv.org/abs/2510.08939", "authors": ["Haotian Deng", "Siyuan He", "Songlin Jia", "Yuyan Bao", "Tiark Rompf"], "title": "Free to Move: Reachability Types with Flow-Sensitive Effects for Safe Deallocation and Ownership Transfer", "comment": null, "summary": "We present a flow-sensitive effect system for reachability types that\nsupports explicit memory management, including Rust-style move semantics, in\nhigher-order impure functional languages. Our system refines the existing\nreachability qualifier with polymorphic \\emph{use} and \\emph{kill} effects that\nrecord how references are read, written, transferred, and deallocated. The\neffect discipline tracks operations performed on each resource using\nqualifiers, enabling the type system to express ownership transfer, contextual\nfreshness, and destructive updates without regions or linearity. We formalize\nthe calculus, its typing and effect rules, and a compositional operational\nsemantics that validates use-after-free safety. All metatheoretic results,\nincluding preservation, progress, and effect soundness, are mechanized. The\nsystem models idioms such as reference deallocation, move semantics, reference\nswapping, while exposing precise safety guarantee. Together, these\ncontributions integrate reachability-based reasoning with explicit resource\ncontrol, advancing the state of the art in safe manual memory management for\nhigher-order functional languages.", "AI": {"tldr": "This paper introduces a novel effect system that allows safe, flow-sensitive manual memory management\u2014including Rust-like move semantics\u2014in higher-order functional languages. It tracks resource operations and ensures use-after-free safety without regions or linear types, achieving precise memory control and advancing language safety guarantees.", "motivation": "Existing systems for memory management in functional languages struggle to support explicit resource control and ownership transfer, especially in higher-order impure contexts. Traditional approaches, such as regions or linear types, can be restrictive or complicated. The motivation is to advance safer manual memory management without those limitations.", "method": "The paper proposes a flow-sensitive effect system for reachability types that refines previous qualifiers with polymorphic 'use' and 'kill' effects. It formalizes a calculus with detailed typing and effect rules, and presents a compositional operational semantics, all of which are mechanized for metatheoretic guarantees like preservation, progress, and effect soundness.", "result": "The system enables tracking and expression of operations on resources such as ownership transfer, freshness, and updates. It models common memory management idioms (deallocation, move semantics, swapping) and provides precise use-after-free safety guarantees, validated by mechanization.", "conclusion": "Combining reachability reasoning with explicit resource management, the system supports safe manual memory management in higher-order functional languages without relying on regions or linearity, signaling a significant advancement."}}
{"id": "2510.08969", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08969", "abs": "https://arxiv.org/abs/2510.08969", "authors": ["Bjarne Stroustrup"], "title": "Concept-Based Generic Programming in C++", "comment": null, "summary": "We present programming techniques to illustrate the facilities and principles\nof C++ generic programming using concepts. Concepts are C++'s way to express\nconstraints on generic code. As an initial example, we provide a simple type\nsystem that eliminates narrowing conversions and provides range checking\nwithout unnecessary notational or run-time overheads. Concepts are used\nthroughout to provide user-defined extensions to the type system. The aim is to\nshow their utility and the fundamental ideas behind them, rather than to\nprovide a detailed or complete explanation of C++'s language support for\ngeneric programming or the extensive support provided by the standard library.\nGeneric programming is an integral part of C++, rather than an isolated\nsub-language. In particular, key facilities support general programming as well\nas generic programming (e.g., uniform notation for types, lambdas, variadic\ntemplates, and C++26 static reflection). Finally, we give design rationales and\norigins for key parts of the concept design, including use patterns, the\nrelationship to Object-Oriented Programming, value arguments, notation, concept\ntype-matching, and definition checking.", "AI": {"tldr": "This paper demonstrates how C++ concepts allow for safer, more expressive generic programming by eliminating unsafe type conversions, enabling range checks, and supporting user-defined type extensions, highlighting concepts' design rationales and integration with everyday C++ programming.", "motivation": "C++ generic programming has been central but challenging due to limited language features for expressing type constraints and safe type manipulation. The motivation is to demonstrate how concepts clarify and enforce constraints, improve safety, and integrate generic programming into general programming practice.", "method": "The authors illustrate C++ generic programming techniques by using concepts, developing a type system that eliminates unsafe conversions and adding range checking, leveraging concepts for user-defined type system extensions, and supporting their discussion with design rationales and historical context.", "result": "Through programming examples and language features like concepts, lambdas, variadic templates, and static reflection, the paper shows improved safety (elimination of narrowing conversions, range checks) and extensibility in user code, without notational or runtime overhead.", "conclusion": "The paper concludes that concepts and new facilities in C++ enhance generic programming, making code more expressive, safer, and easier to extend, integrating generic programming deeply into mainstream C++ usage."}}
{"id": "2510.08576", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08576", "abs": "https://arxiv.org/abs/2510.08576", "authors": ["Justus Flerlage", "Alexander Acker", "Odej Kao"], "title": "Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions", "comment": null, "summary": "Large Language Models (LLMs) have emerged as transformative tools for natural\nlanguage understanding and user intent resolution, enabling tasks such as\ntranslation, summarization, and, increasingly, the orchestration of complex\nworkflows. This development signifies a paradigm shift from conventional,\nGUI-driven user interfaces toward intuitive, language-first interaction\nparadigms. Rather than manually navigating applications, users can articulate\ntheir objectives in natural language, enabling LLMs to orchestrate actions\nacross multiple applications in a dynamic and contextual manner. However,\nextant implementations frequently rely on cloud-based proprietary models, which\nintroduce limitations in terms of privacy, autonomy, and scalability. For\nlanguage-first interaction to become a truly robust and trusted interface\nparadigm, local deployment is not merely a convenience; it is an imperative.\nThis limitation underscores the importance of evaluating the feasibility of\nlocally deployable, open-source, and open-access LLMs as foundational\ncomponents for future intent-based operating systems. In this study, we examine\nthe capabilities of several open-source and open-access models in facilitating\nuser intention resolution through machine assistance. A comparative analysis is\nconducted against OpenAI's proprietary GPT-4-based systems to assess\nperformance in generating workflows for various user intentions. The present\nstudy offers empirical insights into the practical viability, performance\ntrade-offs, and potential of open LLMs as autonomous, locally operable\ncomponents in next-generation operating systems. The results of this study\ninform the broader discussion on the decentralization and democratization of AI\ninfrastructure and point toward a future where user-device interaction becomes\nmore seamless, adaptive, and privacy-conscious through locally embedded\nintelligence.", "AI": {"tldr": "This paper compares open-source, locally deployable language models with proprietary GPT-4 systems for user intent resolution, demonstrating the promise and challenges of open LLMs in privacy-focused, decentralized AI workflows.", "motivation": "Current LLM-powered workflows often depend on cloud-based, proprietary models, which raise concerns about privacy, autonomy, and scalability. There is a need to explore whether open-source, locally deployable LLMs can serve as viable alternatives, especially for intent-based OS operations.", "method": "The study conducts a comparative analysis of several open-source/open-access LLMs against proprietary GPT-4-based solutions. The models are tested on their ability to resolve user intentions and generate cross-application workflows.", "result": "Empirical evidence is presented regarding the performance, trade-offs, and feasibility of using open LLMs locally for dynamic user intent resolution. The analysis reveals practical strengths and weaknesses and informs the discussion of AI decentralization.", "conclusion": "Local, open-access LLMs show potential as core components for future operating systems prioritizing privacy, autonomy, and seamless user interaction. The work highlights their practical viability and possible limitations compared to current proprietary solutions."}}
{"id": "2510.09591", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.09591", "abs": "https://arxiv.org/abs/2510.09591", "authors": ["Saad Ahmed Bazaz", "Mirza Omer Beg"], "title": "A Multilingual Python Programming Language", "comment": "For project homepage, see https://universalpython.github.io/", "summary": "All widely used and useful programming languages have a common problem. They\nrestrict entry on the basis of knowledge of the English language. The lack of\nknowledge of English poses a major hurdle to many newcomers who do not have the\nresources, in terms of time and money, to learn the English language. Studies\nshow that people learn better in their own language. Therefore, we propose a\nlanguage transpiler built on top of the Python programming language, called\nUniversalPython, which allows one to write Python in their own human language.\nWe demonstrate the ability to create an \"Urdu Python\" with this transpiler. In\nthe future, we aim to scale the language to encapsulate more human languages to\nincrease the availability of programming. The source code for this transpiler\nis open-source, and available at\nhttps://github.com/universalpython/universalpython", "AI": {"tldr": "Programming languages often require English proficiency, excluding non-English speakers. The authors developed UniversalPython, a Python-based transpiler allowing code to be written in various human languages. Urdu Python is demonstrated; future expansions will add more languages to improve programming accessibility. The project is open-source.", "motivation": "Entry into programming is hindered for non-English speakers due to the dominance of English-based programming languages, making it difficult and costly for them to learn.", "method": "Authors created UniversalPython, a language transpiler that runs on top of Python, allowing users to write Python code in their native human language. As a proof of concept, they implemented \"Urdu Python.\"", "result": "UniversalPython enables programming in Python using human languages other than English; demonstrated by supporting Urdu. The tool is open-source.", "conclusion": "UniversalPython lowers the entry barrier for programming for non-English speakers by supporting native languages, with future plans to include more languages."}}
{"id": "2510.08609", "categories": ["cs.SE", "cs.CR", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.08609", "abs": "https://arxiv.org/abs/2510.08609", "authors": ["Imranur Rahman", "Jill Marley", "William Enck", "Laurie Williams"], "title": "Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?", "comment": "Accepted to ASE 2025", "summary": "Developers consistently use version constraints to specify acceptable\nversions of the dependencies for their project. \\emph{Pinning} dependencies can\nreduce the likelihood of breaking changes, but comes with a cost of manually\nmanaging the replacement of outdated and vulnerable dependencies. On the other\nhand, \\emph{floating} can be used to automatically get bug fixes and security\nfixes, but comes with the risk of breaking changes. Security practitioners\nadvocate \\emph{pinning} dependencies to prevent against software supply chain\nattacks, e.g., malicious package updates. However, since \\emph{pinning} is the\ntightest version constraint, \\emph{pinning} is the most likely to result in\noutdated dependencies. Nevertheless, how the likelihood of becoming outdated or\nvulnerable dependencies changes across version constraint types is unknown. The\ngoal of this study is to aid developers in making an informed dependency\nversion constraint choice by empirically evaluating the likelihood of\ndependencies becoming outdated or vulnerable across version constraint types at\nscale. In this study, we first identify the trends in dependency version\nconstraint usage and the patterns of version constraint type changes made by\ndevelopers in the npm, PyPI, and Cargo ecosystems. We then modeled the\ndependency state transitions using survival analysis and estimated how the\nlikelihood of becoming outdated or vulnerable changes when using \\emph{pinning}\nas opposed to the rest of the version constraint types. We observe that among\noutdated and vulnerable dependencies, the most commonly used version constraint\ntype is \\emph{floating-minor}, with \\emph{pinning} being the next most common.\nWe also find that \\emph{floating-major} is the least likely to result in\noutdated and \\emph{floating-minor} is the least likely to result in vulnerable\ndependencies.", "AI": {"tldr": "The paper investigates how different dependency version constraints (like pinning or floating) influence the likelihood of dependencies becoming outdated or vulnerable. Floating constraints tend to keep dependencies more up-to-date and secure compared to pinning, which often leads to outdated software.", "motivation": "Dependency version constraints are crucial in software development for managing updates and risks. Pinning can protect against changes but increases maintenance, while floating reduces manual work but could introduce breaking changes or vulnerabilities. Despite recommendations, the true implications of different version constraints on outdatedness or vulnerability are not well understood.", "method": "The study empirically evaluates how likely dependencies become outdated or vulnerable across different version constraint types. Researchers analyzed version constraint usage and change patterns in the npm, PyPI, and Cargo ecosystems, and applied survival analysis to model state transitions and estimate risks.", "result": "Floating-minor is the most common among outdated and vulnerable dependencies, followed by pinning. Floating-major is least likely to result in outdated dependencies, and floating-minor is least likely to result in vulnerable dependencies.", "conclusion": "Choice of version constraint significantly affects the risk of outdated or vulnerable dependencies. Floating-major and floating-minor constraints are recommended to mitigate these risks, as pinning tends to create more outdated dependencies."}}
{"id": "2510.08610", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08610", "abs": "https://arxiv.org/abs/2510.08610", "authors": ["Imranur Rahman", "Md Rayhanur Rahman"], "title": "Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model", "comment": "Accepted to Context Collection Workshop co-located with ASE 2025", "summary": "Code completion can help developers improve efficiency and ease the\ndevelopment lifecycle. Although code completion is available in modern\nintegrated development environments (IDEs), research lacks in determining what\nmakes a good context for code completion based on the information available to\nthe IDEs for the large language models (LLMs) to perform better. In this paper,\nwe describe an effective context collection strategy to assist the LLMs in\nperforming better at code completion tasks. The key idea of our strategy is to\npreprocess the repository into smaller code chunks and later use syntactic and\nsemantic similarity-based code chunk retrieval with relative positioning. We\nfound that code chunking and relative positioning of the chunks in the final\ncontext improve the performance of code completion tasks.", "AI": {"tldr": "This paper introduces a method for improving code completion in IDEs by preprocessing code into smaller chunks and selecting relevant ones using similarity and positioning, resulting in better performance of large language models.", "motivation": "Current research does not sufficiently address what constitutes an effective context for code completion using large language models (LLMs) with information available to IDEs.", "method": "The authors propose a strategy that preprocesses a code repository into smaller chunks, then retrieves these chunks for code completion tasks based on syntactic and semantic similarity, along with their relative positions.", "result": "Using code chunking and relative chunk positioning in context retrieval enhances the performance of LLMs on code completion tasks.", "conclusion": "An effective context collection strategy utilizing code chunking and relative positioning leads to better code completion by LLMs."}}
{"id": "2510.09073", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.09073", "abs": "https://arxiv.org/abs/2510.09073", "authors": ["Matthew Sotoudeh"], "title": "Literate Tracing", "comment": "examples at https://lair.masot.net/trex . SPLASH Onward 2025", "summary": "As computer systems grow ever larger and more complex, a crucial task in\nsoftware development is for one person (the system expert) to communicate to\nanother (the system novice) how a certain program works. This paper reports on\nthe author's experiences with a paradigm for program documentation that we call\nliterate tracing. A literate trace explains a software system using annotated,\nconcrete execution traces of the system. Literate traces complement both\nin-code comments (which often lack global context) and out-of-band design docs\n(which often lack a concrete connection to the code). We also describe TReX,\nour tool for making literate traces that are interactive, visual, and\nguaranteed by construction to be faithful to the program semantics. We have\nused TReX to write literate traces explaining components of large systems\nsoftware including the Linux kernel, Git source control system, and GCC\ncompiler.", "AI": {"tldr": "Literate tracing is a new way to document software using annotated execution traces, making explanations clearer and more accurate than traditional comments or design docs. The TReX tool helps create interactive and precise traces, which have successfully documented parts of major systems like Linux, Git, and GCC.", "motivation": "Effective communication about complex software systems between experts and novices is challenging. Existing documentation methods either lack global context or connections to actual code.", "method": "The author introduces 'literate tracing,' a documentation paradigm using annotated, concrete execution traces. They describe TReX, a tool for interactive and visual literate traces that are guaranteed to reflect true program execution.", "result": "Literate tracing offers documentation that bridges gaps between code comments and design documents. TReX enables creation of interactive, visual, and semantically faithful traces. The tool has been used for components of major software like the Linux kernel, Git, and GCC.", "conclusion": "Literate tracing, supported by TReX, improves program understanding and documentation, especially for substantial and complex systems. It enhances the clarity and fidelity of communication between developers."}}
{"id": "2510.08612", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08612", "abs": "https://arxiv.org/abs/2510.08612", "authors": ["Devang Dhanuka"], "title": "Impact of LLMs on Team Collaboration in Software Development", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being integrated into software\ndevelopment processes, with the potential to transform team workflows and\nproductivity. This paper investigates how LLMs affect team collaboration\nthroughout the Software Development Life Cycle (SDLC). We reframe and update a\nprior study with recent developments as of 2025, incorporating new literature\nand case studies. We outline the problem of collaboration hurdles in SDLC and\nexplore how LLMs can enhance productivity, communication, and decision-making\nin a team context. Through literature review, industry examples, a team survey,\nand two case studies, we assess the impact of LLM-assisted tools (such as code\ngeneration assistants and AI-powered project management agents) on\ncollaborative software engineering practices. Our findings indicate that LLMs\ncan significantly improve efficiency (by automating repetitive tasks and\ndocumentation), enhance communication clarity, and aid cross-functional\ncollaboration, while also introducing new challenges like model limitations and\nprivacy concerns. We discuss these benefits and challenges, present research\nquestions guiding the investigation, evaluate threats to validity, and suggest\nfuture research directions including domain-specific model customization,\nimproved integration into development tools, and robust strategies for ensuring\ntrust and security.", "AI": {"tldr": "This paper analyzes how LLMs impact teamwork in software development, finding they boost productivity and communication but also introduce challenges like privacy and model reliability. Further research is needed to tailor LLMs for specific development contexts and ensure secure, trustworthy integration into workflows.", "motivation": "LLMs are increasingly impacting software development teams, but their effects on collaboration throughout the SDLC are not well understood. The paper aims to update prior research with new evidence and clarify how LLMs can address collaboration hurdles.", "method": "The paper uses a literature review, analysis of industry examples, surveys of team experiences, and two case studies to investigate the impact of LLM-assisted tools on software engineering collaboration.", "result": "LLMs can improve team efficiency by automating repetitive tasks and documentation, clarify communication, and facilitate cross-functional collaboration. However, they present new challenges, such as model limitations and privacy concerns.", "conclusion": "LLMs have the potential to reshape collaborative practices in software engineering, enhancing productivity and communication but requiring careful consideration of security, trust, and domain-specific adaptation. Future directions include customizing models for different domains and strengthening their integration and security."}}
{"id": "2510.08640", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08640", "abs": "https://arxiv.org/abs/2510.08640", "authors": ["Ha Min Son", "Huan Ren", "Xin Liu", "Zhe Zhao"], "title": "Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools", "comment": null, "summary": "Android is the largest mobile platform, yet automatically building\napplications remains a practical challenge. While Large Language Models (LLMs)\nshow promise for code repair, their use for fixing Android build errors remains\nunderexplored. To address this gap, we first introduce AndroidBuildBench, a\nbenchmark of 1,019 build failures curated from the commit histories of 43\nopen-source Android projects. Each problem is paired with a verified solution\nfrom a subsequent commit, ensuring that fixes are feasible. Second, we propose\nGradleFixer, an LLM agent with domain-specific tools for inspecting and\nmanipulating the Gradle build environment. GradleFixer achieves a resolve rate\nof 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent\nthat relies on a general-purpose shell. GradleFixer's success suggests that\nwhile LLMs possess the high-level knowledge to solve these failures, they\nstruggle to translate this knowledge into effective low-level actions using a\ngeneral-purpose shell. We demonstrate the effectiveness of a strategy we term\nTool Bridging, which replaces general-purpose shell commands with domain-aware\nabstractions. We hypothesize this approach works through two mechanisms: 1) it\nprovides tools in an API-like format that LLMs use more reliably, and 2) it\nconstrains the action space to relevant operations. This approach bridges the\ngap between the model's high-level reasoning and effective low-level execution.", "AI": {"tldr": "The paper introduces a new benchmark for Android build errors and presents GradleFixer, an LLM agent with specialized tools, which outperforms general solutions by guiding LLMs with domain-aware actions. Domain-specific tools enable LLMs to bridge high-level reasoning with effective low-level execution for build error repair.", "motivation": "Automatically building Android applications is challenging, and current methods\u2014even those leveraging powerful Large Language Models (LLMs)\u2014are not yet effective at fixing specific Android build errors. The paper identifies the gap in applying LLMs to resolve Android-specific build failures.", "method": "The authors introduce AndroidBuildBench, a curated benchmark of 1,019 real-world build failures from open-source Android projects, each paired with a verified fix. They also present GradleFixer, an LLM agent equipped with domain-specific tools designed to inspect and manipulate the Gradle build environment, leveraging a strategy called Tool Bridging by replacing generic shell commands with Android-specific abstractions.", "result": "GradleFixer achieves an 81.4% resolution rate (pass@1), significantly outperforming a general-purpose coding agent. The tool\u2019s effectiveness stems from providing LLMs with an API-like interface and narrowing down action choices to domain-relevant operations.", "conclusion": "LLMs have the high-level knowledge needed for Android build error resolution, but require domain-aware tooling to execute fixes effectively. Tool Bridging, offering domain-specific abstractions, allows LLMs to better translate knowledge into actionable repairs, outperforming generic agents."}}
{"id": "2510.08664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08664", "abs": "https://arxiv.org/abs/2510.08664", "authors": ["Jianan Mu", "Mingyu Shi", "Yining Wang", "Tianmeng Yang", "Bin Sun", "Xing Hu", "Jing Ye", "Huawei Li"], "title": "Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware", "comment": null, "summary": "LLM-based RTL generation is an interesting research direction, as it holds\nthe potential to liberate the least automated stage in the current chip design.\nHowever, due to the substantial semantic gap between high-level specifications\nand RTL, coupled with limited training data, existing models struggle with\ngeneration accuracy. Drawing on human experience, design with verification\nhelps improving accuracy. However, as the RTL testbench data are even more\nscarce, it is not friendly for LLMs. Although LLMs excel at higher-level\nlanguages like Python/C, they have a huge semantic gap from RTL. When\nimplementing the same functionality, Python/C code and hardware code differ\nsignificantly in the spatiotemporal granularity, requiring the LLM not only to\nconsider high-level functional semantics but also to ensure the low-level\ndetails align with the circuit code. It is not an easy task. In this paper, we\npropose a function abstracted verifiable middleware (Faver) that streamlines\nRTL verification in LLM-based workflows. By mixing LLM-friendly code structures\nwith a rule-based template, Faver decouples the details of circuit\nverification, allowing the LLM to focus on the functionality itself. In our\nexperiments on the SFT model and open-source models, Faver improved the model's\ngeneration accuracy by up to 14%.", "AI": {"tldr": "LLM-generated RTL code struggles with accuracy due to semantic gaps and limited data; Faver, a new middleware, streamlines verification and boosts accuracy by up to 14% in experiments.", "motivation": "LLM-based generation of register-transfer level (RTL) code could automate one of the least automated stages in chip design, but faces significant challenges from a semantic gap between input languages and RTL, as well as limited training and test data.", "method": "The paper introduces Faver, a function-abstracted verifiable middleware that combines LLM-friendly code structures with rule-based templates. This middleware streamlines circuit verification within LLM-based RTL generation workflows by decoupling verification details and allowing LLMs to better focus on functional correctness.", "result": "Experiments conducted on both SFT and open-source models show that Faver improved the generation accuracy of RTL code by up to 14%.", "conclusion": "The Faver middleware helps overcome several core challenges in LLM-based RTL generation, specifically boosting verification capability and generation accuracy despite the semantic gap and data limitations."}}
{"id": "2510.08665", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08665", "abs": "https://arxiv.org/abs/2510.08665", "authors": ["Aofan Liu", "Haoxuan Li", "Bin Wang", "Ao Yang", "Hui Li"], "title": "RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution", "comment": null, "summary": "Code generation models based on large language models (LLMs) have gained wide\nadoption, but challenges remain in ensuring safety, accuracy, and\ncontrollability, especially for complex tasks. Existing methods often lack\ndynamic integration of external tools, transparent reasoning, and user control\nover safety. To address these issues, we propose a controllable code generation\nframework utilizing the ReAct paradigm for multi-agent task execution. This\nframework is a multi-agent system designed to enable efficient, precise, and\ninterpretable code generation through dynamic interactions between LLMs and\nexternal resources. The framework adopts a collaborative architecture\ncomprising four specialized agents: a Planner for task decomposition, a\nSearcher that leverages the ReAct framework for reasoning and tool integration,\na CodeGen agent for accurate code generation, and an Extractor for structured\ndata retrieval. The ReAct-based Searcher alternates between generating\nreasoning traces and executing actions, facilitating seamless integration of\ninternal knowledge with external tools (such as search engines) to enhance\naccuracy and user control. Experimental results show the framework's\neffectiveness across multiple languages, achieving a 94.8% security rate on the\nSVEN dataset with CodeQL, outperforming existing approaches. Its transparent\nreasoning process fosters user trust and improves controllability.", "AI": {"tldr": "A new multi-agent, ReAct-based code generation framework improves safety, accuracy, and user control compared to existing LLM solutions, achieving high performance and security rates in experiments.", "motivation": "Current code generation models based on large language models face challenges with safety, accuracy, and controllability, especially for complex coding tasks. Existing solutions lack dynamic integration with external tools, transparent reasoning, and user control over safety aspects.", "method": "The paper proposes a controllable code generation framework that uses a multi-agent system and the ReAct paradigm. Four specialized agents are employed: a Planner (task decomposition), a Searcher (reasoning and tool integration via ReAct), a CodeGen (code generation), and an Extractor (structured data retrieval). The Searcher agent dynamically alternates between reasoning and executing actions, effectively integrating internal knowledge with external resources.", "result": "The framework demonstrated effectiveness across multiple programming languages and achieved a 94.8% security rate on the SVEN dataset using CodeQL, surpassing existing methods. The transparent reasoning process enhances user trust and control.", "conclusion": "The proposed controllable code generation framework significantly improves code safety, accuracy, and user controllability through dynamic multi-agent collaboration and transparent reasoning, outperforming current approaches."}}
{"id": "2510.08667", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08667", "abs": "https://arxiv.org/abs/2510.08667", "authors": ["Mohammad Baqar"], "title": "RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data", "comment": "13 Pages", "summary": "Modern software teams frequently encounter delays in resolving recurring or\nrelated issues due to fragmented knowledge scattered across JIRA tickets,\ndeveloper discussions, and GitHub pull requests (PRs). To address this\nchallenge, we propose a Retrieval-Augmented Generation (RAG) framework that\nintegrates Sentence-Transformers for semantic embeddings with FAISS-based\nvector search to deliver context-aware ticket resolution recommendations. The\napproach embeds historical JIRA tickets, user comments, and linked PR metadata\nto retrieve semantically similar past cases, which are then synthesized by a\nLarge Language Model (LLM) into grounded and explainable resolution\nsuggestions. The framework contributes a unified pipeline linking JIRA and\nGitHub data, an embedding and FAISS indexing strategy for heterogeneous\nsoftware artifacts, and a resolution generation module guided by retrieved\nevidence. Experimental evaluation using precision, recall, resolution time\nreduction, and developer acceptance metrics shows that the proposed system\nsignificantly improves resolution accuracy, fix quality, and knowledge reuse in\nmodern DevOps environments.", "AI": {"tldr": "A novel system combining semantic embedding, vector search, and language models is introduced to mine and recommend solutions for software tickets by unifying JIRA and GitHub data, demonstrating substantial performance improvements in DevOps workflows.", "motivation": "Software teams often face delays in resolving recurring or related issues due to scattered information across JIRA tickets, developer discussions, and GitHub pull requests.", "method": "The authors propose a Retrieval-Augmented Generation (RAG) framework that uses Sentence-Transformers for semantic embeddings and FAISS-based vector search. This system embeds data from JIRA and GitHub to retrieve similar past cases, which are then synthesized by a Large Language Model (LLM) to generate resolution suggestions.", "result": "Experimental evaluation indicates significant improvements in resolution accuracy, fix quality, and knowledge reuse, as measured by precision, recall, resolution time reduction, and developer acceptance.", "conclusion": "The proposed unified RAG framework effectively leverages linked JIRA and GitHub data to deliver context-aware and explainable ticket resolution recommendations, improving the efficiency and quality of software issue resolution."}}
{"id": "2510.08697", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08697", "abs": "https://arxiv.org/abs/2510.08697", "authors": ["Terry Yue Zhuo", "Xiaolong Jin", "Hange Liu", "Juyong Jiang", "Tianyang Liu", "Chen Gong", "Bhupesh Bishnoi", "Vaisakhi Mishra", "Marek Suppa", "Noah Ziems", "Saiteja Utpala", "Ming Xu", "Guangyu Song", "Kaixin Li", "Yuhan Cao", "Bo Liu", "Zheng Liu", "Sabina Abdurakhmanova", "Wenhao Yu", "Mengzhao Jia", "Jihan Yao", "Kenneth Hamilton", "Kumar Shridhar", "Minh Chien Vu", "Dingmin Wang", "Jiawei Liu", "Zijian Wang", "Qian Liu", "Binyuan Hui", "Meg Risdal", "Ahsen Khaliq", "Atin Sood", "Zhenchang Xing", "Wasi Uddin Ahmad", "John Grundy", "David Lo", "Banghua Zhu", "Xiaoning Du", "Torsten Scholak", "Leandro von Werra"], "title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution", "comment": "Built with love by the BigCode community :)", "summary": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable\nreal-time evaluation from human perspectives to assess the quality of model\nresponses. In the coding domain, manually examining the quality of\nLLM-generated content is extremely challenging, as it requires understanding\nlong chunks of raw code and deliberately simulating code execution. To this\nend, we introduce BigCodeArena, an open human evaluation platform for code\ngeneration backed by a comprehensive and on-the-fly execution environment.\nBuilt on top of Chatbot Arena, BigCodeArena enables the execution of\nLLM-generated code and allows humans to interact with the execution process and\noutcomes. We collected over 14,000 raw code-centric conversation sessions\nacross 10 widely used LLMs, spanning 10 languages and 8 types of execution\nenvironments. Among these conversations, we identified more than 4,700\nmulti-turn samples with pairwise human preferences. Further analysis uncovers\nunderexplored preferences of LLMs in fine-grained domains characterized by\ntasks, languages, and frameworks. To systematically examine code understanding\nand generation capabilities of frontier LLMs, we curated two benchmarks based\non the collected data, namely BigCodeReward and AutoCodeArena. For\nBigCodeReward, we post-processed the 4,700 conversations and evaluated the\nconsistency between reward models and human preferences. The evaluation shows\nthat most LLMs have superior performance in judging coding preferences when the\nexecution results are available. Inspired by these findings, we propose\nAutoCodeArena, an automatic Elo rating benchmark designed to assess the coding\nquality of LLMs without human involvement. We find that proprietary LLMs like\nGPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation\nperformance among recent emerging models.", "AI": {"tldr": "BigCodeArena is a new platform for human-involved and automatic code generation evaluation with live code execution, leading to better benchmarks and revealing proprietary LLMs' superior coding performance.", "motivation": "Evaluating LLM-generated code is challenging due to the need to understand complex code and simulate execution, and existing crowdsourced evaluation platforms lack support for interactive code execution.", "method": "The authors introduce BigCodeArena, an open human evaluation platform for code generation with real-time execution capabilities. They collected extensive conversational data from multiple LLMs, analyzed human preferences in coding scenarios, and created two benchmarks: BigCodeReward (for reward model alignment) and AutoCodeArena (an automatic Elo rating system).", "result": "Analysis of over 14,000 code-centric sessions revealed nuanced human preferences and demonstrated that LLMs perform better in code judgment tasks when execution results are available. Proprietary LLMs, such as GPT-5 and Claude models, show leading performance in code generation according to their new benchmarks.", "conclusion": "BigCodeArena provides an effective, execution-driven platform for real-time code evaluation, offering new benchmarks and insights into LLM performance. The tools and data curated facilitate improved assessment methods for code generation models, reinforcing the strengths of state-of-the-art proprietary LLMs."}}
{"id": "2510.08716", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08716", "abs": "https://arxiv.org/abs/2510.08716", "authors": ["Stephan Lukasczyk", "Gordon Fraser"], "title": "Search-based Hyperparameter Tuning for Python Unit Test Generation", "comment": "Accepted to the 17th Symposium on Search-Based Software Engineering\n  2025 (SSBSE 2025)", "summary": "Search-based test-generation algorithms have countless configuration options.\nUsers rarely adjust these options and usually stick to the default values,\nwhich may not lead to the best possible results. Tuning an algorithm's\nhyperparameters is a method to find better hyperparameter values, but it\ntypically comes with a high demand of resources. Meta-heuristic search\nalgorithms -- that effectively solve the test-generation problem -- have been\nproposed as a solution to also efficiently tune parameters. In this work we\nexplore the use of differential evolution as a means for tuning the\nhyperparameters of the DynaMOSA and MIO many-objective search algorithms as\nimplemented in the Pynguin framework. Our results show that significant\nimprovement of the resulting test suite's coverage is possible with the tuned\nDynaMOSA algorithm and that differential evolution is more efficient than basic\ngrid search.", "AI": {"tldr": "Using differential evolution to tune hyperparameters in test-generation algorithms improves test coverage and is more efficient than grid search.", "motivation": "Users typically do not adjust the many configuration options available in search-based test-generation algorithms, relying on default values that may not provide optimal results. Tuning these options can improve outcomes but is usually resource-intensive.", "method": "The paper explores differential evolution, a meta-heuristic search algorithm, to tune the hyperparameters of DynaMOSA and MIO many-objective search algorithms, using the Pynguin framework for implementation.", "result": "Tuned DynaMOSA algorithm yields significantly better test suite coverage. Differential evolution is more efficient at finding improved configurations than basic grid search.", "conclusion": "Differential evolution is an effective and efficient approach for tuning hyperparameters in search-based test-generation algorithms, leading to improved test suite coverage while reducing resource demands compared to grid search."}}
{"id": "2510.08810", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08810", "abs": "https://arxiv.org/abs/2510.08810", "authors": ["Mohayeminul Islam", "Ajay Kumar Jha", "May Mahmoud", "Sarah Nadi"], "title": "PyMigTool: a tool for end-to-end Python library migration", "comment": "arXiv admin note: text overlap with arXiv:2504.13272", "summary": "Library migration is the process of replacing a library with a similar one in\na software project. Manual library migration is time consuming and error prone,\nas it requires developers to understand the Application Programming Interfaces\n(API) of both libraries, map equivalent APIs, and perform the necessary code\ntransformations. Due to the difficulty of the library migration process, most\nof the existing automated techniques and tooling stop at the API mapping stage\nor support a limited set of libraries and code transformations. In this paper,\nwe develop an end-to-end solution that can automatically migrate code between\nany arbitrary pair of Python libraries that provide similar functionality. Due\nto the promising capabilities of Large Language Models (LLMs) in code\ngeneration and transformation, we use LLMs as the primary engine for migration.\nBefore building the tool, we first study the capabilities of LLMs for library\nmigration on a benchmark of 321 real-world library migrations. We find that\nLLMs can effectively perform library migration, but some post-processing steps\ncan further improve the performance. Based on this, we develop PyMigTool, a\ncommand line application that combines the power of LLMs, static analysis, and\ndynamic analysis to provide accurate library migration. We evaluate PyMigTool\non 717 real-world Python applications that are not from our benchmark. We find\nthat PyMigTool can migrate 32% of the migrations with complete correctness. Of\nthe remaining migrations, only 14% of the migration-related changes are left\nfor developers to fix for more than half of the projects.", "AI": {"tldr": "This paper introduces PyMigTool, an automated tool using Large Language Models, static analysis, and dynamic analysis to migrate code between any Python libraries. It significantly reduces migration effort, performing fully accurate migrations in 32% of real-world cases and leaving only minor fixes for developers in most of the rest.", "motivation": "Library migration is challenging and labor-intensive for developers, involving understanding and mapping APIs and transforming code. Existing automated techniques are limited, often stopping at API mapping or supporting only specific libraries. There is a need for a more effective, automatic, and generalizable solution for library migration.", "method": "The paper investigates the effectiveness of Large Language Models (LLMs) in automating library migration by analyzing 321 real-world library migrations. It then builds PyMigTool, a command line tool that leverages LLMs along with static and dynamic code analysis to automate the migration between arbitrary Python libraries.", "result": "PyMigTool is evaluated on 717 real-world Python applications (not part of the initial benchmark). The tool achieves completely correct migration for 32% of cases. Furthermore, for more than half of the remaining projects, only 14% of the migration-related changes require manual intervention by developers.", "conclusion": "The use of LLMs, combined with static and dynamic analysis, enables effective and automated migration between Python libraries. PyMigTool significantly reduces developer effort, although some migration tasks may still require minimal manual fixes."}}
{"id": "2510.08827", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.08827", "abs": "https://arxiv.org/abs/2510.08827", "authors": ["Erfan Al-Hossami", "Razvan Bunescu"], "title": "McMining: Automated Discovery of Misconceptions in Student Code", "comment": "16 pages, 8 figures", "summary": "When learning to code, students often develop misconceptions about various\nprogramming language concepts. These can not only lead to bugs or inefficient\ncode, but also slow down the learning of related concepts. In this paper, we\nintroduce McMining, the task of mining programming misconceptions from samples\nof code from a student. To enable the training and evaluation of McMining\nsystems, we develop an extensible benchmark dataset of misconceptions together\nwith a large set of code samples where these misconceptions are manifested. We\nthen introduce two LLM-based McMiner approaches and through extensive\nevaluations show that models from the Gemini, Claude, and GPT families are\neffective at discovering misconceptions in student code.", "AI": {"tldr": "The paper introduces McMining, a new task and dataset for discovering student programming misconceptions, and shows that LLMs can effectively identify such misconceptions in code.", "motivation": "Students frequently form misconceptions when learning programming, which hinders their progress and leads to flawed code. Addressing these misconceptions is crucial for effective learning and better code.", "method": "The paper proposes McMining, a novel task focused on automatically identifying programming misconceptions in student code. It includes the construction of a benchmark dataset of misconceptions, paired with code samples, and evaluates two Large Language Model (LLM)-based approaches (McMiner) using prominent LLMs such as Gemini, Claude, and GPT.", "result": "The LLM-based McMiner models, using Gemini, Claude, and GPT, successfully identify misconceptions in student code, demonstrating the effectiveness of LLMs for this purpose.", "conclusion": "The study establishes McMining as a viable task, provides resources for the research community, and shows that current LLMs are able to effectively mine misconceptions from student code."}}
{"id": "2510.08834", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08834", "abs": "https://arxiv.org/abs/2510.08834", "authors": ["Carlos Pinto Gomez", "Fabio Petrillo"], "title": "Identifying Video Game Debugging Bottlenecks: An Industry Perspective", "comment": "8 pages, 3 figures, 4 tables, gas 2026 conference submission", "summary": "Conventional debugging techniques used in traditional software are similarly\nused when debugging video games. However, the reality of video games require\nits own set of unique debugging techniques such as On-Screen Console, Debug\nDraws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this\narticle, we provide insights from a video game studio on how 20 seasoned\nindustry game developers debug during the production of a game. Our experiments\nrely on the recordings of debugging sessions for the most critical bugs\ncategorized as Crashes, Object Behaviors, and Object Persistence. In this\npaper, we focus on identifying the debugging activities that bottleneck bug\nresolution. We also identify the debugging tools used to perform debugging\ntechniques. Lastly, we present how different disciplines collaborate during\ndebugging and how technical roles are at the core of debugging. Our thematic\nanalysis has identified game developers spend 36.6\\% of their time inspecting\ngame artifacts and 35.1\\% of their time reproducing the bug locally.", "AI": {"tldr": "Video game debugging requires unique methods and tools beyond traditional software debugging. By analyzing real studio debugging sessions, the study found that much developer time is used inspecting artifacts and reproducing bugs, relying on specialized tools and multidisciplinary teamwork.", "motivation": "Traditional debugging techniques are used in video games, but the unique nature of video game development may require specialized debugging methods. The goal is to understand how video game developers actually debug in practice, and what techniques and tools are most effective.", "method": "The study involved observing and analyzing recordings of debugging sessions from a video game studio, focusing on 20 experienced developers. The sessions addressed critical bugs such as crashes, object behaviors, and object persistence. Thematic analysis was used to identify bottlenecks and categorize debugging activities, tools, and interdisciplinary collaboration.", "result": "The analysis found that game developers spend about 36.6% of their debugging time inspecting game artifacts and 35.1% reproducing the bug locally. The study also highlighted the use of specialized debugging tools (e.g., On-Screen Console, Debug Draws, Debug Camera, Cheats, In-Game Menus, and Data Scrubbing) and revealed the importance of collaboration among different disciplines, with technical roles being central to debugging.", "conclusion": "Debugging in video games significantly relies on unique tools and cross-disciplinary collaboration, with substantial developer time allocated to inspecting artifacts and reproducing bugs. Traditional debugging techniques are complemented by game-specific methods to address the complex nature of video game development."}}
{"id": "2510.08850", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08850", "abs": "https://arxiv.org/abs/2510.08850", "authors": ["Vasudha Yanuganti", "Ishaan Puri", "Swapnil Chhatre", "Mantinder Singh", "Ashok Jallepalli", "Hritvik Shrivastava", "Pradeep Kumar Sharma"], "title": "Repository-Aware File Path Retrieval via Fine-Tuned LLMs", "comment": null, "summary": "Modern codebases make it hard for developers and AI coding assistants to find\nthe right source files when answering questions like \"How does this feature\nwork?\" or \"Where was the bug introduced?\" Traditional code search (keyword or\nIR based) often misses semantic context and cross file links, while large\nlanguage models (LLMs) understand natural language but lack repository specific\ndetail. We present a method for file path retrieval that fine tunes a strong\nLLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file\npaths directly from a natural language query. To build training data, we\nintroduce six code aware strategies that use abstract syntax tree (AST)\nstructure and repository content to generate realistic question-answer pairs,\nwhere answers are sets of file paths. The strategies range from single file\nprompts to hierarchical repository summaries, providing broad coverage. We fine\ntune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,\nand obtain high retrieval accuracy: up to 91\\% exact match and 93\\% recall on\nheld out queries, clearly beating single strategy training. On a large codebase\nlike PyTorch (about 4,000 Python files), the model reaches 59\\% recall, showing\nscalability. We analyze how multi level code signals help the LLM reason over\ncross file context and discuss dataset design, limits (for example, context\nlength in very large repos), and future integration of retrieval with LLM based\ncode intelligence.", "AI": {"tldr": "The paper introduces a method for training LLMs to retrieve relevant source file paths from natural language queries by leveraging multiple code-aware strategies. The approach achieves high accuracy and recall on several Python projects, demonstrating scalability and effectiveness over traditional code search techniques.", "motivation": "Traditional code search methods often fail to capture semantic context and links between files, making it difficult for developers and AI to answer repository-specific questions. LLMs excel at understanding natural language but lack context about specific repositories.", "method": "The authors fine-tune a Large Language Model (Qwen3-8B) using QLoRA and Unsloth optimizations to directly retrieve relevant file paths from natural language queries. They devise six code-aware training strategies leveraging AST structure and repository content to create realistic question-file path datasets.", "result": "The fine-tuned model achieved up to 91% exact match accuracy and 93% recall on test queries, significantly outperforming models trained with a single strategy. On the large PyTorch codebase (around 4,000 Python files), it achieved 59% recall, indicating good scalability.", "conclusion": "Multi-strategy, code-aware fine-tuning enables LLMs to more accurately retrieve relevant file paths across codebases, surpassing traditional and single-strategy approaches. The limitations include context length for very large repositories, but the approach is promising for future integration into code intelligence tools."}}
{"id": "2510.08876", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08876", "abs": "https://arxiv.org/abs/2510.08876", "authors": ["Kostiantyn Bevziuk", "Andrii Fatula", "Svetozar Lashin Yaroslav Opanasenko", "Anna Tukhtarova", "Ashok Jallepalli Pradeepkumar Sharma", "Hritvik Shrivastava"], "title": "Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval", "comment": null, "summary": "We present a repository decomposition system that converts large software\nrepositories into a vectorized knowledge graph which mirrors project\narchitectural and semantic structure, capturing semantic relationships and\nallowing a significant level of automatization of further repository\ndevelopment. The graph encodes syntactic relations such as containment,\nimplementation, references, calls, and inheritance, and augments nodes with\nLLM-derived summaries and vector embeddings. A hybrid retrieval pipeline\ncombines semantic retrieval with graph-aware expansion, and an LLM-based\nassistant formulates constrained, read-only graph requests and produces\nhuman-oriented explanations.", "AI": {"tldr": "The paper introduces a system that transforms large software repositories into knowledge graphs enriched with semantic information and supports advanced retrieval and human-friendly explanations, facilitating automated understanding and development.", "motivation": "Large software repositories contain complex, interrelated code. Understanding and developing them further requires semantic structuring and better automation. The paper aims to improve repository comprehension and enable advanced tooling by converting repositories into structured, semantically rich graphs.", "method": "The system decomposes repositories into knowledge graphs that capture syntactic relations (containment, references, calls, inheritance) and semantic content (LLM-derived summaries, vector embeddings). A hybrid retrieval approach fuses semantic retrieval with graph-based context expansion. An LLM-based assistant generates read-only graph queries and explains results to users.", "result": "The system builds detailed, vectorized knowledge graphs mirroring both architectural and semantic code relations. Retrieval combines semantic search and structure-aware expansion, while a conversational assistant supports user interaction and comprehension.", "conclusion": "The proposed decomposition and retrieval system automates the understanding and further development of large code repositories by structuring them as semantic knowledge graphs, augmented with LLM summaries and embeddings, and making them accessible via a hybrid retriever and LLM-powered assistant."}}
{"id": "2510.08981", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08981", "abs": "https://arxiv.org/abs/2510.08981", "authors": ["Mandira Roy", "Novarun Deb", "Nabendu Chaki", "Agostino Cortesi"], "title": "SEER: Sustainability Enhanced Engineering of Software Requirements", "comment": "Main Paper: 32 pages, References: 3 pages, Appendix: 13 pages.\n  Submitted to the Journal of Systems and Software, Elsevier", "summary": "The rapid expansion of software development has significant environmental,\ntechnical, social, and economic impacts. Achieving the United Nations\nSustainable Development Goals by 2030 compels developers to adopt sustainable\npractices. Existing methods mostly offer high-level guidelines, which are\ntime-consuming to implement and rely on team adaptability. Moreover, they focus\non design or implementation, while sustainability assessment should start at\nthe requirements engineering phase. In this paper, we introduce SEER, a\nframework which addresses sustainability concerns in the early software\ndevelopment phase. The framework operates in three stages: (i) it identifies\nsustainability requirements (SRs) relevant to a specific software product from\na general taxonomy; (ii) it evaluates how sustainable system requirements are\nbased on the identified SRs; and (iii) it optimizes system requirements that\nfail to satisfy any SR. The framework is implemented using the reasoning\ncapabilities of large language models and the agentic RAG (Retrieval Augmented\nGeneration) approach. SEER has been experimented on four software projects from\ndifferent domains. Results generated using Gemini 2.5 reasoning model\ndemonstrate the effectiveness of the proposed approach in accurately\nidentifying a broad range of sustainability concerns across diverse domains.", "AI": {"tldr": "The paper introduces SEER, a framework using AI methods to embed sustainability into software requirements from the start. It identifies, evaluates, and optimizes sustainability requirements, proving effective across multiple domains and helping developers address global sustainability goals efficiently.", "motivation": "There is a need for software development practices that address sustainability from the earliest stages, as current approaches are either too general or only focus on later design/implementation phases. Early assessment of sustainability in requirements engineering is essential for meeting global goals like the United Nations Sustainable Development Goals (SDGs).", "method": "The paper presents SEER, a framework used during the requirements engineering phase of software development. SEER works in three steps: 1) It identifies relevant sustainability requirements using a general taxonomy, 2) evaluates these requirements for sustainability, and 3) optimizes the requirements that do not meet sustainability standards. The technical implementation employs the reasoning abilities of large language models combined with an agentic Retrieval Augmented Generation (RAG) approach.", "result": "SEER was tested on four projects in different domains, leveraging the Gemini 2.5 reasoning model. The results show SEER can accurately identify a wide range of sustainability requirements and concerns early in the software development process.", "conclusion": "Implementing SEER in the requirements engineering phase helps ensure sustainability is embedded early, allowing software teams to address a wide variety of sustainability issues more effectively and efficiently across domains."}}
{"id": "2510.08990", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08990", "abs": "https://arxiv.org/abs/2510.08990", "authors": ["Mandira Roy", "Novarun Deb", "Nabendu Chaki", "Agostino Cortesi"], "title": "Towards a Taxonomy of Sustainability Requirements for Software Design", "comment": "Paper: 7 pages", "summary": "Software systems are a significant contributor to global sustainability\nconcerns, demanding that environmental, social, technical, and economic factors\nbe systematically addressed from the initial requirements engineering phase.\nAlthough existing research provides various sustainability requirements (SRs),\nthese contributions are often fragmented, specific to certain dimensions, or\nlimited to particular application domains, resulting in a critical lack of a\nunified, comprehensive taxonomy for the software engineering community. To\naddress this gap, this research conducts a Systematic Literature Review (SLR)\nto extract and organize sustainability requirements from the state-of-the-art.\nThe primary contribution is a comprehensive taxonomy of SRs across the four\ndimensions of sustainability (environmental, technical, social, and economic).\nFor each identified category, we provide clear definitions, associated metrics,\nand measures. Furthermore, we depict a correlation matrix that projects the\npositive and negative influences (synergies and conflicts) among categories\nacross different dimensions. This systematized reference assists both software\ndevelopers and researchers in effectively formulating, managing, and\nreconciling trade-offs within sustainable software development.", "AI": {"tldr": "The paper presents a comprehensive taxonomy of sustainability requirements in software engineering, derived from a systematic literature review. It covers four sustainability dimensions, provides definitions, metrics, measures, and highlights their inter-relations, aiding practitioners and researchers in achieving sustainable software development.", "motivation": "Existing sustainability requirements in software engineering are fragmented, domain-specific, or focused on particular sustainability dimensions. There is no unified taxonomy covering all relevant aspects, which hampers systematic consideration of sustainability from the requirements phase.", "method": "The authors conduct a Systematic Literature Review (SLR) to identify, extract, and organize sustainability requirements from current literature. They then synthesize these requirements into a comprehensive taxonomy spanning environmental, technical, social, and economic dimensions.", "result": "The research provides a unified taxonomy of sustainability requirements for software engineering, complete with definitions, metrics, and measures for each category. Additionally, a correlation matrix is presented to illustrate synergies and conflicts among categories across different sustainability dimensions.", "conclusion": "The systematized taxonomy and correlation matrix offer a valuable reference for software engineers and researchers, enabling more holistic and effective sustainable software development by supporting the formulation, management, and reconciliation of sustainability-related trade-offs."}}
{"id": "2510.08996", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08996", "abs": "https://arxiv.org/abs/2510.08996", "authors": ["Spandan Garg", "Ben Steenhoek", "Yufan Huang"], "title": "Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation", "comment": null, "summary": "Current benchmarks for evaluating software engineering agents, such as\nSWE-Bench Verified, are predominantly derived from GitHub issues and fail to\naccurately reflect how developers interact with chat-based coding assistants in\nintegrated development environments (IDEs). We posit that this mismatch leads\nto a systematic overestimation of agent's capabilities in real-world scenarios,\nespecially bug fixing. We introduce a novel benchmarking framework that\ntransforms existing formal benchmarks into realistic user queries through\nsystematic analysis of developer interaction patterns with chat-based agents.\nOur methodology is flexible and can be easily extended to existing benchmarks.\nIn this paper, we apply our testing framework to SWE-Bench Verified, the\nTypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and\ntransform formal GitHub issue descriptions into realistic user-style queries\nbased on telemetry analysis of a popular chat-based agent interactions. Our\nfindings reveal that existing benchmarks significantly overestimate agent\ncapabilities for some models by >50% over baseline performance for public\nbenchmarks and ~10-16% for our internal benchmark. This work establishes a new\nparadigm for evaluating interactive chat-based software engineering agents\nthrough benchmark mutation techniques.", "AI": {"tldr": "Current benchmarks give an inflated sense of chat-based coding assistant abilities. The paper presents a method to make benchmarks more realistic using real interaction data, showing that agent capabilities were previously overestimated by up to 50%. This new approach allows more reliable assessment of such tools.", "motivation": "Current benchmarking approaches for software engineering agents do not accurately reflect real-world developer interaction, particularly in chat-based IDE environments. This results in an overestimation of agent capabilities.", "method": "The authors propose a novel benchmarking framework that transforms formal benchmarks (like GitHub issue descriptions) into realistic user queries by analyzing actual developer interaction patterns with chat-based coding assistants. This is achieved through telemetry analysis and systematic mutation of benchmark tasks.", "result": "When applying this framework to several benchmarks, including SWE-Bench Verified and private datasets, the authors found that existing benchmarks overestimated agent performance by more than 50% for public datasets and 10-16% for their internal benchmark.", "conclusion": "There is a significant mismatch between current benchmark tasks and real developer-chat agent interactions. The proposed benchmark mutation technique enables more realistic evaluation and establishes a new standard for assessing chat-based software engineering agents."}}
{"id": "2510.09045", "categories": ["cs.SE", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09045", "abs": "https://arxiv.org/abs/2510.09045", "authors": ["Manojit Chakraborty", "Madhusudan Ghosh", "Rishabh Gupta"], "title": "Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements", "comment": null, "summary": "In the domain of software development, LLMs have been utilized to automate\ntasks such as code translation, where source code from one programming language\nis translated to another while preserving its functionality. However, LLMs\noften struggle with long source codes that don't fit into the context window,\nwhich produces inaccurate translations. To address this, we propose a novel\nzero-shot code translation method that incorporates identifier replacement. By\nsubstituting user-given long identifiers with generalized placeholders during\ntranslation, our method allows the LLM to focus on the logical structure of the\ncode, by reducing token count and memory usage, which improves the efficiency\nand cost-effectiveness of long code translation. Our empirical results\ndemonstrate that our approach preserves syntactical and hierarchical\ninformation and produces translation results with reduced tokens.", "AI": {"tldr": "This paper presents a zero-shot code translation technique for LLMs that substitutes lengthy identifiers with placeholders, improving translation accuracy and efficiency for long source codes by reducing token usage and preserving code structure.", "motivation": "LLMs face challenges translating long source codes because their context window size is limited, resulting in accuracy issues.", "method": "The proposed method performs zero-shot code translation by replacing long user-defined identifiers with generalized placeholders before feeding the code into the LLM, lowering token count and memory usage.", "result": "Experiments show the method maintains code syntax and structure while reducing translation tokens required.", "conclusion": "Identifier replacement enables more efficient and accurate long code translation using LLMs by focusing on code structure and lowering computational cost."}}
{"id": "2510.09058", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09058", "abs": "https://arxiv.org/abs/2510.09058", "authors": ["Italo Santos", "Cleyton Magalhaes", "Ronnie de Souza Santos"], "title": "Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding", "comment": null, "summary": "Large Language Models have quickly become a central component of modern\nsoftware development workflows, and software practitioners are increasingly\nintegrating LLMs into various stages of the software development lifecycle.\nDespite the growing presence of LLMs, there is still a limited understanding of\nhow these tools are actually used in practice and how professionals perceive\ntheir benefits and limitations. This paper presents preliminary findings from a\nglobal survey of 131 software practitioners. Our results reveal how LLMs are\nutilized for various coding-specific tasks. Software professionals report\nbenefits such as increased productivity, reduced cognitive load, and faster\nlearning, but also raise concerns about LLMs' inaccurate outputs, limited\ncontext awareness, and associated ethical risks. Most developers treat LLMs as\nassistive tools rather than standalone solutions, reflecting a cautious yet\npractical approach to their integration. Our findings provide an early,\npractitioner-focused perspective on LLM adoption, highlighting key\nconsiderations for future research and responsible use in software engineering.", "AI": {"tldr": "A global survey of software professionals reveals LLMs are mainly used as coding aids, boosting productivity but raising concerns about errors, context limitations, and ethics. Most developers use them as supplementary tools, pointing to a cautious but practical adoption and underlining the need for further research and responsible practices.", "motivation": "Large Language Models (LLMs) are becoming increasingly prevalent in software development, but there is limited knowledge about how these models are used in practice and how software professionals perceive their value and challenges.", "method": "The authors conducted a global survey involving 131 software practitioners to gather data on how LLMs are used and perceived in real development workflows.", "result": "LLMs are primarily used for a variety of coding-related tasks. Practitioners report benefits such as greater productivity, lower cognitive effort, and faster learning. However, concerns are raised regarding LLMs' inaccuracy, limited context awareness, and ethical risks. Most practitioners use LLMs as assistive aids instead of fully relying on them.", "conclusion": "The study provides an early, practitioner-oriented understanding of LLM adoption, emphasizing both benefits and challenges. Developers approach LLM integration cautiously, using them as support tools. These insights highlight important considerations for future research and responsible use in software engineering."}}
{"id": "2510.09108", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09108", "abs": "https://arxiv.org/abs/2510.09108", "authors": ["Lukas Krodinger", "Altin Hajdari", "Stephan Lukasczyk", "Gordon Fraser"], "title": "Constraint-Guided Unit Test Generation for Machine Learning Libraries", "comment": "Accepted for SSBSE 2025", "summary": "Machine learning (ML) libraries such as PyTorch and TensorFlow are essential\nfor a wide range of modern applications. Ensuring the correctness of ML\nlibraries through testing is crucial. However, ML APIs often impose strict\ninput constraints involving complex data structures such as tensors. Automated\ntest generation tools such as Pynguin are not aware of these constraints and\noften create non-compliant inputs. This leads to early test failures and\nlimited code coverage. Prior work has investigated extracting constraints from\nofficial API documentation. In this paper, we present PynguinML, an approach\nthat improves the Pynguin test generator to leverage these constraints to\ngenerate compliant inputs for ML APIs, enabling more thorough testing and\nhigher code coverage. Our evaluation is based on 165 modules from PyTorch and\nTensorFlow, comparing PynguinML against Pynguin. The results show that\nPynguinML significantly improves test effectiveness, achieving up to 63.9 %\nhigher code coverage.", "AI": {"tldr": "Automated test generators struggle with ML API constraints. PynguinML uses documentation-extracted constraints to create better tests for PyTorch and TensorFlow, improving code coverage by up to 63.9%.", "motivation": "Machine learning libraries require robust testing for correctness, but automated test generators fail to handle strict API input constraints, leading to low coverage.", "method": "The paper proposes PynguinML, an enhanced version of Pynguin, which utilizes input constraints from API documentation to generate compliant test cases for ML libraries.", "result": "PynguinML was evaluated on 165 modules from PyTorch and TensorFlow, showing up to 63.9% increase in code coverage compared to the original Pynguin tool.", "conclusion": "Leveraging API constraint information substantially improves automated test case generation for ML libraries, leading to more effective and comprehensive testing."}}
{"id": "2510.09134", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.09134", "abs": "https://arxiv.org/abs/2510.09134", "authors": ["Amal Elgammal", "Bernd J. Kr\u00e4mer", "Michael P. Papazoglou", "Mira Raheem"], "title": "A Semantic Framework for Patient Digital Twins in Chronic Care", "comment": "This manuscript is currently under review at Software and Systems\n  Modeling (SoSyM)", "summary": "Personalized chronic care requires the integration of multimodal health data\nto enable precise, adaptive, and preventive decision-making. Yet most current\ndigital twin (DT) applications remain organ-specific or tied to isolated data\ntypes, lacking a unified and privacy-preserving foundation. This paper\nintroduces the Patient Medical Digital Twin (PMDT), an ontology-driven in\nsilico patient framework that integrates physiological, psychosocial,\nbehavioral, and genomic information into a coherent, extensible model.\nImplemented in OWL 2.0, the PMDT ensures semantic interoperability, supports\nautomated reasoning, and enables reuse across diverse clinical contexts. Its\nontology is structured around modular Blueprints (patient, disease and\ndiagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse\nevents), formalized through dedicated conceptual views. These were iteratively\nrefined and validated through expert workshops, questionnaires, and a pilot\nstudy in the EU H2020 QUALITOP project with real-world immunotherapy patients.\nEvaluation confirmed ontology coverage, reasoning correctness, usability, and\nGDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous\ndata, operationalize competency questions, and support descriptive, predictive,\nand prescriptive analytics in a federated, privacy-preserving manner. By\nbridging gaps in data fragmentation and semantic standardization, the PMDT\nprovides a validated foundation for next-generation digital health ecosystems,\ntransforming chronic care toward proactive, continuously optimized, and\nequitable management.", "AI": {"tldr": "The paper introduces PMDT, a privacy-preserving, ontology-based digital twin framework for chronic care that integrates and standardizes disparate health data streams. Validated through clinical study, it supports advanced analytics and serves as a foundation for next-generation digital health systems.", "motivation": "Current digital twin (DT) applications for chronic care are limited: they tend to focus on single organs or data types and do not offer a unified, privacy-preserving system to integrate multimodal health data. There is a pressing need for a framework that can combine various patient information streams (physiological, psychosocial, behavioral, genomic) into a semantically consistent, reusable, and privacy-compliant foundation.", "method": "The paper presents the Patient Medical Digital Twin (PMDT), an ontology-driven framework built in OWL 2.0. PMDT organizes health data into modular 'Blueprints' covering patient profile, disease, treatment, care trajectories, safety, pathways, and adverse events. It uses conceptual views refined via expert workshops, questionnaires, and pilot studies (notably, with immunotherapy patients in the EU H2020 QUALITOP project) to validate, test, and improve the ontology.", "result": "The PMDT framework demonstrates successful integration of heterogeneous health data, accurate automatic reasoning, broad ontology coverage, usability in clinical contexts, and compliance with GDPR. It can operationalize clinical queries and support a range of analytics (descriptive, predictive, prescriptive) in federated, privacy-preserving environments.", "conclusion": "PMDT bridges key gaps related to fragmented data and lack of semantic standardization in chronic care management. It sets the groundwork for future digital health ecosystems that are personalized, proactive, continuously optimized, and equitable, leading to transformed chronic care delivery."}}
{"id": "2510.09308", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09308", "abs": "https://arxiv.org/abs/2510.09308", "authors": ["Mira Raheem", "Amal Elgammal", "Michael Papazoglou", "Bernd Kr\u00e4mer", "Neamat El-Tazi"], "title": "A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms", "comment": "Disclaimer: This manuscript is currently under review at * MDPI\n  Informatics*", "summary": "Artificial intelligence (AI) has the potential to transform healthcare by\nsupporting more accurate diagnoses and personalized treatments. However, its\nadoption in practice remains constrained by fragmented data sources, strict\nprivacy rules, and the technical complexity of building reliable clinical\nsystems. To address these challenges, we introduce a model driven engineering\n(MDE) framework designed specifically for healthcare AI. The framework relies\non formal metamodels, domain-specific languages (DSLs), and automated\ntransformations to move from high level specifications to running software. At\nits core is the Medical Interoperability Language (MILA), a graphical DSL that\nenables clinicians and data scientists to define queries and machine learning\npipelines using shared ontologies. When combined with a federated learning\narchitecture, MILA allows institutions to collaborate without exchanging raw\npatient data, ensuring semantic consistency across sites while preserving\nprivacy. We evaluate this approach in a multi center cancer immunotherapy\nstudy. The generated pipelines delivered strong predictive performance, with\nsupport vector machines achieving up to 98.5 percent and 98.3 percent accuracy\nin key tasks, while substantially reducing manual coding effort. These findings\nsuggest that MDE principles metamodeling, semantic integration, and automated\ncode generation can provide a practical path toward interoperable,\nreproducible, and trustworthy digital health platforms.", "AI": {"tldr": "A novel model-driven engineering framework with graphical tools and federated learning makes AI systems in healthcare easier, more consistent, and privacy-preserving, showing high accuracy and reduced coding burden in a cancer immunotherapy study.", "motivation": "Despite the transformative potential of AI in healthcare, real-world implementation is hindered by fragmented data, privacy concerns, and the complexity of reliable clinical system development. Overcoming these barriers is essential for effective AI adoption in clinical settings.", "method": "The authors present a Model Driven Engineering (MDE) framework tailored for healthcare AI, built on formal metamodels, domain-specific languages (DSLs), and automated software transformations. Central to this framework is the Medical Interoperability Language (MILA), a graphical DSL enabling multi-institutional collaboration via shared ontologies and federated learning without exchanging patient data. The approach is evaluated using a multicenter cancer immunotherapy study.", "result": "The framework enabled strong predictive performance in the study, with support vector machines achieving up to 98.5% accuracy on key tasks. It also substantially reduced manual coding effort and ensured semantic consistency and privacy in cross-site collaboration.", "conclusion": "MDE principles\u2014hybrid metamodeling, semantic integration, and automated code generation\u2014offer a practical solution for building interoperable, reproducible, and trustworthy digital health platforms, facilitating AI's effective and privacy-preserving use in healthcare."}}
{"id": "2510.09400", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09400", "abs": "https://arxiv.org/abs/2510.09400", "authors": ["He Jiang", "Yufu Wang", "Hao Lin", "Peiyu Zou", "Zhide Zhou", "Ang Jia", "Xiaochen Li", "Zhilei Ren"], "title": "TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation", "comment": null, "summary": "Large Language Models (LLMs) have shown strong performance in automated\nsource-to-target code translation through pretraining on extensive code\ncorpora. However, mainstream LLM-based code translation methods suffer from two\ncritical limitations. First, they are highly sensitive to language-specific\nfeatures, which often introduce source-language syntax or lexicon into the\noutput, leading to syntactic confusion. Second, they lack fine-grained semantic\nalignment due to an over-reliance on function-level parallel datasets,\nresulting in semantic misalignment between the translated code and the original\nsource. To overcome these limitations, we propose TIT, a Tree-structured\nInstruction Tuning paradigm for LLM-based code translation. Specifically, TIT\nconsists of three modules. First, to mitigate syntactic confusion, the\nsyntactic information representation module integrates language-agnostic\nsyntactic features via structured parsing. Then, to generate high-quality\nfine-grained parallel data, the fine-grained parallel dataset augmentation\nmodule aligns nodes with code segments through statement-level segmentation and\ncontrastive matching. Finally, we leverage the dual-stage tree instruction\ntuning module to alleviate the contextual processing burden on the LLM caused\nby the introduction of syntactic information. The first stage employs\nsyntax-aware fine-tuning to enable the LLM to autonomously comprehend\nstructured syntactic information, while the second stage utilizes code\ngeneration fine-tuning to guide the model in generating accurate target code\nbased on function-level syntactic dependencies. The experimental results\ndemonstrate that the proposed method significantly outperforms existing\napproaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in\ncode translation while markedly reducing syntactic confusion.", "AI": {"tldr": "TIT introduces tree-structured instruction tuning to improve LLM code translation, mitigating both syntactic confusion and semantic misalignment, and achieves much higher translation success rates than current approaches.", "motivation": "Mainstream LLM-based code translation methods struggle with two main challenges: they are prone to copying source-language syntax or words (leading to syntactic confusion) and lack fine-grained semantic alignment, which causes errors in matching the meaning of code between languages.", "method": "The authors introduce TIT (Tree-structured Instruction Tuning), which includes three modules: (1) a syntactic information representation module that uses structured parsing for language-agnostic features, (2) a fine-grained parallel dataset augmentation module that creates statement-level data with contrastive matching, and (3) a dual-stage tree instruction tuning module that first fine-tunes the model on syntax-aware tasks and then on code generation tasks with function-level dependencies.", "result": "Experimental results show TIT delivers significantly better code translation performance across various LLMs, with a success rate 1.22x-1.75x higher than previous methods and substantially less syntactic confusion.", "conclusion": "Tree-structured Instruction Tuning (TIT) effectively resolves key limitations in LLM-based code translation by introducing syntactic and semantic fine-tuning, achieving superior accuracy and reduced confusion in translated code compared to existing methods."}}
