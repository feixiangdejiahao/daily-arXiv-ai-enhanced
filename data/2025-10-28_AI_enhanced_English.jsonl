{"id": "2510.21902", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21902", "abs": "https://arxiv.org/abs/2510.21902", "authors": ["Timoth\u00e9 Boulet", "Xavier Hinaut", "Cl\u00e9ment Moulin-Frier"], "title": "Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments", "comment": "10 pages, 7 figures", "summary": "Software Engineering Agents (SWE-Agents) have proven effective for\ntraditional software engineering tasks with accessible codebases, but their\nperformance for embodied tasks requiring well-designed information discovery\nremains unexplored. We present the first extended evaluation of SWE-Agents on\ncontroller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to\nsolve 20 diverse embodied tasks from the Minigrid environment. Our experiments\ncompare agent performance across different information access conditions: with\nand without environment source code access, and with varying capabilities for\ninteractive exploration. We quantify how different information access levels\naffect SWE-Agent performance for embodied tasks and analyze the relative\nimportance of static code analysis versus dynamic exploration for task solving.\nThis work establishes controller generation for embodied tasks as a crucial\nevaluation domain for SWE-Agents and provides baseline results for future\nresearch in efficient reasoning systems.", "AI": {"tldr": "This paper extends SWE-Agent evaluation to embodied tasks using the Minigrid environment, analyzing how code access and interactive exploration affect agent success, and sets a benchmark for future research in reasoning systems for controller generation.", "motivation": "Traditional software engineering agents perform well on tasks with accessible code, but their effectiveness on embodied tasks\u2014requiring discovery and interaction with environments\u2014has not been investigated.", "method": "This study adapts the Mini-SWE-Agent (MSWEA) to address 20 varied embodied tasks in the Minigrid environment. It systematically evaluates agent performance under different information access scenarios: having or lacking environment source code, and with varying abilities for interactive exploration.", "result": "The research quantifies the impact of different sources of information (code access and exploration capabilities) on agent performance. It also examines the comparative roles of static code analysis and dynamic exploration in solving embodied tasks.", "conclusion": "Controller generation for embodied tasks is identified as an important benchmark for SWE-Agents. The paper presents baseline quantitative results that will guide future research on reasoning systems for these tasks."}}
{"id": "2510.21903", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21903", "abs": "https://arxiv.org/abs/2510.21903", "authors": ["Xuhui Zhou", "Valerie Chen", "Zora Zhiruo Wang", "Graham Neubig", "Maarten Sap", "Xingyao Wang"], "title": "TOM-SWE: User Mental Modeling For Software Engineering Agents", "comment": null, "summary": "Recent advances in coding agents have made them capable of planning, editing,\nrunning, and testing complex code bases. Despite their growing ability in\ncoding tasks, these systems still struggle to infer and track user intent,\nespecially when instructions are underspecified or context-dependent. To bridge\nthis gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary\nsoftware-engineering (SWE) agent with a lightweight theory-of-mind (ToM)\npartner agent dedicated to modeling the user's mental state. The ToM agent\ninfers user goals, constraints, and preferences from instructions and\ninteraction history, maintains a \\textbf{persistent memory} of the user, and\nprovides user-related suggestions to the SWE agent. In two software engineering\nbenchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task\nsuccess rates and user satisfaction. Notably, on the stateful SWE benchmark, a\nnewly introduced evaluation that provides agents with a user simulator along\nwith previous interaction histories, ToM-SWE achieves a substantially higher\ntask success rate of 59.7\\% compared to 18.1\\% for OpenHands, a\nstate-of-the-art SWE agent. Furthermore, in a three-week study with\nprofessional developers using ToM-SWE in their daily work, participants found\nit useful 86\\% of the time, underscoring the value of stateful user modeling\nfor practical coding agents.", "AI": {"tldr": "ToM-SWE, a dual-agent coding system featuring a theory-of-mind component to track user goals and context, significantly improves task outcomes and user satisfaction over prior methods, underscoring the importance of advanced user modeling in coding agents.", "motivation": "Despite advances in coding agents, these systems still have difficulty understanding and maintaining user intent, particularly when user instructions lack detail or depend heavily on context. There is a need to enhance coding agents' ability to model and track user goals for improved performance.", "method": "The paper introduces ToM-SWE, a dual-agent system composed of a primary software-engineering (SWE) agent and a complementary theory-of-mind (ToM) agent. The ToM agent models the user's mental state by inferring their goals, constraints, and preferences from their instructions and interaction history. It maintains persistent memory of the user and supplies relevant cues to the SWE agent. The framework is evaluated on two benchmarks (ambiguous SWE-bench and stateful SWE-bench) and through a user study with professional developers.", "result": "ToM-SWE outperforms the baselines in both benchmarks. In the stateful SWE-bench evaluation, ToM-SWE achieved a task success rate of 59.7% versus 18.1% for OpenHands, a state-of-the-art SWE agent. In a three-week real-world study, developers rated ToM-SWE as useful 86% of the time.", "conclusion": "Stateful user modeling, as enabled by the ToM agent, significantly enhances the effectiveness and practical usefulness of coding agents. Persistent memory and context-aware reasoning about user intentions lead to higher task success rates and improved developer satisfaction."}}
{"id": "2510.21933", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21933", "abs": "https://arxiv.org/abs/2510.21933", "authors": ["Joao Correia", "Daniel Coutinho", "Marco Castelluccio", "Caio Barbosa", "Rafael de Mello", "Anita Sarma", "Alessandro Garcia", "Marco Gerosa", "Igor Steinmacher"], "title": "A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case", "comment": "13 pages", "summary": "The use of Large Language Models (LLMs) to support tasks in software\ndevelopment has steadily increased over recent years. From assisting developers\nin coding activities to providing conversational agents that answer newcomers'\nquestions. In collaboration with the Mozilla Foundation, this study evaluates\nthe effectiveness of Retrieval-Augmented Generation (RAG) in assisting\ndevelopers within the Mozilla Firefox project. We conducted an empirical\nanalysis comparing responses from human developers, a standard GPT model, and a\nGPT model enhanced with RAG, using real queries from Mozilla's developer chat\nrooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses\nbased on helpfulness, comprehensiveness, and conciseness. The results show that\nRAG-assisted responses were more comprehensive than human developers (62.50% to\n54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to\nenhance developer assistance. However, the RAG responses were not as concise\nand often verbose. The results show the potential to apply RAG-based tools to\nOpen Source Software (OSS) to minimize the load to core maintainers without\nlosing answer quality. Toning down retrieval mechanisms and making responses\neven shorter in the future would enhance developer assistance in massive\nprojects like Mozilla Firefox.", "AI": {"tldr": "The paper finds Retrieval-Augmented Generation tools offer more comprehensive and almost equally helpful answers as humans in OSS developer chats, but tend to be verbose. RAG could lessen maintainers' workload, especially for projects like Firefox, if future improvements focus on conciseness.", "motivation": "The motivation is to improve developer support in open source projects by using advanced AI models, particularly focusing on helping developers address questions and reduce the workload for core maintainers without compromising answer quality.", "method": "The study conducts an empirical comparison of response quality from human developers, a standard GPT model, and a GPT model enhanced with Retrieval-Augmented Generation (RAG), using actual queries from Mozilla Firefox developer chat rooms. Responses are evaluated by Mozilla experts on helpfulness, comprehensiveness, and conciseness.", "result": "RAG-augmented responses are more comprehensive than those from human developers, nearly as helpful, but less concise due to verbosity. This demonstrates that RAG can support developers effectively, though improvements in making responses shorter may be desirable.", "conclusion": "RAG-based language models can provide significant assistance in OSS projects by offering high-quality, comprehensive responses and reducing the burden on maintainers. However, optimizing for brevity can further improve their utility in large-scale projects."}}
{"id": "2510.21966", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21966", "abs": "https://arxiv.org/abs/2510.21966", "authors": ["Musengamana Jean de Dieu", "Ruiyin Li", "Peng Liang", "Mojtaba Shahin", "Muhammad Waseem", "Arif Ali Khan", "Bangchao Wang", "Mst Shamima Aktar"], "title": "ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities", "comment": "42 pages, 14 images, 6 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Stack Overflow (SO), a leading online community forum, is a rich source of\nsoftware development knowledge. However, locating architectural knowledge, such\nas architectural solutions remains challenging due to the overwhelming volume\nof unstructured content and fragmented discussions. Developers must manually\nsift through posts to find relevant architectural insights, which is\ntime-consuming and error-prone. This study introduces ArchISMiner, a framework\nfor mining architectural knowledge from SO. The framework comprises two\ncomplementary components: ArchPI and ArchISPE. ArchPI trains and evaluates\nmultiple models, including conventional ML/DL models, Pre-trained Language\nModels (PLMs), and Large Language Models (LLMs), and selects the\nbest-performing model to automatically identify Architecture-Related Posts\n(ARPs) among programming-related discussions. ArchISPE employs an indirect\nsupervised approach that leverages diverse features, including BERT embeddings\nand local TextCNN features, to extract architectural issue-solution pairs. Our\nevaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in\nARP detection, and ArchISPE outperforms baselines in both SE and NLP fields,\nachieving F1-scores of 0.883 for architectural issues and 0.894 for solutions.\nA user study further validated the quality (e.g., relevance and usefulness) of\nthe identified ARPs and the extracted issue-solution pairs. Moreover, we\napplied ArchISMiner to three additional forums, releasing a dataset of over 18K\narchitectural issue-solution pairs. Overall, ArchISMiner can help architects\nand developers identify ARPs and extract succinct, relevant, and useful\narchitectural knowledge from developer communities more accurately and\nefficiently. The replication package of this study has been provided at\nhttps://github.com/JeanMusenga/ArchISPE", "AI": {"tldr": "ArchISMiner enables automatic, accurate mining of architectural issue-solution pairs from Stack Overflow and other forums, outperforming existing methods and releasing a valuable dataset to aid software architects and developers.", "motivation": "Locating architectural knowledge such as solutions from Stack Overflow is difficult due to the vast volume of unstructured and fragmented discussions, making manual searching time-consuming and prone to errors.", "method": "The study presents ArchISMiner, a mining framework consisting of two components: ArchPI, which selects the best ML/DL/PLM/LLM model to detect architecture-related posts, and ArchISPE, which uses BERT embeddings and local TextCNN features to automatically extract issue-solution pairs through indirect supervised learning.", "result": "ArchPI achieves a high F1-score of 0.960 for detecting architecture-related posts. ArchISPE outperforms baseline methods in Software Engineering and NLP, with F1-scores of 0.883 for issues and 0.894 for solutions. ArchISMiner was further applied to other forums, resulting in a released dataset of over 18,000 architectural issue-solution pairs.", "conclusion": "ArchISMiner significantly improves the efficiency and accuracy of discovering and extracting architectural insights from developer forums, benefiting architects and developers in accessing relevant and useful knowledge."}}
{"id": "2510.23517", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.23517", "abs": "https://arxiv.org/abs/2510.23517", "authors": ["Sidney Congard", "Guillaume Munch-Maccagnoni", "R\u00e9mi Douence"], "title": "Linear effects, exceptions, and resource safety: a Curry-Howard correspondence for destructors", "comment": "26 pages + appendix", "summary": "We analyse the problem of combining linearity, effects, and exceptions, in\nabstract models of programming languages, as the issue of providing some kind\nof strength for a monad $T(- \\oplus E)$ in a linear setting. We consider in\nparticular for $T$ the allocation monad, which we introduce to model and study\nresource-safety properties. We apply these results to a series of two linear\neffectful calculi for which we establish their resource-safety properties.\n  The first calculus is a linear call-by-push-value language with two\nallocation effects $\\mathit{new}$ and $\\mathit{delete}$. The resource-safety\nproperties follow from the linear (and even ordered) character of the typing\nrules.\n  We then explain how to integrate exceptions on top of linearity and effects\nby adjoining default destruction actions to types, as inspired by C++/Rust\ndestructors. We see destructors as objects $\\delta : A\\rightarrow TI$ in the\nslice category over $TI$. This construction gives rise to a second calculus, an\naffine ordered call-by-push-value language with exceptions and destructors, in\nwhich the weakening rule performs a side-effect. As in C++/Rust, a ``move''\noperation is necessary to allow random-order release of resources, as opposed\nto last-in-first-out order. Moving resources is modelled as an exchange rule\nthat performs a side-effect.", "AI": {"tldr": "This paper creates type-theoretic and category-based models for programming languages, effectively combining resource-safety, side effects, and exception handling, capturing properties found in C++/Rust using concepts like destructors and move semantics.", "motivation": "To address the challenge of combining linearity (managing resources safely), computational effects (side effects in programming), and exceptions in programming language models, and specifically to model resource-safety like that found in modern systems languages (e.g., C++/Rust).", "method": "The authors analyze the strength of a monad $T(- \\oplus E)$ within a linear setting, introducing the allocation monad to study resource-safety. They develop two effectful calculi: (1) a linear call-by-push-value language with allocation effects, and (2) an affine ordered variant with exceptions and destructors, using category-theoretic constructions inspired by C++/Rust.", "result": "They establish resource-safety for a linear call-by-push-value calculus with allocation/deallocation effects, thanks to linear/ordered typing. They then show how exceptions and destructors can be integrated via default destruction, supporting side-effects during weakening and random-order resource release through move operations.", "conclusion": "The paper offers abstract models that successfully combine linearity, effects, and exceptions, allowing the formalization and proof of resource-safety properties similar to those in languages like C++/Rust. The exchange rule enables flexible and safe resource management."}}
{"id": "2510.21993", "categories": ["cs.SE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.21993", "abs": "https://arxiv.org/abs/2510.21993", "authors": ["Yupeng Qi", "Ran Xu", "Xu Chu"], "title": "FeaGPT: an End-to-End agentic-AI for Finite Element Analysis", "comment": null, "summary": "Large language models (LLMs) are establishing new paradigms for engineering\napplications by enabling natural language control of complex computational\nworkflows. This paper introduces FeaGPT, the first framework to achieve\ncomplete geometry-mesh-simulation workflows through conversational interfaces.\nUnlike existing tools that automate individual FEA components, FeaGPT\nimplements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline\nthat transforms engineering specifications into validated computational results\nwithout manual intervention. The system interprets engineering intent,\nautomatically generates physics-aware adaptive meshes, configures complete FEA\nsimulations with proper boundary condition inference, and performs\nmulti-objective analysis through closed-loop iteration.\n  Experimental validation confirms complete end-to-end automation capability.\nIndustrial turbocharger cases (7-blade compressor and 12-blade turbine at\n\\SI{110000}{rpm}) demonstrate the system successfully transforms natural\nlanguage specifications into validated CalculiX simulations, producing\nphysically realistic results for rotating machinery analysis. Additional\nvalidation through 432 NACA airfoil configurations confirms scalability for\nparametric design exploration. These results demonstrate that natural language\ninterfaces can effectively democratize access to advanced computational\nengineering tools while preserving analytical rigor.", "AI": {"tldr": "FeaGPT uses natural language commands to fully automate geometry, mesh generation, FEA simulation, and analysis for engineering tasks, validated on industrial and parametric cases. It shows LLM-powered conversational interfaces can make advanced engineering tools much more accessible and user-friendly.", "motivation": "Traditional finite element analysis (FEA) tools require manual intervention and complex workflows, slowing down engineering design and limiting accessibility. The motivation is to make advanced computational engineering more user-friendly and accessible by leveraging natural language interfaces that automate complete FEA processes.", "method": "The paper introduces FeaGPT, a framework that enables conversational, natural language-based control of the full geometry-mesh-simulation-analysis (GMSA) pipeline. The system interprets engineering intent, automatically generates adaptive meshes, configures simulations with proper boundary conditions, and performs iterative multi-objective analysis. It uses large language models to automate the workflow end-to-end.", "result": "FeaGPT is validated with industrial turbocharger cases (7-blade compressor and 12-blade turbine at 110,000 rpm), demonstrating the successful transformation of natural language specifications into accurate CalculiX simulations. Further validation with 432 NACA airfoil cases shows scalability for parametric design. The results confirm that the system produces physically realistic solutions and supports advanced engineering tasks.", "conclusion": "FeaGPT achieves full automation of complex engineering simulation workflows via conversational interfaces, demonstrating the ability of LLMs to democratize and streamline computational engineering for both industrial and parametric design tasks, while maintaining analytical rigor and scalability."}}
{"id": "2510.22003", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22003", "abs": "https://arxiv.org/abs/2510.22003", "authors": ["Stefan Julian Kooy", "Jean Paul Sebastian Piest", "Rob Henk Bemthuis"], "title": "Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review", "comment": "17 pages, 1 figure, 5 tables; to appear in Enterprise Design,\n  Operations, and Computing. EDOC 2025 Workshops, Lecture Notes in Business\n  Information Processing (LNBIP), Springer, 2025. Part of 29th International\n  Conference on Enterprise Design, Operations, and Computing (EDOC)", "summary": "Generative AI (GenAI) is reshaping enterprise architecture work in agile\nsoftware organizations, yet evidence on its effects remains scattered. We\nreport a systematic literature review (SLR), following established SLR\nprotocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies\nacross enterprise, solution, domain, business, and IT architect roles. GenAI\nmost consistently supports (i) design ideation and trade-off exploration; (ii)\nrapid creation and refinement of artifacts (e.g., code, models, documentation);\nand (iii) architectural decision support and knowledge retrieval. Reported\nrisks include opacity and bias, contextually incorrect outputs leading to\nrework, privacy and compliance concerns, and social loafing. We also identify\nemerging skills and competencies, including prompt engineering, model\nevaluation, and professional oversight, and organizational enablers around\nreadiness and adaptive governance. The review contributes with (1) a mapping of\nGenAI use cases and risks in agile architecting, (2) implications for\ncapability building and governance, and (3) an initial research agenda on\nhuman-AI collaboration in architecture. Overall, the findings inform\nresponsible adoption of GenAI that accelerates digital transformation while\nsafeguarding architectural integrity.", "AI": {"tldr": "GenAI is transforming agile enterprise architecture work by supporting creativity, automation, and knowledge retrieval, but also introduces risks related to bias, errors, and privacy. The study maps use cases, risks, and identifies skills and governance needs for responsible adoption.", "motivation": "Generative AI is rapidly influencing enterprise architecture in agile software organizations, but there is fragmented knowledge on its real-world effects and implications.", "method": "A systematic literature review (SLR) guided by Kitchenham and PRISMA protocols, analyzing 1,697 records to synthesize findings from 33 studies across architectural roles.", "result": "Through a systematic literature review of 1,697 records, the study identifies 33 relevant studies that detail GenAI's key benefits, such as enhanced design ideation, accelerated artifact creation, and improved decision support, as well as significant risks like bias, errors, privacy concerns, and reduced social effort.", "conclusion": "The responsible adoption of GenAI can accelerate digital transformation in organizations, provided architectural integrity is safeguarded through new skills, adaptive governance, and awareness of associated risks."}}
{"id": "2510.22210", "categories": ["cs.SE", "cs.AI", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.22210", "abs": "https://arxiv.org/abs/2510.22210", "authors": ["Gwihwan Go", "Quan Zhang", "Chijin Zhou", "Zhao Wei", "Yu Jiang"], "title": "LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation", "comment": "13pages, 6 figures", "summary": "Automated unit test generation is essential for robust software development,\nyet existing approaches struggle to generalize across multiple programming\nlanguages and operate within real-time development. While Large Language Models\n(LLMs) offer a promising solution, their ability to generate high coverage test\ncode depends on prompting a concise context of the focal method. Current\nsolutions, such as Retrieval-Augmented Generation, either rely on imprecise\nsimilarity-based searches or demand the creation of costly, language-specific\nstatic analysis pipelines. To address this gap, we present LSPRAG, a framework\nfor concise-context retrieval tailored for real-time, language-agnostic unit\ntest generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)\nback-ends to supply LLMs with precise symbol definitions and references in real\ntime. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware\ncontext retrieval, requiring minimal per-language engineering effort. We\nevaluated LSPRAG on open-source projects spanning Java, Go, and Python.\nCompared to the best performance of baselines, LSPRAG increased line coverage\nby up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.", "AI": {"tldr": "LSPRAG is a framework that uses Language Server Protocols to provide real-time, language-agnostic context to LLMs for unit test generation. It outperforms existing methods in code coverage across several programming languages, requiring minimal additional engineering.", "motivation": "Automated unit test generation is crucial for software robustness, but existing methods are limited in generalizing across programming languages and providing real-time support. Techniques like Retrieval-Augmented Generation are hindered by imprecise searches and high engineering costs for language-specific solutions.", "method": "The paper introduces LSPRAG, a framework that retrieves precise context for real-time, language-agnostic unit test generation by leveraging Language Server Protocol (LSP) backends. LSPRAG uses LSP servers to supply Large Language Models (LLMs) with real-time, language-aware symbol definitions and references for concise and effective context during unit test generation, requiring minimal effort per language.", "result": "LSPRAG was evaluated on open-source projects written in Java, Go, and Python. It outperformed baselines, improving line coverage by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.", "conclusion": "LSPRAG enables scalable, effective, and efficient language-agnostic automated unit test generation using LLMs. By leveraging LSP backends, it reduces the costs and engineering efforts associated with language-specific pipelines, and significantly increases test coverage across multiple languages."}}
{"id": "2510.22224", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.22224", "abs": "https://arxiv.org/abs/2510.22224", "authors": ["Guan-Yan Yang", "Farn Wang"], "title": "Taming Silent Failures: A Framework for Verifiable AI Reliability", "comment": "This preprint has been accepted by IEEE Reliability Magazine. 10\n  pages, 3 figures", "summary": "The integration of Artificial Intelligence (AI) into safety-critical systems\nintroduces a new reliability paradigm: silent failures, where AI produces\nconfident but incorrect outputs that can be dangerous. This paper introduces\nthe Formal Assurance and Monitoring Environment (FAME), a novel framework that\nconfronts this challenge. FAME synergizes the mathematical rigor of offline\nformal synthesis with the vigilance of online runtime monitoring to create a\nverifiable safety net around opaque AI components. We demonstrate its efficacy\nin an autonomous vehicle perception system, where FAME successfully detected\n93.5% of critical safety violations that were otherwise silent. By\ncontextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards,\nwe provide reliability engineers with a practical, certifiable pathway for\ndeploying trustworthy AI. FAME represents a crucial shift from accepting\nprobabilistic performance to enforcing provable safety in next-generation\nsystems.", "AI": {"tldr": "AI introduces silent failure risks in critical systems. FAME integrates formal synthesis and runtime monitoring to detect these failures, catching 93.5% of safety violations in autonomous vehicles and providing a certifiable path to trustworthy AI.", "motivation": "AI systems in safety-critical domains (like autonomous vehicles) can fail silently, producing confident but wrong outputs with potentially dangerous consequences. Existing approaches lack practical, certifiable methods to reliably detect these failures and assure safety.", "method": "The authors introduced FAME (Formal Assurance and Monitoring Environment), a framework that integrates mathematical offline formal synthesis with online runtime monitoring, designed to create a verifiable safety net for AI components. They demonstrated FAME using an autonomous vehicle perception system and aligned it with industry safety standards (ISO 26262 and ISO/PAS 8800).", "result": "FAME detected 93.5% of otherwise silent critical safety violations in the tested autonomous vehicle perception system. It provided reliability engineers with a practical, certifiable method for trustworthy AI deployment.", "conclusion": "FAME shifts the reliability paradigm from merely probabilistic assurance to provable safety for AI in safety-critical systems, facilitating certified deployment aligned with industry safety standards."}}
{"id": "2510.22249", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22249", "abs": "https://arxiv.org/abs/2510.22249", "authors": ["Ibuki Nakamura", "Yutaro Kashiwa", "Bin Lin", "Hajimu Iida"], "title": "Understanding Self-Admitted Technical Debt in Test Code: An Empirical Study", "comment": null, "summary": "Developers often opt for easier but non-optimal implementation to meet\ndeadlines or create rapid prototypes, leading to additional effort known as\ntechnical debt to improve the code later. Oftentimes, developers explicitly\ndocument the technical debt in code comments, referred to as Self-Admitted\nTechnical Debt (SATD). Numerous researchers have investigated the impact of\nSATD on different aspects of software quality and development processes.\nHowever, most of these studies focus on SATD in production code, often\noverlooking SATD in the test code or assuming that it shares similar\ncharacteristics with SATD in production code. In fact, a significant amount of\nSATD is also present in the test code, with many instances not fitting into\nexisting categories for the production code. This study aims to fill this gap\nand disclose the nature of SATD in the test code by examining its distribution\nand types. Moreover, the relation between its presence and test quality is also\nanalyzed. Our empirical study, involving 17,766 SATD comments (14,987 from\nproduction code, 2,779 from test code) collected from 50 repositories,\ndemonstrates that while SATD widely exists in test code, it is not directly\nassociated with test smells. Our study also presents comprehensive categories\nof SATD types in the test code, and machine learning models are developed to\nautomatically classify SATD comments based on their types for easier\nmanagement. Our results show that the CodeBERT-based model outperforms other\nmachine learning models in terms of recall and F1-score. However, the\nperformance varies on different types of SATD.", "AI": {"tldr": "This study reveals the prevalence and distinct nature of self-admitted technical debt (SATD) in test code, uncovers its categories, and shows that it is not linked to test smells. Machine learning models, especially CodeBERT, can automatically classify SATD comments, supporting efficient management.", "motivation": "Previous research on Self-Admitted Technical Debt (SATD) has largely focused on production code, neglecting test code, even though test code also contains significant SATD with unique characteristics. This paper aims to analyze SATD specifically in test code, explore its distribution and types, and examine its impact on test quality.", "method": "The authors conducted an empirical study by collecting 17,766 SATD comments from 50 repositories, categorizing them into production and test code. They developed comprehensive SATD type categories for test code and built machine learning models (including a CodeBERT-based model) to automatically classify SATD comments.", "result": "SATD is prevalent in test code but does not have a direct association with test smells. The study provides detailed category types of SATD in test code. The CodeBERT-based model achieved superior recall and F1-score compared to other models when classifying SATD comments, although its performance varied across different SATD types.", "conclusion": "SATD exists widely in test code and differs from SATD in production code. Machine learning models can effectively aid in categorizing SATD comments for better management, with CodeBERT demonstrating strong overall results but varying effectiveness according to SATD types."}}
{"id": "2510.22254", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22254", "abs": "https://arxiv.org/abs/2510.22254", "authors": ["Eric W. Bridgeford", "Iain Campbell", "Zijao Chen", "Zhicheng Lin", "Harrison Ritz", "Joachim Vandekerckhove", "Russell A. Poldrack"], "title": "Ten Simple Rules for AI-Assisted Coding in Science", "comment": "9 pages of content; 1 table; 1 page appendix", "summary": "While AI coding tools have demonstrated potential to accelerate software\ndevelopment, their use in scientific computing raises critical questions about\ncode quality and scientific validity. In this paper, we provide ten practical\nrules for AI-assisted coding that balance leveraging capabilities of AI with\nmaintaining scientific and methodological rigor. We address how AI can be\nleveraged strategically throughout the development cycle with four key themes:\nproblem preparation and understanding, managing context and interaction,\ntesting and validation, and code quality assurance and iterative improvement.\nThese principles serve to emphasize maintaining human agency in coding\ndecisions, establishing robust validation procedures, and preserving the domain\nexpertise essential for methodologically sound research. These rules are\nintended to help researchers harness AI's transformative potential for faster\nsoftware development while ensuring that their code meets the standards of\nreliability, reproducibility, and scientific validity that research integrity\ndemands.", "AI": {"tldr": "This paper offers ten practical rules to help researchers use AI coding tools for scientific computing while maintaining code quality, scientific validity, and research integrity. The rules advocate for balanced, strategic use of AI, robust validation, and continued human oversight.", "motivation": "As AI coding tools become more prevalent in scientific computing, concerns arise about maintaining code quality and scientific validity. Researchers need practical guidelines to safely and effectively integrate AI into their workflows.", "method": "This paper proposes ten practical rules for AI-assisted coding, organized around four themes: problem preparation and understanding, managing context and interaction, testing and validation, and code quality assurance and iterative improvement. The approach is principled, focusing on balancing AI capabilities with scientific rigor.", "result": "The paper presents detailed rules to guide researchers in using AI tools responsibly, emphasizing human oversight, robust validation, and preservation of domain expertise. These rules promote reliable, reproducible, and scientifically valid code.", "conclusion": "Researchers can accelerate software development with AI tools if they follow structured guidelines that maintain scientific rigor and research integrity. The ten rules provide a roadmap for responsible and effective AI-assisted coding within scientific computing."}}
{"id": "2510.22318", "categories": ["cs.SE", "cs.AI", "K.3.2, D.2.5"], "pdf": "https://arxiv.org/pdf/2510.22318", "abs": "https://arxiv.org/abs/2510.22318", "authors": ["Tuan-Phong Ngo", "Bao-Ngoc Duong", "Tuan-Anh Hoang", "Joshua Dwight", "Ushik Shrestha Khwakhali"], "title": "Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus", "comment": "7 pages, 3 figures, 3 tables", "summary": "Software testing is a critical component in the software engineering field\nand is important for software engineering education. Thus, it is vital for\nacademia to continuously improve and update educational methods to reflect the\ncurrent state of the field. The International Software Testing Qualifications\nBoard (ISTQB) certification framework is globally recognized and widely adopted\nin industry and academia. However, ISTQB-based learning has been rarely applied\nwith recent generative artificial intelligence advances. Despite the growing\ncapabilities of large language models (LLMs), ISTQB-based learning and\ninstruction with LLMs have not been thoroughly explored. This paper explores\nand evaluates how LLMs can complement the ISTQB framework for higher education.\nThe findings present four key contributions: (i) the creation of a\ncomprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28\nsample exams and 1,145 questions; (ii) the development of a domain-optimized\nprompt that enhances LLM precision and explanation quality on ISTQB tasks;\n(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and\n(iv) actionable insights and recommendations for integrating LLMs into software\ntesting education. These findings highlight the promise of LLMs in supporting\nISTQB certification preparation and offer a foundation for their broader use in\nsoftware engineering at higher education.", "AI": {"tldr": "The paper assesses how LLMs can improve ISTQB-based software testing education, presenting a decade-long exam dataset, optimized prompts for LLM accuracy, systematic LLM evaluation, and practical educational recommendations, concluding that LLMs can significantly support ISTQB preparation and broader engineering instruction.", "motivation": "Software testing is crucial to software engineering and education, but educational methods need updating to reflect industry standards. The ISTQB framework is widely used, but its integration with recent AI advancements, especially LLMs, is largely unexplored.", "method": "The paper creates an extensive ISTQB-aligned dataset of sample exams and questions, designs a domain-optimized prompt for LLMs, evaluates state-of-the-art LLMs on these tasks, and derives actionable recommendations for educational integration.", "result": "The study produced a comprehensive dataset, enhanced LLM precision and explanations for ISTQB topics, systematically evaluated LLM performance, and offered recommendations for implementing LLMs in software testing education.", "conclusion": "LLMs show great potential in assisting ISTQB certification preparation and can serve as a foundation for wider application in software engineering education."}}
{"id": "2510.22338", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22338", "abs": "https://arxiv.org/abs/2510.22338", "authors": ["Aritra Mitra", "Srijoni Majumdar", "Anamitra Mukhopadhyay", "Partha Pratim Das", "Paul D Clough", "Partha Pratim Chakrabarti"], "title": "Operationalizing Large Language Models with Design-Aware Contexts for Code Comment Generation", "comment": null, "summary": "Comments are very useful to the flow of code development. With the increasing\ncommonality of code, novice coders have been creating a significant amount of\ncodebases. Due to lack of commenting standards, their comments are often\nuseless, and increase the time taken to further maintain codes. This study\nintends to find the usefulness of large language models (LLMs) in these cases\nto generate potentially better comments. This study focuses on the feasibility\nof design documents as a context for the LLMs to generate more useful comments,\nas design documents are often used by maintainers to understand code when\ncomments do not suffice.", "AI": {"tldr": "Novice coder comments are often unhelpful for maintenance. This study explores using LLMs, with design documents as context, to automatically generate better code comments.", "motivation": "Novice coders increasingly create codebases with poor or useless comments due to lacking standards, hindering code maintenance. The study aims to address this by exploring automated comment generation using LLMs.", "method": "The research investigates the feasibility of leveraging design documents as context for LLMs to automatically generate code comments, comparing them against those written by novices.", "result": "The study assesses the potential of LLMs, using design documents, to enhance comment quality and code maintainability compared to traditional novice-written comments.", "conclusion": "The study evaluates whether large language models can improve the usefulness of code comments, especially for novice coders, by utilizing design documents as additional context."}}
{"id": "2510.22409", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22409", "abs": "https://arxiv.org/abs/2510.22409", "authors": ["Shahidul Islam", "Md Nahidul Islam Opu", "Shaowei Wang", "Shaiful Chowdhury"], "title": "A First Look at the Self-Admitted Technical Debt in Test Code: Taxonomy and Detection", "comment": null, "summary": "Self-admitted technical debt (SATD) refers to comments in which developers\nexplicitly acknowledge code issues, workarounds, or suboptimal solutions. SATD\nis known to significantly increase software maintenance effort. While extensive\nresearch has examined SATD in source code, its presence and impact in test code\nhave received no focused attention, leaving a significant gap in our\nunderstanding of how SATD manifests in testing contexts.\n  This study, the first of its kind, investigates SATD in test code by manually\nanalyzing 50,000 comments randomly sampled from 1.6 million comments across\n1,000 open-source Java projects. From this sample, after manual analysis and\nfiltering, we identified 615 SATD comments and classified them into 15 distinct\ncategories, building a taxonomy of test code SATD. To investigate whether test\ncode SATD can be detected automatically, we evaluated existing SATD detection\ntools, as well as both open-source and proprietary LLMs. Among the existing\ntools, MAT performed the best, albeit with moderate recall. To our surprise,\nboth open-source and proprietary LLMs exhibited poor detection accuracy,\nprimarily due to low precision. These results indicate that neither existing\napproaches nor current LLMs can reliably detect SATD in test code.\n  Overall, this work provides the first large-scale analysis of SATD in test\ncode, a nuanced understanding of its types, and the limitations of current SATD\ndetection methods. Our findings lay the groundwork for future research on test\ncode-specific SATD.", "AI": {"tldr": "SATD in test code is underexplored. This study manually analyzed test comments, built the first taxonomy for test code SATD, and found that existing tools and LLMs cannot reliably detect it. More research is needed to develop effective detection methods.", "motivation": "The motivation is to address a research gap: while self-admitted technical debt (SATD) is well-studied in source code, its presence and impact in test code have not been explored, leaving an incomplete understanding of SATD in software development.", "method": "The method involved manually analyzing 50,000 comments, randomly sampled from 1.6 million comments across 1,000 open-source Java projects. SATD comments in test code were identified and categorized into a taxonomy of 15 types. Existing SATD detection tools and large language models (LLMs) were evaluated for their ability to detect test code SATD.", "result": "The study identified 615 SATD comments in test code, classified into 15 distinct categories. The best-performing tool (MAT) had only moderate recall, and both open-source and proprietary LLMs performed poorly, especially in precision. Neither traditional tools nor current LLMs reliably detect SATD in test code.", "conclusion": "This work presents the first large-scale study of SATD in test code, introduces a taxonomy of SATD types, and shows that current SATD detection approaches are inadequate for test code. It highlights substantial limitations in both traditional tools and modern LLMs, setting the stage for future research focused on improving SATD detection in test code."}}
{"id": "2510.22457", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22457", "abs": "https://arxiv.org/abs/2510.22457", "authors": ["Shalini Chakraborty", "Sebastian Baltes"], "title": "A Multifaceted View on Discrimination in Software Development Careers", "comment": "11 pages, 1 figure, 5 tables", "summary": "Conversations around diversity and inclusion in software engineering often\nfocus on gender and racial disparities. However, the State of the Developer\nNation 2025 survey with 8,717 participants revealed that other forms of\ndiscrimination are similarly prevalent but receive considerably less attention.\nThis includes discrimination based on age, political perspective, disabilities,\nor cognitive differences such as neurodivergence. We conducted a secondary\nanalysis of 800 open-ended survey responses to examine patterns of perceived\ndiscrimination, as well as related challenges and negative impacts. Our study\ncovers multiple identity facets, including age, gender, race, and disability.\nWe found that age- and gender-related discrimination was the most frequently\nreported workplace issue, but discrimination based on political and religious\nviews emerged as further notable concerns. Most of the participants who\nidentified as female cited gender as the primary source of discrimination,\noften accompanied by intersectional factors such as race, political views, age,\nor sexual orientation. Discrimination related to caregiving responsibilities\nwas reported by all gender identities. Regarding the negative impacts of\nworkplace issues, many participants described modifying their appearance or\nbehavior in response to gender biases. Gender also appeared to influence\nbroader career challenges, as women and non-binary respondents reported\nexperiencing almost all workplace issues at higher rates, particularly\ndiscrimination (35%) and mental health challenges (62%). Our goal is to raise\nawareness in the research community that discrimination in software development\nis multifaceted, and to encourage researchers to select and assess relevant\nfacets beyond age and gender when designing software engineering studies.", "AI": {"tldr": "This study finds age- and gender-related discrimination are common in software engineering, but other forms (political, disability, caregiving) are also significant. Women and non-binary individuals experience more workplace issues, including mental health challenges. Future research should broaden its scope beyond just gender and race discrimination.", "motivation": "Discussions on diversity in software engineering mainly focus on gender and race, overshadowing other prevalent forms of discrimination, such as age, political ideology, disability, and neurodivergence.", "method": "Secondary analysis of 800 open-ended survey responses from 8,717 participants in the State of the Developer Nation 2025 survey. The analysis centered on patterns of perceived discrimination, related challenges, and negative impacts across multiple identity facets.", "result": "Age- and gender-related discrimination were most frequently reported, with political and religious bias also notable. Female and non-binary participants experienced almost all workplace issues at higher rates than male counterparts. Intersectional discrimination\u2014including race, age, political views, and caregiving responsibilities\u2014surfaced. Gender biases led many to alter their appearance or behavior. Non-binary and women respondents reported higher rates of discrimination (35%) and mental health challenges (62%).", "conclusion": "Discrimination in software engineering is multifaceted and not limited to gender and race. Researchers should consider and evaluate various identity facets, such as age, political perspective, disability, and caregiving responsibility, in future studies to more fully address workplace issues."}}
{"id": "2510.22530", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22530", "abs": "https://arxiv.org/abs/2510.22530", "authors": ["Sungmin Kang", "Sumi Yun", "Jingun Hong", "Shin Yoo", "Gabin An"], "title": "Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL", "comment": "11 pages, 8 figures, under review", "summary": "Fault Localization (FL) aims to identify root causes of program failures. FL\ntypically targets failures observed from test executions, and as such, often\ninvolves dynamic analyses to improve accuracy, such as coverage profiling or\nmutation testing. However, for large industrial software, measuring coverage\nfor every execution is prohibitively expensive, making the use of such\ntechniques difficult. To address these issues and apply FL in an industrial\nsetting, this paper proposes AutoCrashFL, an LLM agent for the localization of\ncrashes that only requires the crashdump from the Program Under Test (PUT) and\naccess to the repository of the corresponding source code. We evaluate\nAutoCrashFL against real-world crashes of SAP HANA, an industrial software\nproject consisting of more than 35 million lines of code. Experiments reveal\nthat AutoCrashFL is more effective in localization, as it identified 30%\ncrashes at the top, compared to 17% achieved by the baseline. Through thorough\nanalysis, we find that AutoCrashFL has attractive practical properties: it is\nrelatively more effective for complex bugs, and it can indicate confidence in\nits results. Overall, these results show the practicality of LLM agent\ndeployment on an industrial scale.", "AI": {"tldr": "AutoCrashFL, an LLM-based tool, localizes software crashes using only crash dumps and source code. It outperforms traditional methods, is scalable to industrial-scale software, and is especially effective for complex bugs.", "motivation": "Traditional fault localization (FL) techniques are limited in industrial settings due to their reliance on dynamic analysis methods like coverage profiling or mutation testing, which are expensive and impractical for large-scale software.", "method": "The paper introduces AutoCrashFL, an LLM (large language model) agent that localizes software crashes using only crash dumps and source code repositories, without requiring costly dynamic analysis.", "result": "In experiments with SAP HANA (a 35-million-line industrial codebase), AutoCrashFL localized 30% of crashes at the top ranking, outperforming a baseline that localized only 17%. Additionally, it proved more effective for complex bugs and can indicate confidence in its findings.", "conclusion": "AutoCrashFL is a practical and effective solution for large-scale, industrial fault localization, overcoming key limitations of traditional methods and enabling the scalable use of LLM agents in the software industry."}}
{"id": "2510.22613", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22613", "abs": "https://arxiv.org/abs/2510.22613", "authors": ["Songhan Zhang", "Aoyang Fang", "Yifan Yang", "Ruiyi Cheng", "Xiaoying Tang", "Pinjia He"], "title": "DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices", "comment": null, "summary": "Cloud-native microservices enable rapid iteration and scalable deployment but\nalso create complex, fast-evolving dependencies that challenge reliable\ndiagnosis. Existing root cause analysis (RCA) approaches, even with multi-modal\nfusion of logs, traces, and metrics, remain limited in capturing dynamic\nbehaviors and shifting service relationships. Three critical challenges\npersist: (i) inadequate modeling of cascading fault propagation, (ii)\nvulnerability to noise interference and concept drift in normal service\nbehavior, and (iii) over-reliance on service deviation intensity that obscures\ntrue root causes. To address these challenges, we propose DynaCausal, a dynamic\ncausality-aware framework for RCA in distributed microservice systems.\nDynaCausal unifies multi-modal dynamic signals to capture time-varying\nspatio-temporal dependencies through interaction-aware representation learning.\nIt further introduces a dynamic contrastive mechanism to disentangle true fault\nindicators from contextual noise and adopts a causal-prioritized pairwise\nranking objective to explicitly optimize causal attribution. Comprehensive\nevaluations on public benchmarks demonstrate that DynaCausal consistently\nsurpasses state-of-the-art methods, attaining an average AC@1 of 0.63 with\nabsolute gains from 0.25 to 0.46, and delivering both accurate and\ninterpretable diagnoses in highly dynamic microservice environments.", "AI": {"tldr": "DynaCausal is a new framework for diagnosing faults in cloud-native microservices. It smartly analyzes multiple data sources and prioritizes real causes, outperforming existing methods in both accuracy and explainability.", "motivation": "Cloud-native microservices are increasingly adopted for their scalability and rapid iteration, but they introduce complex and fast-changing dependencies that complicate diagnosing system failures. Existing root cause analysis (RCA) methods struggle with dynamic behaviors, noise, concept drift, and often misidentify root causes due to simplistic reliance on service deviations.", "method": "The paper proposes DynaCausal, a dynamic causality-aware framework for RCA in distributed microservice systems. DynaCausal unifies multi-modal dynamic signals (from logs, traces, and metrics) and applies interaction-aware representation learning to model time-varying spatio-temporal dependencies. It employs a dynamic contrastive mechanism to separate real fault signals from noise and uses a causal-prioritized pairwise ranking objective to optimize causal attribution.", "result": "DynaCausal was comprehensively evaluated on public benchmarks and outperformed state-of-the-art RCA approaches. It achieved an average AC@1 of 0.63, with notable absolute gains between 0.25 and 0.46, providing both accurate and interpretable fault diagnoses.", "conclusion": "DynaCausal significantly improves root cause analysis in complex microservice environments by modeling dynamic causal relationships, effectively filtering noise, and prioritizing true causes, thus setting a new standard for accuracy and interpretability."}}
{"id": "2510.22614", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.22614", "abs": "https://arxiv.org/abs/2510.22614", "authors": ["Roham Koohestani", "Agnia Sergeyuk", "David Gros", "Claudio Spiess", "Sergey Titov", "Prem Devanbu", "Maliheh Izadi"], "title": "Does In-IDE Calibration of Large Language Models work at Scale?", "comment": "Under Review", "summary": "The introduction of large language models into integrated development\nenvironments (IDEs) is revolutionizing software engineering, yet it poses\nchallenges to the usefulness and reliability of Artificial\nIntelligence-generated code. Post-hoc calibration of internal model confidences\naims to align probabilities with an acceptability measure. Prior work suggests\ncalibration can improve alignment, but at-scale evidence is limited. In this\nwork, we investigate the feasibility of applying calibration of code models to\nan in-IDE context. We study two aspects of the problem: (1) the technical\nmethod for implementing confidence calibration and improving the reliability of\ncode generation models, and (2) the human-centered design principles for\neffectively communicating reliability signal to developers. First, we develop a\nscalable and flexible calibration framework which can be used to obtain\ncalibration weights for open-source models using any dataset, and evaluate\nwhether calibrators improve the alignment between model confidence and\ndeveloper acceptance behavior. Through a large-scale analysis of over 24\nmillion real-world developer interactions across multiple programming\nlanguages, we find that a general, post-hoc calibration model based on\nPlatt-scaling does not, on average, improve the reliability of model confidence\nsignals. We also find that while dynamically personalizing calibration to\nindividual users can be effective, its effectiveness is highly dependent on the\nvolume of user interaction data. Second, we conduct a multi-phase design study\nwith 3 expert designers and 153 professional developers, combining\nscenario-based design, semi-structured interviews, and survey validation,\nrevealing a clear preference for presenting reliability signals via\nnon-numerical, color-coded indicators within the in-editor code generation\nworkflow.", "AI": {"tldr": "Calibrating large language model confidence for code generation in IDEs generally doesn't improve reliability at scale, although user-specific adjustments help if enough interaction data is available. Developers best understand reliability through color-coded visual cues rather than numbers.", "motivation": "Integrating large language models (LLMs) into integrated development environments (IDEs) is transforming software engineering, but AI-generated code often lacks reliability and usefulness for developers. Calibration of model confidence could potentially bridge this gap, yet there is insufficient large-scale evidence on its effectiveness and best practices for communicating model reliability to users.", "method": "The study investigates calibration methods and user interface designs for LLMs in IDEs. Technically, the authors build a scalable and flexible calibration framework that uses Platt-scaling and can apply to any dataset, analyzing its effect on model confidence alignment with developer acceptance. Human-centered design principles are explored through scenario-based design, interviews, and surveys with designers and professional developers to determine optimal presentation of reliability signals.", "result": "A large-scale analysis of 24 million developer interactions reveals that generic post-hoc calibration using Platt-scaling does not significantly improve confidence reliability on average. Personalized calibration can help, but only if extensive user interaction data is available. Developer studies show a clear preference for reliability signals delivered as non-numerical, color-coded indicators in the IDE.", "conclusion": "Post-hoc calibration for LLMs in code generation does not reliably improve confidence measures for end users at scale, though personalization shows some promise if enough data exists. The presentational design of reliability signals is crucial\u2014developers prefer color-coded, non-numerical indicators integrated into code editors."}}
{"id": "2510.22787", "categories": ["cs.SE", "cs.AI", "68T07", "I.2.11; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.22787", "abs": "https://arxiv.org/abs/2510.22787", "authors": ["Kamil Szczepanik", "Jaros\u0142aw A. Chudziak"], "title": "Collaborative LLM Agents for C4 Software Architecture Design Automation", "comment": "This paper has been accepted for the upcoming 59th Hawaii\n  International Conference on System Sciences (HICSS-59), 2026, Hawaii, USA.\n  The final published version will appear in the official conference\n  proceedings", "summary": "Software architecture design is a fundamental part of creating every software\nsystem. Despite its importance, producing a C4 software architecture model, the\npreferred notation for such architecture, remains manual and time-consuming. We\nintroduce an LLM-based multi-agent system that automates this task by\nsimulating a dialogue between role-specific experts who analyze requirements\nand generate the Context, Container, and Component views of the C4 model.\nQuality is assessed with a hybrid evaluation framework: deterministic checks\nfor structural and syntactic integrity and C4 rule consistency, plus semantic\nand qualitative scoring via an LLM-as-a-Judge approach. Tested on five\ncanonical system briefs, the workflow demonstrates fast C4 model creation,\nsustains high compilation success, and delivers semantic fidelity. A comparison\nof four state-of-the-art LLMs shows different strengths relevant to\narchitectural design. This study contributes to automated software architecture\ndesign and its evaluation methods.", "AI": {"tldr": "The paper presents an LLM-based system that automates C4 software architecture modeling by simulating expert dialogues and combines rule-based and LLM-driven evaluation to ensure model quality, showing promising results on several case studies.", "motivation": "Designing software architecture using the C4 model is crucial but currently manual and labor-intensive. There is a need to automate and streamline this process to save time and improve efficiency.", "method": "The authors propose a multi-agent system powered by large language models (LLMs). These agents simulate expert dialogues to analyze requirements and automatically generate C4 architectural views (Context, Container, Component). The generated architecture is evaluated using a hybrid framework combining deterministic, rule-based checks with qualitative, LLM-based scoring.", "result": "When tested on five typical software system briefs, the system produced C4 models quickly, with high structural correctness and semantic alignment. Different LLMs showed varied strengths in architectural design tasks.", "conclusion": "The study successfully automates C4 software architecture modeling using LLM-based agents and proposes robust evaluation methods. This advances the field of automated architecture design and its assessment."}}
{"id": "2510.22815", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.22815", "abs": "https://arxiv.org/abs/2510.22815", "authors": ["Vasudev Vikram", "Yuvraj Agarwal", "Rohan Padhye"], "title": "On the Freshness of Pinned Dependencies in Maven", "comment": null, "summary": "Library dependencies in software ecosystems play a crucial role in the\ndevelopment of software. As newer releases of these libraries are published,\ndevelopers may opt to pin their dependencies to a particular version. While\npinning may have benefits in ensuring reproducible builds and avoiding breaking\nchanges, it bears larger risks in using outdated dependencies that may contain\nbugs and security vulnerabilities. To understand the frequency and consequences\nof dependency pinning, we first define the concepts of stale and fresh pins,\nwhich are distinguished based on how outdated the dependency is relative to the\nrelease date of the project. We conduct an empirical study to show that over\n60% of consumers of popular Maven libraries contain stale pins to their\ndependencies, with some outdated versions over a year old. These pinned\nversions often miss out on security fixes; we find that 10% of all dependency\nupgrades in our dataset to the latest minor or patch version would reduce\nsecurity vulnerabilities.\n  We prototype an approach called Pin-Freshener that can encourage developers\nto freshen their pins by leveraging the insight that crowdsourced tests of peer\nprojects can provide additional signal for the safety of an upgrade. Running\nPin-Freshener on dependency upgrades shows that just 1-5 additional test suites\ncan provide 35-100% more coverage of a dependency, compared to that of a single\nconsumer test suite. Our evaluation on real-world pins to the top 500 popular\nlibraries in Maven shows that Pin-Freshener can provide an additional signal of\nat least 5 passing crowdsourced test suites to over 3,000 consumers to safely\nperform an upgrade that reduces security vulnerabilities. Pin-Freshener can\nprovide practical confidence to developers by offering additional signal beyond\ntheir own test suites, representing an improvement over current practices.", "AI": {"tldr": "Over 60% of Maven projects pin dependencies to outdated versions, posing security risks. The proposed Pin-Freshener tool uses crowdsourced tests to help developers confidently update dependencies, increasing test coverage and security.", "motivation": "Developers pin library dependencies to ensure build reproducibility and avoid breaking changes, but this practice can lead to the use of outdated libraries with security vulnerabilities. The paper aims to understand how often this happens and presents a solution to encourage safer upgrade practices.", "method": "The authors conducted an empirical study of Maven library usage, analyzing how frequently dependencies are pinned to outdated versions (explicitly defined as stale pins versus fresh pins). They also developed a tool called Pin-Freshener, which uses crowdsourced test results from peer projects to recommend safe upgrades and improve confidence in dependency freshness.", "result": "The study found that more than 60% of consumers of popular Maven libraries use stale dependency pins, with some pins being over a year old. Upgrading to latest minor or patch versions could mitigate security issues in roughly 10% of cases. Pin-Freshener, by utilizing 1-5 additional crowdsourced test suites, increased coverage by 35-100%, and provided actionable upgrade confidence to thousands of projects.", "conclusion": "Pinning dependencies, while useful for reproducibility, is widespread and leads to outdated, potentially vulnerable software. Pin-Freshener effectively leverages crowdsourced testing to safely encourage upgrading, offering developers practical tools and signals to improve ecosystem security and reliability."}}
{"id": "2510.22986", "categories": ["cs.SE", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.22986", "abs": "https://arxiv.org/abs/2510.22986", "authors": ["Junjie Huang", "Minghua He", "Jinyang Liu", "Yintong Huo", "Domenico Bianculli", "Michael R. Lyu"], "title": "CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs", "comment": null, "summary": "Log-based anomaly detection (LogAD) is critical for maintaining the\nreliability and availability of large-scale online service systems. While\nmachine learning, deep learning, and large language models (LLMs)-based methods\nhave advanced the LogAD, they often suffer from limited interpretability, high\ninference costs, and extensive preprocessing requirements, limiting their\npracticality for real-time, high-volume log analysis. In contrast, rule-based\nsystems offer efficiency and transparency, but require significant manual\neffort and are difficult to scale across diverse and evolving environments. In\nthis paper, We present CodeAD, a novel framework that automatically synthesizes\nlightweight Python rule functions for LogAD using LLMs. CodeAD introduces a\nhierarchical clustering and anchor-grounded sampling strategy to construct\nrepresentative contrastive log windows, enabling LLMs to discern discriminative\nanomaly patterns. To ensure robustness and generalizability, CodeAD employs an\nagentic workflow that iteratively generates, tests, repairs, and refines the\nrules until it meets correctness and abstraction requirements. The synthesized\nrules are interpretable, lightweight, and directly executable on raw logs,\nsupporting efficient and transparent online anomaly detection. Our\ncomprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird)\ndemonstrate that CodeAD achieves an average absolute improvement of 3.6% F1\nscore over the state-of-the-art baselines, while processing large datasets up\nto 4x faster and at a fraction of the cost (total LLM invocation cost under 4\nUSD per dataset). These results highlight CodeAD as a practical and scalable\nsolution for online monitoring systems, enabling interpretable, efficient, and\nautomated LogAD in real-world environment.", "AI": {"tldr": "CodeAD uses LLMs to automatically generate interpretable, efficient Python rules for log anomaly detection, outperforming existing methods in accuracy and speed, with low cost and high scalability.", "motivation": "Log-based anomaly detection is essential for maintaining reliability in large-scale online systems. Existing methods such as machine learning, deep learning, and LLM-based approaches have advanced the field but suffer from low interpretability, high inference costs, and significant preprocessing, limiting their usefulness in real-time, high-volume scenarios. While rule-based systems are more efficient and transparent, they require much manual effort and are hard to scale.", "method": "The paper introduces CodeAD, a framework that leverages LLMs to automatically generate lightweight Python rule functions for LogAD. CodeAD uses hierarchical clustering and anchor-grounded sampling to create representative log windows, enabling LLMs to identify anomaly patterns. It operates through an agentic workflow, iteratively generating, testing, repairing, and refining rules to ensure correctness and abstraction. The resulting rules run directly on raw logs, supporting efficient anomaly detection.", "result": "Comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird) show that CodeAD achieves a 3.6% average absolute improvement in F1 score over existing state-of-the-art baselines, processes large datasets up to 4x faster, and incurs very low cost (under $4 per dataset for LLM invocation).", "conclusion": "CodeAD offers a practical, scalable, interpretable, and efficient solution for online log anomaly detection, making automated LogAD feasible in real-world environments."}}
{"id": "2510.23010", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23010", "abs": "https://arxiv.org/abs/2510.23010", "authors": ["Ming-Tung Shen", "Yuh-Jzer Joung"], "title": "TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation", "comment": null, "summary": "Agentic code generation requires large language models (LLMs) capable of\ncomplex context management and multi-step reasoning. Prior multi-agent\nframeworks attempt to address these challenges through collaboration, yet they\noften suffer from rigid workflows and high reasoning recovery costs. To\novercome these limitations, we propose TALM (Tree-Structured Multi-Agent\nFramework with Long-Term Memory), a dynamic framework that integrates\nstructured task decomposition, localized re-reasoning, and long-term memory\nmechanisms. TALM employs an extensible tree-based collaboration structure. The\nparent-child relationships, when combined with a divide-and-conquer strategy,\nenhance reasoning flexibility and enable efficient error correction across\ndiverse task scopes. Furthermore, a long-term memory module enables semantic\nquerying and integration of prior knowledge, supporting implicit\nself-improvement through experience reuse. Experimental results on HumanEval,\nBigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently\ndelivers strong reasoning performance and high token efficiency, highlighting\nits robustness and practical utility in complex code generation tasks.", "AI": {"tldr": "This work proposes TALM, a dynamic tree-structured multi-agent framework with long-term memory, enhancing LLM code generation by flexible reasoning and efficient error correction. TALM shows strong performance and efficiency across standard coding benchmarks.", "motivation": "Large language models (LLMs) need to manage context and perform multi-step reasoning for agentic code generation, but existing multi-agent frameworks are rigid and inefficient in recovering from reasoning errors.", "method": "TALM introduces a tree-structured multi-agent framework that uses structured task decomposition, localized re-reasoning, and incorporates a long-term memory module. It uses an extensible tree for collaboration, enabling efficient error correction and experience reuse via semantic querying of memory.", "result": "Experiments on HumanEval, BigCodeBench, and ClassEval benchmarks show that TALM consistently achieves strong reasoning performance and high token efficiency.", "conclusion": "TALM is robust and practically useful for complex code generation, overcoming limitations of prior multi-agent frameworks through flexible reasoning and long-term memory."}}
{"id": "2510.23055", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23055", "abs": "https://arxiv.org/abs/2510.23055", "authors": ["Manjeshwar Aniruddh Mallya", "Alessio Ferrari", "Mohammad Amin Zadenoori", "Jacek D\u0105browski"], "title": "From Online User Feedback to Requirements: Evaluating Large Language Models for Classification and Specification Tasks", "comment": null, "summary": "[Context and Motivation] Online user feedback provides valuable information\nto support requirements engineering (RE). However, analyzing online user\nfeedback is challenging due to its large volume and noise. Large language\nmodels (LLMs) show strong potential to automate this process and outperform\nprevious techniques. They can also enable new tasks, such as generating\nrequirements specifications.\n  [Question-Problem] Despite their potential, the use of LLMs to analyze user\nfeedback for RE remains underexplored. Existing studies offer limited empirical\nevidence, lack thorough evaluation, and rarely provide replication packages,\nundermining validity and reproducibility.\n  [Principal Idea-Results] We evaluate five lightweight open-source LLMs on\nthree RE tasks: user request classification, NFR classification, and\nrequirements specification generation. Classification performance was measured\non two feedback datasets, and specification quality via human evaluation. LLMs\nachieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and\nmoderately high specification quality (mean ~ 3/5).\n  [Contributions] We newly explore lightweight LLMs for feedback-driven\nrequirements development. Our contributions are: (i) an empirical evaluation of\nlightweight LLMs on three RE tasks, (ii) a replication package, and (iii)\ninsights into their capabilities and limitations for RE.", "AI": {"tldr": "Lightweight open-source LLMs can moderately automate the analysis and specification generation from user feedback for requirements engineering, showing reasonable accuracy and quality. The study offers empirical evidence, practical resources, and guidance on their use and limitations.", "motivation": "Online user feedback is crucial for requirements engineering (RE), but it is difficult to analyze due to its large volume and noise. Large language models (LLMs) offer strong potential to automate feedback analysis and generate requirements specifications.", "method": "The paper evaluates five lightweight open-source LLMs on three RE tasks: user request classification, non-functional requirement (NFR) classification, and requirements specification generation. Performance is measured on two datasets for classification and by human evaluation for specification quality.", "result": "LLMs achieved moderate-to-high classification accuracy (F1 scores between 0.47 and 0.68) and moderately high specification quality (average rating around 3 out of 5).", "conclusion": "Lightweight LLMs show promise for supporting feedback-driven requirements engineering tasks. The paper provides empirical evaluation, a replication package, and insights into the models' capabilities and limitations."}}
{"id": "2510.23068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23068", "abs": "https://arxiv.org/abs/2510.23068", "authors": ["Ella Dodor", "Cristina V. Lopes"], "title": "Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs", "comment": "11 pages, 9 figures, tool link:\n  https://github.com/ellacodee/CheckstylePlus", "summary": "Good code style improves program readability, maintainability, and\ncollaboration, and is an integral component of software quality. Developers,\nhowever, often cut corners when following style rules, leading to the wide\nadoption of tools such as linters in professional software development\nprojects. Traditional linters like Checkstyle operate using rigid, rule-based\nmechanisms that effectively detect many surface-level violations. However, in\nmost programming languages, there is a subset of style rules that require a\nmore nuanced understanding of code, and fall outside the scope of such static\nanalysis. In this paper, we propose Checkstyle+, a hybrid approach that\naugments Checkstyle with large language model (LLM) capabilities, to identify\nstyle violations that elude the conventional rule-based analysis. Checkstyle+\nis evaluated on a sample of 380 Java code files, drawn from a broader dataset\nof 30,800 real-world Java programs sourced from accepted Codeforces\nsubmissions. The results show that Checkstyle+ achieves superior performance\nover standard Checkstyle in detecting violations of the semantically nuanced\nrules.", "AI": {"tldr": "Checkstyle+ combines rule-based and LLM-driven analysis to better catch subtle code style violations that traditional linters miss, and demonstrates improved performance on real-world Java code.", "motivation": "Although good code style is essential for software quality, traditional rule-based linters often miss more nuanced or complex style issues that require deeper code understanding.", "method": "The authors introduce Checkstyle+, a hybrid linting tool that combines traditional rule-based checks (like those in Checkstyle) with large language model (LLM) capabilities to catch subtle style violations.", "result": "When tested on 380 Java code files from Codeforces, Checkstyle+ outperformed standard Checkstyle in identifying violations of semantically nuanced style rules.", "conclusion": "Hybridizing rule-based linting with LLMs enhances the detection of complex code style violations, improving code quality assurance tools."}}
{"id": "2510.23350", "categories": ["cs.SE", "D.2.1; D.2.4; D.2.5"], "pdf": "https://arxiv.org/pdf/2510.23350", "abs": "https://arxiv.org/abs/2510.23350", "authors": ["Alcino Cunha", "Nuno Macedo"], "title": "Validating Formal Specifications with LLM-generated Test Cases", "comment": null, "summary": "Validation is a central activity when developing formal specifications.\nSimilarly to coding, a possible validation technique is to define upfront test\ncases or scenarios that a future specification should satisfy or not.\nUnfortunately, specifying such test cases is burdensome and error prone, which\ncould cause users to skip this validation task. This paper reports the results\nof an empirical evaluation of using pre-trained large language models (LLMs) to\nautomate the generation of test cases from natural language requirements. In\nparticular, we focus on test cases for structural requirements of simple domain\nmodels formalized in the Alloy specification language. Our evaluation focuses\non the state-of-art GPT-5 model, but results from other closed- and open-source\nLLMs are also reported. The results show that, in this context, GPT-5 is\nalready quite effective at generating positive (and negative) test cases that\nare syntactically correct and that satisfy (or not) the given requirement, and\nthat can detect many wrong specifications written by humans.", "AI": {"tldr": "Automated generation of Alloy test cases using GPT-5 and other LLMs shows promising results, making the validation process easier and more reliable by reducing manual effort and improving accuracy in detecting specification errors.", "motivation": "Validating formal specifications is crucial but manually defining test cases is tedious and prone to errors, leading users to potentially skip this important step.", "method": "The authors conducted an empirical evaluation, using pre-trained large language models (LLMs) to automatically generate test cases from natural language requirements. They focused on structural requirements for domain models specified in Alloy and compared GPT-5's effectiveness with other LLMs.", "result": "GPT-5 is shown to be quite effective, generating syntactically correct positive and negative test cases that align with the requirements and help detect incorrect human-written specifications. Other LLMs were also evaluated, but GPT-5 was the primary focus.", "conclusion": "Automating test case generation with LLMs, particularly GPT-5, significantly aids the validation of formal specifications, reducing manual effort and improving error detection."}}
{"id": "2510.23389", "categories": ["cs.SE", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23389", "abs": "https://arxiv.org/abs/2510.23389", "authors": ["Edoardo Manino", "Bruno Farias", "Rafael S\u00e1 Menezes", "Fedor Shmarov", "Lucas C. Cordeiro"], "title": "Floating-Point Neural Network Verification at the Software Level", "comment": "Pre-print before submission to peer review", "summary": "The behaviour of neural network components must be proven correct before\ndeployment in safety-critical systems. Unfortunately, existing neural network\nverification techniques cannot certify the absence of faults at the software\nlevel. In this paper, we show how to specify and verify that neural networks\nare safe, by explicitly reasoning about their floating-point implementation. In\ndoing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural\nnetwork verification examples that cover activation functions, common layers,\nand full neural networks of up to 170K parameters. Our verification suite is\nwritten in plain C and is compatible with the format of the International\nCompetition on Software Verification (SV-COMP). Thanks to it, we can conduct\nthe first rigorous evaluation of eight state-of-the-art software verifiers on\nneural network code. The results show that existing automated verification\ntools can correctly solve an average of 11% of our benchmark, while producing\naround 3% incorrect verdicts. At the same time, a historical analysis reveals\nthat the release of our benchmark has already had a significantly positive\nimpact on the latter.", "AI": {"tldr": "NeuroCodeBench 2.0 is a new C-based benchmark for verifying neural network correctness at the software level, highlighting that current tools largely fall short but are improving as a result of this benchmark.", "motivation": "Neural networks are increasingly used in safety-critical systems, where correctness is vital. However, current verification techniques do not adequately guarantee software-level safety, particularly concerning floating-point implementations.", "method": "The authors approach this by specifying and verifying neural networks\u2014explicitly considering their floating-point implementations. They develop NeuroCodeBench 2.0, a benchmark suite in C compatible with SV-COMP, containing 912 examples across different neural network components. This suite enables comparison and evaluation of verification tools.", "result": "The benchmark allows a rigorous evaluation of eight leading software verifiers on neural network code. The study finds that these tools can solve only about 11% of the benchmarks correctly and produce roughly 3% incorrect results. However, the release of NeuroCodeBench 2.0 has already spurred improvements in these verification techniques, as evidenced by a historical analysis.", "conclusion": "NeuroCodeBench 2.0 fills a gap by providing a detailed, software-level neural network verification benchmark, revealing significant limitations in current verification tools but also driving progress in the field."}}
{"id": "2510.23528", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23528", "abs": "https://arxiv.org/abs/2510.23528", "authors": ["Joran Leest", "Ilias Gerostathopoulos", "Patricia Lago", "Claudia Raibulet"], "title": "Tracing Distribution Shifts with Causal System Maps", "comment": null, "summary": "Monitoring machine learning (ML) systems is hard, with standard practice\nfocusing on detecting distribution shifts rather than their causes. Root-cause\nanalysis often relies on manual tracing to determine whether a shift is caused\nby software faults, data-quality issues, or natural change. We propose ML\nSystem Maps -- causal maps that, through layered views, make explicit the\npropagation paths between the environment and the ML system's internals,\nenabling systematic attribution of distribution shifts. We outline the approach\nand a research agenda for its development and evaluation.", "AI": {"tldr": "The paper introduces ML System Maps, a causal mapping approach to systematically trace and attribute the causes of distribution shifts in machine learning systems, moving beyond mere detection to support thorough root-cause analysis.", "motivation": "Monitoring machine learning systems is challenging, particularly in identifying the causes of distribution shifts. Current methods focus on detecting the occurrence of shifts but not their root causes, often requiring manual tracing to determine if issues are due to software faults, data quality, or natural changes.", "method": "The paper proposes 'ML System Maps,' which are causal maps featuring layered views that explicitly visualize the propagation paths linking the environment with the internal components of the ML system.", "result": "By using ML System Maps, the approach allows for a more systematic attribution of distribution shifts, mapping them to potential causes across the system.", "conclusion": "ML System Maps offer a structured and visual method to trace and attribute the causes of distribution shifts in ML systems, supporting better root-cause analysis. The paper presents a plan for further research and evaluation of this approach."}}
