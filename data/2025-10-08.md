<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing](https://arxiv.org/abs/2510.05147)
*Yu Zhu*

Main category: cs.SE

TL;DR: The paper proposes an RL-based strategy for dynamic software testing resource allocation, integrating Q-learning and hybrid rewards for adaptability. It outperforms standard methods in changing environments and is applicable beyond software testing.


<details>
  <summary>Details</summary>
Motivation: Exhaustive pre-production software testing is infeasible due to environment heterogeneity and shifting failure probabilities. Current optimization methods lack adaptability for non-stationary settings, necessitating a more dynamic approach.

Method: The authors introduce a RL framework using Q-learning integrated with a hybrid reward design (simulated and real-time feedback). It includes an adaptive online-offline training scheme to respond to changing failure probabilities and maintain stability.

Result: Extensive simulations show that the RL framework surpasses static and optimization-based methods, approaching oracle-level performance, and is broadly applicable to dynamic resource scheduling.

Conclusion: Reinforcement learning (RL), with Q-learning and hybrid rewards, offers superior adaptive configuration allocation for software testing, consistently outperforming traditional static and optimization-based strategies.

Abstract: Ensuring reliability in modern software systems requires rigorous
pre-production testing across highly heterogeneous and evolving environments.
Because exhaustive evaluation is infeasible, practitioners must decide how to
allocate limited testing resources across configurations where failure
probabilities may drift over time. Existing combinatorial optimization
approaches are static, ad hoc, and poorly suited to such non-stationary
settings. We introduce a novel reinforcement learning (RL) framework that
recasts configuration allocation as a sequential decision-making problem. Our
method is the first to integrate Q-learning with a hybrid reward design that
fuses simulated outcomes and real-time feedback, enabling both sample
efficiency and robustness. In addition, we develop an adaptive online-offline
training scheme that allows the agent to quickly track abrupt probability
shifts while maintaining long-run stability. Extensive simulation studies
demonstrate that our approach consistently outperforms static and
optimization-based baselines, approaching oracle performance. This work
establishes RL as a powerful new paradigm for adaptive configuration
allocation, advancing beyond traditional methods and offering broad
applicability to dynamic testing and resource scheduling domains.

</details>


### [2] [VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation](https://arxiv.org/abs/2510.05156)
*Lesly Miculicich,Mihir Parmar,Hamid Palangi,Krishnamurthy Dj Dvijotham,Mirko Montanari,Tomas Pfister,Long T. Le*

Main category: cs.SE

TL;DR: VeriGuard is a framework for formally guaranteeing the safety of LLM agents. It uses a thorough offline process to specify and verify safety policies, and monitors agent actions at runtime to ensure compliance, achieving robust trust and risk reduction.


<details>
  <summary>Details</summary>
Motivation: Autonomous AI agents in sensitive fields like healthcare can fail, violate privacy, or be compromised, and current systems lack mechanisms for guaranteeing safe behavior. The need to formalize and assure agent adherence to strict safety constraints motivated the development of VeriGuard.

Method: VeriGuard consists of a dual-stage architecture: (1) an offline stage that validates user intents, synthesizes behavior policies, and subjects them to both testing and formal verification; and (2) an online stage that monitors and validates agent actions in real time against pre-verified policies.

Result: VeriGuard improves agent trustworthiness by formally guaranteeing adherence to user objectives and safety specifications, using policy synthesis, verification, and runtime monitoring to mitigate various risks.

Conclusion: VeriGuard successfully provides formal safety guarantees for LLM-based agents by separating comprehensive offline policy verification from lightweight online action monitoring. This dual-stage architecture enables robust and practical enforcement of safety constraints in sensitive domains.

Abstract: The deployment of autonomous AI agents in sensitive domains, such as
healthcare, introduces critical risks to safety, security, and privacy. These
agents may deviate from user objectives, violate data handling policies, or be
compromised by adversarial attacks. Mitigating these dangers necessitates a
mechanism to formally guarantee that an agent's actions adhere to predefined
safety constraints, a challenge that existing systems do not fully address. We
introduce VeriGuard, a novel framework that provides formal safety guarantees
for LLM-based agents through a dual-stage architecture designed for robust and
verifiable correctness. The initial offline stage involves a comprehensive
validation process. It begins by clarifying user intent to establish precise
safety specifications. VeriGuard then synthesizes a behavioral policy and
subjects it to both testing and formal verification to prove its compliance
with these specifications. This iterative process refines the policy until it
is deemed correct. Subsequently, the second stage provides online action
monitoring, where VeriGuard operates as a runtime monitor to validate each
proposed agent action against the pre-verified policy before execution. This
separation of the exhaustive offline validation from the lightweight online
monitoring allows formal guarantees to be practically applied, providing a
robust safeguard that substantially improves the trustworthiness of LLM agents.

</details>


### [3] [Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework](https://arxiv.org/abs/2510.05365)
*Irtaza Sajid Qureshi,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: LLMs are decent at basic pattern reproduction and some understanding in test generation, but fail badly when code identifiers change. However, giving models few-shot examples boosts performance. Focusing on technical, structured input helps much more than narratives. This suggests how to better use and assess LLMs for software test automation.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly being used for automated software testing, but it is unclear how well they generalize beyond memorized patterns and reason about natural language bug reports.

Method: They systematically evaluate LLM reasoning in test case generation across Bloom’s taxonomy layers using the LIBRO framework. They apply this on StarCoder and GPT-4o, testing on Defects4J and GHRB datasets, as well as mutated versions that introduce linguistic and semantic challenges.

Result: Both models mainly replicate previous results at the 'Remember' and partially 'Understand' levels. They uncover unique bugs but see large (>60%) performance drops with identifier mutations ('Apply'). Few-shot, open-book setups improve success rates up to three times. Technical elements like test code and method names are more useful than narrative descriptions for successful test generation.

Conclusion: The research sheds light on the cognitive reasoning abilities of LLMs in test generation and provides guidance for improving performance and evaluation paradigms.

Abstract: Large Language Models (LLMs) are increasingly applied to automated software
testing, yet their ability to generalize beyond memorized patterns and reason
about natural language bug reports remains unclear. We present a systematic
evaluation of LLM reasoning in test case generation, structured around the
cognitive layers of Bloom's taxonomy: \textit{Remember}, \textit{Understand},
\textit{Apply}, \textit{Analyze}, \textit{Evaluate}, and \textit{Create}, which
progressively assess higher levels of cognitive and reasoning capabilities.
Building on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,
GHRB, and mutated variants that introduce linguistic and semantic challenges.
Our findings show that both models largely reproduce prior results with minor
deviations (\textit{Remember}), exhibit partial robustness to linguistic
rephrasings and translations while uncovering unique reproducible bugs
(\textit{Understand}), but suffer severe performance drops exceeding 60\% under
identifier mutations (\textit{Apply}). Conversely, providing near-identical
few-shot examples in an open-book setting improves success rates by up to three
times, and component-level analysis reveals that structured technical elements,
such as test code and method names, are far more impactful than narrative
descriptions for successful test generation (\textit{Analyze}). These insights
illuminate the cognitive processes underlying LLM-generated tests, suggest
concrete directions for improving performance, and establish a robust and
realistic evaluation paradigm for this task.

</details>


### [4] [Who Do You Think You Are? Creating RSE Personas from GitHub Interactions](https://arxiv.org/abs/2510.05390)
*Felicity Anderson,Julien Sindt,Neil Chue Hong*

Main category: cs.SE

TL;DR: The paper presents a data-driven personas approach to analyze and classify contributor behaviors in research software repositories, identifying seven typical personas from large-scale GitHub mining, thus helping teams understand and improve Research Software Engineering dynamics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand patterns and dynamics of Research Software Engineering (RSE) contributions by analyzing collaborative behavior in software repositories. The motivation is to help individuals and RS project teams better comprehend their roles, impacts, and repository interactions, which are crucial for improving RSE practices.

Method: The authors combine software repository mining with data-driven persona generation, applying this methodology to research software repositories. They evaluate the method by assessing contributor patterns in mid-sized public RS repositories on GitHub, analyzing data from over 1,284 repositories and more than 115,000 contributors.

Result: They successfully identify and classify seven distinct contributor personas representing various levels of interactivity, such as Ephemeral Contributor, Occasional Contributor, Project Organiser, Moderate Contributor, Low-Process Closer, Low-Coding Closer, and Active Contributor. The study demonstrates the feasibility of large-scale analysis across heterogeneous projects and contributor backgrounds.

Conclusion: The RSE personas method provides actionable insights into contributor behaviors and patterns in research software, supporting teams and individuals in understanding and improving their software engineering practices. It enables meaningful analysis despite differences in project management styles and domains.

Abstract: We describe data-driven RSE personas: an approach combining software
repository mining and data-driven personas applied to research software (RS),
an attempt to describe and identify common and rare patterns of Research
Software Engineering (RSE) development. This allows individuals and RS project
teams to understand their contributions, impact and repository dynamics - an
important foundation for improving RSE. We evaluate the method on different
patterns of collaborative interaction behaviours by contributors to mid-sized
public RS repositories (those with 10-300 committers) on GitHub. We demonstrate
how the RSE personas method successfully characterises a sample of 115,174
repository contributors across 1,284 RS repositories on GitHub, sampled from
42,284 candidate software repository records queried from Zenodo. We identify,
name and summarise seven distinct personas from low to high interactivity:
Ephemeral Contributor; Occasional Contributor; Project Organiser; Moderate
Contributor; Low-Process Closer; Low-Coding Closer; and Active Contributor.
This demonstrates that large datasets can be analysed despite difficulties of
comparing software projects with different project management factors, research
domains and contributor backgrounds.

</details>


### [5] [UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification](https://arxiv.org/abs/2510.05441)
*Yiannis Charalambous,Claudionor N. Coelho Jr,Luis Lamb,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: UnitTenX is an AI-powered multi-agent system that automates generation of unit tests for legacy code, increasing test coverage and reliability. The approach outperforms traditional methods, improves documentation, and addresses limitations of current LLM-based bug detection.


<details>
  <summary>Details</summary>
Motivation: Legacy code often lacks adequate unit tests, making software unreliable and difficult to maintain. Traditional methods struggle with complex, legacy codebases. There is a need for automated, effective solutions that can increase test coverage and improve code quality.

Method: UnitTenX combines AI agents, formal methods, and Large Language Models to automate the generation of unit tests. The system tackles legacy and complex codebases, aiming to improve test coverage and code reliability despite known limitations of LLMs in bug detection.

Result: The results show that UnitTenX can generate high-quality unit tests effectively, identify potential issues, and improve documentation and readability of legacy code.

Conclusion: UnitTenX is a robust, open-source solution that addresses unit test generation for legacy code. It enhances software reliability and maintainability by automating critical value testing and improving documentation, even with LLM limitations.

Abstract: This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent
system designed to generate unit tests for legacy code, enhancing test coverage
and critical value testing. UnitTenX leverages a combination of AI agents,
formal methods, and Large Language Models (LLMs) to automate test generation,
addressing the challenges posed by complex and legacy codebases. Despite the
limitations of LLMs in bug detection, UnitTenX offers a robust framework for
improving software reliability and maintainability. Our results demonstrate the
effectiveness of this approach in generating high-quality tests and identifying
potential issues. Additionally, our approach enhances the readability and
documentation of legacy code.

</details>


### [6] [What Types of Code Review Comments Do Developers Most Frequently Resolve?](https://arxiv.org/abs/2510.05450)
*Saul Goldman,Hong Yi Lin,Jirat Pasuksmit,Patanamon Thongtanunam,Kla Tantithamthavorn,Zhe Wang,Ray Zhang,Ali Behnaz,Fan Jiang,Michael Siers,Ryan Jiang,Mike Buller,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: The paper analyzes differences between human and LLM-generated code review comments, finding that readability, bug, and maintainability issues lead to more code changes than design-related comments. LLMs and humans have complementary strengths, and LLM-generated comments can be practically useful for developers.


<details>
  <summary>Details</summary>
Motivation: LLM-powered code review tools generate comments, but it's unclear which comments truly trigger code changes. The paper aims to understand the types of comments most likely to drive resolution, to improve the actionable value of automated reviews.

Method: The authors create a taxonomy of five comment categories and develop an LLM-as-a-Judge classification system. They empirically analyze and compare human and LLM-generated comments, and study which types are resolved most often by developers.

Result: Readability, bug-related, and maintainability-focused comments—regardless of author—have higher resolution rates than those about code design. LLM and human reviewers show different strengths depending on context.

Conclusion: Significant portions of LLM-generated review comments are actionable and can be resolved by developers. Human and LLM reviewers can complement each other. Improvement suggestions are provided for code review automation tools.

Abstract: Large language model (LLM)-powered code review automation tools have been
introduced to generate code review comments. However, not all generated
comments will drive code changes. Understanding what types of generated review
comments are likely to trigger code changes is crucial for identifying those
that are actionable. In this paper, we set out to investigate (1) the types of
review comments written by humans and LLMs, and (2) the types of generated
comments that are most frequently resolved by developers. To do so, we
developed an LLM-as-a-Judge to automatically classify review comments based on
our own taxonomy of five categories. Our empirical study confirms that (1) the
LLM reviewer and human reviewers exhibit distinct strengths and weaknesses
depending on the project context, and (2) readability, bugs, and
maintainability-related comments had higher resolution rates than those focused
on code design. These results suggest that a substantial proportion of
LLM-generated comments are actionable and can be resolved by developers. Our
work highlights the complementarity between LLM and human reviewers and offers
suggestions to improve the practical effectiveness of LLM-powered code review
tools.

</details>


### [7] [An Empirical Study of Security-Policy Related Issues in Open Source Projects](https://arxiv.org/abs/2510.05604)
*Rintaro Kanaji,Brittany Reid,Yutaro Kashiwa,Raula Gaikovina Kula,Hajimu Iida*

Main category: cs.SE

TL;DR: The paper analyzes how GitHub's SECURITY.md files impact vulnerability reporting, finding most issues are requests to add the file and that issues with resource links are resolved faster, offering suggestions to improve open-source security management.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate the real-world effectiveness and challenges of SECURITY.md files in facilitating vulnerability reporting in open-source projects, which is currently not well understood despite GitHub’s recommendations.

Method: Content classification and quantitative analysis of 711 randomly sampled SECURITY.md-related GitHub issues. Comparative analysis of issue close times and responses for six community health files was conducted.

Result: 79.5% of SECURITY.md-related issues were requests to add the file. Issues that included links were resolved about 2 days faster (median time), providing actionable insights for better security practices in open-source communities.

Conclusion: The study concludes that SECURITY.md files in GitHub projects face operational challenges, especially in their adoption and utility for vulnerability reporting. Many issues are requests to add the file, and including resource links helps resolve these issues faster.

Abstract: GitHub recommends that projects adopt a SECURITY.md file that outlines
vulnerability reporting procedures. However, the effectiveness and operational
challenges of such files are not yet fully understood. This study aims to
clarify the challenges that SECURITY.md files face in the vulnerability
reporting process within open-source communities. Specifically, we classified
and analyzed the content of 711 randomly sampled issues related to SECURITY.md.
We also conducted a quantitative comparative analysis of the close time and
number of responses for issues concerning six community health files, including
SECURITY.md. Our analysis revealed that 79.5% of SECURITY.md-related issues
were requests to add the file, and reports that included links were closed,
with a median time that was 2 days shorter. These findings offer practical
insights for improving security reporting policies and community management,
ultimately contributing to a more secure open-source ecosystem.

</details>


### [8] [The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment](https://arxiv.org/abs/2510.05705)
*Eva Martín del Pico,Josep Lluís Gelpí,Salvador Capella-Gutiérrez*

Main category: cs.SE

TL;DR: The Software Observatory is a new platform that helps researchers and developers analyze and improve research software by consolidating metadata, visualizing trends, and assessing adherence to FAIR principles.


<details>
  <summary>Details</summary>
Motivation: The motivation is to help the scientific community identify current trends and gaps in research software development, which may hinder scientific progress, and to enable actionable insights using the FAIR principles as a framework.

Method: The study introduces the Software Observatory at OpenEBench, which consolidates metadata from various sources and provides comprehensive analytics, including visualization and FAIRness evaluation, using the FAIRsoft Evaluator component.

Result: The Software Observatory aggregates and visualizes research software metadata, enables trend analysis, and provides FAIRness scores, supporting the improvement of research software practices across the life sciences.

Conclusion: The Software Observatory is a valuable tool for researchers, software developers, and stakeholders, promoting improved software development and adherence to FAIR principles in research software.

Abstract: In the ever-changing realm of research software development, it is crucial
for the scientific community to grasp current trends to identify gaps that can
potentially hinder scientific progress. The adherence to the FAIR (Findable,
Accessible, Interoperable, Reusable) principles can serve as a proxy to
understand those trends and provide a mechanism to propose specific actions.
  The Software Observatory at OpenEBench
(https://openebench.bsc.es/observatory) is a novel web portal that consolidates
software metadata from various sources, offering comprehensive insights into
critical research software aspects. Our platform enables users to analyse
trends, identify patterns and advancements within the Life Sciences research
software ecosystem, and understand its evolution over time. It also evaluates
research software according to FAIR principles for research software, providing
scores for different indicators.
  Users have the ability to visualise this metadata at different levels of
granularity, ranging from the entire software landscape to specific communities
to individual software entries through the FAIRsoft Evaluator. Indeed, the
FAIRsoft Evaluator component streamlines the assessment process, helping
developers efficiently evaluate and obtain guidance to improve their software's
FAIRness.
  The Software Observatory represents a valuable resource for researchers and
software developers, as well as stakeholders, promoting better software
development practices and adherence to FAIR principles for research software.

</details>


### [9] [Digital Twins for Software Engineering Processes](https://arxiv.org/abs/2510.05768)
*Robin Kimmel,Judith Michael,Andreas Wortmann,Jingxi Zhang*

Main category: cs.SE

TL;DR: This paper discusses how digital twins could improve software engineering by representing and optimizing processes, helping both software and domain experts, but highlights that substantial work is needed for practical adoption.


<details>
  <summary>Details</summary>
Motivation: A shortage of skilled software engineers and the complex nature of collaborative software development drive the need for advanced representations and tools to optimize processes.

Method: The study outlines the conceptual framework for digital twins in software engineering, discusses potential benefits, proposes a vision, and analyzes gaps for implementation.

Result: Digital twins can help software experts manage their time efficiently and assist domain experts in producing higher-quality software. The paper identifies benefits, potential structures, and existing gaps for deployment.

Conclusion: Digital twins have significant potential for improving software engineering processes, but further research and development are necessary to realize and deploy them effectively.

Abstract: Digital twins promise a better understanding and use of complex systems. To
this end, they represent these systems at their runtime and may interact with
them to control their processes. Software engineering is a wicked challenge in
which stakeholders from many domains collaborate to produce software artifacts
together. In the presence of skilled software engineer shortage, our vision is
to leverage DTs as means for better rep- resenting, understanding, and
optimizing software engineering processes to (i) enable software experts making
the best use of their time and (ii) support domain experts in producing
high-quality software. This paper outlines why this would be beneficial, what
such a digital twin could look like, and what is missing for realizing and
deploying software engineering digital twins.

</details>


### [10] [Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding](https://arxiv.org/abs/2510.05788)
*Nikita Pavlichenko,Iurii Nazarov,Ivan Dolgov,Ekaterina Garanina,Dmitry Ustalov,Ivan Bondyrev,Kseniia Lysaniuk,Evgeniia Vu,Kirill Chekmenev,Joseph Shtok,Yaroslav Golubev,Anton Semenkin,Uladzislau Sazanovich*

Main category: cs.SE

TL;DR: Mellum is an open-source, 4B parameter code completion model optimized for JetBrains IDEs. Through careful data governance and training methods, it achieves high-quality suggestions while being cost- and latency-efficient. Mellum is production-ready, performant at scale, and openly available for use and research.


<details>
  <summary>Details</summary>
Motivation: There is a growing demand for high-quality, efficient code completion features in Integrated Development Environments (IDEs), especially for interactive use. Current models may not balance quality, efficiency, governance, and user needs, particularly in production scenarios. This paper aims to address these challenges by developing an open-source, production-ready code completion model.

Method: The paper introduces the Mellum model family, which follows a Llama-style architecture with 4 billion parameters. The models are pre-trained on approximately 4 trillion tokens of multi-language, permissively licensed code. The authors describe an industrial pipeline: careful data governance, staged training including fill-in-the-middle and project context supervised fine-tuning, and alignment using direct preference optimization based on real user feedback. Multiple large-scale offline and online evaluation methods are used.

Result: The authors demonstrate that careful data curation and staged training significantly enhance model quality. Features such as context packing are shown to be crucial for robust code suggestions. The compact model size allows meeting cost and latency requirements for interactive code completion. Mellum models perform well in both benchmarks and live deployments, serving hundreds of thousands of users.

Conclusion: The Mellum family provides a practical, open-source solution for in-editor code completion, with proven methods for quality, efficiency, and scalable deployment. It serves as a reproducible and pragmatic blueprint for transitioning research models to production-ready tools in real-world IDE environments. The models and pipeline are openly released for the community.

Abstract: We present the Mellum models family, open-weight code completion models
designed for interactive use in JetBrains IDEs. Mellums have 4B parameters,
adopt a Llama-style architecture, and are pre-trained on ~4T tokens of
permissively licensed, multi-language code. Our studies show that (i) careful
data curation and staged training significantly improve the model's quality,
(ii) editor-critical capabilities such as context packing are necessary for
high-quality suggestions, and (iii) a compact, task-focused model can meet the
cost and latency constraints of interactive completion.
  In the paper, we describe an end-to-end industrial pipeline for producing
contextualized in-editor completion: disciplined data governance, multi-stage
training that includes fill-in-the-middle and project context via supervised
fine-tuning, and alignment via direct preference optimization using feedback
from real-world scenarios. Our quality evaluations include both large-scale
offline benchmarks and online telemetry from production deployments in
JetBrains IDEs. Mellums are released under the Apache-2.0 license on
HuggingFace, with a public model card providing a reproducible reference for
practitioners. Our experience offers a pragmatic blueprint for taking a
focused, open model from a research prototype to at scale production for
hundreds of thousands of users.

</details>


### [11] [A Wave of Resignations in the Aftermath of Remote Onboarding](https://arxiv.org/abs/2510.05878)
*Darja Smite,Franz Zieris,Lars-Ola Damm*

Main category: cs.SE

TL;DR: Remote onboarding during the pandemic led to higher resignation rates among new hires at Ericsson, reflecting a lack of organizational attachment. Differentiated hybrid work policies and stronger in-person integration for new employees improved retention. Effective hybrid models with mentorship should guide post-pandemic HR strategies.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by the need to understand the effects of remote, onsite, and hybrid work modalities on employee retention—especially in the context of the COVID-19 pandemic, which forced rapid adoption of remote work structures. It seeks to address challenges faced by software teams surrounding employee resignation and organizational attachment.

Method: The researchers analyzed HR data from Ericsson Sweden between 2016 and 2025. They examined resignation patterns before, during, and after the pandemic, categorizing employees based on their work modality (onsite, remote, hybrid). They also reviewed exit surveys to assess the reasons behind resignations and organizational attachment issues.

Result: The study found a significant increase in resignations from summer 2021 to summer 2023, especially among employees with less than five years at the company. Those who were onboarded remotely during the pandemic were much more likely to resign within their first three years. The successful return to pre-pandemic retention rates was achieved through differentiated work policies, particularly emphasizing mentorship and team integration during hybrid and in-person onboarding.

Conclusion: Hybrid work models, when properly designed to include mentorship and opportunities for organizational attachment, can maintain employee retention in knowledge-intensive industries. Selective return-to-office policies, focused on integrating new hires with their teams and senior staff, demonstrate clear benefits in fostering belonging and long-term retention.

Abstract: The COVID-19 pandemic has permanently altered workplace structures,
normalizing remote work. However, critical evidence highlights challenges with
fully remote arrangements, particularly for software teams. This study
investigates employee resignation patterns at Ericsson, a global developer of
software-intensive systems, before, during, and after the pandemic. Using HR
data from 2016-2025 in Ericsson Sweden, we analyze how different work
modalities (onsite, remote, and hybrid) influence employee retention. Our
findings show a marked increase in resignations from summer 2021 to summer
2023, especially among employees with less than five years of tenure. Employees
onboarded remotely during the pandemic were significantly more likely to resign
within their first three years, even after returning to the office. Exit
surveys suggest that remote onboarding may fail to establish the necessary
organizational attachment, the feeling of belonging and long-term retention. By
contrast, the company's eventual successful return to pre-pandemic retention
rates illustrates the value of differentiated work policies and supports
reconsidering selective return-to-office (RTO) mandates. Our study demonstrates
the importance of employee integration practices in hybrid environments where
the requirement for in-office presence for recent hires shall be accompanied by
in-office presence from their team members and more senior staff whose
mentoring and social interactions contribute to integration into the corporate
work environment. We hope these actionable insights will inform HR leaders and
policymakers in shaping post-pandemic work practices, demonstrating that
carefully crafted hybrid models anchored in organizational attachment and
mentorship can sustain retention in knowledge-intensive companies.

</details>


### [12] [Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications](https://arxiv.org/abs/2510.05968)
*Scott Frees*

Main category: cs.SE

TL;DR: The paper introduces practical architectural patterns—including a dual-response model using ResourceLink—to enable scalable, secure LLM-powered reporting systems by decoupling query generation from data retrieval and addressing context window limitations.


<details>
  <summary>Details</summary>
Motivation: Large language models can translate natural language into database queries but are hindered by context window limitations, especially with large datasets common in reporting systems. Practical implementation patterns for overcoming these limits and building scalable architectures are lacking.

Method: The paper presents architectural patterns, specifically a dual-response pattern that extends the Model Context Protocol's ResourceLink. This supports iterative query refinement and external data access, alongside methods to handle multi-tenant security and resource lifecycle.

Result: The proposed patterns successfully decouple query generation from data retrieval, enable scalable usage of LLMs in reporting systems, and address security and resource management challenges.

Conclusion: The research provides pragmatic solutions and patterns that overcome key limitations in LLM-driven reporting systems, empowering developers to build scalable, secure applications.

Abstract: Large language models translate natural language into database queries, yet
context window limitations prevent direct deployment in reporting systems where
complete datasets exhaust available tokens. The Model Context Protocol
specification defines ResourceLink for referencing external resources, but
practical patterns for implementing scalable reporting architectures remain
undocumented. This paper presents patterns for building LLM-powered reporting
systems that decouple query generation from data retrieval. We introduce a
dual-response pattern extending ResourceLink to support both iterative query
refinement and out-of-band data access, accompanied by patterns for
multi-tenant security and resource lifecycle management. These patterns address
fundamental challenges in LLM-driven reporting applications and provide
practical guidance for developers building them.

</details>


### [13] [Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools](https://arxiv.org/abs/2510.06000)
*Daniel Otten,Trevor Stalnaker,Nathan Wintersgill,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: Surveying 91 software engineers, this work finds GenAI heavily used for code generation, with experienced users expanding its use to debugging and code review via iterative conversations. Documentation tasks are most reliable; complex coding and debugging remain challenging. The findings offer a baseline for future GenAI tool improvements in development workflows.


<details>
  <summary>Details</summary>
Motivation: While prompt engineering as a skill is well-studied, there is a knowledge gap regarding how developers integrate GenAI into their broader workflows beyond individual techniques.

Method: A large-scale survey of 91 software engineers (72 active GenAI users) was used, examining their prompting strategies, conversation patterns, and reliability assessments.

Result: Code generation is a nearly universal application of GenAI. More proficient users apply GenAI to complex tasks—debugging and code review—often using iterative multi-turn conversations. Documentation leveraging GenAI is perceived as the most reliable, but challenges remain for complex code generation and debugging.

Conclusion: The study provides foundational empirical insights into how software developers currently use GenAI tools, highlighting areas for future improvement.

Abstract: The integration of generative artificial intelligence (GenAI) tools has
fundamentally transformed software development. Although prompt engineering has
emerged as a critical skill, existing research focuses primarily on individual
techniques rather than software developers' broader workflows. This study
presents a systematic investigation of how software engineers integrate GenAI
tools into their professional practice through a large-scale survey examining
prompting strategies, conversation patterns, and reliability assessments across
various software engineering tasks.
  We surveyed 91 software engineers, including 72 active GenAI users, to
understand AI usage patterns throughout the development process. Our 14 key
findings show that while code generation is nearly universal, proficiency
strongly correlates with using AI for more nuanced tasks such as debugging and
code review, and that developers prefer iterative multi-turn conversations to
single-shot prompting. Documentation tasks are perceived as most reliable,
while complex code generation and debugging present sizable challenges. Our
insights provide an empirical baseline of current developer practices, from
simple code generation to deeper workflow integration, with actionable insights
for future improvements.

</details>


### [14] [Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations](https://arxiv.org/abs/2510.06104)
*Elijah Kayode Adejumo,Brittany Johnson*

Main category: cs.SE

TL;DR: The paper explores using LLMs to generate clear risk explanations from fault-prediction metrics to help OSS contributors. They propose several explanation styles and plan to test usefulness through direct comparison with conventional metric cues.


<details>
  <summary>Details</summary>
Motivation: OSS projects rely on contributors with varying expertise, and interpreting metric-based warnings about code risks is challenging for less experienced contributors. Improving their understanding and decision quality is vital for project health and safety.

Method: The authors propose to use LLMs for generating descriptive, contextual, and actionable risk explanations based on software fault-prediction metrics, and plan to evaluate this approach via a task-based study comparing LLM-based guidance to traditional metric-only information.

Result: The outlined approach has created explanation types for LLM-enabled code review assistance. The next step is to empirically assess the impact on decision quality, completion time, and error rates with OSS contributors.

Conclusion: The paper concludes that leveraging LLMs to translate defect-prediction metrics into human-readable explanations and actionable advice has the potential to improve the ability of OSS contributors to interpret complex signals and safely plan or review code modifications.

Abstract: Open Source Software (OSS) has become a very important and crucial
infrastructure worldwide because of the value it provides. OSS typically
depends on contributions from developers across diverse backgrounds and levels
of experience. Making safe changes, such as fixing a bug or implementing a new
feature, can be challenging, especially in object-oriented systems where
components are interdependent. Static analysis and defect-prediction tools
produce metrics (e.g., complexity,coupling) that flag potentially fault-prone
components, but these signals are often hard for contributors new or unfamiliar
with the codebase to interpret. Large Language Models (LLMs) have shown strong
performance on software engineering tasks such as code summarization and
documentation generation. Building on this progress, we investigate whether
LLMs can translate fault-prediction metrics into clear, human-readable risk
explanations and actionable guidance to help OSS contributors plan and review
code modifications. We outline explanation types that an LLM-generated
assistant could provide (descriptive, contextual, and actionable explanations).
We also outline our next steps to assess usefulness through a task-based study
with OSS contributors, comparing metric-only baselines to LLM-generated
explanations on decision quality, time-to-completion, and error rates

</details>


### [15] [Automated Program Repair of Uncompilable Student Code](https://arxiv.org/abs/2510.06187)
*Griffin Pitts,Aum Pandya,Darsh Rank,Tirth Bhatt,Muntasir Hoq,Bita Akram*

Main category: cs.SE

TL;DR: The paper shows that LLMs can repair uncompilable student code, enabling richer learning analysis, though the fidelity of repairs to students' original logic and structure varies by model.


<details>
  <summary>Details</summary>
Motivation: Many student programming submissions in CS1 are uncompilable, reducing their value for analyzing learning progress and knowledge tracing. Previous methods exclude these cases, losing potentially valuable learning data.

Method: This study explores automated program repair using large language models (LLMs) such as GPT-5, Claude 3.5 Haiku, and Gemini 2.5 Flash. It evaluates these LLMs under high- and low-context prompting, measuring the compilability, edit distance, and preservation of students' code structure and logic in their repairs.

Result: All three LLMs can produce compilable code from uncompilable submissions, but they differ in how well they preserve student intent, control flow, and code structure. These differences impact their usefulness for educational analysis.

Conclusion: Applying LLMs for repairing uncompilable student code allows more comprehensive analysis of student learning and coding development, as previously excluded data can now be recovered with fidelity to the original intent.

Abstract: A significant portion of student programming submissions in CS1 learning
environments are uncompilable, limiting their use in student modeling and
downstream knowledge tracing. Traditional modeling pipelines often exclude
these cases, discarding observations of student learning. This study
investigates automated program repair as a strategy to recover uncompilable
code while preserving students' structural intent for use in student modeling.
Within this framework, we assess large language models (LLMs) as repair agents,
including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash
(Google), under high- and low-context prompting conditions. Repairs were
evaluated for compilability, edit distance, and preservation of students'
original structure and logic. We find that while all three LLMs are capable of
producing compilable repairs, their behavior diverges in how well they preserve
students' control flow and code structure, which affects their pedagogical
utility. By recovering uncompilable submissions, this work enables richer and
more comprehensive analyses of learners' coding processes and development over
time.

</details>
