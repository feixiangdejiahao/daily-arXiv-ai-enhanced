{"id": "2510.15912", "categories": ["cs.PL", "cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15912", "abs": "https://arxiv.org/abs/2510.15912", "authors": ["Jack Cashman"], "title": "Latency Based Tiling", "comment": null, "summary": "Latency Based Tiling provides a systems based approach to deriving\napproximate tiling solution that maximizes locality while maintaining a fast\ncompile time. The method uses triangular loops to characterize miss ratio\nscaling of a machine avoiding prefetcher distortion. Miss ratio scaling\ncaptures the relationship between data access latency and working set size with\nsharp increases in latency indicating the data footprint exceeds capacity from\na cache level. Through these noticeable increases in latency we can determine\nan approximate location for L1, L2, and L3 memory sizes. These sizes are\nexpected to be under approximations of a systems true memory sizes which is in\nline with our expectations given the shared nature of cache in a multi process\nsystem as described in defensive loop tiling. Unlike auto tuning, which can be\neffective but prohibitively slow, Latency Based Tiling achieves negligible\ncompile time overhead. The implementation in Rust enables a hardware agnostic\napproach which combined with a cache timing based techniques, yields a\nportable, memory safe system running wherever Rust is supported. The tiling\nstrategy is applied to a subset of the polyhedral model, where loop nestings\nare tiled based on both the derived memory hierarchy and the observed data\nfootprint per iteration.", "AI": {"tldr": "Latency Based Tiling quickly estimates cache sizes using latency spikes, then tiles loops accordingly for efficient memory usage without slow auto-tuning. Rust implementation ensures portability and safety.", "motivation": "Traditional auto-tuning methods for loop tiling in compilers, while effective at optimizing locality, are often very slow at compile time. Additionally, existing approaches may not efficiently capture memory hierarchy and working set behaviors, especially in systems with shared caches. There is a need for a faster, portable, and memory-safe tiling system that adapts to real hardware constraints.", "method": "The proposed Latency Based Tiling uses triangular loops to analyze and capture miss ratio scaling\u2014a measure of how memory access latency increases with working set size. This approach identifies sharp increases in latency that correspond to cache size limits (L1, L2, L3), providing estimates for memory hierarchy levels. These estimates are used for tiling loop nests in the polyhedral model. The system is implemented in Rust, which enables hardware agnosticism and memory safety.", "result": "Latency Based Tiling achieves fast compile times with negligible overhead compared to traditional auto-tuning, maintains locality, and provides effective tiling based on dynamically observed cache effects. The generated tile sizes are portable under-approximations matching practical system constraints, and the Rust implementation ensures robust portability and safety.", "conclusion": "Latency Based Tiling offers a practical, efficient, and portable solution to loop tiling by analyzing latency and miss ratio scaling to uncover hardware cache limits. It eliminates the need for slow auto-tuning, makes informed tiling decisions, and can run on any system supporting Rust."}}
{"id": "2510.16133", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.16133", "abs": "https://arxiv.org/abs/2510.16133", "authors": ["Daniel Sainati", "Joseph W. Cutler", "Benjamin C. Pierce", "Stephanie Weirich"], "title": "Typing Strictness (Extended Version)", "comment": "30 pages, 22 figures, extended version of a paper to be published at\n  POPL 2026", "summary": "Strictness analysis is critical to efficient implementation of languages with\nnon-strict evaluation, mitigating much of the performance overhead of laziness.\nHowever, reasoning about strictness at the source level can be challenging and\nunintuitive. We propose a new definition of strictness that refines the\ntraditional one by describing variable usage more precisely. We lay\ntype-theoretic foundations for this definition in both call-by-name and\ncall-by-push-value settings, drawing inspiration from the literature on type\nsystems tracking effects and coeffects. We prove via a logical relation that\nthe strictness attributes computed by our type systems accurately describe the\nuse of variables at runtime, and we offer a strictness-annotation-preserving\ntranslation from the call-by-name system to the call-by-push-value one. All our\nresults are mechanized in Rocq.", "AI": {"tldr": "This paper refines strictness analysis in lazy languages by introducing a more precise type-theoretic definition, mechanizing proofs that connect static analysis to runtime behavior and supporting translation between evaluation strategies.", "motivation": "Strictness analysis is essential for optimizing non-strict (lazy) programming languages, but traditional approaches are difficult and imprecise for source-level reasoning. The work aims to improve precision and usability of strictness analysis.", "method": "The authors develop type-theoretic foundations in call-by-name and call-by-push-value paradigms, prove results via logical relations, and mechanize proofs in Rocq.", "result": "The new definition enables precise tracking of variable usage, proves accurate reflection of runtime behavior through logical relations, and provides a translation retaining strictness annotations across evaluation strategies.", "conclusion": "The paper establishes a refined definition of strictness at the type-theoretic level, supported by logical relations and mechanized proofs, and demonstrates accurate correspondence between static and dynamic variable usage."}}
{"id": "2510.16594", "categories": ["cs.PL", "F.3.2; F.1.1"], "pdf": "https://arxiv.org/pdf/2510.16594", "abs": "https://arxiv.org/abs/2510.16594", "authors": ["Moida Praneeth Jain", "Venkatesh Choppella"], "title": "SimpliPy: A Source-Tracking Notional Machine for Simplified Python", "comment": "15 pages, 1 figure, 1 table. Accepted at the 4th Workshop on Research\n  Highlights in Programming Languages (RHPL 2025), co-located with FSTTCS 2025.\n  Code available at: https://github.com/PraneethJain/simplipy", "summary": "Misconceptions about program execution hinder many novice programmers. We\nintroduce SimpliPy, a notional machine designed around a carefully chosen\nPython subset to clarify core control flow and scoping concepts. Its foundation\nis a precise operational semantics that explicitly tracks source code line\nnumbers for each execution step, making the link between code and behavior\nunambiguous. Complementing the dynamic semantics, SimpliPy uses static analysis\nto generate Control Flow Graphs (CFGs) and identify lexical scopes, helping\nstudents build a structural understanding before tracing. We also present an\ninteractive web-based debugger built on these principles. This tool embodies\nthe formal techniques, visualizing the operational state (environments, stack)\nand using the static CFG to animate control flow directly on the graph during\nstep-by-step execution. SimpliPy thus integrates formal semantics, program\nanalysis, and visualization to offer both a pedagogical approach and a\npractical demonstration of applying formal methods to program understanding.", "AI": {"tldr": "SimpliPy is an educational tool based on a simplified Python semantics. It combines dynamic and static analyses to make control flow and scoping clearer for beginners, with a visual debugger and formal underpinnings that foster better code comprehension.", "motivation": "Novice programmers struggle with misconceptions about program execution, particularly regarding control flow and scoping. Existing tools and teaching methods often fail to make the connection between code and its behavior sufficiently clear.", "method": "The authors developed SimpliPy, a notional machine based on a simplified Python subset with a precise operational semantics that tracks source code line numbers at each execution step. The system uses static analysis to generate Control Flow Graphs (CFGs) and identify lexical scopes, aiding structural understanding prior to dynamic tracing. An interactive web debugger visualizes operational state and animates control flow directly on the CFG during step-by-step execution.", "result": "SimpliPy integrates formal semantics, program analysis, and visualization, allowing students to clearly observe the relationship between code and behavior. The tool\u2019s debugger visualizes environments and control flow, helping learners understand execution and scoping in Python.", "conclusion": "SimpliPy provides a practical pedagogical approach and demonstration of formal methods for program understanding, bridging the gap between theory and practice for novice programmers."}}
{"id": "2510.16883", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.16883", "abs": "https://arxiv.org/abs/2510.16883", "authors": ["Giulia Giusti", "Michele Pagani"], "title": "JAX Autodiff from a Linear Logic Perspective (Extended Version)", "comment": null, "summary": "Autodiff refers to the core of the automatic differentiation systems\ndeveloped in projects like JAX and Dex. Autodiff has recently been formalised\nin a linear typed calculus by Radul et al in arXiv:2204.10923. Although this\nformalisation suffices to express the main program transformations of Autodiff,\nthe calculus is very specific to this task, and it is not clear whether the\ntype system yields a substructural logic that has interest on its own.\n  We propose an encoding of Autodiff into a linear $\\lambda$-calculus that\nenjoys a Curry-Howard correspondence with Girard's linear logic. We prove that\nthe encoding is sound both qualitatively (the encoded terms are extensionally\nequivalent to the original ones) and quantitatively (the encoding preserves the\noriginal work cost as described in arXiv:2204.10923). As a byproduct, we show\nthat unzipping, one of the transformations used to implement backpropagation in\nAutodiff, is, in fact, optional.", "AI": {"tldr": "The paper shows Autodiff can be encoded in linear lambda calculus, grounding it in linear logic and proving this encoding is both correct and preserves computational cost; it also reveals an aspect of current implementations (unzipping) is optional.", "motivation": "Current formalizations of Autodiff, as in JAX and Dex, use a linear typed calculus that is narrowly focused and its alignment with broader substructural logics is unclear. The paper aims to place Autodiff in a more general and theoretically meaningful linear logic context.", "method": "The authors encode Autodiff into a linear lambda calculus that corresponds to Girard's linear logic via Curry-Howard correspondence, then prove soundness both qualitatively (semantic equivalence) and quantitatively (work cost preservation).", "result": "The encoding into linear lambda calculus is shown to be both qualitatively correct (terms are extensionally equivalent) and quantitatively faithful (preserves original cost), and demonstrates that the 'unzipping' transformation used in backpropagation is optional.", "conclusion": "Autodiff can be fully and soundly encoded in a general linear lambda calculus, providing a broader foundational basis for automatic differentiation and showing that some implementation steps, like unzipping, are not necessary."}}
{"id": "2510.17429", "categories": ["cs.PL", "D.3.1"], "pdf": "https://arxiv.org/pdf/2510.17429", "abs": "https://arxiv.org/abs/2510.17429", "authors": ["Jin Sano", "Naoki Yamamoto", "Kazunori Ueda"], "title": "Introducing Linear Implication Types to $\u03bb_{GT}$ for Computing With Incomplete Graphs", "comment": "26 pages, 14 figures, This paper is submitted to PRO2025-3", "summary": "Designing programming languages that enable intuitive and safe manipulation\nof data structures is a critical research challenge. Conventional destructive\nmemory operations using pointers are complex and prone to errors. Existing type\nsystems, such as affine types and shape types, address this problem towards\nsafe manipulation of heaps and pointers, but design of high-level declarative\nlanguages that allow us to manipulate complex pointer data structures at a\nhigher level of abstraction is largely an open problem. The $\\lambda_{GT}$\nlanguage, a purely functional programming language that treats hypergraphs\n(hereafter referred to as graphs) as primary data structures, addresses some of\nthese challenges. By abstracting data with shared references and cycles as\ngraphs, it enables declarative operations through pattern matching and\nleverages its type system to guarantee safety of these operations.\nNevertheless, the previously proposed type system of $\\lambda_{GT}$ leaves two\nsignificant open challenges. First, the type system does not support\n\\emph{incomplete graphs}, that is, graphs in which some elements are missing\nfrom the graphs of user-defined types. Second, the type system relies on\ndynamic type checking during pattern matching. This study addresses these two\nchallenges by incorporating linear implication into the $\\lambda_{GT}$ type\nsystem, while introducing new constraints to ensure its soundness.", "AI": {"tldr": "This paper improves the \u03bb_GT functional programming language's type system to safely support manipulation of graphs\u2014including incomplete ones\u2014without dynamic checks, by using linear implication and soundness constraints.", "motivation": "Conventional destructive memory operations with pointers are error-prone and complex. Existing type systems offer some safety but still lack high-level declarative languages for manipulating complex pointer-based data structures easily and safely.", "method": "The paper proposes enhancements to the type system of the \u03bb_GT functional programming language by incorporating linear implication and introducing new constraints for soundness, aiming to support incomplete graphs and eliminate reliance on dynamic type checking.", "result": "The improved type system supports incomplete graphs and removes the need for dynamic type checking by ensuring soundness with new constraints.", "conclusion": "Incorporating linear implication into the \u03bb_GT type system with new constraints resolves key shortcomings, enabling safe and static manipulation of graph-based data structures in a declarative manner."}}
{"id": "2510.17505", "categories": ["cs.PL", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.17505", "abs": "https://arxiv.org/abs/2510.17505", "authors": ["Jaeyeon Won", "Willow Ahrens", "Joel S. Emer", "Saman Amarasinghe"], "title": "Insum: Sparse GPU Kernels Simplified and Optimized with Indirect Einsums", "comment": null, "summary": "Programming high-performance sparse GPU kernels is notoriously difficult,\nrequiring both substantial effort and deep expertise. Sparse compilers aim to\nsimplify this process, but existing systems fall short in two key ways. First,\nthey are primarily designed for CPUs and rarely produce high-performance GPU\ncode. Second, when computations involve both sparse and dense regions, these\ncompilers often fail to optimize the dense portions effectively. In this paper,\nwe propose a new approach for expressing sparse computations. We start from\nformat-agnostic Einsums over sparse tensors and rewrite them into\nformat-conscious indirect Einsums, which explicitly encode format information\nby mapping sparse data and metadata onto dense tensor operations through\nindirect indexing. To execute indirect Einsums, we introduce the Insum\ncompiler, which generates efficient GPU code for these Einsums by lowering to\nthe PyTorch compiler, extended to better support Tensor Core-enabled indirect\nEinsums. We also present two fixed-length sparse formats, GroupCOO and\nBlockGroupCOO, designed to fit naturally with indirect Einsums. Our approach\nachieves 1.14x to 3.81x speedups across a range of sparse GPU applications\nwhile reducing lines of code by 202x to 4491x compared to hand-written\nimplementations.", "AI": {"tldr": "This paper presents a new method and compiler for more efficient and easier GPU programming of sparse computations, outclassing existing methods in speed and simplicity.", "motivation": "Programming high-performance sparse GPU kernels is complex and demands significant expertise. Existing sparse compilers are CPU-focused and inefficient for GPU code, particularly failing to optimize mixed sparse-dense computations.", "method": "The paper introduces a new way to express sparse computations using format-agnostic Einsums for sparse tensors, which are rewritten into indirect Einsums that incorporate format information through indirect indexing. A compiler called Insum is proposed to generate optimized GPU code for these operations by extending the capabilities of the PyTorch compiler, and two new sparse formats, GroupCOO and BlockGroupCOO, are designed for compatibility.", "result": "Their approach produces notable improvements: achieving 1.14x to 3.81x speedups in sparse GPU applications and reducing code size by 202x to 4491x compared to manual implementations.", "conclusion": "The proposed format-conscious indirect Einsum approach and the Insum compiler significantly enhance the performance and reduce the complexity of programming sparse GPU kernels, addressing previous limitations of sparse compilers."}}
{"id": "2510.16059", "categories": ["cs.SE", "cs.CL", "D.2.2; D.2.3"], "pdf": "https://arxiv.org/pdf/2510.16059", "abs": "https://arxiv.org/abs/2510.16059", "authors": ["Xin Cao", "Nan Yu"], "title": "SIADAFIX: issue description response for adaptive program repair", "comment": "20 pages, 3 figures", "summary": "We propose utilizing fast and slow thinking to enhance the capabilities of\nlarge language model-based agents on complex tasks such as program repair. In\nparticular, we design an adaptive program repair method based on issue\ndescription response, called SIADAFIX. The proposed method utilizes slow\nthinking bug fix agent to complete complex program repair tasks, and employs\nfast thinking workflow decision components to optimize and classify issue\ndescriptions, using issue description response results to guide the\norchestration of bug fix agent workflows. SIADAFIX adaptively selects three\nrepair modes, i.e., easy, middle and hard mode, based on problem complexity. It\nemploys fast generalization for simple problems and test-time scaling\ntechniques for complex problems. Experimental results on the SWE-bench Lite\nshow that the proposed method achieves 60.67% pass@1 performance using the\nClaude-4 Sonnet model, reaching state-of-the-art levels among all open-source\nmethods. SIADAFIX effectively balances repair efficiency and accuracy,\nproviding new insights for automated program repair. Our code is available at\nhttps://github.com/liauto-siada/siada-cli.", "AI": {"tldr": "SIADAFIX uses a combination of fast and slow thinking, adaptively choosing repair modes for program repair. It sets a new benchmark in automated program repair, balancing efficiency and accuracy.", "motivation": "Current LLM-based agents struggle with complex program repair tasks, often unable to efficiently balance speed (efficiency) and accuracy. Enhancing these agents could improve automated program repair.", "method": "SIADAFIX combines two components: a slow thinking bug fix agent for complex repairs and fast workflow decision modules that optimize and classify issue descriptions. It adaptively switches between three repair modes (easy, middle, hard) using generalization and test-time scaling as needed.", "result": "SIADAFIX achieves 60.67% pass@1 performance using the Claude-4 Sonnet model, outperforming other open-source methods on SWE-bench Lite.", "conclusion": "The proposed SIADAFIX method improves automated program repair by adaptively managing repair complexity and workflow. It balances speed and accuracy, providing new perspectives for future research."}}
{"id": "2510.16357", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.16357", "abs": "https://arxiv.org/abs/2510.16357", "authors": ["Jugal Gajjar", "Kamalasankari Subramaniakuppusamy"], "title": "MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema", "comment": "12 pages, 7 figures, 4 tables, 2 algorithms, and 34 references.\n  HuggingFace:\n  https://huggingface.co/datasets/jugalgajjar/MultiLang-Code-Parser-Dataset\n  GitHub: https://github.com/JugalGajjar/MultiLang-Code-Parser-Dataset", "summary": "We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,\nlanguage-agnostic dataset unifying syntactic and structural representations of\ncode across ten major programming languages. MLCPD contains over seven million\nparsed source files normalized under our proposed universal Abstract Syntax\nTree (AST) schema, enabling consistent cross-language reasoning, structural\nlearning, and multilingual software analysis. Unlike existing corpora that\nfocus purely on token-level code or isolated parsers, MLCPD provides both\nhierarchical tree representations and rich metadata for every file, ensuring\nlossless syntactic coverage and structural uniformity. Each entry includes a\nnormalized schema, language-level metadata, and abstracted node semantics\nstored in Parquet format for scalable retrieval. Empirical analyses reveal\nstrong cross-language structural regularities-demonstrating that syntactic\ngraphs from languages as diverse as Python, Java, and Go can be aligned under a\nshared schema. We release the dataset publicly on Hugging Face and the\naccompanying codebase on GitHub, which includes complete pipelines for dataset\nreproduction, grammar compilation, and a visualization tool for exploring the\nunified AST across languages. Together, these resources establish MLCPD as an\nopen, reproducible foundation for future research in cross-language\nrepresentation learning and program analysis.", "AI": {"tldr": "MLCPD is a large, universal code dataset providing normalized ASTs for ten languages, supporting cross-language code analysis, with open resources to aid software research.", "motivation": "There is a lack of large-scale, language-agnostic datasets that unify code structure across multiple programming languages, limiting research in cross-language software analysis and representation learning.", "method": "The authors constructed MLCPD by parsing over seven million source files from ten major programming languages, applying a universal AST schema to normalize and unify their syntactic structure and metadata.", "result": "MLCPD reveals strong cross-language structural regularities, showing that code from varied languages can be aligned under a unified schema. The dataset and associated tools are made publicly available for reproducibility and further research.", "conclusion": "MLCPD establishes a new, open-standard resource for cross-language program analysis and representation learning, enabling lossless and consistent structural reasoning between diverse programming languages."}}
{"id": "2510.16242", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.16242", "abs": "https://arxiv.org/abs/2510.16242", "authors": ["Eva Maxfield Brown", "Isaac Slaughter", "Nicholas Weber"], "title": "Code Contribution and Credit in Science", "comment": null, "summary": "Software development has become essential to scientific research, but its\nrelationship to traditional metrics of scholarly credit remains poorly\nunderstood. We develop a dataset of approximately 140,000 paired research\narticles and code repositories, as well as a predictive model that matches\nresearch article authors with software repository developer accounts. We use\nthis data to investigate how software development activities influence credit\nallocation in collaborative scientific settings. Our findings reveal\nsignificant patterns distinguishing software contributions from traditional\nauthorship credit. We find that nearly 30% of articles include non-author code\ncontributors- individuals who participated in software development but received\nno formal authorship recognition. While code-contributing authors show a modest\n$\\sim$4.2% increase in article citations, this effect becomes non-significant\nwhen controlling for domain, article type, and open access status. First\nauthors are significantly more likely to be code contributors than other author\npositions. Notably, we identify a negative relationship between coding\nfrequency and scholarly impact metrics. Authors who contribute code more\nfrequently exhibit progressively lower h-indices than non-coding colleagues,\neven when controlling for publication count, author position, domain, and\narticle type. These results suggest a disconnect between software contributions\nand credit, highlighting important implications for institutional reward\nstructures and science policy.", "AI": {"tldr": "Software contributors in scientific research often go unrecognized in traditional author credits, and their coding activities do not translate into higher scholarly impact. This highlights a misalignment in credit allocation and calls for science policy reforms.", "motivation": "Software has become vital in scientific research, but it's unclear how software contributions are recognized compared to traditional authorship in scholarly metrics.", "method": "The researchers compiled a large dataset (~140,000 pairs) linking research articles and software repositories. They developed a model to match article authors to developer accounts and analyzed patterns of credit allocation and impact metrics for software contributors versus non-contributors.", "result": "Nearly 30% of articles had code contributors who did not receive formal authorship credit. Code-contributing authors gained only a small citation boost, which was not statistically significant after controlling for factors. First authors were much more likely to contribute code. Frequent code contributors had lower h-indices than their non-coding peers, even when accounting for other variables.", "conclusion": "There is a substantial gap between software development contributions and formal scholarly recognition, with frequent code contributors receiving less institutional credit through traditional metrics."}}
{"id": "2510.16809", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL", "68T50, 68N30, 68W40", "I.2.7; D.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.16809", "abs": "https://arxiv.org/abs/2510.16809", "authors": ["Amirkia Rafiei Oskooei", "Kaan Baturalp Cosdan", "Husamettin Isiktas", "Mehmet S. Aktas"], "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation", "comment": null, "summary": "Large Language Models (LLMs) with vast context windows offer new avenues for\nin-context learning (ICL), where providing many examples (\"many-shot\"\nprompting) is often assumed to enhance performance. We investigate this\nassumption for the complex task of code translation. Through a large-scale\nempirical study of over 90,000 translations, we systematically evaluate the\nimpact of scaling in-context examples from zero-shot to many-shot\nconfigurations of up to 625 examples, with prompts spanning from approximately\n100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while\nstatic similarity metrics may modestly improve with more examples, functional\ncorrectness consistently peaks with few-shot prompting (5-25 examples).\nProviding substantially more examples often degrades this crucial functional\nperformance. This study highlights that for code translation, the quality of a\nfew well-chosen examples outweighs sheer quantity, challenging the universal\nefficacy of \"more is better\" for ICL and underscoring the task-dependent nature\nof optimal prompting strategies. Our results have significant implications for\neffectively leveraging LLMs in software engineering.", "AI": {"tldr": "For code translation tasks using LLMs, giving many examples in the prompt does not improve and may worsen functional accuracy. A handful of well-chosen examples works best, challenging the assumption that bigger prompts are always better.", "motivation": "It is commonly assumed that providing more examples in in-context learning, especially with models having large context windows, will enhance task performance. The paper tests this assumption in the context of code translation, a complex and practical task.", "method": "Large-scale empirical study evaluating over 90,000 code translations. The study systematically scales in-context examples from zero-shot up to many-shot (over 625 examples) and analyzes functional correctness versus static similarity metrics.", "result": "Static similarity metrics improve modestly with more examples, but functional correctness peaks with only a few examples (5-25). Excessive examples degrade functional performance. The optimal number of examples is task-dependent, with 'more is better' not holding universally for code translation.", "conclusion": "The quality of a few well-chosen in-context examples yields better functional correctness in code translation tasks compared to simply providing many examples. More examples do not necessarily improve performance and may degrade results."}}
{"id": "2510.16384", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16384", "abs": "https://arxiv.org/abs/2510.16384", "authors": ["Yuwei Zhao", "Yuan-An Xiao", "Qianyu Xiao", "Zhao Zhang", "Yingfei Xiong"], "title": "SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis", "comment": null, "summary": "Automated code optimization aims to improve performance in programs by\nrefactoring code, and recent studies focus on utilizing LLMs for the\noptimization. Typical existing approaches mine optimization commits from\nopen-source codebases to construct a large-scale knowledge base, then employ\ninformation retrieval techniques such as BM25 to retrieve relevant optimization\nexamples for hotspot code locations, thereby guiding LLMs to optimize these\nhotspots. However, since semantically equivalent optimizations can manifest in\nsyntactically dissimilar code snippets, current retrieval methods often fail to\nidentify pertinent examples, leading to suboptimal optimization performance.\nThis limitation significantly reduces the effectiveness of existing\noptimization approaches.\n  To address these limitations, we propose SemOpt, a novel framework that\nleverages static program analysis to precisely identify optimizable code\nsegments, retrieve the corresponding optimization strategies, and generate the\noptimized results. SemOpt consists of three key components: (1) A strategy\nlibrary builder that extracts and clusters optimization strategies from\nreal-world code modifications. (2) A rule generator that generates Semgrep\nstatic analysis rules to capture the condition of applying the optimization\nstrategy. (3) An optimizer that utilizes the strategy library to generate\noptimized code results. All the three components are powered by LLMs.\n  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its\neffectiveness under different LLMs by increasing the number of successful\noptimizations by 1.38 to 28 times compared to the baseline. Moreover, on\npopular large-scale C/C++ projects, it can improve individual performance\nmetrics by 5.04% to 218.07%, demonstrating its practical utility.", "AI": {"tldr": "SemOpt boosts automated code optimization by using LLMs and static analysis for semantic understanding, outperforming traditional approaches on benchmarks and real-world projects.", "motivation": "Existing automated code optimization systems often rely on information retrieval techniques like BM25 to mine optimization examples from large codebases. However, these approaches struggle because semantically similar optimizations may have different syntactic forms, making it hard to retrieve pertinent examples and resulting in suboptimal optimized code.", "method": "The authors introduce SemOpt, a framework that applies static program analysis and LLMs to automatically identify specific code segments that can be optimized. SemOpt has three components: (1) it builds a library by clustering optimization strategies from real-world code modifications, (2) it generates Semgrep rules to detect applicable code segments, and (3) it leverages the strategy library to synthesize optimized code, all powered by LLMs.", "result": "SemOpt achieves far better optimization rates compared to baseline approaches, improving the number of successful code optimizations from 1.38 up to 28 times on a 151-task benchmark. On real-world C/C++ projects, it yields performance gains ranging from 5.04% up to 218.07% on individual metrics, proving its effectiveness and practical value.", "conclusion": "SemOpt addresses the limitations of current LLM-driven code optimization systems by focusing on semantic understanding and static analysis, resulting in significantly improved optimization success rates and real-world performance gains."}}
{"id": "2510.16395", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16395", "abs": "https://arxiv.org/abs/2510.16395", "authors": ["Xin Peng", "Chong Wang"], "title": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated strong\ncapabilities in software engineering tasks, raising expectations of\nrevolutionary productivity gains. However, enterprise software development is\nlargely driven by incremental evolution, where challenges extend far beyond\nroutine coding and depend critically on tacit knowledge, including design\ndecisions at different levels and historical trade-offs. To achieve effective\nAI-powered support for complex software development, we should align emerging\nAI capabilities with the practical realities of enterprise development. To this\nend, we systematically identify challenges from both software and LLM\nperspectives. Alongside these challenges, we outline opportunities where AI and\nstructured knowledge frameworks can enhance decision-making in tasks such as\nissue localization and impact analysis. To address these needs, we propose the\nCode Digital Twin, a living framework that models both the physical and\nconceptual layers of software, preserves tacit knowledge, and co-evolves with\nthe codebase. By integrating hybrid knowledge representations, multi-stage\nextraction pipelines, incremental updates, LLM-empowered applications, and\nhuman-in-the-loop feedback, the Code Digital Twin transforms fragmented\nknowledge into explicit and actionable representations. Our vision positions it\nas a bridge between AI advancements and enterprise software realities,\nproviding a concrete roadmap toward sustainable, intelligent, and resilient\ndevelopment and evolution of ultra-complex systems.", "AI": {"tldr": "Current LLMs can help in coding, but enterprise software development faces unique challenges due to its complex, evolving nature and reliance on tacit knowledge. The proposed Code Digital Twin is a structured, evolving framework that makes hidden knowledge explicit and usable by both AI and humans, helping enterprises utilize AI for smarter, sustainable software engineering.", "motivation": "Despite rapid advances in LLM capabilities, applying them to enterprise software development is challenging due to its incremental nature and reliance on tacit knowledge (such as design decisions and historical trade-offs) that go beyond routine coding.", "method": "The authors systematically identify challenges from both the software engineering and LLM perspectives, and propose the Code Digital Twin\u2014a framework that models physical and conceptual layers of software, incorporates hybrid knowledge representations and multi-stage extraction pipelines, supports incremental updates and human-in-the-loop feedback, and uses LLM-powered applications.", "result": "The Code Digital Twin framework enables the preservation and explicit representation of tacit knowledge, enhances decision-making tasks (like issue localization and impact analysis), and transforms fragmented knowledge into actionable forms. It can co-evolve with the codebase and support sustainable, resilient enterprise software development.", "conclusion": "Integrating AI capabilities with structured knowledge frameworks (like Code Digital Twin) can align LLM strengths with the realities of enterprise software engineering. This approach delivers a roadmap for intelligent and sustainable development and evolution of complex software systems, bridging the gap between cutting-edge AI and real-world software practices."}}
{"id": "2510.16433", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16433", "abs": "https://arxiv.org/abs/2510.16433", "authors": ["Tatsuya Shirai", "Olivier Nourry", "Yutaro Kashiwa", "Kenji Fujiwara", "Yasutaka Kamei", "Hajimu Iida"], "title": "Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions", "comment": null, "summary": "Software vulnerabilities are constantly being reported and exploited in\nsoftware products, causing significant impacts on society. In recent years, the\nmain approach to vulnerability detection, fuzzing, has been integrated into the\ncontinuous integration process to run in short and frequent cycles. This\ncontinuous fuzzing allows for fast identification and remediation of\nvulnerabilities during the development process. Despite adoption by thousands\nof projects, however, it is unclear how continuous fuzzing contributes to\nvulnerability detection. This study aims to elucidate the role of continuous\nfuzzing in vulnerability detection. Specifically, we investigate the coverage\nand the total number of fuzzing sessions when fuzzing bugs are discovered. We\ncollect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an\nonline service provided by Google that performs fuzzing during continuous\nintegration. Through an empirical study of a total of approximately 1.12\nmillion fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal\nthat (i) a substantial number of fuzzing bugs exist prior to the integration of\ncontinuous fuzzing, leading to a high detection rate in the early stages; (ii)\ncode coverage continues to increase as continuous fuzzing progresses; and (iii)\nchanges in coverage contribute to the detection of fuzzing bugs. This study\nprovides empirical insights into how continuous fuzzing contributes to fuzzing\nbug detection, offering practical implications for future strategies and tool\ndevelopment in continuous fuzzing.", "AI": {"tldr": "Continuous fuzzing in software development quickly detects many existing vulnerabilities early, improves code coverage over time, and new code coverage increases help uncover additional bugs, making continuous fuzzing a valuable strategy for improving software security.", "motivation": "Despite the widespread adoption of continuous fuzzing in thousands of software projects, it remains unclear how much continuous fuzzing improves vulnerability detection and what role it plays in discovering bugs during software development.", "method": "The authors conducted an empirical study using data collected from OSS-Fuzz, including issue reports, coverage reports, and fuzzing logs from around 1.12 million fuzzing sessions across 878 projects.", "result": "The study found: (i) many fuzzing bugs were present before continuous fuzzing started, leading to high bug detection rates in the early phase; (ii) code coverage consistently increases as continuous fuzzing is performed; and (iii) increases in code coverage help find new fuzzing bugs.", "conclusion": "Continuous fuzzing is effective for vulnerability detection, particularly due to increased code coverage and the discovery of pre-existing bugs early in the process. These findings inform future strategies and tool development for software security."}}
{"id": "2510.16502", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16502", "abs": "https://arxiv.org/abs/2510.16502", "authors": ["Sebasti\u00e1n Pizard", "Ramiro Moreira", "Federico Galiano", "Ignacio Sastre", "Lorena Etcheverry"], "title": "On the Use of Large Language Models for Qualitative Synthesis", "comment": null, "summary": "Large language models (LLMs) show promise for supporting systematic reviews\n(SR), even complex tasks such as qualitative synthesis (QS). However, applying\nthem to a stage that is unevenly reported and variably conducted carries\nimportant risks: misuse can amplify existing weaknesses and erode confidence in\nthe SR findings. To examine the challenges of using LLMs for QS, we conducted a\ncollaborative autoethnography involving two trials. We evaluated each trial for\nmethodological rigor and practical usefulness, and interpreted the results\nthrough a technical lens informed by how LLMs are built and their current\nlimitations.", "AI": {"tldr": "Using LLMs in systematic reviews\u2014especially qualitative synthesis\u2014is promising but risky due to variability and reporting inconsistencies. Collaborative trials highlight the need for caution and rigorous evaluation to avoid amplifying weaknesses and undermining review reliability.", "motivation": "Large language models have the potential to assist with systematic reviews, especially in the complex stage of qualitative synthesis, but application risk remains due to variability and reporting inconsistencies.", "method": "A collaborative autoethnography approach was used, involving two trials to assess LLMs' effectiveness in qualitative synthesis. Each trial was evaluated for rigor and practical utility, and results were interpreted technically based on LLM design and current limitations.", "result": "The study identified key challenges associated with using LLMs in qualitative synthesis, focusing on the risks of misuse, impact on methodological rigor, and concerns about eroding confidence in review findings.", "conclusion": "LLMs offer significant opportunities but also present critical risks when applied to unevenly conducted or reported SR stages. Careful evaluation and understanding of these risks are necessary to prevent undermining the credibility of systematic reviews."}}
{"id": "2510.16579", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16579", "abs": "https://arxiv.org/abs/2510.16579", "authors": ["Wendk\u00fbuni C. Ou\u00e9draogo", "Yinghua Li", "Xueqi Dang", "Pawel Borsukiewicz", "Xin Zhou", "Anil Koyuncu", "Jacques Klein", "David Lo", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Human-Aligned Code Readability Assessment with Large Language Models", "comment": null, "summary": "Code readability is crucial for software comprehension and maintenance, yet\ndifficult to assess at scale. Traditional static metrics often fail to capture\nthe subjective, context-sensitive nature of human judgments. Large Language\nModels (LLMs) offer a scalable alternative, but their behavior as readability\nevaluators remains underexplored. We introduce CoReEval, the first large-scale\nbenchmark for evaluating LLM-based code readability assessment, comprising over\n1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.\nThe benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types\n(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),\n9 decoding settings, and developer-guided prompts tailored to junior and senior\npersonas. We compare LLM outputs against human annotations and a validated\nstatic model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and\njustification quality (sentiment, aspect coverage, semantic clustering). Our\nfindings show that developer-guided prompting grounded in human-defined\nreadability dimensions improves alignment in structured contexts, enhances\nexplanation quality, and enables lightweight personalization through persona\nframing. However, increased score variability highlights trade-offs between\nalignment, stability, and interpretability. CoReEval provides a robust\nfoundation for prompt engineering, model alignment studies, and human in the\nloop evaluation, with applications in education, onboarding, and CI/CD\npipelines where LLMs can serve as explainable, adaptable reviewers.", "AI": {"tldr": "The paper presents CoReEval, a large benchmark evaluating LLMs for code readability. It finds developer-guided prompts improve result alignment and explanations but increase score variability. CoReEval supports model alignment, prompt engineering, and personalized code review by LLMs.", "motivation": "Code readability is key for software comprehension and maintenance, but traditional metrics are limited and subjective human assessments are hard to scale. LLMs offer a scalable alternative, but have not been systematically evaluated for this task.", "method": "The authors introduce CoReEval, a large benchmark with over 1.4 million evaluations across 10 LLMs. They assess code readability in Java, Python, and CUDA using multiple code types, prompts, decoding settings, and developer-guided persona framing. Outputs are compared to human annotations and a static model using alignment and qualitative justification analyses.", "result": "Developer-guided prompts improve alignment and explanation quality in structured contexts, allowing personalization for different user personas. However, results show greater score variability, indicating trade-offs between alignment, stability, and interpretability.", "conclusion": "CoReEval establishes a scalable framework for evaluating and aligning LLM-based code readability assessment. The findings support future studies in prompt engineering and human-in-the-loop loops, enhancing education, onboarding, and CI/CD uses."}}
{"id": "2510.16665", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16665", "abs": "https://arxiv.org/abs/2510.16665", "authors": ["Mohamed Sami Rakha", "Andriy Miranskyy", "Daniel Alencar da Costa"], "title": "Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios", "comment": "Accepted to IEEE Transactions on Software Engineering", "summary": "Software defect prediction (SDP) is crucial for delivering high-quality\nsoftware products. Recent research has indicated that prediction performance\nimprovements in SDP are achievable by applying hyperparameter tuning to a\nparticular SDP scenario. However, the positive impact resulting from the\nhyperparameter tuning step may differ based on the targeted SDP scenario.\nComparing the impact of hyperparameter tuning across SDP scenarios is necessary\nto provide comprehensive insights and enhance the robustness, generalizability,\nand, eventually, the practicality of SDP modeling for quality assurance.\n  Therefore, in this study, we contrast the impact of hyperparameter tuning\nacross two pivotal and consecutive SDP scenarios: (1) Inner Version Defect\nPrediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main\ndistinctions between the two scenarios lie in the scope of defect prediction\nand the selected evaluation setups. This study's experiments use common\nevaluation setups, 28 machine learning (ML) algorithms, 53 post-release\nsoftware datasets, two tuning algorithms, and five optimization metrics. We\napply statistical analytics to compare the SDP performance impact differences\nby investigating the overall impact, the single ML algorithm impact, and\nvariations across different software dataset sizes.\n  The results indicate that the SDP gains within the IVDP scenario are\nsignificantly larger than those within the CVDP scenario. The results reveal\nthat asserting performance gains for up to 24 out of 28 ML algorithms may not\nhold across multiple SDP scenarios. Furthermore, we found that small software\ndatasets are more susceptible to larger differences in performance impacts.\nOverall, the study findings recommend software engineering researchers and\npractitioners to consider the effect of the selected SDP scenario when\nexpecting performance gains from hyperparameter tuning.", "AI": {"tldr": "Performance improvements from hyperparameter tuning in software defect prediction vary significantly between different scenarios. IVDP benefits more than CVDP, and in small datasets the difference is greater, meaning scenario selection affects practical gains from tuning.", "motivation": "There is a need to understand the generalized impact of hyperparameter tuning on software defect prediction (SDP) models across different scenarios to enhance model robustness, generalizability, and practical applicability. Existing work lacks comparative insights on how scenario choice affects performance gains from tuning.", "method": "The study contrasts hyperparameter tuning effects across two major SDP scenarios: Inner Version Defect Prediction (IVDP) and Cross Version Defect Prediction (CVDP). It uses 28 machine learning algorithms, 53 datasets, two tuning algorithms, five metrics, and statistical methods to analyze overall, per-algorithm, and dataset size impacts.", "result": "Findings show that hyperparameter tuning yields significantly greater performance improvements in IVDP than in CVDP. Performance gains that hold for most ML algorithms in IVDP do not necessarily translate to CVDP; small datasets experience larger performance impact differences.", "conclusion": "The effect of hyperparameter tuning on SDP is highly dependent on the prediction scenario. Researchers and practitioners should account for scenario choice when anticipating the benefits of tuning."}}
{"id": "2510.16779", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.16779", "abs": "https://arxiv.org/abs/2510.16779", "authors": ["Xiaoyu Guo", "Minggu Wang", "Jianjun Zhao"], "title": "QuanBench: Benchmarking Quantum Code Generation with Large Language Models", "comment": "This paper was accepted by ASE2025", "summary": "Large language models (LLMs) have demonstrated good performance in general\ncode generation; however, their capabilities in quantum code generation remain\ninsufficiently studied. This paper presents QuanBench, a benchmark for\nevaluating LLMs on quantum code generation. QuanBench includes 44 programming\ntasks that cover quantum algorithms, state preparation, gate decomposition, and\nquantum machine learning. Each task has an executable canonical solution and is\nevaluated by functional correctness (Pass@K) and quantum semantic equivalence\n(Process Fidelity). We evaluate several recent LLMs, including general-purpose\nand code-specialized models. The results show that current LLMs have limited\ncapability in generating the correct quantum code, with overall accuracy below\n40% and frequent semantic errors. We also analyze common failure cases, such as\noutdated API usage, circuit construction errors, and incorrect algorithm logic.\nQuanBench provides a basis for future work on improving quantum code generation\nwith LLMs.", "AI": {"tldr": "QuanBench, a benchmark for quantum code generation, demonstrates that recent large language models struggle with accuracy (below 40%) and semantics. The work highlights common errors and serves as a resource for future research to enhance LLM performance in quantum programming.", "motivation": "Large language models have excelled in general code generation, but their abilities in quantum code generation are not well-understood. There is a need for systematic benchmarks to evaluate and improve their performance in this specialized field.", "method": "The authors introduce QuanBench, a benchmark suite comprising 44 quantum programming tasks that span algorithms, state preparation, gate decomposition, and quantum machine learning. Each task includes an executable canonical solution, and models are evaluated using Pass@K for correctness and Process Fidelity for quantum semantic equivalence. Several recent LLMs, both general-purpose and code-specialized, are assessed.", "result": "Current large language models show limited accuracy in quantum code generation, with overall correctness below 40%. Frequent mistakes include outdated API usage, incorrect circuit constructions, and flawed algorithm logic. Semantic errors are common across evaluated models.", "conclusion": "QuanBench reveals significant deficiencies in LLM-based quantum code generation, providing insights into error patterns and laying the foundation for targeted improvements in future models."}}
{"id": "2510.16786", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16786", "abs": "https://arxiv.org/abs/2510.16786", "authors": ["Pengfei Gao", "Chao Peng"], "title": "More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents", "comment": null, "summary": "LLM-powered coding agents, which operate in iterative loops (turns) to solve\nsoftware engineering tasks, are becoming increasingly powerful. However, their\npractical deployment is hindered by significant and unpredictable costs. This\nchallenge arises from a combination of factors: quadratically growing token\ncounts with each turn, the high price of models, the large number of turns\nrequired for real-world tasks, and the tendency of agents to take inefficient\nor unnecessary actions. While existing research focuses on optimizing\nindividual turns, the strategic control of the total number of turns remains an\nunderexplored area for managing agent performance and cost. To address this\ngap, we conduct a comprehensive empirical study on SWE-bench using three\nstate-of-the-art models and evaluate the impact of three distinct turn-control\nstrategies: an unrestricted baseline, a fixed-turn limit with reminders, and a\nnovel dynamic-turn strategy that grants extensions on-demand. Our findings\nfirst reveal a fundamental trade-off in the unrestricted setting, where no\nsingle model excels across performance, cost, and turn efficiency. We then show\nthat a fixed-turn limit, specifically at the 75th percentile of the baseline,\nserves as a \"sweet spot\", substantially reducing costs (by 24%-68%) with\nminimal impact on solve rates. Most significantly, the dynamic-turn strategy\nconsistently outperforms fixed-limit approaches, achieving comparable or better\nsolve rates while further reducing costs by an additional 12%-24% by\nintelligently allocating resources only to tasks that need them. This work\nprovides the first systematic analysis of turn-control strategies, offering\nsimple yet effective guidelines for developers to balance cost and efficacy. We\ndemonstrate that dynamic resource allocation is a superior, easy-to-implement\napproach for deploying powerful yet economically viable coding agents.", "AI": {"tldr": "This study systematically analyzes how to control turns in LLM coding agents for cost-effective deployment. Fixed and dynamic strategies both reduce costs substantially, with dynamic turn-control offering even better results without hurting task performance, making it the recommended approach for practical coding agent deployment.", "motivation": "LLM-powered coding agents are powerful but expensive to deploy due to factors like many iterative turns, growing token counts, costly models, and agent inefficiencies. Existing research has not sufficiently addressed controlling the total number of turns as a way to manage performance and cost.", "method": "A comprehensive empirical study is conducted using the SWE-bench benchmark and three state-of-the-art LLM models. The study evaluates the effect of three turn-control strategies: 1) unrestricted turn baseline, 2) fixed-turn limits with reminders, and 3) a novel dynamic-turn strategy with on-demand extensions.", "result": "Results show that unrestricted turns create cost-performance trade-offs and no model dominates across all metrics. Imposing a fixed-turn limit at the 75th percentile greatly reduces costs (by 24%-68%) with little loss in task completion rates. The dynamic-turn strategy further cuts costs (by 12%-24%) and maintains or improves solve rates by granting extra turns only when necessary.", "conclusion": "Strategic control over the number of agent turns, especially with dynamic resource allocation, enables the deployment of coding agents that are both effective and cost-efficient. Dynamic-turn strategies are simple, powerful, and easy to implement for developers."}}
{"id": "2510.16823", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.16823", "abs": "https://arxiv.org/abs/2510.16823", "authors": ["Yue Liu", "Zhenchang Xing", "Shidong Pan", "Chakkrit Tantithamthavorn"], "title": "When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation", "comment": null, "summary": "In recent years, the AI wave has grown rapidly in software development. Even\nnovice developers can now design and generate complex framework-constrained\nsoftware systems based on their high-level requirements with the help of Large\nLanguage Models (LLMs). However, when LLMs gradually \"take the wheel\" of\nsoftware development, developers may only check whether the program works. They\noften miss security problems hidden in how the generated programs are\nimplemented.\n  In this work, we investigate the security properties of framework-constrained\nprograms generated by state-of-the-art LLMs. We focus specifically on Chrome\nextensions due to their complex security model involving multiple privilege\nboundaries and isolated components. To achieve this, we built ChromeSecBench, a\ndataset with 140 prompts based on known vulnerable extensions. We used these\nprompts to instruct nine state-of-the-art LLMs to generate complete Chrome\nextensions, and then analyzed them for vulnerabilities across three dimensions:\nscenario types, model differences, and vulnerability categories. Our results\nshow that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),\nparticularly in Authentication & Identity and Cookie Management scenarios (up\nto 83% and 78% respectively). Most vulnerabilities exposed sensitive browser\ndata like cookies, history, or bookmarks to untrusted code. Interestingly, we\nfound that advanced reasoning models performed worse, generating more\nvulnerabilities than simpler models. These findings highlight a critical gap\nbetween LLMs' coding skills and their ability to write secure\nframework-constrained programs.", "AI": {"tldr": "The study shows that leading LLMs often generate insecure Chrome extensions, with high vulnerability rates especially in sensitive scenarios. The supposed sophistication of newer LLMs does not guarantee better security, highlighting a pressing gap between code generation ability and secure coding in LLMs.", "motivation": "As LLMs increasingly automate software development, there is concern that developers may overlook hidden security vulnerabilities in generated code, especially when frameworks impose specific constraints. This study aims to address the gap in understanding LLMs' security performance in this setting.", "method": "Developed ChromeSecBench (140 prompts from known vulnerable extensions); used these to instruct nine LLMs to generate Chrome extensions; analyzed the resulting extensions across scenarios, models, and vulnerability categories for security flaws.", "result": "LLMs generated vulnerable Chrome extensions at rates ranging from 18% to 50%; vulnerabilities were particularly severe in authentication/identity (up to 83%) and cookie management (up to 78%) scenarios. Most flaws exposed sensitive user data to untrusted code. Advanced models were surprisingly more prone to generating vulnerabilities than simpler models.", "conclusion": "LLMs generate framework-constrained software (Chrome extensions) with high rates of security vulnerabilities, particularly in key scenarios such as authentication and cookie management, and advanced models may perform worse than simpler ones in this context."}}
{"id": "2510.17056", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17056", "abs": "https://arxiv.org/abs/2510.17056", "authors": ["Luis F. G. Campos", "Leonardo C. Marques", "Walter T. Nakamura"], "title": "Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection", "comment": "Accepted and to be published in SBQS25 - Brazilian Symposium on\n  Software Quality 2025", "summary": "Usability inspection is a well-established technique for identifying\ninteraction issues in software interfaces, thereby contributing to improved\nproduct quality. However, it is a costly process that requires time and\nspecialized knowledge from inspectors. With advances in Artificial Intelligence\n(AI), new opportunities have emerged to support this task, particularly through\ngenerative models capable of interpreting interfaces and performing inspections\nmore efficiently. This study examines the performance of generative AIs in\nidentifying usability problems, comparing them to those of experienced human\ninspectors. A software prototype was evaluated by four specialists and two AI\nmodels (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,\nand F1-score. While inspectors achieved the highest levels of precision and\noverall coverage, the AIs demonstrated high individual performance and\ndiscovered many novel defects, but with a higher rate of false positives and\nredundant reports. The combination of AIs and human inspectors produced the\nbest results, revealing their complementarity. These findings suggest that AI,\nin its current stage, cannot replace human inspectors but can serve as a\nvaluable augmentation tool to improve efficiency and expand defect coverage.\nThe results provide evidence based on quantitative analysis to inform the\ndiscussion on the role of AI in usability inspections, pointing to viable paths\nfor its complementary use in software quality assessment contexts.", "AI": {"tldr": "Generative AI models can't fully replace human experts in usability inspections but can augment the process, discovering new issues and improving coverage. Teaming humans and AI gives the most effective results, pointing to a future of complementary use in software quality assurance.", "motivation": "Usability inspection improves software quality by identifying interface issues, but is resource-intensive and requires specialized human expertise. Recent advances in artificial intelligence present opportunities to ease this process through generative models that may automate or enhance inspections.", "method": "The study compares human and AI performance in usability inspections. A software prototype is evaluated by four experts and two generative AI models (GPT-4o and Gemini 2.5 Flash), using precision, recall, and F1-score as metrics. The inspectors' and AIs' reports are analyzed for effectiveness and overlap.", "result": "Human inspectors showed highest precision and coverage, while AIs uncovered many new defects but produced more false positives and redundant reports. Combining human and AI inspections yielded the most comprehensive results, showing that they complement each other.", "conclusion": "AI cannot currently replace human inspectors in usability inspections, but serves as an effective augmentation tool to improve efficiency and broaden defect coverage. Quantitative evidence supports complementary use of AI and humans in software quality assessment."}}
{"id": "2510.17110", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17110", "abs": "https://arxiv.org/abs/2510.17110", "authors": ["Xiaoyu Guo", "Shinobu Saito", "Jianjun Zhao"], "title": "M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs", "comment": "This paper was accepted by ASE2025", "summary": "With the growing interest in quantum computing, the emergence of quantum\nsupremacy has marked a pivotal milestone in the field. As a result, numerous\nquantum programming languages (QPLs) have been introduced to support the\ndevelopment of quantum algorithms. However, the application of Model-Driven\nDevelopment (MDD) in quantum system engineering remains largely underexplored.\nThis paper presents an MDD-based approach to support the structured design and\nimplementation of quantum systems. Our framework enables the automatic\ngeneration of quantum code for multiple QPLs, thereby enhancing development\nefficiency and consistency across heterogeneous quantum platforms. The\neffectiveness and practicality of our approach have been demonstrated through\nmultiple case studies.", "AI": {"tldr": "The paper introduces an MDD approach to automatically generate quantum code for multiple languages, improving efficiency and reliability in quantum system development, as shown by case studies.", "motivation": "Quantum system engineering lacks structured design approaches, especially using Model-Driven Development (MDD), while quantum programming languages are proliferating.", "method": "This paper proposes an MDD-based framework that allows structured design and automatic code generation for various quantum programming languages.", "result": "The framework successfully generated quantum code for multiple QPLs and was validated with several case studies, showing increased efficiency and consistency.", "conclusion": "Applying MDD to quantum systems enables more efficient and reliable development across different platforms, filling a gap in quantum software engineering."}}
{"id": "2510.17130", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17130", "abs": "https://arxiv.org/abs/2510.17130", "authors": ["Shuzheng Gao", "Chaozheng Wang", "Cuiyun Gao", "Michael R. Lyu"], "title": "SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning", "comment": "The paper was completed in Feb. 2025, submitted to ICSE 2026 in Mar.\n  2025, received a major revision in Jun. 2025, and was finally accepted in\n  Oct. 2025", "summary": "Code generation, the task of creating executable programs from natural\nlanguage requirements, has recently seen tremendous advances through\nChain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to\ndevelop high-level reasoning plans before writing code. Recent research has\nproposed various methods to enhance models' CoT reasoning for code generation\nsuch as prompt engineering and supervised fine-tuning. However, existing\napproaches still face three critical limitations: (1) limited exploration of\ndiverse reasoning paths, which constrains generalization across various\nprogramming scenarios, (2) lack of quality assessment for intermediate\nreasoning steps, which hampers the reliability of the generated plans and code,\nand (3) the potential negative impact of \"overthinking\", potentially leading to\nunnecessarily complex and incorrect solutions. To address these limitations, we\nframe CoT code generation as a decision making problem and present SEER, a\nSElf-Exploring deep Reasoning framework that enables accurate and adaptive\nreasoning for code generation. SEER introduces three key components: (1)\nDiverse reasoning path exploration, which aims at exploring diverse reasoning\npaths and annotating intermediate steps without relying on manual experts or\nclosed-source proprietary models; (2) Reasoning quality-aware model training,\nwhich trains a policy model for generating candidate reasoning steps and a\nvalue model for assessing their quality; and (3) Adaptive CoT reasoning, which\ndynamically switches between direct generation and step-by-step reasoning for\ndifferent problems.", "AI": {"tldr": "SEER is a new framework for code generation with LLMs that improves reasoning diversity, quality assessment, and adaptability, overcoming the main drawbacks of previous Chain-of-Thought reasoning approaches.", "motivation": "Despite improvements in code generation using Chain-of-Thought (CoT) reasoning with Large Language Models (LLMs), existing approaches still face notable challenges such as limited exploration of diverse reasoning paths, poor assessment of intermediate reasoning quality, and an increased risk of overcomplicating solutions. These limitations restrict generalization and reliability across programming tasks.", "method": "The paper introduces SEER, a Self-Exploring deep Reasoning framework that frames CoT code generation as a decision-making problem. SEER includes three main components: diverse reasoning path exploration without expert or proprietary intervention, reasoning quality-aware model training involving policy and value models, and adaptive CoT reasoning that flexibly alternates between direct generation and step-by-step approaches.", "result": "SEER enables more accurate and adaptive code generation reasoning, by systematically exploring diverse reasoning paths, quantitatively evaluating the quality of intermediate steps, and adapting the reasoning strategy to each problem. This results in improved code generation performance and reliability compared to previous CoT approaches.", "conclusion": "SEER effectively addresses key limitations in CoT code generation by providing diverse reasoning exploration, higher reliability through quality assessments, and adaptive reasoning strategies, thereby enhancing the overall effectiveness and generalization of LLM-based code generation."}}
{"id": "2510.17142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17142", "abs": "https://arxiv.org/abs/2510.17142", "authors": ["Xiaoxue Ren", "Jun Wan", "Yun Peng", "Zhongxin Liu", "Ming Liang", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant capability in code\ngeneration, but their potential in code efficiency optimization remains\nunderexplored. Previous LLM-based code efficiency optimization approaches\nexclusively focus on function-level optimization and overlook interaction\nbetween functions, failing to generalize to real-world development scenarios.\nCode editing techniques show great potential for conducting project-level\noptimization, yet they face challenges associated with invalid edits and\nsuboptimal internal functions. To address these gaps, we propose Peace, a novel\nhybrid framework for Project-level code Efficiency optimization through\nAutomatic Code Editing, which also ensures the overall correctness and\nintegrity of the project. Peace integrates three key phases: dependency-aware\noptimizing function sequence construction, valid associated edits\nidentification, and efficiency optimization editing iteration. To rigorously\nevaluate the effectiveness of Peace, we construct PeacExec, the first benchmark\ncomprising 146 real-world optimization tasks from 47 high-impact GitHub Python\nprojects, along with highly qualified test cases and executable environments.\nExtensive experiments demonstrate Peace's superiority over the state-of-the-art\nbaselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and\n0.840 speedup in execution efficiency. Notably, our Peace outperforms all\nbaselines by significant margins, particularly in complex optimization tasks\nwith multiple functions. Moreover, extensive experiments are also conducted to\nvalidate the contributions of each component in Peace, as well as the rationale\nand effectiveness of our hybrid framework design.", "AI": {"tldr": "Peace is a new framework for optimizing Python projects' code efficiency using automated code editing. Unlike previous methods that only work at the function level, Peace optimizes entire projects while keeping them correct and intact. Benchmark tests show Peace works significantly better than existing approaches, especially on tougher, multi-function optimizations.", "motivation": "Previous LLM-based approaches to code efficiency optimization are limited to function-level optimization, ignoring interactions between functions, and are not robust in real-world project scenarios. Additionally, project-level code editing faces issues like invalid edits and suboptimal internal functions. There is a need for a framework that can perform project-level optimization while maintaining code correctness and integrity.", "method": "The paper proposes Peace, a hybrid framework for project-level code efficiency optimization via automatic code editing. Peace consists of three phases: (1) constructing dependency-aware optimizing function sequences, (2) identifying valid associated code edits, and (3) iteratively performing efficiency optimization editing. The effectiveness of Peace is evaluated using PeacExec, a new benchmark with 146 real-world tasks from 47 popular GitHub Python projects, featuring thorough test cases and executable setups.", "result": "Peace achieves a 69.2% correctness rate (pass@1), a 46.9% increase in optimization rate, and a 0.840 speedup in execution efficiency, outperforming state-of-the-art baselines, especially in more complex, multi-function optimization tasks. The experiments also verify the contributions and rationale of each component within Peace.", "conclusion": "Peace is a robust solution for project-level code optimization: it not only surpasses previous methods in accuracy and performance but also maintains project correctness and integrity. The framework's effectiveness and design have been thoroughly validated through extensive empirical evaluation."}}
{"id": "2510.17163", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17163", "abs": "https://arxiv.org/abs/2510.17163", "authors": ["Shuzheng Gao", "Eric John Li", "Man Ho Lam", "Jingyu Xiao", "Yuxuan Wan", "Chaozheng Wang", "Ng Man Tik", "Michael R. Lyu"], "title": "TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework", "comment": null, "summary": "Large foundation models are fundamentally transforming the software\nengineering landscape, demonstrating exceptional capabilities across diverse\ntasks such as code generation, debugging, and testing. Despite this rapid\nprogress, a significant gap remains in how to comprehensively evaluate these\nmodels' trustworthiness in real-world software engineering scenarios. Existing\nbenchmarks suffer from limited task scope and fail to incorporate critical\nevaluation aspects such as the robustness and reliability of models. To bridge\nthis gap, we present an evaluation framework called TREAT (Code LLMs\nTrustworthiness / Reliability Evaluation And Testing) that provides a holistic\nassessment of model performance in code intelligence tasks. Our evaluation\nframework addresses key limitations in existing approaches with four main\nimprovements: (1) Multi-Task Holistic Evaluation that spans diverse software\nengineering activities rather than limited coding tasks; (2) Multi-Language and\nMulti-Modality Assessment that extends beyond traditional single-language,\ntext-only benchmarks to include multi-modality coding tasks; (3) Robustness\nAssessment that evaluates model reliability under semantically-preserving code\ntransformations; and (4) Rigorous Evaluation Methodology that enhances the\ntrustworthiness of evaluation results through diverse evaluation prompts and\nadaptive solution extraction. Based on this evaluation framework, we assess 26\nstate-of-the-art models and uncover both their strengths and limitations,\nyielding several key insights:(1) Current models show substantial performance\nvariation across programming tasks; (2) Multi-modal language models demonstrate\nspecific performance limitations in UI code generation and edit;", "AI": {"tldr": "The paper presents TREAT, a comprehensive framework for evaluating the trustworthiness and reliability of code LLMs across various languages, modalities, and tasks. Assessing 26 models, it reveals strengths, weaknesses, and specific challenges in multi-modal scenarios, offering valuable insights for model development and use.", "motivation": "The motivation of this paper is the lack of comprehensive evaluation methods for large code foundation models. Current benchmarks are limited in scope and do not sufficiently address robustness, reliability, or real-world coding scenarios, which are essential for trusting these models in practical software engineering tasks.", "method": "The paper introduces a new evaluation framework named TREAT (Code LLMs Trustworthiness / Reliability Evaluation And Testing). The method includes multi-task, multi-language, and multi-modality evaluations, robustness checks under code transformations, and a rigorous, adaptive methodology for assessing trustworthiness and reliability.", "result": "Using the TREAT framework, the authors assessed 26 state-of-the-art models and found notable performance variations across different programming tasks. They also identified limitations in multi-modal models, particularly in UI code generation and editing tasks.", "conclusion": "The TREAT framework addresses the deficiencies of existing benchmarks and enables a holistic, multi-dimensional evaluation of code-focused large language models. This comprehensive approach uncovers important strengths and weaknesses, guiding both model development and deployment in software engineering."}}
{"id": "2510.17164", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17164", "abs": "https://arxiv.org/abs/2510.17164", "authors": ["Maria Deolinda Santana", "Cleyton Magalhaes", "Ronnie de Souza Santos"], "title": "Software Testing with Large Language Models: An Interview Study with Practitioners", "comment": null, "summary": "\\textit{Background:} The use of large language models in software testing is\ngrowing fast as they support numerous tasks, from test case generation to\nautomation, and documentation. However, their adoption often relies on informal\nexperimentation rather than structured guidance. \\textit{Aims:} This study\ninvestigates how software testing professionals use LLMs in practice to propose\na preliminary, practitioner-informed guideline to support their integration\ninto testing workflows. \\textit{Method:} We conducted a qualitative study with\n15 software testers from diverse roles and domains. Data were collected through\nsemi-structured interviews and analyzed using grounded theory-based processes\nfocused on thematic analysis. \\textit{Results:} Testers described an iterative\nand reflective process that included defining testing objectives, applying\nprompt engineering strategies, refining prompts, evaluating outputs, and\nlearning over time. They emphasized the need for human oversight and careful\nvalidation, especially due to known limitations of LLMs such as hallucinations\nand inconsistent reasoning. \\textit{Conclusions:} LLM adoption in software\ntesting is growing, but remains shaped by evolving practices and caution around\nrisks. This study offers a starting point for structuring LLM use in testing\ncontexts and invites future research to refine these practices across teams,\ntools, and tasks.", "AI": {"tldr": "Software testers are increasingly using LLMs through iterative and cautious practices, emphasizing the need for human validation. The study offers early guidelines for LLM integration and highlights the importance of structured approaches and future research.", "motivation": "The growing use of LLMs in software testing lacks structured guidance, with practitioners relying on informal experimentation. The study aims to understand real-world usage and develop preliminary guidelines for integration.", "method": "Qualitative study with 15 software testers from diverse roles and domains; data collected via semi-structured interviews and analyzed using grounded theory and thematic analysis.", "result": "Testers employ an iterative process involving prompt engineering, refinement, output evaluation, and learning. Human oversight and validation are necessary due to LLM limitations such as hallucinations and inconsistent reasoning.", "conclusion": "LLM adoption in software testing is increasing but depends on cautious, evolving practices, with human oversight being crucial due to model limitations. The study provides an initial, practitioner-informed guideline for integrating LLMs into testing workflows and calls for further research to refine these practices."}}
{"id": "2510.17184", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17184", "abs": "https://arxiv.org/abs/2510.17184", "authors": ["Nicolas Robert", "Fabien Gandon", "Maxime Lefran\u00e7ois"], "title": "OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development", "comment": null, "summary": "Agile and collaborative approaches to ontologies design are crucial because\nthey contribute to making them userdriven, up-to-date, and able to evolve\nalongside the systems they support, hence proper continuous validation tooling\nis required to ensure ontologies match developers' requirements all along their\ndevelopment. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV\nWorkflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C\nStandards to assist the development of modular ontologies through GitHub\nComposite Actions, pre-commit hooks, or a command line interface. OLIVAW was\ntested on several ontology projects to ensure its usefulness, genericity and\nreusability. A template repository is available for a quick start. OLIVAW is", "AI": {"tldr": "OLIVAW is a tool that integrates ontology development with GitHub workflows, supporting continuous validation and agile ontology evolution. Its effectiveness and generality were confirmed across multiple projects.", "motivation": "Continuous validation and user-driven evolution of ontologies are necessary to keep them aligned with developers' changing requirements during software development.", "method": "OLIVAW, a tool designed around the ACIMOV methodology, uses W3C Standards and integrates with GitHub via Composite Actions, pre-commit hooks, or CLI, to support modular ontology development.", "result": "OLIVAW was tested on several ontology projects and demonstrated usefulness, genericity, and reusability. Additionally, a template repository was provided for quick setup.", "conclusion": "OLIVAW enables agile, collaborative, and validated ontology development workflows, ensuring ontologies stay relevant and effective throughout project lifecycles."}}
{"id": "2510.17376", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17376", "abs": "https://arxiv.org/abs/2510.17376", "authors": ["Yongmin Li", "Jia Li", "Ge Li", "Zhi Jin"], "title": "AdapTrack: Constrained Decoding without Distorting LLM's Output Intent", "comment": "to be published in ICSE 2026", "summary": "Language model-based code generation and completion tools have been widely\nadopted, but they may sometimes produce code that does not meet necessary\nconstraints, such as syntactic correctness or API existence. Constrained\ndecoding techniques are developed to help the model generate code adhering to\nthe constraints by greedily eliminating generation options that violate\nconstraints at each step of the generation process. However, there is a severe\nlimitation of constrained decoding, that it distorts the model's output intent,\nforcing it to produce code that may satisfy the constraint but does not match\nthe development intent and is therefore incorrect. In response to this\nchallenge, we propose AdapTrack. By incorporating backtracking into the\ngeneration process, AdapTrack avoids distorting the output intent of the model,\nthereby producing results that are not only constraint-compliant but also more\nsemantically aligned with model's output intent. On our synthetic API\ncompletion dataset, AdapTrack can achieve up to 360.87% improvement compared to\nconstrained decoding; on the real-world API completion dataset we collect that\nexhibits similar issues, AdapTrack can achieve up to 38.93% improvement over\nconstrained decoding; in general code genration benchmarks, compared to\nconstrained decoding, AdapTrack can achieve up to 7.84% improvement on\nHumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by\nbetter adhering to the model's output intent, AdapTrack can achieve significant\nimprovements. We provide a theoretical proof that the distribution produced by\nAdapTrack aligns with the model's distribution given the generated tokens,\nthereby ensuring that the model's output intent is not distorted. Experiments\non DSL problems show that, compared to existing methods, our approach can\nprovide generation results that are more consistent with the language model's\ndistribution.", "AI": {"tldr": "Current constrained decoding for language model-based code generation can cause outputs to meet constraints but lose intended meaning. AdapTrack adds backtracking, leading to code that both respects constraints and stays true to the model's intent, achieving significant improvements on several benchmarks and aligning better with the model\u2019s natural distribution.", "motivation": "Modern code generation tools using language models often fail to meet required constraints like syntax correctness or valid API usage. Traditional constrained decoding fixes this by eliminating rule-breaking options during generation, but this often distorts the model's intended output, leading to incorrect code. The paper aims to solve this dilemma: producing correct code without sacrificing the original intent generated by the model.", "method": "The paper introduces AdapTrack, a technique that incorporates backtracking into code generation. Instead of purely greedy constraint application, AdapTrack allows the generation process to revisit previous steps, helping maintain the original semantics and output intent of the model, while still meeting constraints. The authors present both theoretical proof and empirical evaluation of its alignment with the true model distribution.", "result": "AdapTrack significantly outperforms traditional constrained decoding on multiple benchmarks. On synthetic API completion, AdapTrack achieves up to 360.87% improvement; on real-world datasets, up to 38.93% improvement; and on standard code generation benchmarks like HumanEval and MBPP, up to 7.84% and 6.42% improvement, respectively. Experimental results further show AdapTrack produces model-aligned outputs, validated by theoretical analysis.", "conclusion": "AdapTrack successfully bridges the gap between constraint satisfaction and preservation of the model's output intent. It offers marked performance improvements over existing constrained decoding techniques, both in accuracy and intent alignment, and its theoretical grounding shows promise for broader application in language model-based code generation."}}
{"id": "2510.17430", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.17430", "abs": "https://arxiv.org/abs/2510.17430", "authors": ["Kuniaki Kudo", "Sherine Devi"], "title": "Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff", "comment": null, "summary": "We have developed a Scalable CI/CD Pipeline to address internal challenges\nrelated to Japan 2025 cliff problem, a critical issue where the mass end of\nservice life of legacy core IT systems threatens to significantly increase the\nmaintenance cost and black box nature of these system also leads to difficult\nupdate moreover replace, which leads to lack of progress in Digital\nTransformation (DX). If not addressed, Japan could potentially lose up to 12\ntrillion yen per year after 2025, which is 3 times more than the cost in\nprevious years. Asahi also faced the same internal challenges regarding legacy\nsystem, where manual maintenance workflows and limited QA environment have left\ncritical systems outdated and difficult to update. Middleware and OS version\nhave remained unchanged for years, leading to now its nearing end of service\nlife which require huge maintenance cost and effort to continue its operation.\nTo address this problem, we have developed and implemented a Scalable CI/CD\nPipeline where isolated development environments can be created and deleted\ndynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate\nGitHub for source code control and branching, Jenkins for pipeline automation,\nAmazon Web Services for scalable environment, and Docker for environment\ncontainerization. This paper presents the design and architecture of the\nScalable CI/CD Pipeline, with the implementation along with some use cases.\nThrough Scalable CI/CD, developers can freely and safely test maintenance\nprocedures and do experiments with new technology in their own environment,\nreducing maintenance cost and drive Digital Transformation (DX).\n  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.", "AI": {"tldr": "The paper proposes a scalable CI/CD pipeline using modern tools (GitHub, Jenkins, AWS, Docker) to tackle Japan's legacy IT system challenges, aiming to minimize maintenance costs and support digital transformation, with demonstrated benefits for developers and organizations.", "motivation": "The motivation is to solve the '2025 Japan Cliff' issue, where legacy IT systems in Japan face mass end-of-service, resulting in rising maintenance costs and difficulties in updating or replacing outdated systems. This threatens both business continuity and digital transformation efforts.", "method": "The method involves developing and implementing a scalable CI/CD pipeline that utilizes GitHub for source control, Jenkins for automation, AWS for dynamic and scalable environments, and Docker for containerization. Isolated environments are created and removed as needed, supporting safe and efficient development and maintenance.", "result": "The result is a flexible and automated development pipeline enabling developers to safely test, update, and experiment with legacy systems. This approach is expected to reduce maintenance costs, facilitate modernization of legacy IT, and support ongoing digital transformation.", "conclusion": "Implementing a Scalable CI/CD Pipeline significantly alleviates challenges posed by legacy core IT systems. It improves maintainability, reduces costs, and advances digital transformation by providing developers with modern, isolated, and scalable environments."}}
