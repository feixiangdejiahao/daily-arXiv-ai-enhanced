{"id": "2511.20709", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20709", "abs": "https://arxiv.org/abs/2511.20709", "authors": ["Abhijeet Pathak", "Suvadra Barua", "Dinesh Gudimetla", "Rupam Patir", "Jiawei Guo", "Hongxin Hu", "Haipeng Cai"], "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation", "comment": null, "summary": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.", "AI": {"tldr": "The paper introduces the DUALGAUGE framework and DUALGAUGE-BENCH, enabling rigorous, automated joint evaluation of both correctness and security for code generated by LLMs. Testing shows current models have critical shortcomings, and these resources offer a reproducible way to accelerate improvements in secure software generation.", "motivation": "There is a growing use of large language models (LLMs) and autonomous coding agents in software generation, but ensuring that the generated code is both secure and functionally correct remains an unsolved challenge. Existing benchmarks either focus on security or correctness separately, or lack mechanisms for joint evaluation.", "method": "The authors propose DUALGAUGE, a fully automated benchmarking framework for the joint evaluation of code security and correctness generated by LLMs. They also introduce DUALGAUGE-BENCH, a dataset that contains diverse coding tasks with manually validated test suites for security and functionality. DUALGAUGE uses an agentic program executor for sandboxed testing and an LLM-based evaluator for assessing both correctness and vulnerabilities.", "result": "The evaluation of DUALGAUGE and DUALGAUGE-BENCH proved their quality and accuracy. Benchmarking ten leading LLMs with thousands of test cases revealed significant gaps in the ability of these models to generate code that is both correct and secure.", "conclusion": "DUALGAUGE and DUALGAUGE-BENCH offer scalable, reproducible, and rigorous joint evaluation tools for the secure and correct code generation by LLMs. The identified gaps in current models highlight the need for improvement, and their open-source tools and datasets will help advance the field."}}
{"id": "2511.20730", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20730", "abs": "https://arxiv.org/abs/2511.20730", "authors": ["Nehal Afifi", "Christoph Wittig", "Lukas Paehler", "Andreas Lindenmann", "Kai Wolter", "Felix Leitenberger", "Melih Dogru", "Patric Grauberger", "Tobias D\u00fcser", "Albert Albers", "Sven Matthiesen"], "title": "Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities", "comment": null, "summary": "The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities.", "AI": {"tldr": "This paper systematically reviews DDM usage in product development, finding ML and statistical methods most prevalent and DL rising. Key challenges are model interpretability, traceability across stages, and real-world validation. It calls for better guidelines and interpretable hybrid models.", "motivation": "Integration of data-driven methods (DDMs) in product development is fragmented due to uncertainty regarding which methods to use and when. Clarifying usage across development stages is needed to establish guidelines and improve adoption.", "method": "A PRISMA systematic literature review was conducted, using the V-model product development framework simplified to four stages. Structured searches in Scopus, Web of Science, and IEEE Xplore for 2014-2024 yielded records that were screened down to 114 papers for full-text analysis.", "result": "ML and statistical methods are most commonly used, with DL showing an increasing trend. Key methods include supervised learning, clustering, regression, and surrogate modeling, especially in early and middle stages. There is limited application in validation. Challenges include interpretability, traceability, and real-world validation.", "conclusion": "The review identifies the dominant use of ML and statistical methods in product development, with DL adoption increasing. Major challenges include interpretability, cross-stage traceability, and validation in real-world conditions. There is a need for clearer guidance linking algorithms to design stages, and for the development of interpretable hybrid models."}}
{"id": "2511.20813", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20813", "abs": "https://arxiv.org/abs/2511.20813", "authors": ["Simon Hacks"], "title": "Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms", "comment": "17 pages, submitted to CAiSE - International Conference on Advanced information Systems Engineering 2026", "summary": "\"Train While You Fight\" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.", "AI": {"tldr": "The paper analyzes the technical needs of continuous, in-operation military training and shows how software engineering solutions can help ADL platforms meet these requirements, based on NATO guidelines and a German military example.", "motivation": "There is a growing need for military training systems that enable continuous learning during operations, rather than only beforehand or afterwards. This is due to the dynamic and distributed nature of modern military environments.", "method": "The paper uses a Design Science Research approach. It first derives technical challenges from NATO documentation and current practice. It then defines solution objectives and systematically maps these challenges to established software engineering patterns.", "result": "Seven key technical challenges for Advanced Distributed Learning (ADL) platforms are identified: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. The paper shows how proven software engineering patterns can address these challenges using a real-world national case from the German armed forces.", "conclusion": "Current ADL platforms need to address several specific technical challenges to support the 'Train While You Fight' concept. Software engineering patterns can help overcome these challenges, as demonstrated in the German armed forces use case."}}
{"id": "2511.20782", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.20782", "abs": "https://arxiv.org/abs/2511.20782", "authors": ["Russel Arbore", "Alvin Cheung", "Max Willsey"], "title": "Optimism in Equality Saturation", "comment": null, "summary": "Equality saturation is a technique for program optimization based on non-destructive rewriting and a form of program analysis called e-class analysis. The current form of e-class analysis is pessimistic and therefore ineffective at analyzing cyclic programs, such as those in SSA form. We propose an abstract interpretation algorithm that can precisely analyze cycles during equality saturation. This results in a unified algorithm for optimistic analysis and non-destructive rewriting. We instantiate this approach on a prototype abstract interpreter for SSA programs using a new semantics of SSA. Our prototype can analyze simple example programs more precisely than clang and gcc.", "AI": {"tldr": "The paper introduces an improved algorithm for equality saturation that more precisely analyzes cyclic programs by unifying abstract interpretation with non-destructive rewriting, outperforming standard compilers on sample analyses.", "motivation": "Existing equality saturation techniques, while powerful for program optimization, struggle with analyzing cyclic programs (like those in SSA form) because current e-class analysis is too pessimistic. This limitation hampers their effectiveness in practical program analysis and optimization tasks.", "method": "The paper proposes a novel abstract interpretation algorithm capable of precisely analyzing cycles during equality saturation. This method combines optimistic analysis with non-destructive rewriting into a unified algorithm. The authors implement a prototype abstract interpreter for SSA programs, using a new semantics of SSA to demonstrate their technique.", "result": "The prototype demonstrates the ability to analyze simple example programs more precisely than established compilers such as clang and gcc.", "conclusion": "A new algorithm for equality saturation is presented, overcoming the limitations of pessimistic e-class analysis for cyclic (SSA-form) programs. The approach enables more precise program optimizations and analyses compared to current mainstream compilers."}}
{"id": "2511.20916", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20916", "abs": "https://arxiv.org/abs/2511.20916", "authors": ["Illia Khudiakov", "Vladyslav Pliuhin", "Sergiy Plankovskyy", "Yevgen Tsegelnyk"], "title": "Application of machine learning for infrastructure reconstruction programs management", "comment": "8 pages, 2 figures, 3 tables", "summary": "The purpose of this article is to describe an adaptive decision-making support model aimed at improving the efficiency of engineering infrastructure reconstruction program management in the context of developing the architecture and work breakdown structure of programs. As part of the study, the existing adaptive program management tools are analyzed, the use of infrastructure systems modelling tools is justified for program architecture and WBS creation. Existing models and modelling methods are viewed, and machine learning and artificial neural networks are selected for the model. The main components of the model are defined, which include a set of decision-maker preferences, decision-making tasks, sets of input data, and applied software components of the model. To support decision-making, the adaptive model applies the method of system modeling and predicting the value of the objective function at a given system configuration. Prediction is done using machine learning methods based on a dataset consisting of historical data related to existing engineering systems. The work describes the components of the redistribution of varied model parameters, which modify the model dataset based on the selected object type, which allows adapting the decision-making process to the existing program implementation goals. The functional composition done in Microsoft Azure Machine Learning Studio is described. The neural network parameters and evaluation results are given. The application of the developed adaptive model is possible in the management of programs for the reconstruction of such engineering systems as systems of heat, gas, electricity supply, water supply, and drainage, etc.", "AI": {"tldr": "The paper presents an adaptive, machine learning-based decision support model for efficiently managing engineering infrastructure reconstruction programs through better prediction and adaptability using system modeling and historical data.", "motivation": "The motivation behind this paper is to enhance the efficiency and adaptability of managing reconstruction programs for engineering infrastructure, particularly during the development of program architecture and work breakdown structures (WBS), given the complexity and evolving nature of such projects.", "method": "The paper reviews current adaptive program management tools and justifies the need for infrastructure systems modeling tools for program architecture and WBS creation. It proposes a machine learning-based model, particularly using artificial neural networks, implemented via Microsoft Azure Machine Learning Studio. The approach combines system modeling, historical data analysis, and the prediction of outcomes for various system configurations.", "result": "The study defines the main model components (e.g., decision-maker preferences, input data, software), demonstrates how the adaptive model redistributes parameters to fit different types of reconstruction projects, and provides details about the neural network's setup and its evaluation. The model can be applied to a variety of engineering infrastructure systems, such as heating, gas, electricity, water supply, and drainage.", "conclusion": "The developed adaptive decision-making support model is effective for managing complex infrastructure reconstruction programs. By leveraging machine learning for predictive modeling and adaptability, it enables more efficient and goal-aligned decision-making processes across diverse engineering systems."}}
{"id": "2511.21209", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.21209", "abs": "https://arxiv.org/abs/2511.21209", "authors": ["Yee-Jian Tan", "Andreas Nuyts", "Dominique Devriese"], "title": "Towards Computational UIP in Cubical Agda", "comment": null, "summary": "Some advantages of Cubical Type Theory, as implemented by Cubical Agda, over intensional Martin-L\u00f6f Type Theory include Quotient Inductive Types (QITs), which exist as instances of Higher Inductive Types, and functional extensionality, which is provable in Cubical Type Theory. However, HoTT features an infinite hierarchy of equalities that may become unwieldy in formalisations. Fortunately, QITs and functional extensionality are both preserved even if the equality levels of Cubical Type Theory are truncated to only homotopical Sets (h-Sets). In other words, removing the univalence axiom from Cubical Type Theory and instead postulating a conflicting axiom: the Uniqueness of Identity Proofs (UIP) postulate. Since univalence is proved in Cubical Type Theory from the so-called Glue Types, therefore, it is known that one can first remove the Glue Types (thus removing univalence) and then set-truncate all equalities (essentially assuming UIP), \u00e0 la XTT. The result is a \"h-Set Cubical Type Theory\" that retains features such as functional extensionality and QITs.\n  However, in Cubical Agda, there are currently only two unsatisfying ways to achieve h-Set Cubical Type Theory. The first is to give up on the canonicity of the theory and simply postulate the UIP axiom, while the second way is to use a standard result stating \"type formers preserve h-levels\" to manually prove UIP for every defined type. The latter is, however, laborious work best suited for an automatic implementation by the proof assistant. In this project, we analyse formulations of UIP and detail their computation rules for Cubical Agda, and evaluate their suitability for implementation. We also implement a variant of Cubical Agda without Glue, which is already compatible with postulated UIP, in anticipation of a future implementation of UIP in Cubical Agda.", "AI": {"tldr": "This paper explores how Cubical Type Theory can be modified to support only set-level equalities (h-Set Cubical Type Theory) while keeping functional extensionality and Quotient Inductive Types. The authors examine current and potential ways to implement the Uniqueness of Identity Proofs (UIP) axiom in Cubical Agda, implement a Glue-free variant compatible with UIP, and discuss solutions for making UIP implementation more practical.", "motivation": "The motivation is to address the difficulty in combining certain desirable features (like functional extensionality and QITs) with truncated equality hierarchies in type theories, particularly in the context of Cubical Type Theory as realized in Cubical Agda. The authors aim to find a more satisfactory and systematic way to achieve h-Set Cubical Type Theory that is both practical and implementable.", "method": "The paper analyzes different formulations of the Uniqueness of Identity Proofs (UIP) axiom and specifies their computation rules within Cubical Agda. It evaluates each method's practical suitability for adoption and implements a version of Cubical Agda with Glue Types removed, to facilitate a system compatible with UIP.", "result": "The authors show that in Cubical Agda, the current ways to achieve h-Set Cubical Type Theory are unsatisfying but feasible (either giving up canonicity and postulating UIP or manually proving UIP for every type). They introduce a variant of Cubical Agda without Glue Types, which is compatible with postulated UIP, setting the groundwork for future, more automated solutions for UIP. Their analysis also clarifies the requirements for computation rules and implementation strategies within Cubical Agda.", "conclusion": "The paper concludes that it is possible to obtain a version of Cubical Type Theory (h-Set Cubical Type Theory) that supports both functional extensionality and Quotient Inductive Types, by removing univalence (and related Glue Types) and assuming UIP. The authors evaluate existing approaches to implement UIP in Cubical Agda, present their own variant of Cubical Agda without Glue, and discuss the requirements and challenges for a fully automatic implementation of UIP in Cubical Agda."}}
{"id": "2511.20933", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20933", "abs": "https://arxiv.org/abs/2511.20933", "authors": ["Mootez Saad", "Boqi Chen", "Jos\u00e9 Antonio Hern\u00e1ndez L\u00f3pez", "D\u00e1niel Varr\u00f3", "Tushar Sharma"], "title": "Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code", "comment": "18 figures", "summary": "Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \\textit{Verification} to \\textit{Guided} and \\textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \\textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.", "AI": {"tldr": "This study finds that large language models like DeepSeek-R1 are good at identifying software design flaws in clean, guided settings, but their performance drops significantly in real-world, noisy, or unguided contexts\u2014especially for coupling, less so for cohesion. Current LLMs are not yet robust enough for autonomous code reasoning in practical scenarios.", "motivation": "Large language models (LLMs) are widely used in software engineering, but it is unclear how well they understand core software design concepts such as cohesion and coupling.", "method": "The researchers conducted an empirical study by generating poorly designed code fragments and testing the DeepSeek-R1 LLM models (sizes 14B, 32B, 70B) on their ability to analyze cohesion and coupling. They used various levels of task guidance and introduced contextual noise (distractor elements) to evaluate the models' robustness.", "result": "LLMs show strong baseline understanding of cohesion and coupling in ideal conditions. However, their capability to reason about coupling drastically weakens in noisy, open-ended scenarios (over 50% drop in F1 scores), whereas cohesion analysis remains robust unless all guidance is removed. Reasoning-trace analysis shows that coupling induces 'cognitive shortcutting,' whereas cohesion is analyzed more thoroughly but still fails without guidance.", "conclusion": "LLMs can reliably assist in identifying design flaws under guided conditions, but their reasoning and robustness in autonomous, realistic environments are limited, indicating the need for more scalable and robust program understanding."}}
{"id": "2511.21509", "categories": ["cs.PL", "cs.SC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21509", "abs": "https://arxiv.org/abs/2511.21509", "authors": ["Dirk Beyer", "Gidon Ernst", "Martin Jon\u00e1\u0161", "Marian Lingsch-Rosenfeld"], "title": "SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks", "comment": null, "summary": "In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.", "AI": {"tldr": "The paper introduces SV-LIB, a language-agnostic exchange format and intermediate language for software verification, designed to promote tool interoperability and easier validation across different languages and verification systems.", "motivation": "Many software verification technologies are language-agnostic, but existing tools are often tied to specific programming languages. A common exchange format would facilitate technology transfer, tool interoperability, and independent validation within the verification community.", "method": "The authors designed SV-LIB based on concepts familiar from imperative programming languages and SMT-LIB, defining its syntax and informal semantics. The paper presents version 1.0 of the format, including support for programs, specifications, verification witnesses, and witness-validation tasks.", "result": "SV-LIB version 1.0 is introduced as an initial standard for representing verification tasks, with support for integration into existing infrastructures and the ability to both reuse existing tooling and support witness validation. Formal semantics and concurrency support are noted as future work.", "conclusion": "SV-LIB provides a standardized and extensible intermediate language and exchange format for software verification tasks, aiming to bridge the gap between different programming and modeling languages and to enhance tool interoperability in the verification community."}}
{"id": "2511.20955", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20955", "abs": "https://arxiv.org/abs/2511.20955", "authors": ["Sanchit Kaul", "Kevin Nhu", "Jason Eissayou", "Ivan Eser", "Victor Borup"], "title": "SpaceX: Exploring metrics with the SPACE model for developer productivity", "comment": "Code available at https://github.com/knhu/ECS260Project", "summary": "This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.", "AI": {"tldr": "Simple productivity metrics miss important developer behaviors. Using advanced analysis of code repositories and developer emotions, this study finds negative moods lead to more frequent code commits, and that social network analysis provides better insights into collaboration. It proposes a new, more comprehensive productivity metric.", "motivation": "The paper is motivated by the limitations of traditional, simplistic productivity metrics in software engineering, which often overlook the complexity and multi-dimensionality of developer productivity.", "method": "The study uses repository mining on open-source projects, employing statistical methods such as Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to create comprehensive productivity metrics. It also analyzes the network topology of contributor interactions.", "result": "Key findings include a significant positive correlation between negative emotional states and commit frequency, suggesting that frustration fuels iterative contribution. The study also finds that analyzing the relationship network among contributors better captures collaboration than volume-based measures.", "conclusion": "The work concludes by proposing the Composite Productivity Score (CPS), a holistic metric designed to more accurately reflect the diverse factors influencing developer productivity."}}
{"id": "2511.21022", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21022", "abs": "https://arxiv.org/abs/2511.21022", "authors": ["Guancheng Lin", "Xiao Yu", "Jacky Keung", "Xing Hu", "Xin Xia", "Alex X. Liu"], "title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations", "comment": null, "summary": "Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines \"Common API Layers\" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to \"Specific API Layers\" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.", "AI": {"tldr": "Model editing can update LLMs' outdated API knowledge more efficiently than full retraining. A new method, AdaLoRA-L, outperforms previous techniques by improving specificity, ensuring updated models generate accurate APIs without harming untargeted knowledge.", "motivation": "Large Language Models (LLMs) for code completion often generate deprecated APIs because their training data quickly becomes outdated. Retraining these models is costly, and while model editing methods are proposed as efficient alternatives, their efficacy in updating API knowledge is unknown.", "method": "The study systematically evaluates 10 state-of-the-art model editing techniques on three LLMs (Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder) using a new benchmark, EDAPIBench, with over 70 APIs and 3,000+ editing tasks. It further introduces AdaLoRA-L, a targeted fine-tuning technique that separates general and specific API layers to improve editing specificity.", "result": "AdaLoRA, a parameter-efficient fine-tuning method, showed the best performance for updating deprecated APIs but had issues with specificity\u2014meaning edits unintentionally affected unrelated knowledge. The newly proposed AdaLoRA-L method improved specificity while maintaining strong overall performance.", "conclusion": "AdaLoRA-L offers an effective and efficient solution for updating LLMs' API knowledge, especially by addressing the problem of specificity in model editing. It allows LLMs to generate up-to-date APIs with minimal unintended effects on other model knowledge."}}
{"id": "2511.21151", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21151", "abs": "https://arxiv.org/abs/2511.21151", "authors": ["M. Alecci", "P. Jim\u00e9nez", "J. Samhi", "T. Bissyand\u00e9", "J. Klein"], "title": "Exploring Hidden Geographic Disparities in Android Apps", "comment": null, "summary": "While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.\n  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.\n  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.", "AI": {"tldr": "Geography affects mobile apps more than expected: functionally identical apps often vary in security, privacy, and behavior depending on the country. This research exposes major, hidden app disparities and their impact on fairness, reproducibility, and user consent.", "motivation": "Previous studies have focused on app evolution but largely ignored geographical differences in app behavior. The motivation is to address this gap, investigate the security and fairness implications of regional app variations, and raise awareness of hidden biases in app assessment.", "method": "A distributed app collection pipeline was built to gather apps from multiple regions. Thousands of apps were analyzed, focusing on functional similarity, permissions, third-party libraries, and privacy disclosures. Variations in Android App Bundles were examined, and a dataset of 81,963 GeoTwins was released.", "result": "The study uncovered two major phenomena: (1) functionally similar apps, called GeoTwins, have significant regional differences in permissions, third-party libraries, and privacy practices; (2) even base.apk files from the Android App Bundle differ by region. These discrepancies can affect malware classification, reproducibility, and raise ethical concerns about transparency.", "conclusion": "The study reveals systemic regional disparities in mobile app behavior, showing that visually identical apps differ across regions in terms of security, privacy, and functionality. These hidden differences compromise research reproducibility and introduce geographic bias, raising ethical concerns about app transparency and user consent."}}
{"id": "2511.21197", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.21197", "abs": "https://arxiv.org/abs/2511.21197", "authors": ["Paolo Buono", "Mary Cerullo", "Stefano Cirillo", "Giuseppe Desolda", "Francesco Greco", "Emanuela Guglielmi", "Grazia Margarella", "Giuseppe Polese", "Simone Scalabrino", "Cesare Tucci"], "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools", "comment": null, "summary": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.", "AI": {"tldr": "Developers view AI bug detectors as critical issue spotters and readability tools as coaching aids, with trust hinging on explanations and user control; the paper offers design suggestions for better Human-Centered AI in IDEs.", "motivation": "While AI-assisted tools are advancing in supporting developers with challenging tasks, little is known about developers' mental models of these tools and how mismatches impact trust, control, and adoption.", "method": "Six co-design workshops with 58 developers were conducted to gather insights on their mental models regarding AI-assisted bug detection and readability features.", "result": "Developers perceive bug detection tools as 'bug detectives' that should only warn for critical issues, ensuring transparency, actionable feedback, and confidence cues. Readability assessment tools are seen as 'quality coaches', offering contextual, personalized, and progressive guidance. Trust depends on explanatory clarity, timing, and user control.", "conclusion": "The study distilled a set of design principles for Human-Centered AI tools in IDEs, emphasizing balance between disruption and support, conciseness and depth, and automation and human agency."}}
{"id": "2511.21380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21380", "abs": "https://arxiv.org/abs/2511.21380", "authors": ["Jingyi Chen", "Xiaoyan Guo", "Songqiang Chen", "Shing-Chi Cheung", "Jiasi Shen"], "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions", "comment": null, "summary": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.", "AI": {"tldr": "LLM-based multi-agent systems show potential for automating dataset adaptation in software engineering but mostly provide incomplete results. Providing contextual feedback (error messages, reference code) greatly improves output quality. Further research is needed for more reliable automation.", "motivation": "Adapting software engineering research artifacts across different datasets is crucial for enabling scalability and reproducibility in research, but the process is largely manual and not well investigated. The study is motivated by the potential of large language model (LLM)-based multi-agent systems to automate these otherwise complex and tedious adaptation tasks.", "method": "The paper conducts an empirical evaluation of state-of-the-art LLM-based multi-agent systems (specifically GitHub Copilot with GPT-4.1 and Claude Sonnet 4) on dataset adaptation tasks. The research utilizes a five-stage evaluation pipeline: file comprehension, code editing, command generation, validation, and final execution. Benchmark datasets from repositories like ROCODE and LogHub2.0 are used to assess system performance, failure modes, and the effect of prompt-based interventions.", "result": "Current multi-agent LLM systems can recognize relevant files and provide partial adaptations but frequently fail to deliver correct, functional implementations. Introducing error messages and reference code as prompt-level interventions significantly boosts the structural similarity to ground truth, from 7.25% to 67.14%. This demonstrates that context and feedback greatly enhance agent performance.", "conclusion": "Present multi-agent LLM systems for dataset adaptation in SE research exhibit significant promise but also clear limitations. While they can assist with initial adaptation steps, their functional output is often incomplete. Effective use of contextual and feedback-driven prompts can significantly improve results. The study outlines directions for future development towards more robust, self-correcting agent systems."}}
{"id": "2511.21382", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21382", "abs": "https://arxiv.org/abs/2511.21382", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead", "comment": "33 pages, 8 figures", "summary": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.", "AI": {"tldr": "The paper reviews the use of Large Language Models (LLMs) for automated unit test generation, highlighting the dominance of prompt engineering, the rise of validation and repair loops, and current challenges like weak fault detection. It provides a roadmap for future research to industrialize LLM-based software testing.", "motivation": "Unit testing is essential for ensuring software quality, but manual and automated approaches often fall short in producing semantically rich and realistic tests. Large Language Models (LLMs) are emerging as promising tools to address these shortcomings, but the landscape and best practices in LLM-driven test generation are unclear.", "method": "The authors performed a systematic literature review of 115 research papers published between May 2021 and August 2025. They built a unified taxonomy based on the lifecycle of unit test generation, categorizing each approach by generative strategy and enhancement technique, and analyzed trends and gaps in the literature.", "result": "Prompt engineering has become the dominant strategy for leveraging LLMs in test generation, used in 89% of studies due to its adaptability. Iterative validation and repair mechanisms are now common, improving test robustness and pass rates. Despite these advances, generated tests still have weak fault detection, and the field lacks standard benchmarks for evaluation.", "conclusion": "LLMs hold great promise for advancing automated unit testing, but critical challenges remain, such as improving fault detection and establishing benchmark standards. The field is moving towards autonomous and hybrid testing systems. This survey provides a foundation and research agenda for future work in LLM-supported test generation."}}
