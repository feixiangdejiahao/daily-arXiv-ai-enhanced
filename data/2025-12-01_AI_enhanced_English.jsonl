{"id": "2511.21769", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21769", "abs": "https://arxiv.org/abs/2511.21769", "authors": ["Royer David Estrada-Esponda", "Gerardo Matturro", "Jose Reinaldo Sabogal-Pinilla"], "title": "Technical knowledge and soft skills in software startups within the Colombian entrepreneurial ecosystem", "comment": "30 pages", "summary": "The technical knowledge and soft skills of entrepreneurial team members significantly impact the early stages of software startups. It is widely recognized that the success or failure of a startup is determined by the quality of the individuals who constitute the founding team. This article presents the findings of a study conducted within the Colombian entrepreneurial ecosystem, focusing on which technical knowledge and soft skills are the most valued by founding teams of software startups, and how the needs for knowledge and skills evolve as the startup grows. A survey of software startup representatives revealed that the most valued knowledge includes requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting. The most valued soft skills are typically communication, leadership, and teamwork. The outcomes of this work are relevant to software entrepreneurs, incubators, and researchers.", "AI": {"tldr": "The success of Colombian software startups heavily depends on founding teams' technical and soft skills. A survey reveals that requirements engineering, software testing, project management, agile methods, and key business skills, along with communication, leadership, and teamwork, are most valued. Priorities shift as startups grow, guiding where entrepreneurs and incubators should focus skill development efforts.", "motivation": "The paper is motivated by the recognition that the success or failure of software startups is heavily influenced by the technical knowledge and soft skills of the founding team members. It seeks to understand which specific skills and knowledge areas are most valued during the initial development of software startups within the Colombian ecosystem and how these needs change as startups mature.", "method": "The authors conducted a survey among representatives of software startups within the Colombian entrepreneurial ecosystem in order to identify and analyze the most valued technical and soft skills, as well as how these requirements evolve over time.", "result": "The study found that the most valued technical knowledge areas are requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting. In terms of soft skills, communication, leadership, and teamwork are considered most important. The paper also discusses how the importance of different skills changes as the startup grows.", "conclusion": "The research highlights specific technical and soft skills crucial for the success of early-stage software startups. The findings are valuable for software entrepreneurs, incubators, and researchers as they emphasize priority areas for skill development and team composition."}}
{"id": "2511.21788", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21788", "abs": "https://arxiv.org/abs/2511.21788", "authors": ["Md. Raihan Tapader", "Md. Mostafizer Rahman", "Ariful Islam Shiplu", "Md Faizul Ibne Amin", "Yutaka Watanobe"], "title": "Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings", "comment": null, "summary": "In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring.", "AI": {"tldr": "The paper introduces an LLM-powered code refactoring framework using prompt engineering and few-shot learning for multiple programming languages. This framework outperforms traditional rule-based methods by achieving high correctness and compilability, especially in Java and Python, and offers robust, generalizable refactoring solutions across diverse languages.", "motivation": "The motivation is to address the limitations of current code refactoring methods, which struggle to generalize across multiple programming languages and coding styles because they rely on handcrafted rules. There is a need for a more adaptable and efficient refactoring approach that can improve code quality in various languages.", "method": "This study develops a Large Language Model (LLM)-based framework capable of cross-language code refactoring. The method involves fine-tuning the model with prompt engineering (adjusting parameters like Temperature, using different shot algorithms, and instruction fine-tuning) and employs few-shot learning for better code transformation. The framework's effectiveness is evaluated using metrics such as compilability, correctness, code distance, similarity, code length, and cyclomatic complexity, along with human assessments.", "result": "Experimental results demonstrate that for Java code, the proposed model achieves a top correctness rate of 99.99% in a 10-shot setting and a high compilability rate of 94.78%, maintaining a similarity of about 53-54%. This indicates strong structural improvements with preserved semantics. For Python, the model delivers the lowest code distance, denoting minimal structural disruption but still decent similarity rates (44-48%), pointing to minimally invasive yet effective refactoring.", "conclusion": "LLM-based, prompt-engineered, multilingual code refactoring is both generalizable and effective. The approach not only ensures correct and compilable code across several programming languages but also strikes a balance between significant structural improvements and maintenance of semantic integrity."}}
{"id": "2511.21877", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21877", "abs": "https://arxiv.org/abs/2511.21877", "authors": ["Nenad Petrovic", "Norbert Kroth", "Axel Torschmied", "Yinglei Song", "Fengjunjie Pan", "Vahid Zolfaghari", "Nils Purschke", "Sven Kirchner", "Chengdong Wu", "Andre Schamschurko", "Yi Zhang", "Alois Knoll"], "title": "LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems", "comment": null, "summary": "This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining.", "AI": {"tldr": "This paper introduces an LLM-based workflow that leverages event chains and signal retrieval to generate reliable automotive code from natural language, achieving solid results in an emergency braking case study without LLM retraining.", "motivation": "Existing approaches to generating automotive code from natural-language requirements struggle with hallucinations, architectural correctness, and adapting to evolving vehicle signal catalogs. The authors aim to address these challenges while ensuring real-time feasibility and behavioral consistency.", "method": "An event-chain-driven workflow augmented with Retrieval-Augmented Generation (RAG) retrieves and validates relevant signals from Vehicle Signal Specification (VSS) catalogs, maps them into event chains encoding constraints, and guides LLM-based code synthesis under these constraints.", "result": "Initial findings from an emergency braking case study confirm that the approach ensures valid signal usage and consistent code generation, without the need for LLM retraining.", "conclusion": "The proposed workflow successfully generates validated and consistent automotive code from natural-language requirements, achieving valid signal usage and behavioral consistency without requiring LLM retraining."}}
{"id": "2511.21878", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.21878", "abs": "https://arxiv.org/abs/2511.21878", "authors": ["Kaiyao Ke", "Ali Reza Ibrahimzada", "Rangeet Pan", "Saurabh Sinha", "Reyhaneh Jabbarvand"], "title": "Advancing Automated In-Isolation Validation in Repository-Level Code Translation", "comment": null, "summary": "Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.", "AI": {"tldr": "Validating repository-wide code translation is hard. TRAM uses LLMs and mock objects to automate type resolution and fragment validation, outperforming existing methods for Java-to-Python translation.", "motivation": "Validating repository-level code translations automatically across languages is difficult and existing approaches are either costly or require significant manual effort.", "method": "TRAM retrieves API documentation and contextual information for type resolution, uses LLMs for precise cross-language type mapping, and constructs mock objects via custom serialization/deserialization for in-isolation validation of translated method fragments.", "result": "TRAM achieves state-of-the-art performance in Java-to-Python repository-level translation, effectively integrating context-aware type resolution with reliable mock-based validation.", "conclusion": "TRAM provides automatic, high-quality translations and validation between programming languages, addressing previous bottlenecks in cost and manual labor."}}
{"id": "2511.22075", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22075", "abs": "https://arxiv.org/abs/2511.22075", "authors": ["Doruk Alp Mutlu"], "title": "Expanding Specification Capabilities of a Gradual Verifier with Pure Functions", "comment": "Submitted to the 53rd ACM SIGPLAN Symposium on Principles of Programming Languages (POPL 2026) Student Research Competition", "summary": "Gradual verification soundly combines static checking and dynamic checking to provide an incremental approach for software verification. With gradual verification, programs can be partially specified first, and then the full specification of a program can be achieved in incremental steps. The first and only practicable gradual verifier based on symbolic execution, Gradual C0, supports recursive heap data structures. Despite recent efforts to improve the expressivity of Gradual C0's specification language, Gradual C0's specification language is still limited in its capabilities for complex expressions. This work explores an extension to Gradual C0's design with a common construct supported by many static verification tools, pure functions, which both extend the specification capabilities of Gradual C0 and increase the ease of encoding observer methods in Gradual C0. Our approach addresses the technical challenges related to the axiomatisation of pure functions with imprecise specifications.", "AI": {"tldr": "The paper enhances Gradual C0, a gradual verifier, by adding pure functions to its specification language. This makes specifying and verifying complex program behaviors easier and more expressive, addressing previous limitations and supporting incremental verification with improved technical solutions.", "motivation": "Gradual C0's current specification language is limited in its ability to express complex conditions and observers, making verification cumbersome. The motivation is to extend its expressive power by introducing pure functions, a feature common to many static verification tools, to overcome these limitations.", "method": "The authors propose an extension to the symbolic execution-based Gradual C0 verifier by incorporating pure functions into its specification language. They address technical challenges related to axiomatisation of these pure functions, particularly when dealing with imprecise specifications.", "result": "The extension enables Gradual C0 to better express complex specifications, including observer methods, and resolves technical issues associated with imprecise specification of pure functions. This leads to more flexible and robust software verification.", "conclusion": "The paper concludes that by extending Gradual C0's specification language with pure functions, both the expressivity and ease of encoding complex specifications are improved, while ensuring sound verification."}}
{"id": "2511.21920", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21920", "abs": "https://arxiv.org/abs/2511.21920", "authors": ["Apu Kumar Chakroborti", "Yi Ding", "Lipeng Wan"], "title": "Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code", "comment": null, "summary": "As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools.", "AI": {"tldr": "This paper shows that LLMs can help automate scientific data workflows, but they currently have reliability issues. The authors evaluate their performance and propose solutions that improve execution, though challenges remain. Their benchmark and methods contribute to making AI-assisted research more trustworthy and accessible.", "motivation": "Modern scientific research generates complex, large-scale datasets, but many domain scientists struggle to create custom data analysis workflows due to limited programming skills. There is a need for accessible tools that bridge this gap and facilitate scientific discovery.", "method": "The paper builds a benchmark suite of domain-specific prompts mimicking real research tasks and systematically evaluates open-source LLMs' ability to generate executable and correct Python scripts. It proposes and tests three strategies to improve code reliability: data-aware prompt disambiguation, retrieval-augmented enhancement, and iterative error repair.", "result": "Initial LLM-generated scripts exhibit frequent failures due to ambiguous prompts and inadequate domain understanding. The proposed enhancement strategies substantially improve code execution rates and quality, but further improvements are required for robust applicability.", "conclusion": "LLMs can automate scientific data analysis and visualization, but current models struggle with prompt ambiguity and domain-specific nuances. The authors offer actionable techniques and a reusable benchmark to advance LLM reliability and inclusivity in research workflows, though continued refinement is needed."}}
{"id": "2511.22419", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22419", "abs": "https://arxiv.org/abs/2511.22419", "authors": ["Ken Sakayori", "Andrea Colledan", "Ugo Dal Lago"], "title": "On Circuit Description Languages, Indexed Monads, and Resource Analysis", "comment": "Extended version of a paper to be published at POPL 2026", "summary": "In this paper, a monad-based denotational model is introduced and shown adequate for the Proto-Quipper family of calculi, themselves being idealized versions of the Quipper programming language. The use of a monadic approach allows us to separate the value to which a term reduces from the circuit that the term itself produces as a side effect. In turn, this enables the denotational interpretation and validation of rich type systems in which the size of the produced circuit can be controlled. Notably, the proposed semantic framework, through the novel concept of circuit algebra, suggests forms of effect typing guaranteeing quantitative properties about the resulting circuit, even in presence of optimizations.", "AI": {"tldr": "This paper presents a monad-based semantic framework for idealized Quipper languages, separating program values from generated circuits and enabling formal guarantees on circuit size through advanced type systems and the newly introduced concept of circuit algebra.", "motivation": "The motivation is to create a semantic model for Proto-Quipper, an idealized version of the Quipper quantum programming language, enabling precise control and reasoning about the quantum circuits generated by programs, especially regarding their size and type systems.", "method": "The method used involves introducing a denotational semantics based on monads. This approach separates term evaluation from circuit generation and validates advanced type systems for circuit size control.", "result": "The results include a new semantic framework for Proto-Quipper calculi that effectively mediates between value computation and circuit generation, and introduces the concept of circuit algebra to enable effect typing ensuring circuit size guarantees even under optimizations.", "conclusion": "The monad-based denotational model is adequate for the Proto-Quipper family, provides precise semantic interpretation, and supports quantitative guarantees about quantum circuit outputs within rich type systems."}}
{"id": "2511.21956", "categories": ["cs.SE", "physics.acc-ph"], "pdf": "https://arxiv.org/pdf/2511.21956", "abs": "https://arxiv.org/abs/2511.21956", "authors": ["M. Polzin", "M. Guzman"], "title": "Beyond Like-for-Like: A User-centered Approach to Modernizing Legacy Applications", "comment": "The 20th International Conference on Accelerator and Large Experimental Physics Control Systems", "summary": "When modernizing a legacy application, it is easy to fall back on a like-for-like replica with new tools and updated design stylings, but this is an opportunity to explore making a more intuitive application that supports user tasks and efficiency. Rather than having a blank canvas-unburdened by legacy tech debt-to create a new application, you are working with an existing application that is integral to accelerator operations and one that expert users are already familiar with. Due to this, you might assume people will prefer the like-for-like, but you could be carrying forward the pain points, processes that are inefficient, and ultimately wind up with an application that no one wants to use because it doesn't solve existing problems. Getting users involved can make all the difference in your approach to modernizing a legacy application that caters to both newer and expert users. It also can bridge the gap between like-for-like and introducing new GUI design. Having a legacy application doesn't have to make the modernized one difficult to develop, as the existing application is a tool in how you move forward with the new application. It provides insight into areas that an application with a clean slate doesn't give you.", "AI": {"tldr": "Don't just replicate legacy apps\u2014modernize by engaging users, fixing old issues, and improving usability for both new and expert users, leveraging the strengths of the existing system.", "motivation": "Legacy applications are often modernized as direct replicas, missing the chance to improve user experience and address inefficiencies. The motivation is to leverage modernization as an opportunity to make applications more intuitive, effective, and suitable for diverse users.", "method": "Engage current users in the modernization process to understand their needs and pain points. Use the legacy application as a reference tool and source of insights for developing the new application, rather than duplicating its shortcomings.", "result": "Involving users during modernization helps create applications that are both familiar and improved, addressing existing issues while introducing better GUI design. Legacy applications offer valuable context that new builds lack, aiding a more informed development process.", "conclusion": "Modernizing legacy applications should go beyond cosmetic updates and direct replication. By involving users and learning from the old system, more efficient and user-friendly applications can be built, bridging the gap between old and new approaches and enhancing productivity."}}
{"id": "2511.22692", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22692", "abs": "https://arxiv.org/abs/2511.22692", "authors": ["David Castro-Perez", "Francisco Ferreira", "Sung-Shik Jongmans"], "title": "A Synthetic Reconstruction of Multiparty Session Types (with Appendix)", "comment": null, "summary": "Multiparty session types (MPST) provide a rigorous foundation for verifying the safety and liveness of concurrent systems. However, existing approaches often force a difficult trade-off: classical, projection-based techniques are compositional but limited in expressiveness, while more recent techniques achieve higher expressiveness by relying on non-compositional, whole-system model checking, which scales poorly.\n  This paper introduces a new approach to MPST that delivers both expressiveness and compositionality, called the synthetic approach. Our key innovation is a type system that verifies each process directly against a global protocol specification, represented as a labelled transition system (LTS) in general, with global types as a special case. This approach uniquely avoids the need for intermediate local types and projection.\n  We demonstrate that our approach, while conceptually simpler, supports a benchmark of challenging protocols that were previously beyond the reach of compositional techniques in the MPST literature. We generalise our type system, showing that it can validate processes against any specification that constitutes a \"well-behaved\" LTS, supporting protocols not expressible with the standard global type syntax. The entire framework, including all theorems and many examples, has been formalised and mechanised in Agda, and we have developed a prototype implementation as an extension to VS Code.", "AI": {"tldr": "This paper proposes a new, simpler type system for verifying concurrent systems that is both highly expressive and compositional. The approach verifies each process directly against a labelled transition system, eliminating the need for traditional projection techniques. It supports protocols previously too complex for compositional methods, is fully formalized in Agda, and comes with a VS Code prototype.", "motivation": "Current methods for verifying concurrent systems using multiparty session types are either expressive but not compositional, leading to scalability issues, or compositional but limited in expressiveness. There is a need for an approach that combines both.", "method": "The paper introduces a synthetic approach to MPST, featuring a type system that allows each process to be verified directly against a global protocol specification, modeled as a labelled transition system (LTS). This approach removes the need for projecting global types into local types. It is supported by formal proofs mechanized in Agda and a VS Code extension as a prototype.", "result": "The synthetic approach is shown to support more complex and expressive protocols that were previously unattainable by compositional methods. It can validate any process against a well-behaved LTS, not just those described by standard global types. The implementation and theorems have been formalized in Agda and prototyped in VS Code.", "conclusion": "The synthetic approach enables both compositionality and expressiveness in verifying concurrent protocols, overcoming limitations of prior methods. It broadens the applicability of MPST and improves scalability without sacrificing soundness or rigour."}}
{"id": "2511.21964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21964", "abs": "https://arxiv.org/abs/2511.21964", "authors": ["Ali Sayedsalehi", "Peter C. Rigby", "Audris Mockus"], "title": "DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction", "comment": "8 pages, 4 figures, includes system architecture diagrams, Web UI screenshots, GitHub App examples, and an appendix with API endpoints. Full replication package and demo materials available", "summary": "In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website.", "AI": {"tldr": "DRS-OSS is an open-source system that accurately predicts the risk of regressions in pull requests using a fine-tuned large language model. It integrates with standard developer tools and achieves strong benchmark results, enabling better review and test prioritization and reducing defect rates in large codebases.", "motivation": "Large-scale open-source projects handle hundreds of pull requests daily, each carrying a risk of introducing defects. There is a critical need for automated tools to estimate and mitigate these risks to optimize review, testing, and deployment processes.", "method": "The authors build DRS-OSS, an open-source Diff Risk Scoring system. It features a public API, web UI, and a GitHub plugin. DRS-OSS utilizes a fine-tuned Llama 3.1 8B sequence classifier, trained on the ApacheJIT dataset. It processes long-context inputs combining commit messages, structured diffs, and change metrics, with efficient training enabled by 4-bit QLoRA and DeepSpeed ZeRO-3 CPU offloading on limited GPU resources.", "result": "DRS-OSS achieves state-of-the-art results on the ApacheJIT benchmark (F1 = 0.64, ROC-AUC = 0.89). Simulations indicate that gating the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system is seamlessly integrated with developer workflows via APIs, dashboards, and a GitHub App.", "conclusion": "DRS-OSS provides a practical, high-performing solution for risk prediction in code changes, supporting better prioritization and defect prevention in open-source workflows. The tool is fully open-sourced, with code, scripts, and deployment artifacts publicly available."}}
{"id": "2511.23283", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23283", "abs": "https://arxiv.org/abs/2511.23283", "authors": ["Alexandre Moine", "Sam Westrick", "Joseph Tassarotti"], "title": "All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs", "comment": "32 pages, 26 figures, extended version of the same paper accepted at POPL 2026", "summary": "Nondeterminism makes parallel programs challenging to write and reason about. To avoid these challenges, researchers have developed techniques for internally deterministic parallel programming, in which the steps of a parallel computation proceed in a deterministic way. Internal determinism is useful because it lets a programmer reason about a program as if it executed in a sequential order. However, no verification framework exists to exploit this property and simplify formal reasoning about internally deterministic programs.\n  To capture the essence of why internally deterministic programs should be easier to reason about, this paper defines a property called schedule-independent safety. A program satisfies schedule-independent safety, if, to show that the program is safe across all orderings, it suffices to show that one terminating execution of the program is safe. We then present a separation logic called Musketeer for proving that a program satisfies schedule-independent safety. Once a parallel program has been shown to satisfy schedule-independent safety, we can verify it with a new logic called Angelic, which allows one to dynamically select and verify just one sequential ordering of the program.\n  Using Musketeer, we prove the soundness of MiniDet, an affine type system for enforcing internal determinism. MiniDet supports several core algorithmic primitives for internally deterministic programming that have been identified in the research literature, including a deterministic version of a concurrent hash set. Because any syntactically well-typed MiniDet program satisfies schedule-independent safety, we can apply Angelic to verify such programs.\n  All results in this paper have been verified in Rocq using the Iris separation logic framework.", "AI": {"tldr": "This paper introduces the concept of schedule-independent safety to simplify verification of internally deterministic parallel programs. It presents Musketeer and Angelic, two new logics for proving and verifying such properties, and shows that programs written in MiniDet, an internally deterministic type system, can be formally and easily verified if well-typed. All results have been mechanized in Rocq using Iris.", "motivation": "Parallel programs are difficult to write and verify due to nondeterminism. Internal determinism makes reasoning simpler by aligning the execution with sequential reasoning, but there was no verification framework to leverage this property and facilitate formal reasoning about such programs.", "method": "The authors define 'schedule-independent safety', wherein verifying safety on a single terminating execution is sufficient to guarantee safety across all schedules. They introduce 'Musketeer', a separation logic for proving that programs satisfy this property, and 'Angelic', a logic that allows verification of any one sequential ordering once schedule-independent safety is established. MiniDet, an affine type system, is used to enforce internal determinism, and its soundness is proved using Musketeer. All results are mechanized and verified using the Rocq tool and the Iris separation logic framework.", "result": "They show that MiniDet programs, when well-typed, satisfy schedule-independent safety, enabling simpler verification via Angelic logic. Core primitives for internally deterministic programming, including a deterministic concurrent hash set, are supported and formally verified. All results are mechanized in Rocq and based on the Iris framework.", "conclusion": "By introducing schedule-independent safety and corresponding logical tools (Musketeer and Angelic), the paper allows for much simpler formal reasoning and verification of internally deterministic parallel programs, reducing the burden of reasoning about all possible schedules to just one."}}
{"id": "2511.22118", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22118", "abs": "https://arxiv.org/abs/2511.22118", "authors": ["Yihan Dai", "Dimitrios Stamatios Bouras", "Haoxiang Jia", "Sergey Mechtaev"], "title": "Statistical Independence Aware Caching for LLM Workflows", "comment": null, "summary": "Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.", "AI": {"tldr": "LLM caching improves performance but risks statistical issues; Mnimi offers a type-based solution ensuring both efficiency and statistical correctness, validated in a program repair case study.", "motivation": "Large language models (LLMs) have high inference costs and latency. Local caching can alleviate these issues, but naive caching threatens statistical independence, which is crucial for many probabilistic workflows and reliable evaluation metrics, especially in LLMs used for code applications.", "method": "The authors propose Mnimi, a cache design pattern that encapsulates statistical constraints within the types of LLM references. This enables modular LLM workflows that ensure statistical integrity. The implementation in Python leverages decorators and iterators over infinite sequences.", "result": "In a case study with SpecFix, an automated program specification repair system, Mnimi improved reproducibility, debugging, and efficiency in terms of time and cost, all while maintaining statistical correctness.", "conclusion": "Mnimi allows for practical and efficient caching in LLM workflows without compromising statistical independence. It provides a modular approach that maintains statistical integrity and operational benefits for LLM-based systems."}}
{"id": "2511.23358", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23358", "abs": "https://arxiv.org/abs/2511.23358", "authors": ["Alexandre Moine", "Stephanie Balzer", "Alex Xu", "Sam Westrick"], "title": "TypeDis: A Type System for Disentanglement", "comment": "34 pages, 24 figures, extended version of the same paper accepted at POPL 2026", "summary": "Disentanglement is a runtime property of parallel programs guaranteeing that parallel tasks remain oblivious to each other's allocations. As demonstrated in the MaPLe compiler and run-time system, disentanglement can be exploited for fast automatic memory management, especially task-local garbage collection with no synchronization between parallel tasks. However, as a low-level property, disentanglement can be difficult to reason about for programmers. The only means of statically verifying disentanglement so far has been DisLog, an Iris-fueled variant of separation logic, mechanized in the Rocq proof assistant. DisLog is a fully-featured program logic, allowing for proof of functional correctness as well as verification of disentanglement. Yet its employment requires significant expertise and per-program proof effort.\n  This paper explores the route of automatic verification via a type system, ensuring that any well-typed program is disentangled and lifting the burden of carrying out manual proofs from the programmer. It contributes TypeDis, a type system inspired by region types, where each type is annotated with a timestamp, identifying the task that allocated it. TypeDis supports iso-recursive types as well as polymorphism over both types and timestamps. Crucially, timestamps are allowed to change during type-checking, at join points as well as via a form of subtyping, dubbed subtiming. The paper illustrates TypeDis and its features on a range of examples. The soundness of TypeDis and the examples are mechanized in the Rocq proof assistant, using an improved version of DisLog, dubbed DisLog2.", "AI": {"tldr": "The paper introduces TypeDis, a type system that automatically verifies disentanglement in parallel programs by annotating types with timestamps, eliminating the need for complex manual proofs and enabling practical, sound verification.", "motivation": "Disentanglement is valuable for efficient parallel memory management but is hard for programmers to reason about and has only been verifiable via complex manual proofs using program logics like DisLog.", "method": "The paper proposes TypeDis, an automatic verification type system inspired by region types, with timestamp annotations that track allocation origins. TypeDis supports iso-recursive types and polymorphism, with timestamps that can change through subtyping ('subtiming'). The system is illustrated with examples and mechanized in Rocq using DisLog2.", "result": "TypeDis ensures that any well-typed program is disentangled, lifting the burden of manual verification from programmers. The paper mechanizes soundness proofs and example verifications using DisLog2.", "conclusion": "The type system TypeDis automatically guarantees disentanglement in parallel programs, making verification practical and automatable while supported by mechanized soundness proofs."}}
{"id": "2511.22186", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22186", "abs": "https://arxiv.org/abs/2511.22186", "authors": ["Chayanid Termphaiboon", "Raula Gaikovina Kula", "Youmei Fan", "Morakot Choetkiertikul", "Chaiyong Ragkhitwetsagul", "Thanwadee Sunetnanta", "Kenichi Matsumoto"], "title": "Exploring the SECURITY.md in the Dependency Chain: Preliminary Analysis of the PyPI Ecosystem", "comment": "8 pages, 5 figures, accepted to ISE 2025 (International Workshop on Intelligent Software Engineering)", "summary": "Security policies, such as SECURITY.md files, are now common in open-source projects. They help guide responsible vulnerability reporting and build trust among users and contributors. Despite their growing use, it is still unclear how these policies influence the structure and evolution of software dependencies. Software dependencies are external packages or libraries that a project relies on, and their interconnected nature affects both functionality and security. This study explores the relationship between security policies and dependency management in PyPI projects. We analyzed projects with and without a SECURITY.md file by examining their dependency trees and tracking how dependencies change over time. The analysis shows that projects with a security policy tend to rely on a broader set of direct dependencies, while overall depth and transitive dependencies remain similar. Historically, projects created after the introduction of SECURITY.md, particularly later adopters, show more frequent dependency updates. These results suggest that security policies are linked to more modular and feature-rich projects, and highlight the role of SECURITY.md in promoting proactive dependency management and reducing risks in the software supply chain.", "AI": {"tldr": "Having a SECURITY.md policy in an open-source project is associated with greater modularity and more proactive dependency updates, helping reduce supply chain security risks.", "motivation": "To understand how the adoption of formal security policies in open-source projects affects the structure and evolution of their software dependencies, particularly regarding security and risk management.", "method": "The authors conducted an empirical analysis by comparing PyPI projects with and without SECURITY.md files. They examined the projects\u2019 dependency trees and tracked how dependencies evolved over time.", "result": "Projects with SECURITY.md files have more direct dependencies, but similar dependency tree depth and transitive dependencies compared with those without. More recent projects, especially those adopting SECURITY.md later, update dependencies more frequently.", "conclusion": "Projects with security policies, such as SECURITY.md files, tend to be more modular, feature-rich, and engage in more proactive dependency management, which helps reduce risks in the software supply chain."}}
{"id": "2511.23472", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23472", "abs": "https://arxiv.org/abs/2511.23472", "authors": ["Yusuke Matsushita", "Kengo Hirata", "Ryo Wakizaka", "Emanuele D'Osualdo"], "title": "RapunSL: Untangling Quantum Computing with Separation, Linear Combination and Mixing", "comment": "Full version of the conference paper at POPL 2026. The first two authors contributed equally to this work", "summary": "Quantum Separation Logic (QSL) has been proposed as an effective tool to improve the scalability of deductive reasoning for quantum programs. In QSL, separation is interpreted as disentanglement, and the frame rule brings a notion of entanglement-local specification (one that only talks about the qubits entangled with those acted upon by the program). In this paper, we identify two notions of locality unique to the quantum domain, and we construct a novel quantum separation logic, RapunSL, which is able to soundly reduce reasoning about superposition states to reasoning about pure states (basis-locality), and reasoning about mixed states arising from measurement to reasoning about pure states (outcome-locality). To do so, we introduce two connectives, linear combination and mixing, which together with separation provide a dramatic improvement in the scalability of reasoning, as we demonstrate on a series of challenging case studies.", "AI": {"tldr": "The paper presents RapunSL, a new quantum separation logic that dramatically improves scalable reasoning about quantum programs by reducing complex quantum states to pure states, thanks to two novel logic connectives and a focus on quantum-local behaviors.", "motivation": "Classical separation logic does not adequately address the unique aspects of quantum entanglement and state complexity, hindering scalable reasoning in quantum programming. The paper aims to resolve this by developing logic specifically for quantum-local behaviors, allowing for more efficient analysis and specification.", "method": "The authors identify unique quantum localities (basis-locality and outcome-locality) and design two new logical connectives (linear combination and mixing) for their quantum separation logic framework. They prove soundness and apply their logic to challenging quantum programming scenarios as case studies to validate effectiveness.", "result": "RapunSL enables sound reduction of reasoning about quantum superposition and mixed states to pure states, leading to dramatic improvements in the scalability of quantum program verification as demonstrated in various complex case studies.", "conclusion": "The paper introduces RapunSL, a quantum separation logic that enables scalable and sound reasoning about quantum programs by reducing complex quantum state reasoning to simpler cases involving pure states. The proposed framework effectively handles the unique challenges of quantum locality and demonstrates significant improvements through case studies."}}
{"id": "2511.22359", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.22359", "abs": "https://arxiv.org/abs/2511.22359", "authors": ["Vadim Safronov", "Ionut Bostan", "Nicholas Allott", "Andrew Martin"], "title": "UniBOM -- A Unified SBOM Analysis and Visualisation Tool for IoT Systems and Beyond", "comment": "This paper has been accepted at the ACM 15th International Conference on the Internet of Things (ACM IoT 2025)", "summary": "Modern networked systems rely on complex software stacks, which often conceal vulnerabilities arising from intricate interdependencies. A Software Bill of Materials (SBOM) is effective for identifying dependencies and mitigating security risks. However, existing SBOM solutions lack precision, particularly in binary analysis and non-package-managed languages like C/C++.\n  This paper introduces UniBOM, an advanced tool for SBOM generation, analysis, and visualisation, designed to enhance the security accountability of networked systems. UniBOM integrates binary, filesystem, and source code analysis, enabling fine-grained vulnerability detection and risk management. Key features include historical CPE tracking, AI-based vulnerability classification by severity and memory safety, and support for non-package-managed C/C++ dependencies.\n  UniBOM's effectiveness is demonstrated through a comparative vulnerability analysis of 258 wireless router firmware binaries and the source code of four popular IoT operating systems, highlighting its superior detection capabilities compared to other widely used SBOM generation and analysis tools. Packaged for open-source distribution, UniBOM offers an end-to-end unified analysis and visualisation solution, advancing SBOM-driven security management for dependable networked systems and broader software.", "AI": {"tldr": "UniBOM is a new SBOM tool that surpasses existing solutions in detecting vulnerabilities, notably for binary and C/C++-based systems. It integrates multiple analysis methods, provides AI-driven classifications, and greatly enhances security management for networked systems.", "motivation": "Networked systems use complex software stacks that may hide vulnerabilities, particularly due to intricate dependencies. Existing Software Bill of Materials (SBOM) solutions are imprecise, especially for binary analysis and languages like C/C++ that lack package management.", "method": "The paper introduces UniBOM, a tool that combines binary, filesystem, and source code analysis for SBOM generation, analysis, and visualization. It provides fine-grained vulnerability detection and risk management, features historical CPE tracking, AI-based vulnerability classification, and specifically supports non-package-managed dependencies.", "result": "UniBOM was validated by analyzing 258 wireless router firmware binaries and four IoT OS source codes, demonstrating superior vulnerability detection compared to other SBOM tools.", "conclusion": "UniBOM offers a unified and advanced SBOM solution that improves security accountability for networked software systems, especially with open-source accessibility and enhanced analysis features."}}
{"id": "2511.22409", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22409", "abs": "https://arxiv.org/abs/2511.22409", "authors": ["Polydoros Giannouris", "Sophia Ananiadou"], "title": "NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements", "comment": null, "summary": "Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows.", "AI": {"tldr": "NOMAD, a multi-agent system inspired by human reasoning, decomposes UML diagram generation with LLMs into specialized steps. It shows improved results over existing methods and provides new insights into errors and verification challenges, marking progress towards more reliable AI-driven software modelling.", "motivation": "While LLMs are applied in software engineering, their effectiveness at generating structured outputs like UML diagrams is not well understood. The work seeks to create more interpretable and verifiable LLM-driven modelling workflows.", "method": "A modular multi-agent system decomposes UML diagram generation into specialized subtasks (entity extraction, relationship classification, diagram synthesis), and performance is evaluated through a large case study (Northwind) and human-authored UML exercises. The study also introduces a taxonomy of errors and investigates verification strategies.", "result": "NOMAD outperformed baseline methods in generating UML diagrams, provided systematic error categorization, and revealed ongoing issues with attribute extraction and verification strategy effectiveness.", "conclusion": "NOMAD significantly improves UML diagram generation with LLMs, outperforming baselines and enabling better error analysis, but challenges remain in extracting detailed attributes and verification strategies need further development."}}
{"id": "2511.22513", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22513", "abs": "https://arxiv.org/abs/2511.22513", "authors": ["J\u00e9r\u00f4me Pfeiffer", "Nicolai Maisch", "Sebastian Friedl", "Matthias Milan Strljic", "Armin Lechler", "Oliver Riedel", "Andreas Wortmann"], "title": "Declarative Policy Control for Data Spaces: A DSL-Based Approach for Manufacturing-X", "comment": null, "summary": "The growing adoption of federated data spaces, such as in the GAIA-X and the International Data Spaces (IDS) initiative, promises secure and sovereign data sharing across organizational boundaries in Industry 4.0. In manufacturing ecosystems, this enables use cases, such as cross-factory process optimization, predictive maintenance, and supplier integration. Frameworks and standards, such as the Asset Administration Shell (AAS), Eclipse Dataspace Connector (EDC), ID-Link and Open Platform Communications Unified Architecture (OPC UA) provide a strong foundation to realize this ecosystem. However, a major open challenge is the practical description and enforcement of context-dependent data usage policies using these base technologies - especially by domain experts without software engineering backgrounds. Therefore, this article proposes a method for leveraging domain-specific languages (DSLs) to enable declarative, human-readable, and machine-executable policy definitions for sovereign data sharing via data space connectors. The DSL empowers domain experts to specify fine-grained data governance requirements - such as restricting access to data from specific production batches or enforcing automatic deletion after a defined retention period - without writing imperative code.", "AI": {"tldr": "Federated data spaces promise secure, cross-organization data sharing in manufacturing, but practical policy enforcement is complex for non-programmers. This paper proposes domain-specific languages that let domain experts declaratively define and enforce data governance policies, making data sharing more secure and manageable.", "motivation": "Federated data spaces for Industry 4.0 promise secure, sovereign cross-organization data sharing, supporting use cases like process optimization, predictive maintenance, and supplier integration. There are robust frameworks and standards, but enabling practical and context-dependent data usage policy enforcement, especially for non-programmers, remains challenging.", "method": "The paper proposes using domain-specific languages (DSLs) to allow domain experts to declaratively specify data usage policies in a human-readable and machine-executable format. These policies can be incorporated into data space connectors to enforce data governance without imperative coding.", "result": "The DSL approach empowers domain experts to easily define fine-grained policies (e.g., restricting access by production batch, enforcing retention-based deletion) that are automatically enforced via federated data space connectors.", "conclusion": "By using DSLs for policy specification, practical, granular, and context-aware data governance in federated manufacturing data spaces becomes feasible for domain experts without programming skills."}}
{"id": "2511.22726", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22726", "abs": "https://arxiv.org/abs/2511.22726", "authors": ["Ethan Friesen", "Sasha Morton-Salmon", "Md Nahidul Islam Opu", "Shahidul Islam", "Shaiful Chowdhury"], "title": "The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods", "comment": null, "summary": "Identifying the small subset of source code that repeatedly attracts bugs is critical for reducing long-term maintenance effort. We define ExtremelyBuggy methods as those involved in more than one bug fix and present the first large-scale study of their prevalence, characteristics, and predictability. Using a dataset of over 1.25 million methods from 98 open-source Java projects, we find that ExtremelyBuggy methods constitute only a tiny fraction of all methods, yet frequently account for a disproportionately large share of bugs. At their inception, these methods are significantly larger, more complex, less readable, and less maintainable than both singly-buggy and non-buggy methods. However, despite these measurable differences, a comprehensive evaluation of five machine learning models shows that early prediction of ExtremelyBuggy methods remains highly unreliable due to data imbalance, project heterogeneity, and the fact that many bugs emerge through subsequent evolution rather than initial implementation. To complement these quantitative findings, we conduct a thematic analysis of 265 ExtremelyBuggy methods, revealing recurring visual issues (e.g., confusing control flow, poor readability), contextual roles (e.g., core logic, data transformation, external resource handling), and common defect patterns (e.g., faulty conditionals, fragile error handling, misuse of variables). These results highlight the need for richer, evolution-aware representations of code and provide actionable insights for practitioners seeking to prioritize high-risk methods early in the development lifecycle.", "AI": {"tldr": "ExtremelyBuggy methods (a small subset of code often involved in bug fixes) are much larger and more complex than other methods and account for many bugs. Current machine learning models cannot predict them early due to project differences and data issues. The study suggests more nuanced code analysis and targeted strategies to reduce risk.", "motivation": "Reducing long-term maintenance costs in software projects relies on identifying source code segments that attract recurring bugs. The study is motivated by a lack of systematic understanding of these critical, bug-prone methods.", "method": "The study analyzes over 1.25 million methods from 98 open-source Java projects, defines 'ExtremelyBuggy' methods (involved in multiple bug fixes), and uses both statistical analysis and a thematic qualitative review. Five machine learning models are evaluated for predictive reliability.", "result": "ExtremelyBuggy methods are rare but responsible for a disproportionately high number of bugs. They start life being larger, more complex, and harder to maintain. Existing machine learning approaches cannot reliably predict them early, primarily due to data imbalance and project diversity. Qualitative analysis reveals common patterns and defects shared by these methods.", "conclusion": "Simple metrics and existing ML models are insufficient for early prediction of ExtremelyBuggy methods. Richer, evolution-aware code representations are needed. Practitioners can benefit from focusing on evolving risk factors and visual/contextual cues for method prioritization."}}
{"id": "2511.22921", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.22921", "abs": "https://arxiv.org/abs/2511.22921", "authors": ["Hengyuan Liu", "Xia Song", "Yong Liu", "Zheng Li"], "title": "MBFL-DKMR: Improving Mutation-based Fault Localization through Denoising-based Kill Matrix Refinement", "comment": null, "summary": "Software debugging is a critical and time-consuming aspect of software development, with fault localization being a fundamental step that significantly impacts debugging efficiency. Mutation-Based Fault Localization (MBFL) has gained prominence due to its robust theoretical foundations and fine-grained analysis capabilities. However, recent studies have identified a critical challenge: noise phenomena, specifically the false kill relationships between mutants and tests, which significantly degrade localization effectiveness. While several approaches have been proposed to rectify the final localization results, they do not directly address the underlying noise. In this paper, we propose a novel approach to refine the kill matrix, a core data structure capturing mutant-test relationships in MBFL, by treating it as a signal that contains both meaningful fault-related patterns and high-frequency noise. Inspired by signal processing theory, we introduce DKMR (Denoising-based Kill Matrix Refinement), which employs two key stages: (1) signal enhancement through hybrid matrix construction to improve the signal-to-noise ratio for better denoising, and (2) signal denoising via frequency domain filtering to suppress noise while preserving fault-related patterns. Building on this foundation, we develop MBFL-DKMR, a fault localization framework that utilizes the refined matrix with fuzzy values for suspiciousness calculation. Our evaluation on Defects4J v2.0.0 demonstrates that MBFL-DKMR effectively mitigates the noise and outperforms the state-of-the-art MBFL techniques. Specifically, MBFL-DKMR achieves 129 faults localized at Top-1 compared to 85 for BLMu and 103 for Delta4Ms, with negligible additional computational overhead (0.11 seconds, 0.001\\% of total time). This work highlights the potential of signal processing techniques to enhance the effectiveness of MBFL by refining the kill matrix.", "AI": {"tldr": "This paper presents a new method (DKMR) to clean up noisy data in the core process of mutation-based fault localization using signal processing techniques, resulting in significantly better debugging accuracy with no meaningful speed penalty.", "motivation": "Debugging software is often slow and challenging, with fault localization being a major bottleneck. Mutation-Based Fault Localization (MBFL) is widely used but suffers from noise in the data (false kill relationships), which undermines effectiveness. Existing methods do not address this inherent noise directly.", "method": "The authors propose DKMR (Denoising-based Kill Matrix Refinement), which treats the kill matrix in MBFL as a signal with both informative data and noise. They apply a two-stage process: first, they enhance the signal using hybrid matrix construction to increase the signal-to-noise ratio; second, they denoise the matrix through frequency domain filtering to remove noise while preserving relevant fault patterns. This refined matrix is then used within a new framework, MBFL-DKMR, to calculate suspiciousness scores for fault localization.", "result": "MBFL-DKMR was evaluated on the Defects4J v2.0.0 dataset and showed superior performance compared to current MBFL techniques. It localized 129 faults at Top-1, outperforming BLMu (85 faults) and Delta4Ms (103 faults), with almost no additional computational overhead (only 0.11 seconds).", "conclusion": "Signal processing methods (such as denoising the kill matrix) can significantly improve the accuracy and efficiency of mutation-based fault localization. DKMR demonstrates that addressing noise at its source in the kill matrix yields substantial benefits over state-of-the-art methods."}}
{"id": "2511.23007", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23007", "abs": "https://arxiv.org/abs/2511.23007", "authors": ["Yizheng Wang", "Tao Jiang", "Jinyan Bai", "Zhengbin Zou", "Tiancheng Xue", "Nan Zhang", "Jie Luan"], "title": "A transfer learning approach for automatic conflicts detection in software requirement sentence pairs based on dual encoders", "comment": "22 pages, 7 figures, 3 tables", "summary": "Software Requirement Document (RD) typically contain tens of thousands of individual requirements, and ensuring consistency among these requirements is critical for the success of software engineering projects. Automated detection methods can significantly enhance efficiency and reduce costs; however, existing approaches still face several challenges, including low detection accuracy on imbalanced data, limited semantic extraction due to the use of a single encoder, and suboptimal performance in cross-domain transfer learning. To address these issues, this paper proposes a Transferable Software Requirement Conflict Detection Framework based on SBERT and SimCSE, termed TSRCDF-SS. First, the framework employs two independent encoders, Sentence-BERT (SBERT) and Simple Contrastive Sentence Embedding (SimCSE), to generate sentence embeddings for requirement pairs, followed by a six-element concatenation strategy. Furthermore, the classifier is enhanced by a two-layer fully connected feedforward neural network (FFNN) with a hybrid loss optimization strategy that integrates a variant of Focal Loss, domain-specific constraints, and a confidence-based penalty term. Finally, the framework synergistically integrates sequential and cross-domain transfer learning. Experimental results demonstrate that the proposed framework achieves a 10.4% improvement in both macro-F1 and weighted-F1 scores in in-domain settings, and an 11.4% increase in macro-F1 in cross-domain scenarios.", "AI": {"tldr": "This paper introduces TSRCDF-SS, a dual-encoder-based framework for detecting conflicts in software requirements, which significantly improves accuracy and transfer learning ability over existing methods.", "motivation": "Ensuring consistency among large numbers of software requirements is crucial but challenging due to low detection accuracy on imbalanced data, limited semantic extraction, and poor cross-domain transfer learning performance in existing automated methods.", "method": "The paper proposes TSRCDF-SS, a conflict detection framework that utilizes two independent sentence encoders (SBERT and SimCSE) and a six-element concatenation strategy. The classifier uses a two-layer FFNN optimized with a hybrid loss function that combines a variant of Focal Loss, domain-specific constraints, and a confidence-based penalty. It also incorporates sequential and cross-domain transfer learning techniques.", "result": "Experiments show TSRCDF-SS outperforms existing methods, achieving a 10.4% improvement in both macro-F1 and weighted-F1 scores for in-domain tasks, and an 11.4% improvement in macro-F1 for cross-domain scenarios.", "conclusion": "The proposed TSRCDF-SS framework effectively addresses the challenges of accuracy, semantic extraction, and transfer learning in software requirement conflict detection, delivering significant performance gains in both in-domain and cross-domain contexts."}}
{"id": "2511.23009", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23009", "abs": "https://arxiv.org/abs/2511.23009", "authors": ["D. Sree Yashaswinee", "Gargie Tambe", "Y. Raghu Reddy", "Karthik Vaidhyanathan"], "title": "APDT: A Digital Twin for Assessing Access Point Characteristics in a Network", "comment": "7 pages, 1 framework, 2 figures", "summary": "Digital twins (DT) have emerged as a transformative technology, enabling real-time monitoring, simulations, and predictive maintenance across various domains, though their Application in the networking domain remains underexplored. This paper focuses on issues such as increasing client density and traffic congestion by proposing a digital twin for computer networks. Our Digital Twin, named Access Point Digital Twin (APDT) is used for tracking user behavior and changing bandwidth demands, directly impacting network performance and Quality of Service (QoS) parameters like latency, jitter, etc. APDT captures the real-time state of networks with data from access points (APs), enabling simulation-based analyses and predictive modelling. APDT facilitates the simulation of various what-if scenarios thereby providing a better understanding of various aspects of the network characteristics. We tested APDT on our University network. APDT uses data collected from three access points via the Ruckus SmartZone API and incorporates NS-3 based simulations. The simulation replicates a real-time snapshot from a Ruckus access point and models metrics such as latency and inter-packet transfer time. Additionally, a forecasting model predicts traffic congestion and suggests proactive client offloading, enhancing network management and performance optimization. Preliminary results indicate that APDT can successfully predict short-term traffic surges, leading to improved QoS and reduced traffic congestion.", "AI": {"tldr": "The paper presents APDT, a digital twin for network access points that uses real data and simulations to predict and manage network congestion. Tested on a university network, it shows promise for improving performance and QoS by forecasting surges and recommending client offloading.", "motivation": "There is a lack of exploration of Digital Twin applications in computer networks, despite rising issues such as increasing client density and traffic congestion that can negatively impact network performance and Quality of Service (QoS).", "method": "The paper introduces Access Point Digital Twin (APDT), which uses real-time data from access points gathered via the Ruckus SmartZone API and NS-3 simulations. APDT tracks user behavior and bandwidth demands to analyze and predict network performance. The system is tested on a university network and simulates scenarios based on real access point data, focusing on metrics like latency and inter-packet transfer time. It also incorporates a forecasting model to predict congestion and recommend client offloading.", "result": "Preliminary tests on the university network show that APDT can effectively predict short-term traffic surges and suggest proactive network management strategies, leading to improved QoS and reduced congestion.", "conclusion": "APDT provides an effective digital twin framework for computer networks, utilizing real-time data and simulation-based analysis for predicting congestion and optimizing performance."}}
{"id": "2511.23050", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23050", "abs": "https://arxiv.org/abs/2511.23050", "authors": ["Nikita Repnkiov", "Vladimir Faerman"], "title": "Software for Studying CASCADE Error Correction Protocols in Quantum Communications", "comment": "Reported in Omsk State Technical University, November 13", "summary": "This article addresses the development of quantum communication methods in the context of emerging quantum computing threats and emphasizes the importance of key reconciliation in quantum communication systems. The study focuses on the CASCADE protocol and the design of a software prototype intended for research and educational purposes. A parallel error-correction algorithm based on the actor model was implemented, improving the efficiency of key reconciliation and reducing the amount of exchanged data. Evaluation of the prototype revealed limitations, including the computational cost of message passing, complexity of error handling, and code redundancy due to iterative development. Experimental results confirmed the correct implementation of the core CASCADE algorithms and informed the design of future improvements. Proposed enhancements include redesigning the system architecture, developing interfaces for exporting intermediate data, defining the communication channel as a separate component, and expanding tools for systematic verification and comparative analysis of blind key-reconciliation methods.", "AI": {"tldr": "This paper presents and evaluates a parallelized software prototype of the CASCADE protocol for quantum key reconciliation. While the prototype improved efficiency and demonstrated correct functionality, several implementation limitations were noted. The authors suggest concrete enhancements to guide ongoing research and development in secure quantum communications.", "motivation": "The study is motivated by the rise of quantum computing threats, which require secure quantum communication systems. Key reconciliation is identified as a crucial step in these systems, necessitating efficient and reliable protocols and tools, both for practical implementation and educational use.", "method": "The research focuses on implementing a parallel error-correction algorithm (based on the actor model) within the CASCADE protocol. A software prototype was developed for research and educational applications, and its performance and limitations were evaluated experimentally.", "result": "The prototype successfully implemented the core CASCADE algorithms and improved the efficiency of key reconciliation, reducing the amount of exchanged data. However, there were identified limitations, such as computational cost of message passing, complex error handling, and code redundancy.", "conclusion": "The study validated the parallel error-correction approach and CASCADE implementation, providing insights into future improvements for quantum communication systems. The authors proposed architectural redesign, interface development for data exporting, defining the communication channel separately, and expanding tools for verification and analysis of reconciliation methods."}}
{"id": "2511.23157", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.23157", "abs": "https://arxiv.org/abs/2511.23157", "authors": ["Hana Kataoka", "Jialong Li", "Yutaka Matsuno"], "title": "Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning", "comment": "Accepted by ICSE-SEET (ACM/IEEE 48th International Conference on Software Engineering: Software Engineering Education and Training)", "summary": "As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as \"equalizers,\" boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as \"amplifiers,\" dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities.", "AI": {"tldr": "A two-year study found that advanced LLMs in project-based SE education improve overall student outcomes but also widen the gap between strong and weak students, posing new challenges for educational equity.", "motivation": "As LLMs become increasingly central to software development, it is crucial to investigate how their integration impacts software engineering (SE) education, especially in authentic, open-ended learning environments rather than isolated tasks.", "method": "A two-year longitudinal study was conducted, comparing two cohorts: 2024 students using early free LLMs (n=48) and 2025 students using the latest paid LLMs (n=46), within the context of open-ended Project-Based Learning (PBL) in SE education.", "result": "The latest powerful LLMs serve as both 'equalizers'\u2014improving average student performance, including those previously weak in programming\u2014and as 'amplifiers'\u2014increasing the gap between the strongest and weakest students, thus introducing new challenges for addressing educational inequity.", "conclusion": "Integrating advanced LLMs into SE education offers both significant opportunities for enhancing authenticity and inclusivity, but also presents emerging challenges regarding widening performance disparities that educators must address."}}
{"id": "2511.23159", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23159", "abs": "https://arxiv.org/abs/2511.23159", "authors": ["Bertrand Meyer"], "title": "AI for software engineering: from probable to provable", "comment": null, "summary": "Vibe coding, the much-touted use of AI techniques for programming, faces two overwhelming obstacles: the difficulty of specifying goals (\"prompt engineering\" is a form of requirements engineering, one of the toughest disciplines of software engineering); and the hallucination phenomenon. Programs are only useful if they are correct or very close to correct.\n  The solution? Combine the creativity of artificial intelligence with the rigor of formal specification methods and the power of formal program verification, supported by modern proof tools.", "AI": {"tldr": "Vibe coding faces challenges in requirements clarity and code correctness. The paper suggests solving these by combining AI with formal specification and program verification tools to produce reliable, accurate software.", "motivation": "The motivation is to address two major challenges facing AI-powered programming (termed 'vibe coding'): the difficulty in specifying clear goals through prompt engineering (akin to requirements engineering), and the issue of AI-generated code hallucinating or producing incorrect outputs.", "method": "The paper proposes a solution combining the creativity of AI with the rigor of formal specification methods and leveraging formal program verification, using modern proof tools to ensure correctness.", "result": "By integrating AI techniques with formal methods and verification tools, the approach aims to produce programs that are correct or very close to correct, overcoming the limitations of prompt engineering and reducing hallucinated outputs.", "conclusion": "A hybrid approach, merging AI-driven coding with formal specification and verification, can address the major pitfalls in AI programming and enhance code reliability significantly."}}
{"id": "2511.23213", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23213", "abs": "https://arxiv.org/abs/2511.23213", "authors": ["Samuele Doria", "Eleonora Losiouk"], "title": "GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis", "comment": null, "summary": "Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.\n  We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.\n  On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\\%, and Guardian, an LLM-based UI automator, reaches 17.12\\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\\% and 9.48\\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.\n  Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\\% of the target methods and dynamically reaches 59.86\\% of them.", "AI": {"tldr": "GAPS is a new system for guiding execution to target methods in Android apps, dramatically outperforming previous tools in both speed and coverage, particularly for hard-to-reach, non-GUI code, and is validated on benchmarks and real-world apps.", "motivation": "Current tools for driving execution toward specific target methods in Android apps\u2014especially non-GUI components\u2014are insufficient. This is crucial for tasks such as vulnerability validation, debugging, and behavioral analysis, yet effective solutions are lacking.", "method": "The method presented is GAPS, a system combining static, method-guided call graph analysis with dynamic, interaction-driven execution. Specifically, GAPS uses a lightweight, backward traversal of the call graph, informed by data-flow analysis, to reconstruct paths that reach target methods. These paths are then converted into runtime instructions to guide app exploration.", "result": "On the AndroTest benchmark, GAPS statically identifies paths to 88.24% of target methods in 4.27 seconds per app and dynamically reaches 57.44% of them. This is significantly better than APE (12.82%), GoalExplorer (9.69%), and Guardian (17.12%). Static tools, FlowDroid and DroidReach, also perform worse both in coverage and speed. On top 50 real-world apps, GAPS reconstructs static paths to 62.03% and dynamically reaches 59.86% of methods, with an average static analysis time of 278.9 seconds per app.", "conclusion": "GAPS demonstrates clear improvement over existing dynamic and static tools in reliably reaching target methods within Android applications, especially non-GUI methods. It is both efficient and effective, making it practical for real-world security analysis."}}
{"id": "2511.23302", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23302", "abs": "https://arxiv.org/abs/2511.23302", "authors": ["Hengyuan Liu", "Zheng Li", "Donghua Wang", "Yankai Wu", "Xiang Chen", "Yong Liu"], "title": "FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation", "comment": null, "summary": "Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.", "AI": {"tldr": "This paper introduces MBFL-FLIM, a fault localization framework that uses LLM-based semantic analysis to recognize and mitigate interference mutants in mutation-based fault localization. MBFL-FLIM significantly boosts localization accuracy and robustness compared to traditional and advanced approaches, validated on a large benchmark with strong empirical results.", "motivation": "Mutation-based Fault Localization (MBFL) is hampered by interference mutants, which are non-faulty entities killed by failing tests and thus mislead fault localization. This paper aims to address the challenge of these interference mutants, improve MBFL's effectiveness, and preserve accurate fault-revealing information.", "method": "The authors propose the concept of Fault Localization Interference Mutants (FLIMs) and analyze them under the RIPR (Reachability, Infection, Propagation, Revealability) model to identify interference causes. They introduce a novel method to semantically recognize and mitigate FLIMs by leveraging LLM-based semantic analysis, further enhanced by fine-tuning and confidence estimation techniques. This mitigation is incorporated into suspiciousness score calculations in a new workflow, MBFL-FLIM.", "result": "Empirical experiments on the Defects4J benchmark (395 program versions, eight LLMs) show that MBFL-FLIM outperforms SBFL, MBFL, advanced dynamic feature-based, and recent LLM-based fault localization techniques. MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric and demonstrates robust performance in multi-fault scenarios. Ablation studies confirm the benefits of fine-tuning and confidence estimation.", "conclusion": "MBFL-FLIM effectively addresses the interference mutant problem in mutation-based fault localization using LLM-based semantic analysis and mitigation strategies, significantly improving fault localization accuracy and robustness over previous methods."}}
{"id": "2511.23321", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.23321", "abs": "https://arxiv.org/abs/2511.23321", "authors": ["Yifei Wang", "Jacky Keung", "Zhenyu Mao", "Jingyu Zhang", "Yuchen Cao"], "title": "Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing", "comment": null, "summary": "Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.", "AI": {"tldr": "C2C-MoLA is a new, modular framework for translating charts to code that uses expert routing and efficient parameter updates. It outperforms existing models in accuracy, memory use, and speed, especially on complex charts, and is scalable for practical use.", "motivation": "Chart-to-code generation remains challenging, with shortcomings in cross-type generalization, memory efficiency, and modularity in prior methods. This motivates the need for a framework addressing these issues in automated data visualization.", "method": "The paper introduces C2C-MoLA, a multimodal framework combining Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). MoE uses complexity-aware routing with specialized experts and sparse gating, while LoRA enables efficient parameter tuning. These are integrated with a training strategy aligning routing and semantic accuracy.", "result": "C2C-MoLA improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates training convergence by 20% over existing baselines, especially for complex charts. Optimal model configurations and strong scalability for real-world tasks are also demonstrated.", "conclusion": "C2C-MoLA significantly enhances chart-to-code generation by improving accuracy, memory efficiency, and training speed. It shows scalability and effectiveness, especially for complex chart types, validated by ablation studies."}}
