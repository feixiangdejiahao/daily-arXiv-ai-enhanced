{"id": "2511.09987", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09987", "abs": "https://arxiv.org/abs/2511.09987", "authors": ["Shiv Sundram", "Akhilesh Balasingam", "Nathan Zhang", "Kunle Olukotun", "Fredrik Kjolstad"], "title": "Cyclotron: Compilation of Recurrences to Distributed and Systolic Architectures", "comment": null, "summary": "We present Cyclotron, a framework and compiler for using recurrence equations to express streaming dataflow algorithms, which then get portably compiled to distributed topologies of interlinked processors. Our framework provides an input language of recurrences over logical tensors, which then gets lowered into an intermediate language of recurrences over logical iteration spaces, and finally into programs of send, receive, and computation operations specific to each individual processor. In Cyclotron's IR, programs are optimized such that external memory interactions are confined to the boundaries of the iteration space. Within inner iteration spaces, all data accesses become local: data accesses target values residing in local fast memory or on neighboring processing units, avoiding costly memory movement. We provide a scheduling language allowing users to define how data gets streamed and broadcasted between processors, enabling pipelined execution of computation kernels over distributed topologies of processing elements. We demonstrate the portability of our approach by compiling our IR to a reconfigurable simulator of systolic arrays and chiplet style distributed hardware, as well as to distributed-memory CPU clusters. In the simulated reconfigurable setting, we use our compiler for hardware design space exploration in which link costs and latencies can be specified. In the distributed CPU setting, we show how to use recurrences and our scheduling language to express various matrix multiplication routines (Cannon, SUMMA, PUMMA, weight stationary) and solvers (Triangular solve and Cholesky). For matrix multiplication and the triangular solve, we generate distributed implementations competitive with ScaLAPACK.", "AI": {"tldr": "Cyclotron is a compiler and framework for portably expressing and deploying streaming dataflow algorithms over distributed architectures, offering both performance and hardware flexibility, validated by its competitive results in distributed matrix operations compared to ScaLAPACK.", "motivation": "Streaming dataflow algorithms are critical for high-performance computing on distributed systems, but programming them efficiently, portably, and for various distributed hardware topologies is challenging. There is a need for a unified framework that simplifies the development, optimization, and deployment of these algorithms.", "method": "The authors introduce Cyclotron, a framework and compiler that allows streaming dataflow algorithms to be specified using recurrence equations over logical tensors. Cyclotron lowers these specifications into intermediate representations that map onto distributed processor topologies. Programs are optimized to confine external memory accesses to iteration space boundaries; Cyclotron also offers a scheduling language to control data streaming and broadcasting.", "result": "Cyclotron enables portable compilation to various distributed hardware (systolic arrays, chiplets, distributed-memory CPU clusters) and supports hardware design space exploration. The framework successfully expresses and generates efficient distributed implementations for matrix multiplication routines and solvers, benchmarking competitive performance with ScaLAPACK in certain cases.", "conclusion": "Cyclotron provides an effective and portable means to express and optimize streaming dataflow algorithms, demonstrating competitive performance and flexible deployment across diverse distributed architectures."}}
{"id": "2511.10343", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.10343", "abs": "https://arxiv.org/abs/2511.10343", "authors": ["Alistair O'Brien", "Didier R\u00e9my", "Gabriel Scherer"], "title": "Omnidirectional type inference for ML: principality any way", "comment": "39 pages + appendices", "summary": "The Damas-Hindley-Milner (ML) type system owes its success to principality, the property that every well-typed expression has a unique most general type. This makes inference predictable and efficient. Unfortunately, many extensions of ML (GADTs, higher-rank polymorphism, and static overloading) endanger princpality by introducing _fragile_ constructs that resist principal inference. Existing approaches recover principality through directional inference algorithms, which propagate _known_ type information in a fixed (or static) order (e.g. as in bidirectional typing) to disambiguate such constructs. However, the rigidity of a static inference order often causes otherwise well-typed programs to be rejected.\n  We propose _omnidirectional_ type inference, where type information flows in a dynamic order. Typing constraints may be solved in any order, suspending when progress requires known type information and resuming once it becomes available, using _suspended match constraints_. This approach is straightforward for simply typed systems, but extending it to ML is challenging due to let-generalization. Existing ML inference algorithms type let-bindings (let x = e1 in e2) in a fixed order: type e1, generalize its type, and then type e2. To overcome this, we introduce _incremental instantiation_, allowing partially solved type schemes containing suspended constraints to be instantiated, with a mechanism to incrementally update instances as the scheme is refined.\n  Omnidirectionality provides a general framework for restoring principality in the presence of fragile features. We demonstrate its versatility on two fundamentally different features of OCaml: static overloading of record labels and datatype constructors and semi-explicit first-class polymorphism. In both cases, we obtain a principal type inference algorithm that is more expressive than OCaml's current typechecker.", "AI": {"tldr": "Omnidirectional type inference dynamically resolves type constraints, overcoming rigidity in traditional ML inference and restoring principality for advanced features like overloading and polymorphism, resulting in more expressive principal type inference than current OCaml implementations.", "motivation": "The motivation is to address the loss of principality in ML type systems when extended with fragile features like GADTs, higher-rank polymorphism, and static overloading. Existing inference algorithms using fixed orders (directional) reject valid programs because they are too rigid.", "method": "The paper proposes omnidirectional type inference, which allows dynamic order for solving typing constraints. Progress on constraints can be suspended and resumed as type information becomes available, using suspended match constraints. The extension to ML's let-generalization is enabled by incremental instantiation: type schemes with suspended constraints are partially instantiated and updated incrementally.", "result": "The proposed framework restores principality for features that previously endangered it. The method is applicable to diverse OCaml features (static overloading and first-class polymorphism) and yields principal type inference algorithms more expressive than OCaml's existing typechecker.", "conclusion": "Omnidirectional type inference provides a general solution to the principality problems caused by fragile extensions to ML, supporting more expressive type inference without sacrificing principality."}}
{"id": "2511.10361", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.10361", "abs": "https://arxiv.org/abs/2511.10361", "authors": ["Rodrigo Mesquita", "Bernardo Toninho"], "title": "Lazy Linearity for a Core Functional Language", "comment": "Extended version of POPL 2026 paper", "summary": "Traditionally, in linearly typed languages, consuming a linear resource is synonymous with its syntactic occurrence in the program. However, under the lens of non-strict evaluation, linearity can be further understood semantically, where a syntactic occurrence of a resource does not necessarily entail using that resource when the program is executed. While this distinction has been largely unexplored, it turns out to be inescapable in Haskell's optimising compiler, which heavily rewrites the source program in ways that break syntactic linearity but preserve the program's semantics. We introduce Linear Core, a novel system which accepts the lazy semantics of linearity statically and is suitable for lazy languages such as the Core intermediate language of the Glasgow Haskell Compiler. We prove that Linear Core is sound, guaranteeing linear resource usage, and that multiple optimising transformations preserve linearity in Linear Core while failing to do so in Core. We have implemented Linear Core as a compiler plugin to validate the system against linearity-heavy libraries, including linear-base.", "AI": {"tldr": "Linear Core is a new system ensuring robust linear type guarantees in lazy languages like Haskell, even in the presence of aggressive compiler optimizations, and is validated by implementation as a compiler plugin.", "motivation": "Traditional linearly typed languages treat the use of a linear resource as its appearance in code, but in non-strict (lazy) languages like Haskell, this syntactic approach can break due to aggressive compiler optimizations. There is a need to reconcile linear type guarantees with lazy evaluation and real-world compiler behaviors.", "method": "The authors introduce 'Linear Core', a static system that upholds the semantics of linearity in lazy, non-strict languages like Haskell. They prove the soundness of Linear Core, show that it maintains linear resource usage through optimizing transformations, and implement it as a compiler plugin to test with linearity-intensive libraries.", "result": "Linear Core is sound and guarantees linear resource usage under lazy evaluation. Optimizing compiler transformations that break linearity in standard Core still preserve it in Linear Core. The Linear Core plugin successfully validates against libraries such as linear-base.", "conclusion": "Linear Core provides a principled foundation for linearity that is robust to optimizations in lazy languages like Haskell, solving limitations of prior approaches."}}
{"id": "2511.10374", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.10374", "abs": "https://arxiv.org/abs/2511.10374", "authors": ["Somashekaracharya G Bhaskaracharya", "Aravind Acharya", "Bastian Hagedorn", "Vinod Grover"], "title": "Modeling Layout Abstractions Using Integer Set Relations", "comment": null, "summary": "Modern deep learning compilers rely on layout abstractions to manage the complex mapping between logical tensor structures and physical memory arrangements. CuTe layouts and Triton linear layouts are widely adopted industry standards. However, these layout systems operate independently with distinct mathematical underpinnings, preventing unified formal analysis and cross-system reasoning. We bridge this gap by introducing a novel approach that leverages the Integer Set Library (ISL) to create a unified mathematical representation for both layout systems through integer set relations, thereby enabling rigorous formal analysis, correctness verification, and the foundation for future cross-system optimization strategies. Our approach models CuTe layouts through integer set relations that encode the transformation from multi-dimensional coordinates to linear indices using stride-based calculations, including sophisticated swizzle operations that perform bit-level manipulations for enhanced memory access patterns. For Triton linear layouts, we construct integer set relations that model the binary vector space transformations where arithmetic operations follow finite field F_2 rules. We implement a complete suite of layout manipulation algorithms for composition, inversion, complement using built-in operations in ISL to ensure mathematical correctness and preserve layout semantics. Experimental evaluation shows that the system handles the full spectrum of layout complexity, from elementary identity transformations to sophisticated multi-dimensional tensor arrangements with complex stride configurations and swizzle patterns, validating the mathematical modeling approach across different layout paradigms.", "AI": {"tldr": "The paper presents an ISL-based framework to unify analysis and manipulation of tensor layouts in deep learning compilers, bridging CuTe and Triton systems and enabling cross-system reasoning.", "motivation": "CuTe and Triton tensor layouts are prevalent in industry but have incompatible mathematical bases, preventing unified reasoning. A single formal representation enables better analysis, verification, and optimization across systems, addressing a key limitation in current compiler infrastructure.", "method": "Integer Set Library (ISL) is used to model multi-dimensional-to-linear index transformations for both CuTe (stride-based with swizzle) and Triton (binary vector space) layouts. Layout manipulation algorithms are built using ISL\u2019s operations and validated through experimental testing.", "result": "This paper introduces a unified mathematical framework for tensor layout management in deep learning compilers, using the Integer Set Library (ISL) to represent both CuTe and Triton layout systems. The system models transformations and manipulations, including composition, inversion, and complement, for layouts with complex stride and swizzle patterns. Experiments demonstrate the capability to handle a wide range of layout complexities and validate the mathematical modeling across different paradigms.", "conclusion": "The unified approach facilitates formal analysis and correctness verification of layout transformations, supports complex tensor arrangements, and lays groundwork for future cross-system optimizations in deep learning compilers."}}
{"id": "2511.09794", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09794", "abs": "https://arxiv.org/abs/2511.09794", "authors": ["Wasique Islam Shafin", "Md Nakhla Rafi", "Zhenhao Li", "Tse-Hsun Chen"], "title": "Evaluating Software Process Models for Multi-Agent Class-Level Code Generation", "comment": null, "summary": "Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\\% for GPT-4o-mini and -39.8\\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation.", "AI": {"tldr": "This paper finds that structured multi-agent LLM workflows (using Waterfall-style processes) improve code cleanliness and maintainability but often reduce correctness\u2014except for certain models. Workflow discipline shifts error types and influences collaboration, revealing fundamental trade-offs for LLM-driven code generation beyond single-agent approaches.", "motivation": "Most prior research in LLM-driven code generation centers on single-agent, function-level scenarios. However, real-world software development requires maintainable, well-structured code produced through team processes and specialized roles. This paper addresses how multi-agent workflows, mimicking common software development cycles, affect code generation quality and characteristics.", "method": "The study simulates a Waterfall-style software development cycle, involving Requirement, Design, Implementation, and Testing stages, using three different LLMs (GPT-4o-mini, DeepSeek-Chat, Claude-3.5-Haiku). 100 Python tasks from the ClassEval benchmark are used to assess the impact of multi-agent collaboration and process structure on class-level code generation.", "result": "Multi-agent workflows reorganize model output rather than uniformly improving it. The Waterfall-style process yields cleaner and more maintainable code but reduces functional correctness for GPT-4o-mini (-37.8%) and DeepSeek-Chat (-39.8%), while Claude-3.5-Haiku shows improvement (+9.5%). Structural errors decrease, but semantic and validation errors rise. Testing stage increases test coverage but brings new reasoning errors; Requirement and Design have less appreciable impact.", "conclusion": "Software process structure, such as staged multi-agent collaboration, profoundly affects how LLMs generate and validate code, altering failure patterns and quality characteristics. There is a fundamental trade-off between strict workflow discipline (which favors maintainability) and flexible problem-solving (which favors correctness), highlighting new challenges for applying LLMs in collaborative software engineering."}}
{"id": "2511.10565", "categories": ["cs.PL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10565", "abs": "https://arxiv.org/abs/2511.10565", "authors": ["Rahul Krishnan", "Ashley Samuelson", "Emily Yao", "Ethan Cecchetti"], "title": "zkStruDul: Programming zkSNARKs with Structural Duality", "comment": null, "summary": "Non-Interactive Zero Knowledge (NIZK) proofs, such as zkSNARKS, let one prove knowledge of private data without revealing it or interacting with a verifier. While existing tooling focuses on specifying the predicate to be proven, real-world applications optimize predicate definitions to minimize proof generation overhead, but must correspondingly transform predicate inputs. Implementing these two steps separately duplicates logic that must precisely match to avoid catastrophic security flaws. We address this shortcoming with zkStruDul, a language that unifies input transformations and predicate definitions into a single combined abstraction from which a compiler can project both procedures, eliminating duplicate code and problematic mismatches. zkStruDul provides a high-level abstraction to layer on top of existing NIZK technology and supports important features like recursive proofs. We provide a source-level semantics and prove its behavior is identical to the projected semantics, allowing straightforward standard reasoning.", "AI": {"tldr": "zkStruDul is a new language for NIZK proofs that merges input transformation and predicate definition, eliminating duplicated code and preventing security issues. It supports advanced features and comes with formal guarantees, making NIZK applications easier and safer to build.", "motivation": "In non-interactive zero-knowledge (NIZK) proof systems, applications often need to optimize predicate definitions for performance, which requires transforming predicate inputs. Existing tools treat input transformation and predicate logic separately, leading to duplicated code and a risk of mismatches that may cause security vulnerabilities.", "method": "The authors introduce zkStruDul, a programming language that unifies input transformations and predicate definitions into a single abstraction. This language allows a compiler to automatically generate both procedures from the same source code, eliminating duplicate logic and potential errors.", "result": "zkStruDul enables applications to write high-level abstractions over existing NIZK technologies, including features such as recursive proofs. The language provides source-level semantics and proves that its execution matches the semantics of the compiled procedures, ensuring reliable reasoning.", "conclusion": "zkStruDul streamlines NIZK proof development by merging input transformation and predicate definition into one step, reducing code duplication and minimizing security risks. Its design and formal guarantees facilitate safer and more efficient adoption of NIZK proofs in real-world applications."}}
{"id": "2511.09964", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09964", "abs": "https://arxiv.org/abs/2511.09964", "authors": ["Noah van der Vleuten", "Anthony Flores", "Shray Mathur", "Max Rakitin", "Thomas Hopkins", "Kevin G. Yager", "Esther H. R. Tsai"], "title": "EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines", "comment": null, "summary": "Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI.", "AI": {"tldr": "The paper presents EnvTrace, a simulation-based method to evaluate LLM-generated instrument control code using digital twins. Testing 30+ LLMs, the study finds that top models can approach human-level correctness, suggesting LLMs and digital twins could work together for autonomous AI control.", "motivation": "Standard algorithmic benchmarks for large language models (LLMs) are inadequate for evaluating instrument control tasks, as physical systems require more complex and dynamic assessment methods. There is a need for better evaluation techniques that capture real-world execution and correctness for such control logic.", "method": "The paper introduces EnvTrace, a simulation-based evaluation framework that utilizes digital twins to assess code generated by LLMs for instrument control. EnvTrace evaluates code execution traces instead of relying on static tests, aligning these traces against reference behavior for semantic equivalence.", "result": "Over 30 LLMs were tested using this approach on a beamline control-logic digital twin. The multi-faceted scoring revealed that many advanced LLMs can nearly match humans in generating functional and correct instrument control code rapidly.", "conclusion": "EnvTrace demonstrates that LLMs, when evaluated in realistic simulation environments, can perform at near human-level for control code generation. This framework supports a vision where LLMs and digital twins interact symbiotically for intuitive, reliable, and autonomous control in embodied AI applications."}}
{"id": "2511.10049", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10049", "abs": "https://arxiv.org/abs/2511.10049", "authors": ["Divyanshu Saxena", "Rishikesh Maurya", "Xiaoxuan Ou", "Gagan Somashekar", "Shachee Mishra Gupta", "Arun Iyer", "Yu Kang", "Chetan Bansal", "Aditya Akella", "Saravan Rajmohan"], "title": "Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents", "comment": "5 pages", "summary": "The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements.", "AI": {"tldr": "The paper introduces a benchmark generation method leveraging developer intent and LLMs for better AI agent evaluation in changing enterprise environments, demonstrated with a service migration case study.", "motivation": "Current fixed benchmarks are inadequate for enterprise-scale AI agent evaluation due to evolving requirements and lack of ground-truth examples.", "method": "Development of a benchmark generation process using semi-structured developer documents and LLMs to dynamically generate benchmarks.", "result": "Successful application in a large public enterprise for service migration, enabling dynamic evaluation and rapid feedback for improving AI agents.", "conclusion": "The proposed benchmark generation process leads to a maintainable and adaptive evaluation framework for AI agents, especially in evolving enterprise settings."}}
{"id": "2511.10271", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10271", "abs": "https://arxiv.org/abs/2511.10271", "authors": ["Xin Sun", "Daniel St\u00e5hl", "Kristian Sandahl", "Christoph Kessler"], "title": "Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics", "comment": null, "summary": "In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.", "AI": {"tldr": "LLM-generated code is mostly evaluated on whether it works, not on how well it works. Academia focuses mostly on security and performance, industry cares more about maintainability, and the models often show trade-offs between various quality aspects. There's a need for better quality assurance in LLM-generated code.", "motivation": "There is a growing use of large language models (LLMs) for code generation, but evaluations have mostly focused on functional correctness rather than on non-functional quality aspects (such as security, maintainability, and performance efficiency). The paper is motivated by the lack of systematic understanding and evaluation of these non-functional qualities in code generated by LLMs.", "method": "The study used three complementary approaches: (1) a systematic review of 108 papers on the topic, (2) two industry workshops involving practitioners from various organizations, and (3) an empirical analysis of real-world software patches generated by three LLMs, focusing on their non-functional qualities.", "result": "The literature survey showed that security and performance efficiency are dominant topics in academic works, whereas maintainability and similar qualities are relatively understudied. In contrast, industry practitioners prioritize maintainability and readability and are concerned about technical debt from generated code. The empirical results demonstrated that improving one non-functional quality often compromises another, and there is high variance in runtime and memory usage across models and optimization techniques. These findings expose a disconnect among academic research, industry practice, and actual LLM model performance.", "conclusion": "There is a significant mismatch between the aspects of code quality emphasized in the academic literature, those prioritized by industry, and what LLMs actually deliver. There is an urgent need for integrated quality assurance mechanisms in LLM-based code generation workflows to ensure output code meets higher-quality standards, not just functional correctness."}}
{"id": "2511.10323", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10323", "abs": "https://arxiv.org/abs/2511.10323", "authors": ["D\u00e1vid K\u00f3sz\u00f3", "Tam\u00e1s Aladics", "Rudolf Ferenc", "P\u00e9ter Heged\u0171s"], "title": "A Large-Scale Collection Of (Non-)Actionable Static Code Analysis Reports", "comment": "Under publication to Nature Scientific Data journal", "summary": "Static Code Analysis (SCA) tools, while invaluable for identifying potential coding problems, functional bugs, or vulnerabilities, often generate an overwhelming number of warnings, many of which are non-actionable. This overload of alerts leads to ``alert fatigue'', a phenomenon where developers become desensitized to warnings, potentially overlooking critical issues and ultimately hindering productivity and code quality. Analyzing these warnings and training machine learning models to identify and filter them requires substantial datasets, which are currently scarce, particularly for Java. This scarcity impedes efforts to improve the accuracy and usability of SCA tools and mitigate the effects of alert fatigue. In this paper, we address this gap by introducing a novel methodology for collecting and categorizing SCA warnings, effectively distinguishing actionable from non-actionable ones. We further leverage this methodology to generate a large-scale dataset of over 1 million entries of Java source code warnings, named NASCAR: (Non-)Actionable Static Code Analysis Reports. To facilitate follow-up research in this domain, we make both the dataset and the tools used to generate it publicly available.", "AI": {"tldr": "The authors introduce a systematic way to categorize SCA warnings and release a large dataset of Java warnings, facilitating research to improve code analysis tools and reduce developer alert fatigue.", "motivation": "Existing Static Code Analysis (SCA) tools generate too many warnings, with many not helpful for developers (non-actionable warnings), causing alert fatigue and reduced code quality. Improvement efforts are hampered by the lack of large, public datasets for SCA warnings, especially for Java.", "method": "The paper proposes a methodology for systematically collecting and categorizing SCA warnings, separating actionable from non-actionable ones. This approach is then used to build a comprehensive dataset.", "result": "A large-scale dataset (over 1 million Java SCA warnings) is produced and made publicly available. The tools for generating the dataset are also shared to promote further research.", "conclusion": "The paper fills a key need by providing a methodology and dataset for studying SCA warnings, supporting the development of better alert filtering and machine learning approaches to mitigate alert fatigue."}}
{"id": "2511.10326", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10326", "abs": "https://arxiv.org/abs/2511.10326", "authors": ["Shuangyu Lyu", "Chuan Luo", "Ruizhi Shi", "Wei Wu", "Chanjuan Liu", "Chunming Hu"], "title": "Towards Comprehensive Sampling of SMT Solutions", "comment": null, "summary": "This work focuses on effectively generating diverse solutions for satisfiability modulo theories (SMT) formulas, targeting the theories of bit-vectors, arrays, and uninterpreted functions, which is a critical task in software and hardware testing. Generating diverse SMT solutions helps uncover faults and detect safety violations during the verification and testing process, resulting in the SMT sampling problem, i.e., constructing a small number of solutions while achieving comprehensive coverage of the constraint space. While high coverage is crucial for exploring system behaviors, reducing the number of solutions is of great importance, as excessive solutions increase testing time and resource usage, undermining efficiency. In this work, we introduce PanSampler, a novel SMT sampler that achieves high coverage with a small number of solutions. It incorporates three novel techniques, i.e., diversity-aware SMT algorithm, abstract syntax tree (AST)-guided scoring function and post-sampling optimization technology, enhancing its practical performance. It iteratively samples solutions, evaluates candidates, and employs local search to refine solutions, ensuring high coverage with a small number of samples. Extensive experiments on practical benchmarks demonstrate that PanSampler exhibits a significantly stronger capability to reach high target coverage, while requiring fewer solutions than current samplers to achieve the same coverage level. Furthermore, our empirical evaluation on practical subjects, which are collected from real-world software systems, shows that PanSampler achieves higher fault detection capability and reduces the number of required test cases from 32.6\\% to 76.4\\% to reach the same fault detection effectiveness, leading to a substantial improvement in testing efficiency. PanSampler advances SMT sampling, reducing the cost of software testing and hardware verification.", "AI": {"tldr": "PanSampler is a novel SMT sampling tool that maximizes coverage and fault detection with fewer test cases, thanks to diversity-aware algorithms and optimization techniques. It demonstrates significant efficiency gains over existing methods, reducing testing costs.", "motivation": "Efficiently generating diverse solutions for SMT formulas is essential in software and hardware testing to uncover faults and safety violations. Traditional approaches often require generating excessive solutions to achieve high coverage, which is inefficient and resource-consuming.", "method": "The paper introduces PanSampler, an SMT sampler utilizing three key techniques: a diversity-aware SMT algorithm, abstract syntax tree (AST)-guided scoring, and post-sampling optimization. It samples solutions iteratively, evaluates candidate diversity, and refines solutions via local search to optimize coverage with fewer samples.", "result": "Experimental results on real-world benchmarks demonstrate that PanSampler achieves higher target coverage and fault detection with significantly fewer solutions compared to existing samplers. It reduces the number of required test cases by 32.6% to 76.4% to maintain effectiveness, thereby improving testing efficiency.", "conclusion": "PanSampler substantially improves the efficiency and effectiveness of SMT sampling, reducing the test case count and resource usage while maintaining or increasing coverage and fault detection capabilities. This leads to lower costs in software testing and hardware verification."}}
