<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Roadmap for Tamed Interactions with Large Language Models](https://arxiv.org/abs/2510.24819)
*Vincenzo Scotti,Jan Keim,Tobias Hey,Andreas Metzger,Anne Koziolek,Raffaela Mirandola*

Main category: cs.SE

TL;DR: LLMs are widely adopted but unreliable. This paper proposes a scripting language (LSL) to better control, validate, and structure LLM interactions, aiming to increase reliability and trustworthiness in AI-driven software applications.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in AI-powered software, but their unreliability and tendency to hallucinate limit adoption, especially in automated workflows. Software engineering offers formal tools to specify and validate outputs, but there is a lack of integrated solutions for managing LLM behavior effectively.

Method: The paper proposes the development of a Domain Specific Language (DSL) called LLM Scripting Language (LSL) to script and control interactions with LLMs. LSL is intended to enforce structure, allow validation/verification, and provide explainability for LLM outputs.

Result: The paper presents a vision for LSL, highlighting how it could address current challenges by providing programmable, reliable, and trustworthy interactions with LLMs, independent from model training or implementation details.

Conclusion: Making LLM interaction programmable through a dedicated scripting language can increase reliability, robustness, and trustworthiness of AI applications by integrating specification, verification, and validation techniques.

Abstract: We are witnessing a bloom of AI-powered software driven by Large Language
Models (LLMs). Although the applications of these LLMs are impressive and
seemingly countless, their unreliability hinders adoption. In fact, the
tendency of LLMs to produce faulty or hallucinated content makes them
unsuitable for automating workflows and pipelines. In this regard, Software
Engineering (SE) provides valuable support, offering a wide range of formal
tools to specify, verify, and validate software behaviour. Such SE tools can be
applied to define constraints over LLM outputs and, consequently, offer
stronger guarantees on the generated content. In this paper, we argue that the
development of a Domain Specific Language (DSL) for scripting interactions with
LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based
applications. Currently, LLMs and LLM-based software still lack reliability,
robustness, and trustworthiness, and the tools or frameworks to cope with these
issues suffer from fragmentation. In this paper, we present our vision of LSL.
With LSL, we aim to address the limitations above by exploring ways to control
LLM outputs, enforce structure in interactions, and integrate these aspects
with verification, validation, and explainability. Our goal is to make LLM
interaction programmable and decoupled from training or implementation.

</details>


### [2] [Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification](https://arxiv.org/abs/2510.24749)
*Aofan Liu,Shiyuan Song,Haoxuan Li,Cehao Yang,Yiyan Qi*

Main category: cs.SE

TL;DR: This paper tackles the difficulty of finding code relevant to specific change requests in large repositories, introducing a new benchmark and an advanced retrieval model that significantly outperforms current methods.


<details>
  <summary>Details</summary>
Motivation: Existing code search systems mainly focus on matching functions and struggle to understand broader cross-component change intents within large code repositories, especially in response to specific change requests.

Method: The paper introduces RepoAlign-Bench, a repository-level benchmark with 52,000 annotated instances for code retrieval tasks related to change requests. It also proposes ReflectCode, a dual-tower model enhanced with adversarial reflection, disentangled encoders for code and documentation, and LLM-guided integration of syntactic and semantic patterns.

Result: ReflectCode achieves a 12.2% improvement in Top-5 Accuracy and a 7.1% improvement in Recall compared to previous state-of-the-art models.

Conclusion: RepoAlign-Bench benchmarks context-aware repository-level code retrieval, and ReflectCode sets a new standard in addressing change-driven search scenarios, outperforming existing baselines and promoting holistic code understanding.

Abstract: The escalating complexity of modern codebases has intensified the need for
retrieval systems capable of interpreting cross-component change intents, a
capability fundamentally absent in conventional function-level search
paradigms. While recent studies have improved the alignment between natural
language queries and code snippets, retrieving contextually relevant code for
specific change requests remains largely underexplored. To address this gap, we
introduce RepoAlign-Bench, the first benchmark specifically designed to
evaluate repository-level code retrieval under change request driven scenarios,
encompassing 52k annotated instances. This benchmark shifts the retrieval
paradigm from function-centric matching to holistic repository-level reasoning.
Furthermore, we propose ReflectCode, an adversarial reflection augmented
dual-tower architecture featuring disentangled code_encoder and doc_encoder
components. ReflectCode dynamically integrates syntactic patterns, function
dependencies, and semantic expansion intents through large language model
guided reflection. Comprehensive experiments demonstrate that ReflectCode
achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over
state-of-the-art baselines, establishing a new direction for context-aware code
retrieval.

</details>


### [3] [Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering](https://arxiv.org/abs/2510.24799)
*Filipe R. Cogo,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Compiler.next is a new compiler that automates software generation from human intent using dynamic optimization and search, aiming to make AI-powered development more accessible, scalable, and efficient.


<details>
  <summary>Details</summary>
Motivation: AI-assisted software engineering has great potential but currently faces major limitations, such as cognitive overload, inefficient tool integration, and limited AI copilot functionality.

Method: The authors propose Compiler.next, a search-based compiler that dynamically optimizes cognitive architectures and system parameters by automatically generating working software from human-written intents. The approach balances objectives like accuracy, cost, and latency.

Result: The paper presents the architecture of Compiler.next, showing how it can democratize software development, lower technical barriers, and facilitate scalable, adaptable, reliable AI-powered software. It also discusses challenges and lays out a roadmap for future developments.

Conclusion: Compiler.next represents a foundational shift towards fully automated, search-driven software development, enabling faster innovation and more efficient, accessible AI-native systems for a broader range of users.

Abstract: The rapid advancement of AI-assisted software engineering has brought
transformative potential to the field of software engineering, but existing
tools and paradigms remain limited by cognitive overload, inefficient tool
integration, and the narrow capabilities of AI copilots. In response, we
propose Compiler.next, a novel search-based compiler designed to enable the
seamless evolution of AI-native software systems as part of the emerging
Software Engineering 3.0 era. Unlike traditional static compilers,
Compiler.next takes human-written intents and automatically generates working
software by searching for an optimal solution. This process involves dynamic
optimization of cognitive architectures and their constituents (e.g., prompts,
foundation model configurations, and system parameters) while finding the
optimal trade-off between several objectives, such as accuracy, cost, and
latency. This paper outlines the architecture of Compiler.next and positions it
as a cornerstone in democratizing software development by lowering the
technical barrier for non-experts, enabling scalable, adaptable, and reliable
AI-powered software. We present a roadmap to address the core challenges in
intent compilation, including developing quality programming constructs,
effective search heuristics, reproducibility, and interoperability between
compilers. Our vision lays the groundwork for fully automated, search-driven
software development, fostering faster innovation and more efficient AI-driven
systems.

</details>


### [4] [VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus](https://arxiv.org/abs/2510.25015)
*Chuyue Sun,Yican Sun,Daneshvar Amrollahi,Ethan Zhang,Shuvendu Lahiri,Shan Lu,David Dill,Clark Barrett*

Main category: cs.SE

TL;DR: VeriStruct is a new framework that lets AI help verify entire Rust data structure modules, not just single functions. By guiding syntax and automatically fixing mistakes, it succeeded in verifying nearly all functions across 11 tested modules, showing important progress in AI-assisted code verification.


<details>
  <summary>Details</summary>
Motivation: Existing AI-assisted verification tools are limited to verifying single functions and struggle with the complexity of real-world data structure modules, particularly due to annotation syntax and semantics challenges in the Verus verification system.

Method: The authors developed VeriStruct, a framework with a planner module that automates the process of generating abstractions, type invariants, specifications, and proof code. It uses syntax-guidance within prompts and a repair mechanism to correct annotation errors automatically.

Result: VeriStruct was evaluated on eleven Rust data structure modules, successfully verifying 128 out of 129 functions (99.2%), demonstrating its effectiveness at automating formal verification for more complex code.

Conclusion: VeriStruct significantly advances AI-assisted formal verification beyond single functions, by enabling reliable verification of complex data structure modules and overcoming syntax and semantic challenges. It brings the field closer to robust, automated formal verification.

Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated
verification from single functions to more complex data structure modules in
Verus. VeriStruct employs a planner module to orchestrate the systematic
generation of abstractions, type invariants, specifications, and proof code. To
address the challenge that LLMs often misunderstand Verus' annotation syntax
and verification-specific semantics, VeriStruct embeds syntax guidance within
prompts and includes a repair stage to automatically correct annotation errors.
In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on
ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in
total. These results represent an important step toward the goal of automatic
AI-assisted formal verification.

</details>


### [5] [Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study](https://arxiv.org/abs/2510.25016)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: Manual requirements engineering is being revolutionized by AI, but automation raises issues like bias and ethics. This paper introduces a Human-AI hybrid framework (HARE-SM) that combines automated analysis with human checks, demonstrates a prototype, and sets out a research plan for applying AI to RE in collaborative settings.


<details>
  <summary>Details</summary>
Motivation: Current requirements engineering (RE) depends heavily on manual, error-prone work, but emerging AI technologies offer transformative opportunities to automate and enhance this process. However, using AI introduces risks such as bias, lack of explainability, and ethical challenges.

Method: The study proposes the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that integrates AI-driven analysis (LLMs, NLP, generative AI) with human oversight. The methodology includes preparing RE datasets, fine-tuning AI models, and creating collaborative human-AI workflows. A preliminary prototype of the framework has been implemented.

Result: The study presents the conceptual HARE-SM model and a working prototype, demonstrating initial viability for applying intelligent data science techniques to semi-structured and unstructured RE data. It sets a research agenda and practical design direction.

Conclusion: AI, particularly through frameworks like HARE-SM, can enhance requirements engineering by combining automated analysis with essential human oversight and ethical safeguards. The study provides an early prototype and a roadmap for further research.

Abstract: The future of Requirements Engineering (RE) is increasingly driven by
artificial intelligence (AI), reshaping how we elicit, analyze, and validate
requirements. Traditional RE is based on labor-intensive manual processes prone
to errors and complexity. AI-powered approaches, specifically large language
models (LLMs), natural language processing (NLP), and generative AI, offer
transformative solutions and reduce inefficiencies. However, the use of AI in
RE also brings challenges like algorithmic bias, lack of explainability, and
ethical concerns related to automation. To address these issues, this study
introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that
integrates AI-driven analysis with human oversight to improve requirements
elicitation, analysis, and validation. The model emphasizes ethical AI use
through transparency, explainability, and bias mitigation. We outline a
multi-phase research methodology focused on preparing RE datasets, fine-tuning
AI models, and designing collaborative human-AI workflows. This preliminary
study presents the conceptual framework and early-stage prototype
implementation, establishing a research agenda and practical design direction
for applying intelligent data science techniques to semi-structured and
unstructured RE data in collaborative environments.

</details>


### [6] [Automating Benchmark Design](https://arxiv.org/abs/2510.25039)
*Amanda Dsouza,Harit Vishwakarma,Zhengyang Qi,Justin Bauer,Derek Pham,Thomas Walshe,Armin Parchami,Frederic Sala,Paroma Varma*

Main category: cs.SE

TL;DR: Static LLM benchmarks quickly become outdated and dynamic ones are costly. BeTaL uses LLMs to automate and tune benchmark design, making benchmarks with more accurate difficulty levels much more efficiently than previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for LLMs, primarily static benchmarks, rapidly become obsolete while dynamic benchmarks, although superior, are costly and difficult to maintain. The field needs a better solution for continuously and efficiently assessing evolving LLM capabilities.

Method: The paper proposes BeTaL (Benchmark Tuning with an LLM-in-the-loop), a framework that automates dynamic benchmark design. It uses environment design principles, parameterizes benchmark templates, and employs LLMs to efficiently explore and generate benchmarks with specific properties (e.g., difficulty, realism).

Result: BeTaL creates benchmarks with difficulty levels closer to those desired, with average deviations between 5.3% and 13.2%, outperforming baseline methods by 2-4 times. The framework is validated through the creation of two new benchmarks and the extension of an existing one.

Conclusion: BeTaL provides an efficient, LLM-driven approach to dynamic benchmark creation, helping keep evaluation methods relevant and accurate as LLM capabilities evolve. It significantly reduces the gap between intended and achieved benchmark difficulty.

Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents
has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are
the primary tool for assessing model capabilities, but these quickly become
saturated. In contrast, dynamic benchmarks evolve alongside the models they
evaluate, but are expensive to create and continuously update. To address these
challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a
framework that leverages environment design principles to automate the process
of dynamic benchmark design. BeTaL works by parameterizing key design choices
in base benchmark templates and uses LLMs to reason through the resulting
parameter space to obtain target properties (such as difficulty and realism) in
a cost-efficient manner. We validate this approach on its ability to create
benchmarks with desired difficulty levels. Using BeTaL, we create two new
benchmarks and extend a popular agentic benchmark $\tau$-bench. Extensive
evaluation on these three tasks and multiple target difficulty levels shows
that BeTaL produces benchmarks much closer to the desired difficulty, with
average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the
baselines.

</details>


### [7] [Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection](https://arxiv.org/abs/2510.25057)
*Robin Maisch,Larissa Schmid,Timur Sağlam,Nils Niehues*

Main category: cs.SE

TL;DR: The paper proposes a code property graph-based framework that greatly improves the ability to detect plagiarism in student code even when advanced refactoring-based obfuscation techniques are used.


<details>
  <summary>Details</summary>
Motivation: Plagiarism detection in programming education is becoming harder due to increasingly advanced obfuscation techniques, especially refactoring-based methods that preserve program behavior while changing code structure.

Method: The paper introduces a novel framework that uses code property graphs and graph transformations to improve existing plagiarism detectors, making them more robust against refactoring-based obfuscation techniques.

Result: Comprehensive tests on real-world student code, obfuscated using both algorithmic and AI-based methods, show that the new approach significantly enhances the detection of plagiarized code.

Conclusion: Leveraging code property graphs and graph transformation techniques offers a substantial advancement in detecting structurally obfuscated (refactored) plagiarized code in educational settings.

Abstract: Plagiarism detection in programming education faces growing challenges due to
increasingly sophisticated obfuscation techniques, particularly automated
refactoring-based attacks. While code plagiarism detection systems used in
education practice are resilient against basic obfuscation, they struggle
against structural modifications that preserve program behavior, especially
caused by refactoring-based obfuscation. This paper presents a novel and
extensible framework that enhances state-of-the-art detectors by leveraging
code property graphs and graph transformations to counteract refactoring-based
obfuscation. Our comprehensive evaluation of real-world student submissions,
obfuscated using both algorithmic and AI-based obfuscation attacks,
demonstrates a significant improvement in detecting plagiarized code.

</details>


### [8] [Adaptive Proof Refinement with LLM-Guided Strategy Selection](https://arxiv.org/abs/2510.25103)
*Minghai Lu,Zhe Zhou,Danning Xie,Songlin Jia,Benjamin Delaware,Tianyi Zhang*

Main category: cs.SE

TL;DR: Formal verification is hard to scale, even with LLM help, because existing methods use fixed refinement strategies. Adapt proposes a dynamic LLM-guided approach to select better strategies depending on the context, resulting in substantially higher theorem-proving success rates and showing good generalizability across multiple LLMs.


<details>
  <summary>Details</summary>
Motivation: Formal verification via theorem proving is critical for software correctness but is challenging to scale due to manual labor and expertise required. While LLMs offer hope, their proofs often require iterative refinement, and current systems use static strategies that cannot adapt to specific errors, limiting effectiveness.

Method: This paper introduces Adapt, a framework that uses an LLM-guided decision-maker to dynamically select the most effective proof refinement strategy based on the proof assistant's state and the specific context of an incorrect proof. Adapt is evaluated on benchmarks, compared to existing methods, and tested across multiple LLMs with additional ablation studies.

Result: Adapt significantly outperforms the best baseline methods, proving 16.63% and 18.58% more theorems on two benchmarks. It generalizes well across five different LLMs. Ablation studies also show the value and trade-offs of various decision-maker components.

Conclusion: Adapt's dynamic, context-sensitive strategy selection enables more efficient and successful formal verification with LLMs, representing a marked advance over traditional fixed-strategy refinement methods.

Abstract: Formal verification via theorem proving enables the expressive specification
and rigorous proof of software correctness, but it is difficult to scale due to
the significant manual effort and expertise required. While Large Language
Models (LLMs) show potential in proof generation, they frequently produce
incorrect proofs on the first attempt and require additional strategies for
iterative refinement. However, existing approaches employ fixed refinement
strategies and cannot dynamically choose an effective strategy based on the
particular issues in a generated proof, which limits their performance. To
overcome this limitation, we introduce Adapt, a novel proof refinement
framework that leverages an LLM-guided decision-maker to dynamically select a
suitable refinement strategy according to the state of the proof assistant and
available context of an incorrect proof. We evaluate Adapt on two benchmarks
against four existing methods and find that it significantly outperforms the
best baseline on both by proving 16.63% and 18.58% more theorems, respectively.
Furthermore, we demonstrate Adapt's generalizability by evaluating it across
five different LLMs. We also conduct ablation studies to measure the
contribution of each component and compare the trade-offs of alternative
decision-maker designs.

</details>


### [9] [Automated Program Repair Based on REST API Specifications Using Large Language Models](https://arxiv.org/abs/2510.25148)
*Katsuki Yamagishi,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: dcFix is a method that uses LLMs to automatically detect and repair REST API misuses by explicitly incorporating non-conforming code and API specs into prompts. It outperforms simpler approaches and streamlines API debugging.


<details>
  <summary>Details</summary>
Motivation: Developers interacting with cloud service REST APIs often face difficulties diagnosing and correcting API misuse, as error messages are vague and issues are generally discovered only during testing. Diagnosing these issues is inefficient, often involving significant trial and error.

Method: The paper proposes dcFix, an automated method for detecting and repairing misuses of REST APIs in client code. dcFix identifies code fragments that do not conform to API specifications and creates prompts by combining these fragments with relevant specifications. These prompts are then given to a Large Language Model (LLM), which generates corrected code.

Result: Evaluations show dcFix can accurately detect REST API misuses in client programs and generate effective repairs. It outperforms a baseline approach where LLM prompts lack information about non-conforming fragments.

Conclusion: Integrating information about detected non-conforming code with REST API specifications results in significantly improved detection and automated repair of API misuses using LLMs. This can offer more precise and effective debugging for developers.

Abstract: Many cloud services provide REST API accessible to client applications.
However, developers often identify specification violations only during
testing, as error messages typically lack the detail necessary for effective
diagnosis. Consequently, debugging requires trial and error. This study
proposes dcFix, a method for detecting and automatically repairing REST API
misuses in client programs. In particular, dcFix identifies non-conforming code
fragments, integrates them with the relevant API specifications into prompts,
and leverages a Large Language Model (LLM) to produce the corrected code. Our
evaluation demonstrates that dcFix accurately detects misuse and outperforms
the baseline approach, in which prompts to the LLM omit any indication of code
fragments non conforming to REST API specifications.

</details>


### [10] [Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models](https://arxiv.org/abs/2510.25195)
*Shuochuan Li,Zan Wang,Xiaoning Du,Zhuo Wu,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: KUMIC is a new framework for generating code comments tailored to different user intents using LLMs and Chain-of-Thought reasoning. It significantly outperforms existing methods in generating accurate, purpose-driven comments.


<details>
  <summary>Details</summary>
Motivation: Generic code comments are insufficient to address the varying needs of developers and users, who require comments with distinct implementation insights or clear usage instructions. There is a necessity to generate multi-intent comments that cater to these diverse expectations, a task where existing LLM approaches often fail to correctly relate code, intent, and comments when given limited demonstration examples.

Method: The authors propose a framework called KUMIC, which utilizes in-context learning and Chain-of-Thought (CoT) prompting with large language models (LLMs). KUMIC retrieves demonstration examples with high code-comment consistency and uses CoT to guide LLMs to generate intent-specific comments. It builds a mapping knowledge chain linking code, intent-specific statements, and comments, improving the reasoning process in multi-intent comment generation.

Result: Experimental results show that KUMIC outperforms state-of-the-art LLM-based baselines, achieving improvements of 14.49% in BLEU, 22.41% in METEOR, 20.72% in ROUGE-L, and 12.94% in SBERT scores.

Conclusion: KUMIC effectively addresses the challenges of multi-intent code comment generation, substantially outperforming existing methods in both intent alignment and comment quality.

Abstract: Code comment generation aims to produce a generic overview of a code snippet,
helping developers understand and maintain code. However, generic summaries
alone are insufficient to meet the diverse needs of practitioners; for example,
developers expect the implementation insights to be presented in an untangled
manner, while users seek clear usage instructions. This highlights the
necessity of multi-intent comment generation. With the widespread adoption of
Large Language Models (LLMs) for code-related tasks, these models have been
leveraged to tackle the challenge of multi-intent comment generation. Despite
their successes, state-of-the-art LLM-based approaches often struggle to
construct correct relationships among intents, code, and comments within a
smaller number of demonstration examples. To mitigate this issue, we propose a
framework named KUMIC for multi-intent comment generation. Built upon
in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize
knowledge utilization for LLMs to generate intent-specific comments.
Specifically, KUMIC first designs a retrieval mechanism to obtain similar
demonstration examples, which exhibit high code-comment consistency. Then,
KUMIC leverages CoT to guide LLMs to focus on statements facilitating the
derivation of code comments aligned with specific intents. In this context,
KUMIC constructs a mapping knowledge chain, linking code to intent-specific
statements to comments, which enables LLMs to follow similar reasoning steps
when generating the desired comments. We conduct extensive experiments to
evaluate KUMIC, and the results demonstrate that KUMIC outperforms
state-of-the-art baselines by 14.49\%, 22.41\%, 20.72\%, and 12.94\% in terms
of BLEU, METEOR, ROUGE-L, and SBERT, respectively.

</details>


### [11] [TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices](https://arxiv.org/abs/2510.25242)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: To support complex embedded and IoT systems, TECS/Rust-OE offers memory safety and efficient exclusive control in Rust via call flows and OS mechanisms, automatically generating reusable, high-performance code.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of embedded systems, especially with IoT growth, requires reliable, secure programming frameworks. Previous approaches using Rust and component-based development have suffered from performance issues due to thread safety mechanisms.

Method: The paper introduces TECS/Rust-OE, a new memory-safe component-based development framework. It uses call flows and real-time OS exclusive control mechanisms to optimize Rust code for embedded systems. Rust code is auto-generated from component descriptions to enhance scalability and reliability.

Result: Experimental evaluations show that TECS/Rust-OE reduces overhead from exclusion controls and retains high code reusability.

Conclusion: TECS/Rust-OE effectively balances memory safety, performance, and code reusability for scalable embedded system design, outperforming previous Rust-CBD approaches.

Abstract: The diversification of functionalities and the development of the IoT are
making embedded systems larger and more complex in structure. Ensuring system
reliability, especially in terms of security, necessitates selecting an
appropriate programming language. As part of existing research, TECS/Rust has
been proposed as a framework that combines Rust and component-based development
(CBD) to enable scalable system design and enhanced reliability. This framework
represents system structures using static mutable variables, but excessive
exclusive controls applied to ensure thread safety have led to performance
degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework
utilizing call flows to address these limitations. The proposed Rust code
leverages real-time OS exclusive control mechanisms, optimizing performance
without compromising reusability. Rust code is automatically generated based on
component descriptions. Evaluations demonstrate reduced overhead due to
optimized exclusion control and high reusability of the generated code.

</details>


### [12] [TECS/Rust: Memory-safe Component Framework for Embedded Systems](https://arxiv.org/abs/2510.25270)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: Switching from C to Rust for embedded component frameworks increases memory safety with little performance cost, while automating much of the code generation.


<details>
  <summary>Details</summary>
Motivation: Embedded systems are becoming more complex, making memory safety an increasing concern, especially when using C in component-based development. The motivation is to find a safer, yet flexible, solution for developing embedded system components.

Method: The paper proposes TECS/Rust, a Rust-based framework for TECS component systems. It leverages Rust's compile-time memory-safety features and automates Rust code generation for CBD components. It also benchmarks code amount and execution time to validate efficiency.

Result: TECS/Rust generates a large portion of the code automatically, significantly aiding development. The framework introduces negligible overhead compared to manually developed code, preserving runtime efficiency.

Conclusion: TECS/Rust successfully enhances memory safety while maintaining flexibility and efficiency in component-based development for embedded systems, making it a promising alternative to C.

Abstract: As embedded systems grow in complexity and scale due to increased functional
diversity, component-based development (CBD) emerges as a solution to
streamline their architecture and enhance functionality reuse. CBD typically
utilizes the C programming language for its direct hardware access and
low-level operations, despite its susceptibility to memory-related issues. To
address these concerns, this paper proposes TECS/Rust, a Rust-based framework
specifically designed for TECS, which is a component framework for embedded
systems. It leverages Rust's compile-time memory-safe features, such as
lifetime and borrowing, to mitigate memory vulnerabilities common with C. The
proposed framework not only ensures memory safety but also maintains the
flexibility of CBD, automates Rust code generation for CBD components, and
supports efficient integration with real-time operating systems. An evaluation
of the amount of generated code indicates that the code generated by this paper
framework accounts for a large percentage of the actual code. Compared to code
developed without the proposed framework, the difference in execution time is
minimal, indicating that the overhead introduced by the proposed framework is
negligible.

</details>


### [13] [Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases](https://arxiv.org/abs/2510.25297)
*Hidetake Tanaka,Haruto Tanaka,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: Traditional example-based testing often misses edge cases in LLM-generated code. Property-based testing and example-based testing, when combined, detect more bugs (81.25%) compared to using either alone (68.75%). Each method finds different types of issues, so a hybrid approach improves code reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the reliability of code generated by large language models, addressing the problem that traditional example-based testing often misses edge cases and defects relevant to boundary conditions or input extremes.

Method: The study analyzed 16 HumanEval problems where standard solutions failed on extended test cases, and generated both property-based testing (PBT) and example-based testing (EBT) code using Claude-4-sonnet. The authors compared the bug detection rates of the two methods both individually and in combination.

Result: Both PBT and EBT individually detected 68.75% of bugs, but combining them increased bug detection to 81.25%. PBT was more effective for performance and extensive input exploration, while EBT excelled at finding issues related to boundary conditions and special patterns.

Conclusion: A hybrid approach that leverages both PBT and EBT improves the reliability of LLM-generated code. The complementary nature of these methods provides valuable guidance for testing strategies in LLM-based software development.

Abstract: As Large Language Models (LLMs) increasingly generate code in software
development, ensuring the quality of LLM-generated code has become important.
Traditional testing approaches using Example-based Testing (EBT) often miss
edge cases -- defects that occur at boundary values, special input patterns, or
extreme conditions. This research investigates the characteristics of
LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge
cases. We analyze 16 HumanEval problems where standard solutions failed on
extended test cases, generating both PBT and EBT test codes using
Claude-4-sonnet. Our experimental results reveal that while each method
individually achieved a 68.75\% bug detection rate, combining both approaches
improved detection to 81.25\%. The analysis demonstrates complementary
characteristics: PBT effectively detects performance issues and edge cases
through extensive input space exploration, while EBT effectively detects
specific boundary conditions and special patterns. These findings suggest that
a hybrid approach leveraging both testing methods can improve the reliability
of LLM-generated code, providing guidance for test generation strategies in
LLM-based code generation.

</details>


### [14] [Dissect-and-Restore: AI-based Code Verification with Transient Refactoring](https://arxiv.org/abs/2510.25406)
*Changjie Wang,Mariano Scazzariello,Anoud Alshnaka,Roberto Guanciale,Dejan Kostić,Marco Chiesa*

Main category: cs.SE

TL;DR: Prometheus is a new AI-assisted tool for formal code verification that uses modular decomposition and allows natural language guidance. It boosts verification rates dramatically over standard methods, making formal verification more practical and accessible, especially for complex programs.


<details>
  <summary>Details</summary>
Motivation: Formal verification of software is essential for reliability but requires significant expertise and cost, making it less accessible. Current AI systems have potential but are not yet effectively integrated into verification workflows.

Method: The paper introduces Prometheus, an AI-assisted verification system that uses modular software engineering principles. It works by decomposing complex program logic into smaller components, verifying them individually, and then recomposing them to prove the original program. When automation fails, users can give natural language input to aid the process.

Result: Prometheus outperforms the baseline, successfully verifying 86% of tasks (versus 68%). Improvements are even more substantial for complex specifications (from 30% to 69%) and when adding proof outlines for complex programs (from 25% to 87%).

Conclusion: AI-assisted modular decomposition and user-guided proof workflows, as embodied in Prometheus, significantly improve both the effectiveness and accessibility of formal verification, especially in challenging or complex cases.

Abstract: Formal verification is increasingly recognized as a critical foundation for
building reliable software systems. However, the need for specialized expertise
to write precise specifications, navigate complex proof obligations, and learn
annotations often makes verification an order of magnitude more expensive than
implementation. While modern AI systems can recognize patterns in mathematical
proofs and interpret natural language, effectively integrating them into the
formal verification process remains an open challenge. We present Prometheus, a
novel AI-assisted system that facilitates automated code verification with
current AI capabilities in conjunction with modular software engineering
principles (e.g., modular refactoring). Our approach begins by decomposing
complex program logic, such as nested loops, into smaller, verifiable
components. Once verified, these components are recomposed to construct a proof
of the original program. This decomposition-recomposition workflow is
non-trivial. Prometheus addresses this by guiding the proof search through
structured decomposition of complex lemmas into smaller, verifiable sub-lemmas.
When automated tools are insufficient, users can provide lightweight natural
language guidance to steer the proof process effectively. Our evaluation
demonstrates that transiently applying modular restructuring to the code
substantially improves the AI's effectiveness in verifying individual
components. This approach successfully verifies 86% of tasks in our curated
dataset, compared to 68% for the baseline. Gains are more pronounced with
increasing specification complexity, improving from 30% to 69%, and when
integrating proof outlines for complex programs, from 25% to 87%.

</details>


### [15] [What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow](https://arxiv.org/abs/2510.25423)
*Ali Asgari,Annibale Panichella,Pouria Derakhshanfar,Mitchell Olsthoorn*

Main category: cs.SE

TL;DR: This paper analyzes Stack Overflow discussions to systematically identify and categorize persistent challenges developers face when working with AI agents, revealing key technical issues and providing guidance to improve agent development and support.


<details>
  <summary>Details</summary>
Motivation: AI agents, which extend large language models with capabilities such as planning and tool usage, are increasingly popular, but developers face significant, often insufficiently explored challenges in building, deploying, and maintaining these systems.

Method: The authors analyzed developer discussions on Stack Overflow by constructing a taxonomy of developer challenges using tag expansion/filtering, applying LDA-MALLET for topic modeling, and manually validating the identified themes.

Result: The analysis identified seven major areas of recurring issues, describing 77 specific technical challenges such as runtime integration, dependency management, orchestration complexity, and evaluation reliability. The study also quantified the popularity and difficulty of topics, mapped development tools/languages, and tracked trends from 2021-2025.

Conclusion: The paper highlights widespread technical challenges in developing AI agents, and offers actionable guidance to improve agent reliability and developer support for practitioners, researchers, and educators.

Abstract: AI agents have rapidly gained popularity across research and industry as
systems that extend large language models with additional capabilities to plan,
use tools, remember, and act toward specific goals. Yet despite their promise,
developers face persistent and often underexplored challenges when building,
deploying, and maintaining these emerging systems. To identify these
challenges, we study developer discussions on Stack Overflow, the world's
largest developer-focused Q and A platform with about 60 million questions and
answers and 30 million users. We construct a taxonomy of developer challenges
through tag expansion and filtering, apply LDA-MALLET for topic modeling, and
manually validate and label the resulting themes. Our analysis reveals seven
major areas of recurring issues encompassing 77 distinct technical challenges
related to runtime integration, dependency management, orchestration
complexity, and evaluation reliability. We further quantify topic popularity
and difficulty to identify which issues are most common and hardest to resolve,
map the tools and programming languages used in agent development, and track
their evolution from 2021 to 2025 in relation to major AI model and framework
releases. Finally, we present the implications of our results, offering
concrete guidance for practitioners, researchers, and educators on agent
reliability and developer support.

</details>


### [16] [Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies](https://arxiv.org/abs/2510.25506)
*Florian Angermeir,Maximilian Amougou,Mark Kreitz,Andreas Bauer,Matthias Linhuber,Davide Fucci,Fabiola Moyón C.,Daniel Mendez,Tony Gorschek*

Main category: cs.SE

TL;DR: This paper analyzes the reproducibility of LLM-centric studies at major conferences and finds that most research cannot be reliably reproduced, underlining the urgent need for better documentation and study design in the field.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing academic interest in Large Language Models (LLMs) and the need to understand and improve the reproducibility of empirical studies involving these models.

Method: The authors reviewed 86 LLM-related studies from ICSE 2024 and ASE 2024, focusing on 18 studies that provided artefacts and used OpenAI models, and attempted to replicate their results.

Result: Out of the 18 studies attempted for replication, only five were sufficiently well-documented for reproduction. However, none could be fully reproduced: two were partially reproducible and three were not reproducible at all.

Conclusion: There are significant reproducibility issues in current LLM-related research, highlighting the need for stricter artefact evaluation and stronger study design to enable reproducible results.

Abstract: Large Language Models have gained remarkable interest in industry and
academia. The increasing interest in LLMs in academia is also reflected in the
number of publications on this topic over the last years. For instance, alone
78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.
Conducting empirical studies with LLMs remains challenging and raises questions
on how to achieve reproducible results, for both other researchers and
practitioners. One important step towards excelling in empirical research on
LLMs and their application is to first understand to what extent current
research results are eventually reproducible and what factors may impede
reproducibility. This investigation is within the scope of our work. We
contribute an analysis of the reproducibility of LLM-centric studies, provide
insights into the factors impeding reproducibility, and discuss suggestions on
how to improve the current state. In particular, we studied the 86 articles
describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86
articles, 18 provided research artefacts and used OpenAI models. We attempted
to replicate those 18 studies. Of the 18 studies, only five were fit for
reproduction. For none of the five studies, we were able to fully reproduce the
results. Two studies seemed to be partially reproducible, and three studies did
not seem to be reproducible. Our results highlight not only the need for
stricter research artefact evaluations but also for more robust study designs
to ensure the reproducible value of future publications.

</details>


### [17] [Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL](https://arxiv.org/abs/2510.25665)
*Ayse Irmak Ercevik,Aidan Dakhama,Melane Navaratnarajah,Yazhuo Cao,Leo Fernandes*

Main category: cs.SE

TL;DR: GreenAFL makes fuzzing more eco-friendly by reducing energy use and maintaining high coverage, through energy-aware corpus management and mutation strategies; experiments validate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Continuous fuzzing requires heavy computational resources, resulting in considerable energy consumption and environmental impact. Classical fuzzers focus solely on coverage and neglect energy efficiency.

Method: The paper introduces GreenAFL, an energy-aware fuzzing framework that modifies traditional fuzzing by (1) minimizing the corpus using energy consumption as a factor and (2) applying energy-guided heuristics to direct mutations towards low-energy yet high-coverage inputs. An ablation study is performed to separately analyze the effect of each component.

Result: Using GreenAFL, experimental results showed that either energy-aware corpus minimisation or energy-guided heuristics (or both) led to both higher code coverage and reduced energy usage compared to standard approaches.

Conclusion: Incorporating power consumption into fuzzing heuristics (as in GreenAFL) is effective for reducing energy footprint while maintaining or improving test coverage, suggesting that energy-aware fuzzing should be adopted for sustainable software testing.

Abstract: Fuzzing has become a key search-based technique for software testing, but
continuous fuzzing campaigns consume substantial computational resources and
generate significant carbon footprints. Existing grey-box fuzzing approaches
like AFL++ focus primarily on coverage maximisation, without considering the
energy costs of exploring different execution paths. This paper presents
GreenAFL, an energy-aware framework that incorporates power consumption into
the fuzzing heuristics to reduce the environmental impact of automated testing
whilst maintaining coverage. GreenAFL introduces two key modifications to
traditional fuzzing workflows: energy-aware corpus minimisation considering
power consumption when reducing initial corpora, and energy-guided heuristics
that direct mutation towards high-coverage, low-energy inputs. We conduct an
ablation study comparing vanilla AFL++, energy-based corpus minimisation, and
energy-based heuristics to evaluate the individual contributions of each
component. Results show that highest coverage, and lowest energy usage is
achieved whenever at least one of our modifications is used.

</details>


### [18] [A Configuration-First Framework for Reproducible, Low-Code Localization](https://arxiv.org/abs/2510.25692)
*Tim Strnad,Blaž Bertalanič,Carolina Fortuna*

Main category: cs.SE

TL;DR: LOCALIZE is a low-code framework for radio-based localization that simplifies experiment setup, ensures reproducibility, and allows easy integration of new components. It outperforms traditional workflows in ease of use and scalability without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: There is a gap in existing machine learning tools for radio-based localization: researchers require low coding effort, default reproducibility, and extensibility, but current solutions rarely deliver all these features.

Method: The paper introduces LOCALIZE, a low-code, configuration-first framework that enables reproducible and extensible experimentation in radio localization. Experiments are declared through human-readable configurations, and the framework orchestrates standardized pipelines from data preparation to reporting, with versioning for all artifacts.

Result: LOCALIZE is shown to reduce authoring effort compared to standard Jupyter notebook workflows, while maintaining comparable runtime and memory usage. The design allows for scaling with increasing training data with bounded overhead. Extension points let experts add new functionalities efficiently.

Conclusion: LOCALIZE makes rigorous, reproducible, and extensible machine-learning-based localization experimentation practical and accessible, filling a key gap in the current research workflow.

Abstract: Machine learning is increasingly permeating radio-based localization
services. To keep results credible and comparable, everyday workflows should
make rigorous experiment specification and exact repeatability the default,
without blocking advanced experimentation. However, in practice, researchers
face a three-way gap that could be filled by a framework that offers (i) low
coding effort for end-to-end studies, (ii) reproducibility by default including
versioned code, data, and configurations, controlled randomness, isolated runs,
and recorded artifacts, and (iii) built-in extensibility so new models,
metrics, and stages can be added with minimal integration effort. Existing
tools rarely deliver all three for machine learning in general and localization
workflows in particular. In this paper we introduce LOCALIZE, a low-code,
configuration-first framework for radio localization in which experiments are
declared in human-readable configuration, a workflow orchestrator runs
standardized pipelines from data preparation to reporting, and all artifacts,
such as datasets, models, metrics, and reports, are versioned. The
preconfigured, versioned datasets reduce initial setup and boilerplate,
speeding up model development and evaluation. The design, with clear extension
points, allows experts to add components without reworking the infrastructure.
In a qualitative comparison and a head-to-head study against a plain Jupyter
notebook baseline, we show that the framework reduces authoring effort while
maintaining comparable runtime and memory behavior. Furthermore, using a
Bluetooth Low Energy dataset, we show that scaling across training data (1x to
10x) keeps orchestration overheads bounded as data grows. Overall, the
framework makes reproducible machine-learning-based localization
experimentation practical, accessible, and extensible.

</details>


### [19] [Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents](https://arxiv.org/abs/2510.25694)
*Jiayi Kuang,Yinghui Li,Xin Zhang,Yangning Li,Di Yin,Xing Sun,Ying Shen,Philip S. Yu*

Main category: cs.SE

TL;DR: Enconda-bench is a new benchmark that evaluates software engineering agents at the process-level for environment configuration. Unlike previous benchmarks that only measure end results, Enconda-bench examines specific capabilities like error localization and repair. Findings show agents can find errors but struggle to fix them using feedback, highlighting a key area for improvement.


<details>
  <summary>Details</summary>
Motivation: While large language model-based agents have made progress in automating software engineering tasks, configuring software environments still requires substantial manual work, largely due to the lack of quality, large-scale datasets and effective evaluation methods.

Method: The authors introduce 'Enconda-bench,' a benchmark that assesses agents' ability to configure software environments at a granular, process-level. The benchmark operates by automatically injecting realistic errors into README files, validating scenarios in Docker, and analyzing the agent's actions during setup, error diagnosis, repair, and final configuration.

Result: Experiments on state-of-the-art LLMs and agent frameworks reveal that these agents are capable of detecting and localizing errors, but often struggle to effectively interpret feedback and make successful corrections, which limits their overall performance in environment configuration tasks.

Conclusion: Enconda-bench provides a unique and detailed framework for internal capability assessment of software engineering agents in environment configuration, offering actionable feedback for future improvements. It advances evaluation beyond simple success rates by opening the 'black box' of the agent's decision-making processes.

Abstract: Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [20] [The Singularity Theory of Concurrent Programs: A Topological Characterization and Detection of Deadlocks and Livelocks](https://arxiv.org/abs/2510.25112)
*Di Zhang*

Main category: cs.PL

TL;DR: This paper proposes using algebraic topology to analyze concurrent programs, modeling execution as a branched space. Deadlocks and livelocks are mathematically characterized and can be detected via topological invariants, providing a foundation that avoids exhaustive state traversal and surpasses classical model checking limits.


<details>
  <summary>Details</summary>
Motivation: Traditional model checking for concurrent programs often struggles with state explosion and exhaustive traversal, making it difficult to systematically detect deadlocks and livelocks. The paper seeks a more comprehensive and mathematically grounded approach.

Method: The authors model program execution as a branched topological space and represent program states and transitions using algebraic topology. Deadlocks are viewed as attractors, and livelocks as non-contractible loops. Tools such as homotopy and homology groups are used to define topological invariants that help to detect and classify problematic states.

Result: The paper demonstrates that deadlocks and livelocks can be systematically detected and classified using topological invariants, without the need for exhaustive state enumeration.

Conclusion: A new geometric and topological foundation for concurrent program verification is established, offering systematic and scalable detection of concurrency issues beyond what traditional methods allow.

Abstract: This paper introduces a novel paradigm for the analysis and verification of
concurrent programs -- the Singularity Theory. We model the execution space of
a concurrent program as a branched topological space, where program states are
points and state transitions are paths. Within this framework, we characterize
deadlocks as attractors and livelocks as non-contractible loops in the
execution space. By employing tools from algebraic topology, particularly
homotopy and homology groups, we define a series of concurrent topological
invariants to systematically detect and classify these concurrent
"singularities" without exhaustively traversing all states. This work aims to
establish a geometric and topological foundation for concurrent program
verification, transcending the limitations of traditional model checking.

</details>


### [21] [Have a thing? Reasoning around recursion with dynamic typing in grounded arithmetic](https://arxiv.org/abs/2510.25369)
*Elliot Bobrow,Bryan Ford,Stefan Milenković*

Main category: cs.PL

TL;DR: Grounded Arithmetic (GA) is a novel logic system enabling direct and safe use of arbitrary recursive functions in formal reasoning, by treating non-terminating computations as undefined rather than inconsistent. Its quantifier-free fragment is proven consistent and it paves the way to more practical and expressive computational logics.


<details>
  <summary>Details</summary>
Motivation: Traditional logic systems (classical and intuitionistic) face difficulties handling arbitrary general-recursive functions without risking inconsistency, limiting their usefulness for formal reasoning about computation.

Method: The paper introduces Grounded Arithmetic (GA), a minimalistic logic system that modifies standard inference rules so that non-terminating recursive functions denote an undefined value (bottom) rather than introducing inconsistency. GA enables 'dynamic typing' or symbolic reverse execution to prove termination of recursive functions, integrating them seamlessly once termination is established. A mechanically-checked consistency proof is provided for the quantifier-free fragment using Isabelle/HOL.

Result: GA successfully allows direct expression and reasoning with arbitrary recursive definitions without inconsistency, provided non-terminating terms are treated as semantically undefined. The quantifier-free GA system is shown to be consistent, and classical reasoning is recovered for proven terminating functions. Quantifiers can be safely added as computations.

Conclusion: Grounded Arithmetic presents a promising foundation for formal reasoning about arbitrary recursive functions in a manner that preserves consistency. While an initial step, this approach may enable more practical and expressive reasoning about computation in both manual and automated formal systems.

Abstract: Neither the classical nor intuitionistic logic traditions are
perfectly-aligned with the purpose of reasoning about computation, in that
neither logical tradition can normally permit the direct expression of
arbitrary general-recursive functions without inconsistency. We introduce
grounded arithmetic or GA, a minimalistic but nonetheless powerful foundation
for formal reasoning that allows the direct expression of arbitrary recursive
definitions. GA adjusts the traditional inference rules such that terms that
express nonterminating computations harmlessly denote no semantic value (i.e.,
"bottom") instead of leading into logical paradox or inconsistency. Recursive
functions may be proven terminating in GA essentially by "dynamically typing"
terms, or equivalently, symbolically reverse-executing the computations they
denote via GA's inference rules. Once recursive functions have been proven
terminating, logical reasoning about their results reduce to the familiar
classical rules. A mechanically-checked consistency proof in Isabelle/HOL
exists for the basic quantifier-free fragment of GA. Quantifiers may be added
atop this foundation as ordinary computations, whose inference rules are thus
admissible and do not introduce new inconsistency risks. While GA is only a
first step towards richly-typed grounded deduction practical for everyday use
in manual or automated computational reasoning, it shows the promise that the
expressive freedom of arbitrary recursive definition can in principle be
incorporated into formal systems.

</details>
