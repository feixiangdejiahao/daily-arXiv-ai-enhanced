{"id": "2601.09741", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.09741", "abs": "https://arxiv.org/abs/2601.09741", "authors": ["James Uther"], "title": "Putting green software principles into practice", "comment": "1st International Workshop on Low Carbon Computing (LOCO 2024)", "summary": "The need and theoretical methods for measuring and reducing CO2 emitted by computing systems are well understood, but real-world examples are still limited. We describe a journey towards green software for a live product running on a public cloud. We discuss practical solutions found, in particular using the cost implications of serverless systems to drive efficiency. We end with some `green software' principles that worked well in this project."}
{"id": "2601.09744", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09744", "abs": "https://arxiv.org/abs/2601.09744", "authors": ["Vignesh Alagappan"], "title": "A Governance Model for IoT Data in Global Manufacturing", "comment": null, "summary": "Industrial IoT platforms in global manufacturing environments generate continuous operational data across production assets, utilities, and connected products. While data ingestion and storage capabilities have matured significantly, enterprises continue to face systemic challenges in governing IoT data at scale. These challenges are not rooted in tooling limitations but in the absence of a governance model that aligns with the realities of distributed operational ownership, heterogeneous source systems, and continuous change at the edge. This paper presents a federated governance model that emphasizes contract-driven interoperability, policy-as-code enforcement, and asset-centric accountability across global manufacturing organizations. The model addresses governance enforcement at architectural boundaries, enabling semantic consistency, quality assurance, and regulatory compliance without requiring centralized control of operational technology systems. This work contributes a systems architecture and design framework grounded in analysis of manufacturing IoT requirements and constraints; empirical validation remains future work"}
{"id": "2601.09745", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09745", "abs": "https://arxiv.org/abs/2601.09745", "authors": ["Antonio Abu Nassar", "Eitan Farchi"], "title": "Enhancing Formal Software Specification with Artificial Intelligence", "comment": null, "summary": "Formal software specification is known to enable early error detection and explicit invariants, yet it has seen limited industrial adoption due to its high notation overhead and the expertise required to use traditional formal languages. This paper presents a case study showing that recent advances in artificial intelligence make it possible to retain many of the benefits of formal specification while substantially reducing these costs. The necessity of a clear distinction between what is controlled by the system analyst and can highly benefits from the rigor of formal specification and what need not be controlled is demonstrated. We use natural language augmented with lightweight mathematical notation and written in \\LaTeX\\ as an intermediate specification language, which is reviewed and refined by AI prior to code generation. Applied to a nontrivial simulation of organizational knowledge growth, this approach enables early validation, explicit invariants, and correctness by design, while significantly reducing development effort and producing a correct implementation on the first attempt."}
{"id": "2601.09749", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09749", "abs": "https://arxiv.org/abs/2601.09749", "authors": ["Suriya Sureshkumar"], "title": "R-LAM: Reproducibility-Constrained Large Action Models for Scientific Workflow Automation", "comment": "9 pages, 3 figures, 1 Table, 2 Artifacts", "summary": "Large Action Models (LAMs) extend large language models by enabling autonomous decision-making and tool execution, making them promising for automating scientific workflows. However, scientific workflows impose strict requirements on reproducibility, auditability, and deterministic execution, which are not satisfied by generic LLM-based agents. Unconstrained action generation can lead to silent state changes, non-deterministic executions, and irreproducible experimental results, limiting the applicability of LAMs in scientific settings.\n  In this paper, we propose R-LAM, a reproducibility-constrained framework for applying Large Action Models to scientific workflow automation. R-LAM introduces structured action schemas, deterministic execution policies, and explicit provenance tracking to ensure that every action and intermediate artifact is auditable and replayable. The framework supports failure-aware execution loops and controlled workflow forking, enabling iterative experimentation without compromising reproducibility.\n  We implement R-LAM as a lightweight Python framework and release it as an open-source PyPI package to facilitate reproducible research. An experimental evaluation of representative scientific workflows demonstrates that R-LAM improves reproducibility success rates and execution reliability compared to unconstrained LLM-based agents, while retaining adaptive control over workflow execution."}
{"id": "2601.09808", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.09808", "abs": "https://arxiv.org/abs/2601.09808", "authors": ["Chen Ling", "Yachen Wang"], "title": "From Dynamic to Lexical: A Comparative Exploration of Scoping Rules in SAS and R", "comment": "This paper was originally published in the SESUG 2025 Conference Proceedings. Cary, NC", "summary": "Variable scoping dictates how and where variables are accessible within programming languages, playing a crucial role in code efficiency and organization. This paper examines the distinct scoping rules in SAS and R, focusing on SAS's dynamic scoping and R's lexical scoping. In SAS, dynamic scoping utilizes symbol tables, resolving variables at runtime by dynamically searching through active macro layers. R, in contrast, employs lexical scoping, using environments to resolve variables based on the structure in which functions are defined. Illustrative examples highlight the differences between these scoping strategies, showcasing their impact on code behavior. Additionally, the paper outlines methods for inspecting variables in SAS's symbol tables and R's environments, offering practical insights for debugging and optimization. Strategies for controlling variable scope in both languages are discussed, enhancing code precision and reliability. This exploration equips programmers with critical understanding to optimize variable management, improving their programming practices in SAS and R."}
{"id": "2601.09750", "categories": ["cs.SE", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.09750", "abs": "https://arxiv.org/abs/2601.09750", "authors": ["Robert K. Strehlow", "Tobias Küster", "Oskar F. Kupke", "Brandon Llanque Kurps", "Fikret Sivrikaya", "Sahin Albayrak"], "title": "SAGE: Tool-Augmented LLM Task Solving Strategies in Scalable Multi-Agent Environments", "comment": null, "summary": "Large language models (LLMs) have proven to work well in question-answering scenarios, but real-world applications often require access to tools for live information or actuation. For this, LLMs can be extended with tools, which are often defined in advance, also allowing for some fine-tuning for specific use cases. However, rapidly evolving software landscapes and individual services require the constant development and integration of new tools. Domain- or company-specific tools can greatly elevate the usefulness of an LLM, but such custom tools can be problematic to integrate, or the LLM may fail to reliably understand and use them. For this, we need strategies to define new tools and integrate them into the LLM dynamically, as well as robust and scalable zero-shot prompting methods that can make use of those tools in an efficient manner. In this paper, we present SAGE, a specialized conversational AI interface, based on the OPACA framework for tool discovery and execution. The integration with OPACA makes it easy to add new tools or services for the LLM to use, while SAGE itself presents rich extensibility and modularity. This not only provides the ability to seamlessly switch between different models (e.g. GPT, LLAMA), but also to add and select prompting methods, involving various setups of differently prompted agents for selecting and executing tools and evaluating the results. We implemented a number of task-solving strategies, making use of agentic concepts and prompting methods in various degrees of complexity, and evaluated those against a comprehensive set of benchmark services. The results are promising and highlight the distinct strengths and weaknesses of different task-solving strategies. Both SAGE and the OPACA framework, as well as the different benchmark services and results, are available as Open Source/Open Data on GitHub."}
{"id": "2601.09839", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.09839", "abs": "https://arxiv.org/abs/2601.09839", "authors": ["Chen Ling", "Yachen Wang"], "title": "Lazy Evaluation: A Comparative Analysis of SAS MACROs and R Functions", "comment": "This paper was originally published in SESUG 2025 Conference Proceedings. Cary, NC: SouthEast SAS Users Group", "summary": "Lazy evaluation is a powerful technique that can optimize code execution by deferring evaluations until their results are required, thus enhancing efficiency. In most modern programming languages, like R, lazy evaluation is commonly applied to function arguments. However, the application of lazy evaluation in SAS has not been extensively explored. This paper focuses on the mechanisms of lazy evaluation in SAS MACROs and R functions, offering a comparative analysis of the underlying principles that drive these processes.\n  R's lazy evaluation is driven by a data structure called Promise, which postpones evaluation and does not occupy memory until the value is needed, utilizing a call-by-need strategy. SAS, on the other hand, achieves lazy evaluation through its symbol tables, employing memory to store parameters, and operates on a call-by-name basis. These discrepancies in lazy evaluation strategies can notably impact the results of R functions and SAS MACROs. By examining these distinct approaches, the paper illuminates the impact of lazy evaluation on programming efficiency, supported by illustrative examples. As the shift from SAS to R becomes increasingly prevalent in the pharmaceutical industry, understanding these techniques enables programmers to optimize their code for greater efficacy. This exploration serves as a guide to enhance programming capabilities and performance in both languages."}
{"id": "2601.09760", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09760", "abs": "https://arxiv.org/abs/2601.09760", "authors": ["Jiali Cheng", "Rui Pan", "Hadi Amiri"], "title": "Investigating Tool-Memory Conflicts in Tool-Augmented LLMs", "comment": "R2-FM Workshop @ ICML 2025", "summary": "Tool-augmented large language models (LLMs) have powered many applications. However, they are likely to suffer from knowledge conflict. In this paper, we propose a new type of knowledge conflict -- Tool-Memory Conflict (TMC), where the internal parametric knowledge contradicts with the external tool knowledge for tool-augmented LLMs. We find that existing LLMs, though powerful, suffer from TMC, especially on STEM-related tasks. We also uncover that under different conditions, tool knowledge and parametric knowledge may be prioritized differently. We then evaluate existing conflict resolving techniques, including prompting-based and RAG-based methods. Results show that none of these approaches can effectively resolve tool-memory conflicts."}
{"id": "2601.09986", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.09986", "abs": "https://arxiv.org/abs/2601.09986", "authors": ["Cheng Zhang", "Qiancheng Fu", "Hang Ji", "Ines Santacruz Del Valle", "Alexandra Silva", "Marco Gaboardi"], "title": "Outrunning Big KATs: Efficient Decision Procedures for Variants of GKAT", "comment": "Conditionally Accepted at ESOP 2026", "summary": "This paper presents several efficient decision procedures for trace equivalence of GKAT automata, which make use of on-the-fly symbolic techniques via SAT solvers. To demonstrate applicability of our algorithms, we designed symbolic derivatives for CF-GKAT, a practical system based on GKAT designed to validate control-flow transformations. We implemented the algorithms in Rust and evaluated them on both randomly generated benchmarks and real-world control-flow transformations. Indeed, we observed order-of-magnitude performance improvements against existing implementations for both KAT and CF-GKAT. Notably, our experiments also revealed a bug in Ghidra, an industry-standard decompiler, highlighting the practical viability of these systems."}
{"id": "2601.09762", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09762", "abs": "https://arxiv.org/abs/2601.09762", "authors": ["Zhiyi Xue", "Xiaohong Chen", "Min Zhang"], "title": "Explicating Tacit Regulatory Knowledge from LLMs to Auto-Formalize Requirements for Compliance Test Case Generation", "comment": "16 pages, 8 figures, 4 tables", "summary": "Compliance testing in highly regulated domains is crucial but largely manual, requiring domain experts to translate complex regulations into executable test cases. While large language models (LLMs) show promise for automation, their susceptibility to hallucinations limits reliable application. Existing hybrid approaches mitigate this issue by constraining LLMs with formal models, but still rely on costly manual modeling. To solve this problem, this paper proposes RAFT, a framework for requirements auto-formalization and compliance test generation via explicating tacit regulatory knowledge from multiple LLMs. RAFT employs an Adaptive Purification-Aggregation strategy to explicate tacit regulatory knowledge from multiple LLMs and integrate it into three artifacts: a domain meta-model, a formal requirements representation, and testability constraints. These artifacts are then dynamically injected into prompts to guide high-precision requirement formalization and automated test generation. Experiments across financial, automotive, and power domains show that RAFT achieves expert-level performance, substantially outperforms state-of-the-art (SOTA) methods while reducing overall generation and review time."}
{"id": "2601.09822", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09822", "abs": "https://arxiv.org/abs/2601.09822", "authors": ["Yongjian Tang", "Thomas Runkler"], "title": "LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities", "comment": "Accepted to GenSE 2026 workshop", "summary": "Despite recent advancements in Large Language Models (LLMs), complex Software Engineering (SE) tasks require more collaborative and specialized approaches. This concept paper systematically reviews the emerging paradigm of LLM-based multi-agent systems, examining their applications across the Software Development Life Cycle (SDLC), from requirements engineering and code generation to static code checking, testing, and debugging. We delve into a wide range of topics such as language model selection, SE evaluation benchmarks, state-of-the-art agentic frameworks and communication protocols. Furthermore, we identify key challenges and outline future research opportunities, with a focus on multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection. This work aims to provide researchers and practitioners with valuable insights into the current forefront landscape of agentic systems within the software engineering domain."}
{"id": "2601.09832", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09832", "abs": "https://arxiv.org/abs/2601.09832", "authors": ["Alvari Kupari", "Nasser Giacaman", "Valerio Terragni"], "title": "Adoption and Evolution of Code Style and Best Programming Practices in Open-Source Projects", "comment": "Published in IEEE International Conference on Software Maintenance and Evolution (ICSME 2025). Authors' version", "summary": "Following code style conventions in software projects is essential for maintaining overall code quality. Adhering to these conventions improves maintainability, understandability, and extensibility. Additionally, following best practices during software development enhances performance and reduces the likelihood of errors. This paper analyzes 1,036 popular open-source JAVA projects on GITHUB to study how code style and programming practices are adopted and evolve over time, examining their prevalence and the most common violations. Additionally, we study a subset of active repositories on a monthly basis to track changes in adherence to coding standards over time. We found widespread violations across repositories, with Javadoc and Naming violations being the most common. We also found a significant number of violations of the GOOGLE Java Style Guide in categories often missed by modern static analysis tools. Furthermore, repositories claiming to follow code-style practices exhibited slightly higher overall adherence to code-style and best-practices. The results provide valuable insights into the adoption of code style and programming practices, highlighting key areas for improvement in the open-source development community. Furthermore, the paper identifies important lessons learned and suggests future directions for improving code quality in JAVA projects."}
{"id": "2601.09842", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09842", "abs": "https://arxiv.org/abs/2601.09842", "authors": ["Walid Maalej"], "title": "On Fun for Teaching Large Programming Courses", "comment": "Accepted at 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE-SEET '26)", "summary": "Teaching software development basics to hundreds of students in a frontal setting is cost-efficient and thus still common in universities. However, in a large lecture hall, students can easily get bored, distracted, and disengaged. The frontal setting can also frustrate lecturers since interaction opportunities are limited and hard to scale. Fun activities can activate students and, if well designed, can also help remember and reflect on abstract software development concepts. We present a novel catalogue of ten physical fun activities, developed over years to reflect on basic programming and software development concepts. The catalogue includes the execution of a LA-OLA algorithm as in stadiums, using paper planes to simulate object messages and pointers, and traversing a lecture hall as a tree or a recursive structure. We report our experience of using the activities in a large course with 500+ students three years in a row. We also conducted an interview study with 15 former students of the course and 14 experienced educators from around the globe. The results suggest that the fun activities can enable students to stay focused, remember key concepts, and reflect afterwards. However, keeping the activities concise and clearly linked to the concepts taught seems to be key to their acceptance and effectiveness."}
{"id": "2601.09873", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09873", "abs": "https://arxiv.org/abs/2601.09873", "authors": ["Saymon Souza", "Amanda Santana", "Eduardo Figueiredo", "Igor Muzetti", "João Eduardo Montandon", "Lionel Briand"], "title": "Beyond Strict Rules: Assessing the Effectiveness of Large Language Models for Code Smell Detection", "comment": null, "summary": "Code smells are symptoms of potential code quality problems that may affect software maintainability, thus increasing development costs and impacting software reliability. Large language models (LLMs) have shown remarkable capabilities for supporting various software engineering activities, but their use for detecting code smells remains underexplored. However, unlike the rigid rules of static analysis tools, LLMs can support flexible and adaptable detection strategies tailored to the unique properties of code smells. This paper evaluates the effectiveness of four LLMs -- DeepSeek-R1, GPT-5 mini, Llama-3.3, and Qwen2.5-Code -- for detecting nine code smells across 30 Java projects. For the empirical evaluation, we created a ground-truth dataset by asking 76 developers to manually inspect 268 code-smell candidates. Our results indicate that LLMs perform strongly for structurally straightforward smells, such as Large Class and Long Method. However, we also observed that different LLMs and tools fare better for distinct code smells. We then propose and evaluate a detection strategy that combines LLMs and static analysis tools. The proposed strategy outperforms LLMs and tools in five out of nine code smells in terms of F1-Score. However, it also generates more false positives for complex smells. Therefore, we conclude that the optimal strategy depends on whether Recall or Precision is the main priority for code smell detection."}
{"id": "2601.09905", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09905", "abs": "https://arxiv.org/abs/2601.09905", "authors": ["Zackary Okun Dunivin", "Mobina Noori", "Seth Frey", "Curtis Atkinson"], "title": "Self-reflection in Automated Qualitative Coding: Improving Text Annotation through Secondary LLM Critique", "comment": null, "summary": "Large language models (LLMs) allow for sophisticated qualitative coding of large datasets, but zero- and few-shot classifiers can produce an intolerable number of errors, even with careful, validated prompting. We present a simple, generalizable two-stage workflow: an LLM applies a human-designed, LLM-adapted codebook; a secondary LLM critic performs self-reflection on each positive label by re-reading the source text alongside the first model's rationale and issuing a final decision. We evaluate this approach on six qualitative codes over 3,000 high-content emails from Apache Software Foundation project evaluation discussions. Our human-derived audit of 360 positive annotations (60 passages by six codes) found that the first-line LLM had a false-positive rate of 8% to 54%, despite F1 scores of 0.74 and 1.00 in testing. Subsequent recoding of all stage-one annotations via a second self-reflection stage improved F1 by 0.04 to 0.25, bringing two especially poor performing codes up to 0.69 and 0.79 from 0.52 and 0.55 respectively. Our manual evaluation identified two recurrent error classes: misinterpretation (violations of code definitions) and meta-discussion (debate about a project evaluation criterion mistaken for its use as a decision justification). Code-specific critic clauses addressing observed failure modes were especially effective with testing and refinement, replicating the codebook-adaption process for LLM interpretation in stage-one. We explain how favoring recall in first-line LLM annotation combined with secondary critique delivers precision-first, compute-light control. With human guidance and validation, self-reflection slots into existing LLM-assisted annotation pipelines to reduce noise and potentially salvage unusable classifiers."}
{"id": "2601.10068", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10068", "abs": "https://arxiv.org/abs/2601.10068", "authors": ["Lianjing Wang", "Yufeng Zhang", "Kenli Li", "Zhenbang Chen", "Xu Zhou", "Pengfei Wang", "Guangning Song", "Ji Wang"], "title": "S$^2$F: Principled Hybrid Testing With Fuzzing, Symbolic Execution, and Sampling", "comment": null, "summary": "Hybrid testing that integrates fuzzing, symbolic execution, and sampling has demonstrated superior testing efficiency compared to individual techniques. However, the state-of-the-art (SOTA) hybrid testing tools do not fully exploit the capabilities of symbolic execution and sampling in two key aspects. First, the SOTA hybrid testing tools employ tailored symbolic execution engines that tend to over-prune branches, leading to considerable time wasted waiting for seeds from the fuzzer and missing opportunities to discover crashes. Second, existing methods do not apply sampling to the appropriate branches and therefore cannot utilize the full capability of sampling. To address these two limitations, we propose a novel hybrid testing architecture that combines the precision of conventional symbolic execution with the scalability of tailored symbolic execution engines. Based on this architecture, we propose several principles for combining fuzzing, symbolic execution, and sampling. We implement our method in a hybrid testing tool S$^2$F. To evaluate its effectiveness, we conduct extensive experiments on 15 real-world programs. Experimental results demonstrate that S$^2$F outperforms the SOTA tool, achieving an average improvement of 6.14% in edge coverage and 32.6% in discovered crashes. Notably, our tool uncovers three previously unknown crashes in real-world programs."}
{"id": "2601.10093", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10093", "abs": "https://arxiv.org/abs/2601.10093", "authors": ["Yiding Qiu", "Seyed Mahdi Azimi", "Artem Lensky"], "title": "Mark My Works Autograder for Programming Courses", "comment": null, "summary": "Large programming courses struggle to provide timely, detailed feedback on student code. We developed Mark My Works, a local autograding system that combines traditional unit testing with LLM-generated explanations. The system uses role-based prompts to analyze submissions, critique code quality, and generate pedagogical feedback while maintaining transparency in its reasoning process.\n  We piloted the system in a 191-student engineering course, comparing AI-generated assessments with human grading on 79 submissions. While AI scores showed no linear correlation with human scores (r = -0.177, p = 0.124), both systems exhibited similar left-skewed distributions, suggesting they recognize comparable quality hierarchies despite different scoring philosophies. The AI system demonstrated more conservative scoring (mean: 59.95 vs 80.53 human) but generated significantly more detailed technical feedback."}
{"id": "2601.10112", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10112", "abs": "https://arxiv.org/abs/2601.10112", "authors": ["Tsvi Cherny-Shahar", "Amiram Yehudai"], "title": "Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants", "comment": "35 pages, 5 figures", "summary": "Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects where cross language dependencies are encoded across heterogeneous build systems and tooling. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence backed architectural map that represents buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges that trace back to concrete build and test definitions. We also present SPADE, a deterministic extractor that constructs RIG from build and test artifacts (currently with an automatic CMake plugin based on the CMake File API and CTest metadata), and exposes RIG as an LLM friendly JSON view that agents can treat as the authoritative description of repository structure.\n  We evaluate three commercial agents (Claude Code, Cursor, Codex) on eight repositories spanning low to high build oriented complexity, including the real world MetaFFI project. Each agent answers thirty structured questions per repository with and without RIG in context, and we measure accuracy, wall clock completion time, and efficiency (seconds per correct answer). Across repositories and agents, providing RIG improves mean accuracy by 12.2\\% and reduces completion time by 53.9\\%, yielding a mean 57.8\\% reduction in seconds per correct answer. Gains are larger in multilingual repositories, which improve by 17.7\\% in accuracy and 69.5\\% in efficiency on average, compared to 6.6\\% and 46.1\\% in single language repositories. Qualitative analysis suggests that RIG shifts failures from structural misunderstandings toward reasoning mistakes over a correct structure, while rare regressions highlight that graph based reasoning quality remains a key factor."}
{"id": "2601.10164", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10164", "abs": "https://arxiv.org/abs/2601.10164", "authors": ["Themistoklis Diamantopoulos", "Dimosthenis Natsos", "Andreas L. Symeonidis"], "title": "Towards Online Malware Detection using Process Resource Utilization Metrics", "comment": "6 pages, 6 figures", "summary": "The rapid growth of Cloud Computing and Internet of Things (IoT) has significantly increased the interconnection of computational resources, creating an environment where malicious software (malware) can spread rapidly. To address this challenge, researchers are increasingly utilizing Machine Learning approaches to identify malware through behavioral (i.e. dynamic) cues. However, current approaches are limited by their reliance on large labeled datasets, fixed model training, and the assumption that a trained model remains effective over time-disregarding the ever-evolving sophistication of malware. As a result, they often fail to detect evolving malware attacks that adapt over time. This paper proposes an online learning approach for dynamic malware detection, that overcomes these limitations by incorporating temporal information to continuously update its models using behavioral features, specifically process resource utilization metrics. By doing so, the proposed models can incrementally adapt to emerging threats and detect zero-day malware effectively. Upon evaluating our approach against traditional batch algorithms, we find it effective in detecting zero-day malware. Moreover, we demonstrate its efficacy in scenarios with limited data availability, where traditional batch-based approaches often struggle to perform reliably."}
{"id": "2601.10220", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10220", "abs": "https://arxiv.org/abs/2601.10220", "authors": ["Simin Sun", "Miroslaw Staron"], "title": "Agentic Pipelines in Embedded Software Engineering: Emerging Practices and Challenges", "comment": null, "summary": "A new transformation is underway in software engineering, driven by the rapid adoption of generative AI in development workflows. Similar to how version control systems once automated manual coordination, AI tools are now beginning to automate many aspects of programming. For embedded software engineering organizations, however, this marks their first experience integrating AI into safety-critical and resource-constrained environments. The strict demands for determinism, reliability, and traceability pose unique challenges for adopting generative technologies.\n  In this paper, we present findings from a qualitative study with ten senior experts from four companies who are evaluating generative AI-augmented development for embedded software. Through semi-structured focus group interviews and structured brainstorming sessions, we identified eleven emerging practices and fourteen challenges related to the orchestration, responsible governance, and sustainable adoption of generative AI tools. Our results show how embedded software engineering teams are rethinking workflows, roles, and toolchains to enable a sustainable transition toward agentic pipelines and generative AI-augmented development."}
{"id": "2601.10258", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.10258", "abs": "https://arxiv.org/abs/2601.10258", "authors": ["Agnia Sergeyuk", "Eric Huang", "Dariia Karaeva", "Anastasiia Serova", "Yaroslav Golubev", "Iftekhar Ahmed"], "title": "Evolving with AI: A Longitudinal Analysis of Developer Logs", "comment": "Accepted to ICSE'26 Research track. 12 pages, 5 figures, 1 table", "summary": "AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling."}
{"id": "2601.10496", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10496", "abs": "https://arxiv.org/abs/2601.10496", "authors": ["Ali Al-Kaswan", "Claudio Spiess", "Prem Devanbu", "Arie van Deursen", "Maliheh Izadi"], "title": "Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs", "comment": "MSR 2026 Technical Track", "summary": "Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice."}
