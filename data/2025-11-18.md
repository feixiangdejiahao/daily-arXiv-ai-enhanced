<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Modular GPU Programming with Typed Perspectives](https://arxiv.org/abs/2511.11939)
*Manya Bansal,Daniel Sainati,Joseph W. Cutler,Saman Amarasinghe,Jonathan Ragan-Kelley*

Main category: cs.PL

TL;DR: Prism is a new GPU programming language that overcomes the tension between modular code and fine-grained control needed for high-performance collective operations, using a type-based system to safely manage thread behavior and deliver both modularity and performance.


<details>
  <summary>Details</summary>
Motivation: Programming modern GPUs for high performance requires balancing detailed control over individual threads and coordinated collective operations, making modular code error-prone. Existing programming models fail to provide modularity while retaining precise, low-level control.

Method: The authors introduce Prism, a new GPU language based on typed perspectives, to reintroduce modularity while maintaining control over thread behavior. They describe the language design, a core calculus (Bundl), and implement a Prism compiler. They evaluate Prism by implementing GPU kernels and analyzing their safety and performance.

Result: Prism allows programmers to write modular GPU code with built-in safety guarantees and without loss of performance. The language and its compiler successfully deliver both modularity and low-level control needed for collective operations.

Conclusion: Prism's type-level approach enables safe, modular GPU programming while achieving peak performance, proving that modular design and low-level control can coexist on modern GPUs.

Abstract: To achieve peak performance on modern GPUs, one must balance two frames of mind: issuing instructions to individual threads to control their behavior, while simultaneously tracking the convergence of many threads acting in concert to perform collective operations like Tensor Core instructions. The tension between these two mindsets makes modular programming error prone. Functions that encapsulate collective operations, despite being called per-thread, must be executed cooperatively by groups of threads.
  In this work, we introduce Prism, a new GPU language that restores modularity while still giving programmers the low-level control over collective operations necessary for high performance. Our core idea is typed perspectives, which materialize, at the type level, the granularity at which the programmer is controlling the behavior of threads. We describe the design of Prism, implement a compiler for it, and lay its theoretical foundations in a core calculus called Bundl. We implement state-of-the-art GPU kernels in Prism and find that it offers programmers the safety guarantees needed to confidently write modular code without sacrificing performance.

</details>


### [2] [The Search for Constrained Random Generators](https://arxiv.org/abs/2511.12253)
*Harrison Goldstein,Hila Peleg,Cassia Torczon,Daniel Sainati,Leonidas Lampropoulos,Benjamin C. Pierce*

Main category: cs.PL

TL;DR: This paper introduces a novel method for efficiently generating test inputs that satisfy given constraints in property-based testing, using deductive synthesis and Lean theorem prover automation, culminating in the Palamedes library.


<details>
  <summary>Details</summary>
Motivation: Property-based testing requires generating valid test cases that satisfy complex predicates, but finding efficient random sampling methods for constrained values is challenging because such values are often sparse and subject to preconditions.

Method: The authors propose using deductive program synthesis techniques. They introduce synthesis rules grounded in the denotational semantics of generators, enabling automatic construction of generators. Recursive predicates are handled by rewriting them as catamorphisms and matching them with appropriate anamorphisms, simplifying the synthesis process.

Result: Their system, Palamedes, is implemented as a library for the Lean theorem prover. It leverages existing proof-search tactics in Lean for generator synthesis, making it extensible and able to adopt further automation advances.

Conclusion: The approach offers a theoretically simpler yet highly expressive solution to the constrained random generation problem for property-based testing, especially for recursive predicates. The implementation in Lean creates a practical and extensible tool for the community.

Abstract: Among the biggest challenges in property-based testing (PBT) is the constrained random generation problem: given a predicate on program values, randomly sample from the set of all values satisfying that predicate, and only those values. Efficient solutions to this problem are critical, since the executable specifications used by PBT often have preconditions that input values must satisfy in order to be valid test cases, and satisfying values are often sparsely distributed.
  We propose a novel approach to this problem using ideas from deductive program synthesis. We present a set of synthesis rules, based on a denotational semantics of generators, that give rise to an automatic procedure for synthesizing correct generators. Our system handles recursive predicates by rewriting them as catamorphisms and then matching with appropriate anamorphisms; this is theoretically simpler than other approaches to synthesis for recursive functions, yet still extremely expressive.
  Our implementation, Palamedes, is an extensible library for the Lean theorem prover. The synthesis algorithm itself is built on standard proof-search tactics, reducing implementation burden and allowing the algorithm to benefit from further advances in Lean proof automation.

</details>


### [3] [Equivalence Checking of ML GPU Kernels](https://arxiv.org/abs/2511.12638)
*Kshitij Dubey,Benjamin Driscoll,Anjiang Wei,Neeraj Kayal,Rahul Sharma,Alex Aiken*

Main category: cs.PL

TL;DR: The paper presents VOLTA, the first tool for formally verifying the equivalence—and thus correctness—of GPU kernels, effectively guaranteeing that optimizations by humans, LLMs, or compilers do not break ML computations.


<details>
  <summary>Details</summary>
Motivation: Rapid advances in deep learning and LLMs have led to widespread use of GPU kernels, making their optimization crucial for both performance and cost. However, generated or optimized GPU kernels lack formal correctness guarantees.

Method: The authors introduce VOLTA, an equivalence checker designed to formally verify the correctness of GPU kernels, whether they are optimized manually, by LLMs, or by compilers.

Result: VOLTA is proven to be sound and complete for a well-defined class of GPU kernels, and it can successfully verify complex ML computations including convolutions, matrix multiplications, and attention mechanisms.

Conclusion: VOLTA fills the verification gap by providing formal, automated equivalence checking for GPU kernels in ML, thus ensuring correctness in hand-optimized, LLM-generated, and compiled code.

Abstract: With the rapid progress of deep learning and large language models (LLMs), companies now spend enormous sums executing GPU kernels. These kernels have, therefore, become prime targets for aggressive optimization. Recent efforts increasingly leverage LLMs to generate GPU kernels, but make no formal guarantees about the generated kernels. We present the first equivalence checker for GPU kernels and use it to formally verify the correctness of machine learning (ML) kernels optimized by hand, by LLMs, and by compilers. We show that our equivalence checker is sound and, for a well-defined class of GPU kernels which includes the programs of interest, complete. Our implementation, VOLTA, can verify ML computations such as convolutions, matrix multiplications, and various attention mechanisms.

</details>


### [4] [Cost-Driven Synthesis of Sound Abstract Interpreters](https://arxiv.org/abs/2511.13663)
*Qiuhan Gu,Avaljot Singh,Gagandeep Singh*

Main category: cs.PL

TL;DR: This paper presents a unified, mathematically principled framework combining LLMs, syntactic/semantic validation, and quantitative feedback to automatically synthesize sound abstract interpreters for neural network verification. The approach matches and surpasses hand-crafted methods, discovering new interpreters for complex cases.


<details>
  <summary>Details</summary>
Motivation: The difficulty of constructing abstract interpreters that offer guaranteed global soundness is a major challenge in the field of abstract interpretation, especially for neural network verification.

Method: The authors formulate the problem of synthesizing sound interpreters as a constrained optimization problem. They introduce a novel cost function to measure unsoundness based on mathematical foundations, under strict syntactic and semantic constraints. They further propose a framework integrating LLM-based candidate generation, syntactic and semantic validation, and cost-guided feedback.

Result: Empirical evidence shows that their framework matches the quality of manually designed transformers and discovers sound, high-precision transformers for complex nonlinear operators not found in current literature.

Conclusion: Modern LLMs can effectively aid in the automatic synthesis of sound abstract interpreters, reducing manual effort and even surpassing existing solutions for some complex tasks.

Abstract: Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.

</details>
