{"id": "2510.13857", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13857", "abs": "https://arxiv.org/abs/2510.13857", "authors": ["Qiang Xu", "Xiangyu Wen", "Changran Xu", "Zeju Li", "Jianyuan Zhong"], "title": "From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering", "comment": null, "summary": "The advent of powerful Large Language Models (LLMs) has ushered in an ``Age\nof the Agent,'' enabling autonomous systems to tackle complex goals. However,\nthe transition from prototype to production is hindered by a pervasive ``crisis\nof craft,'' resulting in agents that are brittle, unpredictable, and ultimately\nuntrustworthy in mission-critical applications. This paper argues this crisis\nstems from a fundamental paradigm mismatch -- attempting to command inherently\nprobabilistic processors with the deterministic mental models of traditional\nsoftware engineering. To solve this crisis, we introduce a governance-first\nparadigm for principled agent engineering, embodied in a formal architecture we\ncall ArbiterOS.", "AI": {"tldr": "LLMs empower autonomous agents, but existing engineering approaches lead to unreliable systems. This paper proposes the ArbiterOS architecture, emphasizing governance as the key to building robust, trustworthy AI agents.", "motivation": "LLMs have enabled autonomous agents with complex capabilities, but current engineering practices fail to create robust and trustworthy systems for critical tasks.", "method": "Proposal of a 'governance-first' paradigm and the formal architecture ArbiterOS to address agent brittleness and unpredictability.", "result": "Introduction of ArbiterOS, a novel governance framework aimed at principled agent engineering for reliable deployment.", "conclusion": "Resolving the mismatch between probabilistic AI and deterministic software design requires new paradigms like ArbiterOS for creating trustworthy agents."}}
{"id": "2510.13859", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13859", "abs": "https://arxiv.org/abs/2510.13859", "authors": ["Ruchit Rawal", "Jeffrey Yang Fan Chiang", "Chihao Shen", "Jeffery Siyuan Tian", "Aastha Mahajan", "Tom Goldstein", "Yizheng Chen"], "title": "Benchmarking Correctness and Security in Multi-Turn Code Generation", "comment": null, "summary": "AI coding assistants powered by large language models (LLMs) have transformed\nsoftware development, significantly boosting productivity. While existing\nbenchmarks evaluate the correctness and security of LLM-generated code, they\nare typically limited to single-turn tasks that do not reflect the iterative\nnature of real-world development. We introduce MT-Sec, the first benchmark to\nsystematically evaluate both correctness and security in multi-turn coding\nscenarios. We construct this using a synthetic data pipeline that transforms\nexisting single-turn tasks into semantically aligned multi-turn interaction\nsequences, allowing reuse of original test suites while modeling the complexity\nof real-world coding processes. We evaluate 32 open- and closed-source models,\nand three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in\n\"correct and secure\" outputs from single-turn to multi-turn settings -- even\namong state-of-the-art models. Beyond full-program generation, we also evaluate\nmodels on multi-turn code-diff generation -- an unexplored yet practically\nrelevant setting -- and find that models perform worse here, with increased\nrates of functionally incorrect and insecure outputs. Finally, we find that\nwhile agent scaffoldings boost single-turn code generation performance, they\nare not quite as effective in multi-turn evaluations. Together, these findings\nhighlight the need for benchmarks that jointly evaluate correctness and\nsecurity in multi-turn, real-world coding workflows.", "AI": {"tldr": "Current benchmarks overlook the iterative, multi-turn nature of actual coding, leading to inflated assessments of LLM-assisted development. MT-Sec reveals that coding assistants have significantly reduced performance and security in realistic, multi-turn scenarios, emphasizing an urgent need for improved, context-aware evaluation methods.", "motivation": "Current benchmarks for evaluating AI coding assistants largely focus on single-turn tasks, which fail to mirror the iterative nature of genuine software development and neglect multi-step interactions crucial for real-world coding.", "method": "The authors introduce MT-Sec, a novel benchmark that systematically assesses both correctness and security in multi-turn coding scenarios using a synthetic data pipeline. This pipeline adapts single-turn tasks into multi-turn sequences, enabling more realistic evaluation without discarding established test suites. Furthermore, the study benchmarks 32 diverse models and assesses agent-scaffolding techniques in these scenarios.", "result": "There is a consistent 20-27% decrease in 'correct and secure' outputs when moving from single-turn to multi-turn coding tasks, even for state-of-the-art models. Models also perform worse in multi-turn code-diff generation, a relevant real-world setting, with higher rates of incorrect and insecure outputs. Agent scaffoldings, although beneficial in single-turn code generation, are less effective in multi-turn scenarios.", "conclusion": "Multi-turn, joint correctness and security evaluation is crucial, as current benchmarks and agent techniques do not sufficiently capture the challenges of real-world iterative coding workflows. The introduction of MT-Sec reveals notable shortcomings of AI coding assistants and points to a need for further benchmark development."}}
{"id": "2510.13914", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13914", "abs": "https://arxiv.org/abs/2510.13914", "authors": ["Janghan Yoon", "Jaegwan Cho", "Junhyeok Kim", "Jiwan Chung", "Jaehyun Jeon", "Youngjae Yu"], "title": "A11YN: aligning LLMs for accessible web UI code generation", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating functional and aesthetic web interfaces directly from\ninstructions. However, these models often replicate accessibility flaws from\ntheir training data, resulting in interfaces that exclude users with diverse\nneeds and contexts. To address this gap, we introduce A11yn, the first method\nthat aligns code-generating LLMs to reliably produce accessibility-compliant\nweb UIs. A11yn optimizes a novel reward function that penalizes violations of\nthe Web Content Accessibility Guidelines (WCAG), with penalties scaled to the\nseverity of each violation as identified by an accessibility testing engine. To\nsupport training, we construct UIReq-6.8K, a dataset of 6,800 diverse\ninstructions for web UI generation. For evaluation, we introduce RealUIReq-300,\na benchmark of 300 real-world web UI requests grounded and manually curated\nfrom public web pages, spanning a broad range of use cases. Empirical results\nshow that A11yn significantly outperforms strong baselines, lowering the\nInaccessibility Rate by 60% over the base model while preserving semantic\nfidelity and visual quality of generated UIs. These findings demonstrate that\naccessibility can be systematically optimized within LLMs, showing the\nfeasibility of aligning code generation for accessibility.", "AI": {"tldr": "A11yn introduces a novel reward-based approach for training LLMs to generate web UIs that comply with accessibility standards (WCAG), achieves a 60% reduction in accessibility issues, and shows that LLMs can be systematically improved for accessibility without sacrificing UI quality.", "motivation": "The motivation is to address the problem of large language models (LLMs) generating web interfaces that carry forward accessibility flaws found in their training data, making web UIs less inclusive for users with diverse needs.", "method": "The paper introduces A11yn, a method that aligns code-generating LLMs to produce accessibility-compliant web UIs. This is done by optimizing a reward function that penalizes violations of the Web Content Accessibility Guidelines (WCAG), with penalties scaled according to the severity identified by an accessibility testing engine. The authors also create the UIReq-6.8K dataset for training, and the RealUIReq-300 benchmark for evaluation.", "result": "A11yn significantly reduces the Inaccessibility Rate by 60% compared to the base model, while maintaining the semantic fidelity and visual quality of generated web UIs.", "conclusion": "The study demonstrates that accessibility can be systematically optimized in LLMs, proving the feasibility of aligning code generation processes for increased web accessibility."}}
{"id": "2510.13992", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13992", "abs": "https://arxiv.org/abs/2510.13992", "authors": ["Quoc Hung Le", "Thanh Le-Cong", "Bach Le", "Bowen Xu"], "title": "Signature in Code Backdoor Detection, how far are we?", "comment": "20 pages, 3 figures", "summary": "As Large Language Models (LLMs) become increasingly integrated into software\ndevelopment workflows, they also become prime targets for adversarial attacks.\nAmong these, backdoor attacks are a significant threat, allowing attackers to\nmanipulate model outputs through hidden triggers embedded in training data.\nDetecting such backdoors remains a challenge, and one promising approach is the\nuse of Spectral Signature defense methods that identify poisoned data by\nanalyzing feature representations through eigenvectors. While some prior works\nhave explored Spectral Signatures for backdoor detection in neural networks,\nrecent studies suggest that these methods may not be optimally effective for\ncode models. In this paper, we revisit the applicability of Spectral\nSignature-based defenses in the context of backdoor attacks on code models. We\nsystematically evaluate their effectiveness under various attack scenarios and\ndefense configurations, analyzing their strengths and limitations. We found\nthat the widely used setting of Spectral Signature in code backdoor detection\nis often suboptimal. Hence, we explored the impact of different settings of the\nkey factors. We discovered a new proxy metric that can more accurately estimate\nthe actual performance of Spectral Signature without model retraining after the\ndefense.", "AI": {"tldr": "Standard settings for Spectral Signature defenses are often inadequate for detecting backdoors in code models. This paper finds better configurations and introduces a new technique for evaluating defense effectiveness without retraining, improving LLM security in development environments.", "motivation": "Large Language Models (LLMs), especially those used in software development, are vulnerable to backdoor attacks via adversarial manipulation of training data. Current detection methods, like Spectral Signature, are promising but may not perform optimally on code models, necessitating a closer evaluation.", "method": "The paper systematically evaluates the effectiveness of Spectral Signature-based defenses against backdoor attacks in code models, examining various attack scenarios and defense configurations. The study also explores different settings of key Spectral Signature parameters to identify their impacts.", "result": "The authors found that the commonly used configurations for Spectral Signature in code backdoor detection are often not optimal. By testing alternative settings, they identified a new proxy metric that more accurately estimates the defense\u2019s performance without requiring model retraining.", "conclusion": "Spectral Signature-based defenses require careful tuning for effective use in code model backdoor detection. The proposed proxy metric offers a practical improvement, enabling more accurate performance estimation and potentially more robust defenses."}}
{"id": "2510.14558", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.14558", "abs": "https://arxiv.org/abs/2510.14558", "authors": ["Amir Mohammad Fadaei Ayyam", "Michael Sammler"], "title": "HITrees: Higher-Order Interaction Trees", "comment": null, "summary": "Recent years have witnessed the rise of compositional semantics as a\nfoundation for formal verification of complex systems. In particular,\ninteraction trees have emerged as a popular denotational semantics. Interaction\ntrees achieve compositionality by providing a reusable library of effects.\nHowever, their notion of effects does not support higher-order effects, i.e.,\neffects that take or return monadic computations. Such effects are essential to\nmodel complex semantic features like parallel composition and call/cc.\n  We introduce Higher-Order Interaction Trees (HITrees), the first variant of\ninteraction trees to support higher-order effects in a non-guarded type theory.\nHITrees accomplish this through two key techniques: first, by designing the\nnotion of effects such that the fixpoints of effects with higher-order input\ncan be expressed as inductive types inside the type theory; and second, using\ndefunctionalization to encode higher-order outputs into a first-order\nrepresentation. We implement HITrees in the Lean proof assistant, accompanied\nby a comprehensive library of effects including concurrency, recursion, and\ncall/cc. Furthermore, we provide two interpretations of HITrees, as state\ntransition systems and as monadic programs. To demonstrate the expressiveness\nof HITrees, we apply them to define the semantics of a language with parallel\ncomposition and call/cc.", "AI": {"tldr": "HITrees extend interaction trees to support higher-order effects, enabling sophisticated semantic modeling (e.g., concurrency, call/cc) inside a non-guarded type theory and implemented in Lean. This increases the compositional power of formal verification tools.", "motivation": "Compositional semantics are important for formally verifying complex systems, and interaction trees are popular in this domain due to their reuse and modularity. However, traditional interaction trees do not support higher-order effects (effects that yield or consume monadic computations), which are critical for modeling advanced features like parallelism and control flow (call/cc).", "method": "The authors introduce 'Higher-Order Interaction Trees' (HITrees), a new type of interaction tree that supports higher-order effects within a non-guarded type theory. They achieve this through (1) designing effects such that higher-order input fixpoints can be inductive types, and (2) using defunctionalization to encode higher-order outputs as first-order representations. The HITree framework is implemented in the Lean proof assistant, including a comprehensive library of effects (e.g., concurrency, call/cc).", "result": "The authors demonstrate two interpretations of HITrees: as state transition systems and as monadic programs. They show that HITrees can express the semantics of languages with features like parallel composition and call/cc, showcasing their greater expressiveness over previous approaches.", "conclusion": "HITrees provide the first support for higher-order effects in interaction trees within non-guarded type theory, enabling formalization and verification of more complex system semantics. Their Lean implementation and comprehensive effect library further bolster the practical utility of the approach."}}
{"id": "2510.14036", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14036", "abs": "https://arxiv.org/abs/2510.14036", "authors": ["Qiushi Wu", "Yue Xiao", "Dhilung Kirat", "Kevin Eykholt", "Jiyong Jang", "Douglas Lee Schales"], "title": "One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery", "comment": null, "summary": "Fixing bugs in large programs is a challenging task that demands substantial\ntime and effort. Once a bug is found, it is reported to the project\nmaintainers, who work with the reporter to fix it and eventually close the\nissue. However, across the program, there are often similar code segments,\nwhich may also contain the bug, but were missed during discovery. Finding and\nfixing each recurring bug instance individually is labor intensive. Even more\nconcerning, bug reports can inadvertently widen the attack surface as they\nprovide attackers with an exploitable pattern that may be unresolved in other\nparts of the program.\n  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear\nrepeatedly across various code segments of a program or even in different\nprograms, stemming from a same root cause, but are unresolved. Our\ninvestigation reveals that RPBs are widespread and can significantly compromise\nthe security of software programs. This paper introduces BugStone, a program\nanalysis system empowered by LLVM and a Large Language Model (LLM). The key\nobservation is that many RPBs have one patched instance, which can be leveraged\nto identify a consistent error pattern, such as a specific API misuse. By\nexamining the entire program for this pattern, it is possible to identify\nsimilar sections of code that may be vulnerable. Starting with 135 unique RPBs,\nBugStone identified more than 22K new potential issues in the Linux kernel.\nManual analysis of 400 of these findings confirmed that 246 were valid. We also\ncreated a dataset from over 1.9K security bugs reported by 23 recent top-tier\nconference works. We manually annotate the dataset, identify 80 recurring\npatterns and 850 corresponding fixes. Even with a cost-efficient model choice,\nBugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.", "AI": {"tldr": "The paper presents BugStone, a system that uses LLVM and a Large Language Model to automatically detect recurring pattern bugs (RPBs) in programs, revealing thousands of previously unnoticed issues in the Linux kernel and demonstrating high precision and accuracy.", "motivation": "Fixing bugs individually is laborious and incomplete, as similar vulnerabilities can persist elsewhere. Public bug reports may inadvertently reveal exploitable patterns to attackers. There is a need for scalable techniques to discover and address recurring bugs across large codebases.", "method": "The method involves leveraging patched instances of bugs to extract recurring patterns, which are then used to scan codebases for similar vulnerabilities using BugStone, built on LLVM and a Large Language Model. Manual validation and dataset creation support empirical results.", "result": "BugStone identified over 22,000 potential issues in the Linux kernel from 135 RPBs, with manual review confirming 246 of 400 sampled findings. A further dataset of 1,900+ bugs and 850 fixes was compiled, with BugStone achieving 92.2% precision and 79.1% pairwise accuracy.", "conclusion": "BugStone successfully identifies RPBs at scale, significantly improving bug detection efficiency and potentially enhancing software security. Its combination of program analysis and LLM demonstrates strong results with high precision and accuracy."}}
{"id": "2510.14279", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.14279", "abs": "https://arxiv.org/abs/2510.14279", "authors": ["Evangelos Lamprou", "Seong-Heon Jung", "Mayank Keoliya", "Lukas Lazarek", "Konstantinos Kallas", "Michael Greenberg", "Nikos Vasilakis"], "title": "Caruca: Effective and Efficient Specification Mining for Opaque Software Components", "comment": null, "summary": "A wealth of state-of-the-art systems demonstrate impressive improvements in\nperformance, security, and reliability on programs composed of opaque\ncomponents, such as Unix shell commands. To reason about commands, these\nsystems require partial specifications. However, creating such specifications\nis a manual, laborious, and error-prone process, limiting the practicality of\nthese systems. This paper presents Caruca, a system for automatic specification\nmining for opaque commands. To overcome the challenge of language diversity\nacross commands, Caruca first instruments a large language model to translate a\ncommand's user-facing documentation into a structured invocation syntax. Using\nthis representation, Caruca explores the space of syntactically valid command\ninvocations and execution environments. Caruca concretely executes each\ncommand-environment pair, interposing at the system-call and filesystem level\nto extract key command properties such as parallelizability and filesystem pre-\nand post-conditions. These properties can be exported in multiple specification\nformats and are immediately usable by existing systems. Applying Caruca across\n60 GNU Coreutils, POSIX, and third-party commands across several\nspecification-dependent systems shows that Caruca generates correct\nspecifications for all but one case, completely eliminating manual effort from\nthe process and currently powering the full specifications for a\nstate-of-the-art static analysis tool.", "AI": {"tldr": "Caruca uses a language model and dynamic analysis to automatically generate formal specifications for command-line tools, replacing manual effort and supporting advanced systems with accurate specs for nearly all tested commands.", "motivation": "Modern systems use powerful tools to improve software quality, but require partial specifications of command-line utilities. Manually creating these specifications is difficult, time-consuming, and prone to error, which limits the adoption and scalability of these advanced systems.", "method": "The authors introduce Caruca, a system that automatically mines specifications for opaque commands. Caruca uses a large language model to translate user documentation into a structured syntax for command invocation. It then synthesizes and executes possible invocations in varied environments, monitoring system calls and filesystem access to infer command properties such as parallelizability and file dependencies. It exports these as formal specifications.", "result": "Caruca was tested on 60 commands (GNU Coreutils, POSIX, and third-party), and successfully generated correct specifications for all but one command. These specifications were immediately usable by various systems and enabled complete automation of the formerly manual process.", "conclusion": "Caruca automates and accelerates the generation of command specifications, previously a laborious manual step, and demonstrably improves the practicality of systems relying on these specifications by eliminating human effort and errors."}}
{"id": "2510.14115", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14115", "abs": "https://arxiv.org/abs/2510.14115", "authors": ["Philipp Bauerfeind", "Amir Salarpour", "David Fernandez", "Pedram MohajerAnsari", "Johannes Reschke", "Mert D. Pes\u00e9"], "title": "David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation", "comment": null, "summary": "Scenario simulation is central to testing autonomous driving systems. Scenic,\na domain-specific language (DSL) for CARLA, enables precise and reproducible\nscenarios, but NL-to-Scenic generation with large language models (LLMs)\nsuffers from scarce data, limited reproducibility, and inconsistent metrics. We\nintroduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a\ndifficulty-stratified 30-case test split, an Example Retriever, and 14\nprompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four\nproprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine\nopen-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using\ntext metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics\n(compilation and generation), and compare them with an expert study (n=11).\nEDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of\nEDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking\nfidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88\npercent of its expert score on local hardware. Retrieval-augmented prompting,\nFew-Shot with Example Retriever (FSER), consistently boosts smaller models, and\nscaling shows diminishing returns beyond mid-size, with Qwen2.5Coder\noutperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a\nstandardized, reproducible basis for evaluating Scenic code generation and\nindicate that mid-size open-source models are practical, cost-effective options\nfor autonomous-driving scenario programming.", "AI": {"tldr": "The paper introduces NL2Scenic, a new dataset and framework for generating test scenarios for autonomous driving from natural language using LLMs. They benchmark 13 models, finding that some open-source, mid-size models are nearly as effective as proprietary leaders, especially with retrieval-augmented prompting. A new evaluation metric, EDIT-COMP, reliably proxies human judgment. The work advances standardized, reproducible, and practical evaluation for scenario programming in autonomous driving.", "motivation": "Testing autonomous driving systems requires precise, reproducible scenario simulations. Existing methods for generating scenario code (in Scenic DSL for CARLA) from natural language using large language models are hampered by a lack of high-quality data, inconsistent metrics, and limited reproducibility.", "method": "The authors introduce NL2Scenic, an open dataset with 146 NL/Scenic code pairs, a stratified test split, a retrieval-augmented prompting framework, and 14 prompt variants. They evaluate 13 large language models (both proprietary and open-source) using a range of text and execution metrics, and validate scoring against human expert judgments. They further propose EDIT-COMP, a new metric combining similarity and compilation success.", "result": "GPT-4o achieved the best results overall, but the open-source Qwen2.5Coder-14B model reaches about 88% of the expert-level score when run locally. Retrieval-augmented prompting, especially Few-Shot with Example Retriever, consistently helps smaller models. Scaling up model size showed diminishing returns after a certain point. Qwen2.5Coder outperforms CodeLlama at similar sizes. The EDIT-SIM metric best aligns with human evaluation, and their new EDIT-COMP metric provides a reliable automated proxy.", "conclusion": "NL2Scenic and the EDIT-COMP metric offer a standardized, reproducible basis for evaluating LLMs on Scenic code generation, and show that mid-size, open-source models are practical and cost-effective for programming autonomous driving scenarios."}}
{"id": "2510.14292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14292", "abs": "https://arxiv.org/abs/2510.14292", "authors": ["Haolin Pan", "Hongbin Zhang", "Mingjie Xing", "Yanjun Wu"], "title": "A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning", "comment": null, "summary": "Compiler pass auto-tuning is critical for enhancing software performance, yet\nfinding the optimal pass sequence for a specific program is an NP-hard problem.\nTraditional, general-purpose optimization flags like -O3 and -Oz adopt a\none-size-fits-all approach, often failing to unlock a program's full\nperformance potential. To address this challenge, we propose a novel Hybrid,\nKnowledge-Guided Evolutionary Framework. This framework intelligently guides\nonline, personalized optimization using knowledge extracted from a large-scale\noffline analysis phase. During the offline stage, we construct a comprehensive\ncompilation knowledge base composed of four key components: (1) Pass Behavioral\nVectors to quantitatively capture the effectiveness of each optimization; (2)\nPass Groups derived from clustering these vectors based on behavior similarity;\n(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a\nlibrary of Prototype Pass Sequences evolved for distinct program types. In the\nonline stage, a bespoke genetic algorithm leverages this rich knowledge base\nthrough specially designed, knowledge-infused genetic operators. These\noperators transform the search by performing semantically-aware recombination\nand targeted, restorative mutations. On a suite of seven public datasets, our\nframework achieves an average of 11.0% additional LLVM IR instruction reduction\nover the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art\ncapability in discovering personalized, high-performance optimization\nsequences.", "AI": {"tldr": "This paper introduces a novel framework that uses large-scale offline analysis and a knowledge-driven evolutionary algorithm to personalize compiler pass tuning. It outperforms traditional optimization flags, achieving an average 11% reduction in LLVM IR instructions over opt -Oz on seven datasets.", "motivation": "Optimizing compiler pass sequences for individual programs can yield significant performance benefits, but the search for optimal sequences is an NP-hard problem. Traditional compiler flags such as -O3 and -Oz are generic and do not provide personalized optimizations.", "method": "The authors proposed a Hybrid, Knowledge-Guided Evolutionary Framework. It uses an offline phase to build a comprehensive compilation knowledge base with: Pass Behavioral Vectors, Pass Groups (via clustering), a Synergy Pass Graph, and a library of Prototype Pass Sequences. In the online phase, a bespoke genetic algorithm uses this knowledge base, employing knowledge-infused genetic operators for semantically-aware recombination and targeted mutations.", "result": "On seven public datasets, their framework achieves 11.0% additional LLVM IR instruction reduction on average compared to the opt -Oz baseline, suggesting improved personalized, high-performance optimization sequences.", "conclusion": "The proposed approach offers a state-of-the-art solution for personalized compiler optimization, surpassing traditional methods with a significant improvement in instruction reduction, thanks to its knowledge-guided, hybrid evolutionary process."}}
{"id": "2510.14339", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14339", "abs": "https://arxiv.org/abs/2510.14339", "authors": ["Jialu Zhang", "Jialiang Gu", "Wangmeiyu Zhang", "Jos\u00e9 Pablo Cambronero", "John Kolesar", "Ruzica Piskac", "Daming Li", "Hanyuan Shi"], "title": "A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments", "comment": null, "summary": "Online programming platforms such as Codeforces and LeetCode attract millions\nof users seeking to learn to program or refine their skills for industry\ninterviews. A major challenge for these users is the Time Limit Exceeded (TLE)\nerror, triggered when a program exceeds the execution time bound. Although\ndesigned as a performance safeguard, TLE errors are difficult to resolve: error\nmessages provide no diagnostic insight, platform support is minimal, and\nexisting debugging tools offer little help. As a result, many users abandon\ntheir submissions after repeated TLE failures.\n  This paper presents the first large-scale empirical study of TLE errors in\nonline programming. We manually analyzed 1000 Codeforces submissions with TLE\nerrors, classified their root causes, and traced how users attempted to fix\nthem. Our analysis shows that TLE errors often arise not only from inefficient\nalgorithms but also from infinite loops, improper data structure use, and\ninefficient I/O, challenging the conventional view that TLEs are purely\nperformance issues.\n  Guided by these findings, we introduce Nettle, the first automated repair\ntool specifically designed for TLE errors, and Nettle-Eval, the first framework\nfor evaluating TLE repairs. Integrating LLMs with targeted automated feedback\ngenerated by the compiler and test cases, Nettle produces small, correct code\nedits that eliminate TLEs while preserving functionality. Evaluated on the same\n1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the\nstrongest LLM baseline, and all of its repairs pass both Nettle-Eval and the\nplatform's official checker, confirming the reliability of our framework.", "AI": {"tldr": "TLE errors on platforms like Codeforces/LeetCode stem from various code issues, not just algorithm efficiency. This paper analyzes real TLE cases and introduces Nettle, an automated tool that fixes almost all TLEs using LLMs and smart feedback. Nettle outperforms alternatives and reliably resolves TLEs, improving programming practice.", "motivation": "Millions of users on online programming platforms suffer from Time Limit Exceeded (TLE) errors, which are hard to diagnose and resolve due to vague error messages and limited support tools.", "method": "The authors manually analyzed 1000 Codeforces submissions with TLE errors to classify root causes and user repair strategies. They then created Nettle\u2014an automated repair tool using LLMs and compiler-generated feedback, along with Nettle-Eval, a dedicated evaluation framework.", "result": "TLE errors arise from various causes beyond inefficient algorithms, such as infinite loops, poor data structures, and bad I/O handling. The Nettle tool fixes TLEs in 98.5% of cases, outperforming current LLM solutions; all fixes pass rigorous functional checks.", "conclusion": "TLE errors are more nuanced than previously thought, with multiple root causes. The Nettle tool and Nettle-Eval framework offer effective, reliable automated repair for TLE errors, significantly improving user outcomes on programming platforms."}}
{"id": "2510.14341", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14341", "abs": "https://arxiv.org/abs/2510.14341", "authors": ["Xu He", "Shu Wang", "Kun Sun"], "title": "PathFix: Automated Program Repair with Expected Path", "comment": "This is the author's version of a paper accepted at SecDev 2025\n  (IEEE)", "summary": "Automated program repair (APR) techniques are effective in fixing inevitable\ndefects in software, enhancing development efficiency and software robustness.\nHowever, due to the difficulty of generating precise specifications, existing\nAPR methods face two main challenges: generating too many plausible patch\ncandidates and overfitting them to partial test cases. To tackle these\nchallenges, we introduce a new APR method named PathFix, which leverages\npath-sensitive constraints extracted from correct execution paths to generate\npatches for repairing buggy code. It is based on one observation: if a buggy\nprogram is repairable, at least one expected path is supposed to replace the\nfault path in the patched program. PathFix operates in four main steps. First,\nit traces fault paths reaching the fault output in the buggy program. Second,\nit derives expected paths by analyzing the desired correct output on the\ncontrol flow graph, where an expected path defines how a feasible patch leads\nto the correct execution. Third, PathFix generates and evaluates patches by\nsolving state constraints along the expected path. Fourth, we validate the\ncorrectness of the generated patch. To further enhance repair performance and\nmitigate scalability issues introduced by path-sensitive analysis, we integrate\na large language model (LLM) into our framework. Experimental results show that\nPathFix outperforms existing solutions, particularly in handling complex\nprogram structures such as loops and recursion.", "AI": {"tldr": "PathFix is a new automated program repair method that uses path-sensitive constraints and a large language model, outperforming existing approaches, especially with complex code.", "motivation": "Existing automated program repair (APR) techniques struggle due to the difficulty in generating precise specifications, resulting in too many plausible patches and overfitting on partial test cases.", "method": "PathFix is proposed, which uses path-sensitive constraints from correct execution paths to generate program repair patches. The method involves tracing faulty paths, deriving expected paths using control flow analysis, generating/evaluating patches via constraint solving, validating patches, and integrating a large language model (LLM) to improve repair performance and scalability.", "result": "Experimental results demonstrate that PathFix surpasses existing APR methods, especially for complex program structures like loops and recursion.", "conclusion": "PathFix provides an effective solution for automated program repair by addressing candidate patch overgeneration and overfitting, aided by path-sensitive analysis and LLM integration."}}
{"id": "2510.14465", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14465", "abs": "https://arxiv.org/abs/2510.14465", "authors": ["Adem Ait", "Gwendal Jouneaux", "Javier Luis C\u00e1novas Izquierdo", "Jordi Cabot"], "title": "Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects", "comment": "Accepted in the 40th IEEE/ACM International Conference on Automated\n  Software Engineering, ASE 2025", "summary": "The stakeholders involved in software development are becoming increasingly\ndiverse, with both human contributors from varied backgrounds and AI-powered\nagents collaborating together in the process. This situation presents unique\ngovernance challenges, particularly in Open-Source Software (OSS) projects,\nwhere explicit policies are often lacking or unclear. This paper presents the\nvision and foundational concepts for a novel Domain-Specific Language (DSL)\ndesigned to define and enforce rich governance policies in systems involving\ndiverse stakeholders, including agents. This DSL offers a pathway towards more\nrobust, adaptable, and ultimately automated governance, paving the way for more\neffective collaboration in software projects, especially OSS ones.", "AI": {"tldr": "The paper proposes a new Domain-Specific Language to manage governance among diverse human and AI stakeholders in OSS projects, aiming for better collaboration and automated policy enforcement.", "motivation": "The increasing diversity of stakeholders in software development\u2014including human contributors from various backgrounds and AI-powered agents\u2014creates governance challenges, especially within open-source software (OSS) projects where policies are often missing or unclear.", "method": "The paper introduces foundational concepts for a Domain-Specific Language (DSL) specifically designed to define and enforce governance policies in collaborative systems.", "result": "The proposed DSL enables more robust, adaptable, and potentially automated governance mechanisms, facilitating effective collaboration among varied stakeholders including AI agents in OSS projects.", "conclusion": "By leveraging a tailored DSL, software projects can achieve clearer and more effective governance, addressing the complexities brought by stakeholders' diversity and the inclusion of AI agents."}}
{"id": "2510.14509", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.14509", "abs": "https://arxiv.org/abs/2510.14509", "authors": ["Jingyao Liu", "Chen Huang", "Zhizhao Guan", "Wenqiang Lei", "Yang Deng"], "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task", "comment": null, "summary": "E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple\nBDD test scenarios with corresponding Python step implementations for each\nrequirement}, and (iii) a fully automated testing pipeline built on the Behave\nframework. To ensure its quality while reducing the annotation effort, E2EDev\nleverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework\n(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with\nE2EDev}, our analysis reveals a persistent struggle to effectively solve these\ntasks, underscoring the critical need for more effective and cost-efficient\nE2ESD solutions. Our codebase and benchmark are publicly available at\nhttps://github.com/SCUNLP/E2EDev.", "AI": {"tldr": "E2EDev is a new benchmark with automated testing and reduced annotation for end-to-end software development. Existing frameworks and LLMs still struggle, showing the need for improved solutions.", "motivation": "The motivation is to improve the effectiveness and efficiency of End-to-End Software Development (E2ESD) solutions, especially by reducing manual annotation effort while maintaining high quality.", "method": "The authors introduce E2EDev, which includes detailed user requirements, multiple Behavior-Driven Development (BDD) test scenarios with Python implementations, and an automated testing pipeline using the Behave framework. Additionally, a novel Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA) is leveraged to enhance quality and reduce annotation workload.", "result": "Analysis of various E2ESD frameworks and Large Language Model (LLM) backbones on E2EDev shows them struggling to solve these tasks effectively. This highlights the need for better, more cost-effective solutions.", "conclusion": "Current state-of-the-art E2ESD and LLM frameworks are insufficient for fully addressing end-to-end software development tasks, emphasizing a pressing demand for more advanced approaches."}}
{"id": "2510.14625", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14625", "abs": "https://arxiv.org/abs/2510.14625", "authors": ["Mehrdad Saadatmand", "Abbas Khan", "Beatriz Marin", "Ana C. R Paiva", "Nele Van Asch", "Graham Moran", "Felix Cammaerts", "Monique Snoeck", "Alexandra Mendes"], "title": "Software Testing Education and Industry Needs - Report from the ENACTEST EU Project", "comment": "* The paper is going to appear in the proceedings of the 26th\n  International Conference on Product-Focused Software Process Improvement\n  (PROFES 2025). To cite the paper, please check and refer to the PROFES 2025\n  proceedings", "summary": "The evolving landscape of software development demands that software testers\ncontinuously adapt to new tools, practices, and acquire new skills. This study\ninvestigates software testing competency needs in industry, identifies\nknowledge gaps in current testing education, and highlights competencies and\ngaps not addressed in academic literature. This is done by conducting two focus\ngroup sessions and interviews with professionals across diverse domains,\nincluding railway industry, healthcare, and software consulting and performing\na curated small-scale scoping review. The study instrument, co-designed by\nmembers of the ENACTEST project consortium, was developed collaboratively and\nrefined through multiple iterations to ensure comprehensive coverage of\nindustry needs and educational gaps. In particular, by performing a thematic\nqualitative analysis, we report our findings and observations regarding:\nprofessional training methods, challenges in offering training in industry,\ndifferent ways of evaluating the quality of training, identified knowledge gaps\nwith respect to academic education and industry needs, future needs and trends\nin testing education, and knowledge transfer methods within companies. Finally,\nthe scoping review results confirm knowledge gaps in areas such as AI testing,\nsecurity testing and soft skills.", "AI": {"tldr": "This paper explores the mismatch between current software testing educational offerings and what industry requires, identifying major gaps, especially in AI testing, security, and soft skills. Through focus groups, interviews, and a review, it calls for better alignment and modernized training.", "motivation": "The motivation of the paper is to address the rapidly changing landscape of software development, which necessitates that software testers constantly update their skills and knowledge. There is a need to identify the current competency requirements in the industry and understand the disconnects between what is taught in education and what is needed in practice.", "method": "The study uses two key qualitative research methods: focus group sessions and interviews with professionals from several industries (railway, healthcare, software consulting). Additionally, the authors conducted a curated small-scale scoping review. The study instrument was collaboratively designed and iteratively refined with industry and academic partners to ensure relevance. Thematic qualitative analysis was used for data analysis.", "result": "The study identified significant knowledge gaps between academic software testing education and industry needs. Specific gaps were noted in areas like AI testing, security testing, and soft skills. The research also offers insights into current professional training methods, challenges in training delivery, various ways of evaluating training quality, as well as methods for transferring knowledge within companies.", "conclusion": "There are crucial gaps between academic software testing education and the competencies required in industry, particularly in AI testing, security testing, and soft skills. Ongoing collaboration between industry and academia is necessary to ensure that educational programs remain relevant. The findings underline the importance of evolving training methods, evaluation practices, and effective knowledge transfer to keep up with industry demands."}}
{"id": "2510.14635", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.14635", "abs": "https://arxiv.org/abs/2510.14635", "authors": ["Qingyao Li", "Xinyi Dai", "Weiwen Liu", "Xiangyang Li", "Yasheng Wang", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "ATGen: Adversarial Reinforcement Learning for Test Case Generation", "comment": null, "summary": "Large Language Models (LLMs) excel at code generation, yet their outputs\noften contain subtle bugs, for which effective test cases are a critical\nbottleneck. Existing test generation methods, whether based on prompting or\nsupervised fine-tuning, rely on static datasets. This imposes a\n``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover\nnovel or more complex bugs beyond their training scope. To overcome this, we\nintroduce ATGen, a framework that trains a test case generator via adversarial\nreinforcement learning. ATGen pits a test generator against an adversarial code\ngenerator that continuously crafts harder bugs to evade the current policy.\nThis dynamic loop creates a curriculum of increasing difficulty challenging\ncurrent policy. The test generator is optimized via Reinforcement Learning (RL)\nto jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to\nlearn a progressively stronger policy that breaks the fixed-difficulty ceiling\nof static training. Extensive experiments demonstrate that ATGen significantly\noutperforms state-of-the-art baselines. We further validate its practical\nutility, showing it serves as both a more effective filter for Best-of-N\ninference and a higher-quality reward source for training code generation\nmodels. Our work establishes a new, dynamic paradigm for improving the\nreliability of LLM-generated code.", "AI": {"tldr": "ATGen uses adversarial RL to dynamically train test generators for LLM-generated code, outperforming existing methods and enhancing reliability by breaking through previous limitations of static datasets.", "motivation": "Existing automatic test generation methods for LLM-generated code rely on static datasets, inherently limiting their ability to find new or complex bugs outside their training data. There is a need for a dynamic approach that can adapt and challenge LLMs beyond this fixed-difficulty ceiling.", "method": "The paper introduces ATGen, a framework based on adversarial reinforcement learning. In ATGen, a test case generator is pitted against an adversarial code generator that tries to evade existing tests by generating harder bugs. The test generator is trained with reinforcement learning to both increase output accuracy and attack success, progressively learning to find more challenging bugs.", "result": "Experiments show ATGen significantly outperforms state-of-the-art baselines in test case generation for code produced by LLMs. ATGen also proves valuable as an effective filter for Best-of-N inference and as a superior reward source for training code generation models.", "conclusion": "ATGen provides a dynamic, adversarial curriculum for improving LLM code reliability, surpassing static training approaches and establishing a new paradigm for more effective test case generation for LLM-generated code."}}
{"id": "2510.14653", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14653", "abs": "https://arxiv.org/abs/2510.14653", "authors": ["Sven Tarlowski", "Lutz Eckstein"], "title": "Requirement Identification for Traffic Simulations in Driving Simulators", "comment": "2 Pages, 1 figure", "summary": "This paper addresses the challenge of ensuring realistic traffic conditions\nby proposing a methodology that systematically identifies traffic simulation\nrequirements. Using a structured approach based on sub-goals in each study\nphase, specific technical needs are derived for microscopic levels, agent\nmodels, and visual representation. The methodology aims to maintain a high\ndegree of fidelity, enhancing both the validity of experimental outcomes and\nparticipant engagement. By providing a clear link between study objectives and\ntraffic simulation design, this approach supports robust automotive development\nand testing.", "AI": {"tldr": "The paper introduces a systematic methodology to make traffic simulations more realistic and closely tied to study goals, improving both the quality of experimental results and the development process in automotive testing.", "motivation": "There is a challenge in ensuring realistic traffic conditions for simulation studies, which impacts the validity and engagement of experiments in automotive development.", "method": "The paper proposes a structured methodology that systematically identifies traffic simulation requirements by using sub-goals in each study phase. This allows derivation of specific technical needs for microscopic levels, agent models, and visual representation.", "result": "The methodology maintains high fidelity in traffic simulations, which in turn enhances the validity of experimental outcomes and participant engagement.", "conclusion": "The approach enables a clear link between study objectives and traffic simulation design, thus supporting robust automotive development and testing."}}
{"id": "2510.14700", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.14700", "abs": "https://arxiv.org/abs/2510.14700", "authors": ["Bin Liu", "Yanjie Zhao", "Guoai Xu", "Haoyu Wang"], "title": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?", "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable capabilities\nin software engineering and cybersecurity tasks, including code generation,\nvulnerability discovery, and automated testing. One critical but underexplored\napplication is automated web vulnerability reproduction, which transforms\nvulnerability reports into working exploits. Although recent advances suggest\npromising potential, challenges remain in applying LLM agents to real-world web\nvulnerability reproduction scenarios. In this paper, we present the first\ncomprehensive evaluation of state-of-the-art LLM agents for automated web\nvulnerability reproduction. We systematically assess 20 agents from software\nengineering, cybersecurity, and general domains across 16 dimensions, including\ntechnical capabilities, environment adaptability, and user experience factors,\non 3 representative web vulnerabilities. Based on the results, we select three\ntop-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation\non our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types\nand 6 web technologies. Our results reveal that while LLM agents achieve\nreasonable success on simple library-based vulnerabilities, they consistently\nfail on complex service-based vulnerabilities requiring multi-component\nenvironments. Complex environment configurations and authentication barriers\ncreate a gap where agents can execute exploit code but fail to trigger actual\nvulnerabilities. We observe high sensitivity to input guidance, with\nperformance degrading by over 33% under incomplete authentication information.\nOur findings highlight the significant gap between current LLM agent\ncapabilities and the demands of reliable automated vulnerability reproduction,\nemphasizing the need for advances in environmental adaptation and autonomous\nproblem-solving capabilities.", "AI": {"tldr": "This paper comprehensively evaluates LLM agents for automated web vulnerability reproduction, finding they handle simple cases reasonably but struggle with complex, multi-component environments. Missing authentication and sophisticated setup lead to frequent failures, highlighting a big gap between current agent abilities and real-world needs.", "motivation": "While LLM agents have shown promise in software engineering and security tasks, their effectiveness at turning vulnerability reports into working web exploits\u2014a critical step in automating security research\u2014remains underexplored and challenging. Addressing this knowledge gap is important for understanding and improving automated vulnerability reproduction.", "method": "The paper systematically evaluates 20 state-of-the-art LLM agents across 16 dimensions (technical capability, adaptability, user experience, etc.) on 3 typical web vulnerabilities. The best 3 agents are then tested in-depth on 80 real-world CVEs covering 7 vulnerability types and 6 web technologies, assessing their practical abilities in realistic scenarios.", "result": "LLM agents perform reasonably well in reproducing simple, library-based web vulnerabilities but consistently underperform on complex, service-based vulnerabilities that require advanced environmental setup and authentication steps. Performance is also significantly affected when input guidance, like authentication info, is incomplete, with over a 33% drop observed. Currently, agents can execute exploit code but often fail to fully trigger real vulnerabilities in complex scenarios.", "conclusion": "There is a significant performance gap between the capabilities of current LLM agents and the requirements for thorough and reliable automated web vulnerability reproduction, especially in complex, real-world situations. Advances in environment adaptation and autonomous problem-solving are necessary to close this gap."}}
{"id": "2510.14778", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14778", "abs": "https://arxiv.org/abs/2510.14778", "authors": ["Maor Reuben", "Ido Mendel", "Or Feldman", "Moshe Kravchik", "Mordehai Guri", "Rami Puzis"], "title": "Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks", "comment": null, "summary": "Supply chain attacks significantly threaten software security with malicious\ncode injections within legitimate projects. Such attacks are very rare but may\nhave a devastating impact. Detecting spurious code injections using automated\ntools is further complicated as it often requires deciphering the intention of\nboth the inserted code and its context. In this study, we propose an\nunsupervised approach for highlighting spurious code injections by quantifying\ncohesion disruptions in the source code. Using a name-prediction-based cohesion\n(NPC) metric, we analyze how function cohesion changes when malicious code is\nintroduced compared to natural cohesion fluctuations. An analysis of 54,707\nfunctions over 369 open-source C++ repositories reveals that code injection\nreduces cohesion and shifts naming patterns toward shorter, less descriptive\nnames compared to genuine function updates. Considering the sporadic nature of\nreal supply-chain attacks, we evaluate the proposed method with extreme\ntest-set imbalance and show that monitoring high-cohesion functions with NPC\ncan effectively detect functions with injected code, achieving a Precision@100\nof 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that\nautomated cohesion measurements, in general, and name-prediction-based\ncohesion, in particular, may help identify supply chain attacks, improving\nsource code integrity.", "AI": {"tldr": "This paper introduces an unsupervised metric that detects malicious code injections by highlighting disruptions in code cohesion and naming patterns, showing promising results on large open-source datasets despite attack rarity and data imbalance.", "motivation": "Supply chain attacks via code injection are rare but highly damaging, and current automated tools struggle due to the complexity of interpreting injected code's intention and context.", "method": "Unsupervised detection of spurious code injections based on quantifying cohesion disruptions using a name-prediction-based cohesion (NPC) metric.", "result": "Analysis of over 54,000 functions in open-source C++ projects shows that code injection decreases cohesion and leads to shorter, less meaningful function names. The NPC metric effectively highlights injected code, achieving a Precision@100 of 36.41% (1:1,000 ratio) and 12.47% (1:10,000 ratio), even in highly imbalanced test sets.", "conclusion": "Automated cohesion measures, especially name-prediction-based cohesion, provide a practical way to detect supply chain attacks and enhance software code integrity."}}
{"id": "2510.14928", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14928", "abs": "https://arxiv.org/abs/2510.14928", "authors": ["Eric Christopher", "Kevin Crossan", "Wolff Dobson", "Chris Kennelly", "Drew Lewis", "Kun Lin", "Martin Maas", "Parthasarathy Ranganathan", "Emma Rapati", "Brian Yang"], "title": "Instruction Set Migration at Warehouse Scale", "comment": null, "summary": "Migrating codebases from one instruction set architecture (ISA) to another is\na major engineering challenge. A recent example is the adoption of Arm (in\naddition to x86) across the major Cloud hyperscalers. Yet, this problem has\nseen limited attention by the academic community. Most work has focused on\nstatic and dynamic binary translation, and the traditional conventional wisdom\nhas been that this is the primary challenge.\n  In this paper, we show that this is no longer the case. Modern ISA migrations\ncan often build on a robust open-source ecosystem, making it possible to\nrecompile all relevant software from scratch. This introduces a new and\nmultifaceted set of challenges, which are different from binary translation.\n  By analyzing a large-scale migration from x86 to Arm at Google, spanning\nalmost 40,000 code commits, we derive a taxonomy of tasks involved in ISA\nmigration. We show how Google automated many of the steps involved, and\ndemonstrate how AI can play a major role in automatically addressing these\ntasks. We identify tasks that remain challenging and highlight research\nchallenges that warrant further attention.", "AI": {"tldr": "Migrating codebases between ISAs (e.g., x86 to Arm) is evolving beyond binary translation due to open-source tools allowing recompilation. A Google study on large-scale migration reveals new challenges, the usefulness of AI/automation, and unresolved issues needing future research.", "motivation": "Codebase migration from one ISA (e.g., x86 to Arm) is a complex and pressing issue, especially with cloud providers increasingly adopting Arm in addition to x86. Despite its importance, academic research has mainly focused on binary translation and not on broader migration challenges.", "method": "The authors analyze a large-scale migration from x86 to Arm at Google, examining around 40,000 code commits. They derive a taxonomy of migration tasks, investigate how automation and AI assist with the process, and identify remaining challenges.", "result": "Google was able to automate many of the migration steps, with AI playing a significant role in handling tasks. The authors categorize tasks involved in ISA migration and point out which steps remain difficult, proposing areas for further research.", "conclusion": "Modern ISA migration is no longer primarily about binary translation, thanks to open-source ecosystems and recompilation opportunities. However, it introduces diverse new challenges that require automation, AI, and ongoing research to fully address."}}
