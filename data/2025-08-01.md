<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: LLMs are being tested for generating smart contract code from business process descriptions, but their reliability is not yet sufficient for real-world requirements. The authors provide a new automated benchmarking framework and empirical results, suggesting further improvements and responsible integration are needed.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based code generation approaches have limitations when translating business process descriptions into smart contract code. Recent advances with LLMs provide new opportunities, but existing work has not robustly evaluated the correctness and execution of generated code. There is a need for empirical, automated, and scalable evaluation approaches in this context.

Method: The authors conducted an exploratory study using an automated evaluation framework to test various LLMs in generating smart contract code from business process descriptions. They used larger datasets of process models and measured LLMs' ability to enforce crucial aspects like process flow, resource allocation, and data-based conditions.

Result: LLMs do not yet meet the high reliability standards required for smart contract development, as their code generation is imperfect. Empirical benchmarking with the new framework highlights these reliability gaps across different LLM types and sizes.

Conclusion: While LLMs offer promise for smart contract generation, current performance is insufficient for production-level requirements. The introduced benchmarking framework establishes a foundation for responsible integration of LLMs and further research on improving reliability in code generation.

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [2] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: FlowETL introduces an automated, example-based ETL pipeline that creates and applies transformation plans with minimal human input, achieving strong results across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Current ETL solutions require extensive human intervention to design and implement context-specific data transformations that are not easily generalizable, demonstrating a need for more automated and general solutions.

Method: FlowETL is introduced as an example-based autonomous ETL pipeline architecture. It uses an ecosystem of components including a Planning Engine that constructs transformation plans from paired input-output dataset samples, an ETL worker that executes these plans, and monitoring tools for observability.

Result: The experiments demonstrate that FlowETL has promising generalization capabilities, effectively standardizing and preparing datasets across 14 different datasets with varied domains, structures, and sizes.

Conclusion: FlowETL offers a significant advancement in ETL automation by reducing the need for manual, context-specific transformations, enabling automated, generalized dataset preparation through example-based planning and execution.

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [3] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: This paper proposes 'vibe modeling'—a novel approach that integrates AI (LLMs) with model-driven engineering to address growing complexity and demands in software development. It discusses the opportunities and challenges of this hybrid technique.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing demand and complexity in software systems, as well as emerging challenges such as new user interfaces, intelligent components, and sustainability. Existing approaches like model-driven engineering (MDE) improve quality and productivity, but managing complex models is difficult. At the same time, large language models (LLMs) enable code generation from natural language, introducing new risks with code quality and maintainability.

Method: The authors introduce 'vibe modeling' as a new approach that integrates AI-powered techniques (like LLMs) with model-driven engineering. This hybrid method aims to combine the strengths of both approaches to improve software development processes.

Result: The paper outlines the key concepts of vibe modeling and discusses its potential benefits and challenges. It does not report empirical evaluation but sets an agenda for integrating AI and MDE.

Conclusion: Vibe modeling is introduced as a promising direction for merging AI and model-driven engineering, with the goal of developing reliable, complex systems more efficiently. The approach offers new opportunities but also presents open problems that need further research.

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [4] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: The paper analyzes the growth and redundancy of CI tools in GitHub Marketplace, showing that most new tools duplicate existing features quickly. The results help developers identify opportunities for innovation and guide strategic tool launches, supported by a comprehensive dataset for further research.


<details>
  <summary>Details</summary>
Motivation: The paper investigates the rapid growth and tool redundancy in the GitHub Marketplace, specifically within Continuous Integration (CI) tools, and aims to provide insights that will help developers and researchers understand innovation and competition dynamics in software ecosystems.

Method: The authors linked 6,983 CI Actions to 3,869 providers, mined their version histories, and modeled the data as a graph. This graph model timestamps functionality debuts, tracks adoption, and clusters redundant tools.

Result: About 65% of new CI Actions replicate existing capabilities, typically appearing within six months of the original. A small set of early Actions become the basis for most subsequent forks and extensions.

Conclusion: The study offers actionable recommendations for developers and maintainers, such as when to launch new tools and which functionalities are underserved. The published dataset and model provide a resource for future research and practical guidance in software product strategy.

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [5] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge automatically generates and debugs IoT integration code with high accuracy, greatly reducing the need for human expertise and exceeding expert-level performance.


<details>
  <summary>Details</summary>
Motivation: Integrating new IoT devices into centralized platforms typically requires complex and expert-level programming of integration code, making it hard, time-consuming, and inaccessible for non-experts.

Method: The authors propose AutoBridge, an automated system for generating IoT integration code. AutoBridge uses a divide-and-conquer approach: it retrieves device- and platform-specific knowledge to automate code synthesis. It also has a multi-stage debugging process including virtual device testing and hardware-in-the-loop debugging using simple binary user feedback.

Result: AutoBridge was evaluated on 34 IoT devices across two open-source IoT platforms, achieving an average success rate of 93.87% and function coverage of 94.87% autonomously. With minimal binary feedback from users, function coverage reached 100%. In a user study, AutoBridge outperformed expert programmers (even those using LLMs) by 50%-80% in code accuracy.

Conclusion: AutoBridge significantly reduces the human effort and expertise required to integrate new IoT devices, automating code generation and debugging to outperform expert programmers.

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [6] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: ABPs can revolutionize business processes but bring trust, accountability, and compliance issues. Making ABPs explainable (XABPs) is essential, and the paper outlines methods and research challenges for achieving this.


<details>
  <summary>Details</summary>
Motivation: Autonomous business processes (ABPs), powered by AI/ML, can greatly enhance business operations but also introduce significant concerns such as trust, debugging challenges, accountability issues, biases, and regulatory compliance risks.

Method: The paper presents a systematic approach to creating explainable autonomous business processes (XABPs), including the characterization of explainability forms, structuring explainability, and identification of key research challenges in business process management (BPM) for XABPs.

Result: The authors propose a structured framework for integrating explainability into ABPs, define the forms that explainability can take in these systems, and highlight essential research problems that need to be addressed for effective adoption of XABPs.

Conclusion: XABPs are necessary for balancing the operational advantages of ABPs with stakeholder concerns, and a systematic approach to explainability will be crucial for building trustworthy, reliable, and compliant autonomous business processes.

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [7] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate introduces a multi-agent debate system for software issue resolution that coordinates diverse reasoning agents to consolidate solutions, resulting in superior performance compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing agent-based issue resolution frameworks in software engineering rely largely on independent exploration by agents. This approach often leads to agents getting stuck in local solutions and failing to recognize issue patterns that span multiple parts of the codebase.

Method: The authors propose SWE-Debate, a competitive multi-agent debate framework. SWE-Debate generates multiple fault propagation traces as localization proposals by traversing a code dependency graph. It then conducts a three-round debate among specialized agents, each representing distinct reasoning strategies along the propagation trace. The consensus fix plan from the debate is used by a Monte Carlo Tree Search (MCTS)-based code modification agent for generating patches.

Result: SWE-Debate achieves state-of-the-art performance on the SWE-bench benchmark. The approach notably outperforms existing open-source agent frameworks and other baseline methods by a significant margin.

Conclusion: A competitive multi-agent debate framework, SWE-Debate, can overcome the limitations of independent exploration, consolidate reasoning paths, and significantly improve issue resolution in complex software engineering tasks.

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [8] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: An automated evaluation system combining analytic and LLM-based methods provides scalable, continuous assessment of COBOL-to-Java translations, improving benchmarking and reducing manual review for modernization projects.


<details>
  <summary>Details</summary>
Motivation: Assessing the quality of COBOL-to-Java code translation, especially through LLM-based translators, is challenging due to model opacity and the difficulty in evaluating translation outputs at scale. There is a need for automated, objective, and comprehensive evaluation methods to support modernization projects and continuous integration workflows.

Method: The paper introduces an automated evaluation system that integrates analytic checkers with LLM-as-a-judge (LaaJ) techniques. The system is designed to be scalable and to support continuous integration, providing multi-faceted assessment and benchmarking of translation quality. They detail the system’s architecture, the evaluation strategies employed, and the reporting mechanisms for stakeholders.

Result: The system enables automated, scalable, and continuous evaluation of COBOL-to-Java translations. It reduces the dependency on manual review, supports large-scale benchmarking, and delivers actionable feedback to developers and project managers, thus aiding modernization efforts.

Conclusion: The proposed evaluation system addresses critical challenges in assessing LLM-based code translation through a hybrid approach, supporting quality assurance and modernization at scale. The combination of analytic and LLM-based judging techniques offers reliable, multi-dimensional evaluation, streamlining the process for developers and managers.

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [9] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Exp enhances LLM agents for software issue resolution by enabling experience accumulation and reuse, significantly boosting problem-solving performance and creating a more strategic, knowledge-driven workflow.


<details>
  <summary>Details</summary>
Motivation: Current large language model (LLM) agents for software issue resolution do not utilize prior experience, resulting in redundant efforts and inability to apply learned methods to new but similar issues. This inefficiency motivates the search for a way to enable agents to learn from and reuse past repair experiences.

Method: The proposed method, SWE-Exp, creates an experience-enhanced approach by building a multi-faceted experience bank. This bank collects and distills actionable experience from previous agent trajectories, capturing both successful and failed attempts at various granularity levels, such as overall problem understanding and specific code edits. This allows agents to leverage stored knowledge in future problem-solving endeavors.

Result: Experiments demonstrate that SWE-Exp achieves a state-of-the-art resolution rate of 41.6% Pass@1 on the SWE-bench-Verified dataset when used with open-source agent frameworks.

Conclusion: SWE-Exp establishes a new paradigm for automated software engineering agents, enabling them to systematically learn from and leverage past repair attempts. This shifts the field from inefficient trial-and-error approaches to more effective, experience-driven problem resolution.

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [10] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: Trae Agent is a new agent-based system for software issue resolution that significantly outperforms previous LLM ensemble methods, notably by handling large solution spaces and repository-level challenges. It ranks first in the SWE-bench benchmark and is available open-source.


<details>
  <summary>Details</summary>
Motivation: Software issue resolution remains a core challenge in software engineering, especially with the increasing potential of large language models (LLMs). Recent ensemble reasoning approaches have improved LLM-based issue resolution, but face difficulties with effectively searching large ensemble spaces and understanding code at the repository level. These limitations reduce their practical effectiveness.

Method: The authors propose Trae Agent, an agent-based ensemble reasoning approach designed explicitly for repository-level software issue resolution. Trae Agent frames issue resolution as an optimal solution search and utilizes modular agents for three main tasks: generation of solutions, pruning of suboptimal solutions, and selection of the best solution. The method is evaluated using three state-of-the-art LLMs on the SWE-bench benchmark and compared with four leading ensemble reasoning approaches.

Result: Experiments show that Trae Agent outperforms all baseline methods, achieving an average Pass@1 improvement of 10.22%. Notably, it secures first place on the SWE-bench Verified leaderboard with a Pass@1 score of 75.20%.

Conclusion: Trae Agent, through its agent-based and modular design, effectively addresses the limitations of previous prompting-based methods, particularly in handling large ensemble spaces and repository-level context. It establishes itself as a new state-of-the-art approach for LLM-based software issue resolution and is released open-source.

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [11] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: This paper extends the Kieker observability framework to support Python by developing a pipeline that combines static and dynamic analysis, addressing the growing need for insights into Python applications.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is the growing popularity of Python and the increasing need for structural insights into Python applications. Since the Kieker observability framework was originally designed for Java, there is value in extending its capabilities to include Python.

Method: The approach involves developing a Python analysis pipeline that integrates both static and dynamic analysis, allowing for comprehensive observation and understanding of Python applications within the Kieker framework.

Result: The result is an enhanced Kieker framework that supports Python, enabling users to analyze Python applications with a combination of static and dynamic techniques to gain more complete system insights.

Conclusion: Supporting Python in the Kieker observability framework extends its usefulness and applicability, allowing users to design custom observability pipelines for Python applications and gain structural insights similar to those possible with Java.

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [12] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: This paper studies code review effort in GitLab by measuring code changes after submission and develops a machine learning model that predicts this effort with high accuracy, finding key factors like code complexity, developer experience, and textual features are most predictive.


<details>
  <summary>Details</summary>
Motivation: While code review is essential, the effort required for code changes during review is underexplored, especially in GitLab Merge Requests. Existing studies have focused on delays and iterations, not on the actual code modification effort and its predictors.

Method: Analyzed over 23,600 GitLab Merge Requests from four projects, defined code review (CR) effort as post-submission code changes, and trained interpretable machine learning models using multifaceted metrics including text, code complexity, developer experience, review history, and branching.

Result: Up to 71% of MRs needed post-submission adjustments, with 28% involving over 200 lines of code. CR effort was not linked to review time or number of participants. The machine learning model achieved high performance (AUC 0.84-0.88) and identified code complexity, experience, and text features as key predictors.

Conclusion: Machine learning models can effectively predict code review effort based on code complexity, developer experience, and text features, offering strong performance in anticipating integration effort during code review.

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Abstractions of Sequences, Functions and Operators](https://arxiv.org/abs/2507.23151)
*Louis Rustenholz,Pedro Lopez-Garcia,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: This paper proposes new theoretical tools to abstract and analyze recursively defined numerical functions, enabling automatic inference of complex bounds and supporting advanced program analysis and hybrid systems.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to enable automatic inference of closed-form bounds for recursively defined functions, which is important for applications in program analysis (such as cost analysis and loop acceleration) and hybrid systems involving differential equations.

Method: The authors develop theoretical foundations in order theory using Galois connections and construct a new family of abstract domains (B-bound domains) for functions. They also introduce a domain abstraction functor and build transfer functions using an operator language for various classes of functions.

Result: The introduced B-bound domains allow the abstraction of functions via bounds derived from selected boundary functions, enabling the inference of complex, non-linear invariants. The work also identifies a convexity property in constraint space that can simplify or automate transfer function construction. The domain abstraction functor aids abstraction and dimensionality reduction.

Conclusion: The paper introduces new constraint-based abstract domains (B-bound domains) for abstracting numerical functions, facilitating the inference of complex non-linear invariants. It also presents a domain abstraction functor to generalize mappings to function space, supporting advanced program analysis and dimensionality reduction.

Abstract: We present theoretical and practical results on the order theory of lattices
of functions, focusing on Galois connections that abstract (sets of) functions
- a topic known as higher-order abstract interpretation.
  We are motivated by the challenge of inferring closed-form bounds on
functions which are defined recursively, i.e. as the fixed point of an operator
or, equivalently, as the solution to a functional equation. This has multiple
applications in program analysis (e.g. cost analysis, loop acceleration,
declarative language analysis) and in hybrid systems governed by differential
equations.
  Our main contribution is a new family of constraint-based abstract domains
for abstracting numerical functions, B-bound domains, which abstract a function
f by a conjunction of bounds from a preselected set of boundary functions. They
allow inferring highly non-linear numerical invariants, which classical
numerical abstract domains struggle with. We uncover a convexity property in
the constraint space that simplifies, and, in some cases, fully automates,
transfer function design.
  We also introduce domain abstraction, a functor that lifts arbitrary mappings
in value space to Galois connections in function space. This supports
abstraction from symbolic to numerical functions (i.e. size abstraction), and
enables dimensionality reduction of equations.
  We base our constructions of transfer functions on a simple operator
language, starting with sequences, and extending to more general functions,
including multivariate, piecewise, and non-discrete domains.

</details>


### [14] [Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks](https://arxiv.org/abs/2507.23205)
*Hebi Li,Forrest Sheng Bao,Qi Xiao,Jin Tian*

Main category: cs.PL

TL;DR: Kernel-FFI is a new, automatic, and language-agnostic FFI framework designed specifically for interactive notebooks like Jupyter, making it much easier to mix languages, handle objects, and invoke functions across languages with minimal effort and without traditional constraints.


<details>
  <summary>Details</summary>
Motivation: Foreign Function Interfaces (FFIs) are crucial for interoperability between programming languages, but current FFI tools are cumbersome and incompatible with the interactive workflows of modern notebook environments like Jupyter. These limitations hinder productivity in multi-language development, especially due to manual configuration, excessive boilerplate, and poor support for features like recursive calls and object-oriented programming.

Method: Kernel-FFI uses source-level transformation to automatically rewrite cross-language calls within code, removing the need for manual binding or boilerplate. It also introduces a novel side-channel communication mechanism for managing asynchronous and recursive foreign function calls in environments where standard kernel communication is blocking. The framework provides full support for object-oriented programming, including foreign object referencing and automatic resource management across different programming languages.

Result: Kernel-FFI enables transparent, language-agnostic cross-language function calls and object manipulation in interactive notebooks. It supports robust object-oriented programming features and overcomes the blocking limitations of Jupyter kernels, allowing recursive and asynchronous foreign calls. The framework will be open-sourced and made publicly available.

Conclusion: Kernel-FFI significantly streamlines multi-language development in interactive notebook environments by automating cross-language invocations, supporting OOP features, and enabling asynchronous, recursive calls without boilerplate or manual configuration.

Abstract: Foreign Function Interfaces (FFIs) are essential for enabling
interoperability between programming languages, yet existing FFI solutions are
ill-suited for the dynamic, interactive workflows prevalent in modern notebook
environments such as Jupyter. Current approaches require extensive manual
configuration, introduce significant boilerplate, and often lack support for
recursive calls and object-oriented programming (OOP) constructs-features
critical for productive, multi-language development.
  We present Kernel-FFI, a transparent, language-agnostic framework that
enables seamless cross-language function calls and object manipulation within
interactive notebooks. Kernel-FFI employs source-level transformation to
automatically rewrite cross-language invocations, eliminating the need for
manual bindings or boilerplate. Kernel-FFI provides robust support for OOP by
enabling foreign object referencing and automatic resource management across
language boundaries. Furthermore, to address the blocking nature of Jupyter
kernels and support recursive and asynchronous foreign calls, we introduce a
novel side-channel communication mechanism. Our tool will be open-sourced and
available at https://codepod.io/docs/kernel-ffi

</details>
