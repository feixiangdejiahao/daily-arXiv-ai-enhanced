<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation](https://arxiv.org/abs/2511.20709)
*Abhijeet Pathak,Suvadra Barua,Dinesh Gudimetla,Rupam Patir,Jiawei Guo,Hongxin Hu,Haipeng Cai*

Main category: cs.SE

TL;DR: The paper introduces DUALGAUGE, a novel framework and benchmark for jointly evaluating the security and correctness of code generated by large language models. Their results show current LLMs struggle with producing code that is both secure and correct, and their open-source tools fill an important gap in evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) are widely used to generate code, but there is no adequate method to simultaneously evaluate both the security and correctness of such generated code. Existing evaluations are lacking because they focus only on vulnerabilities, ignore correctness, or use different datasets for assessing security and functionality, which do not capture the joint requirement for secure and correct code.

Method: The authors introduce DUALGAUGE, an automated benchmarking framework that evaluates both security and correctness of LLM-generated code concurrently. They also introduce DUALGAUGE-BENCH, a curated benchmark suite pairing coding tasks with manually validated test suites, enabling comprehensive joint evaluation. DUALGAUGE uses an agentic program executor in sandboxed environments and an LLM-based evaluator to assess code behavior.

Result: The authors applied DUALGAUGE to ten leading LLMs over thousands of test scenarios. The results revealed significant gaps in the ability of current LLMs to generate both correct and secure code. The quality and accuracy of the framework and benchmark were rigorously evaluated.

Conclusion: DUALGAUGE is the first framework that allows rigorous, scalable, and reproducible joint evaluation of security and correctness in LLM-generated code. The open-source framework and datasets aim to accelerate progress in secure code generation by addressing a critical unmet evaluation need.

Abstract: Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.

</details>


### [2] [Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities](https://arxiv.org/abs/2511.20730)
*Nehal Afifi,Christoph Wittig,Lukas Paehler,Andreas Lindenmann,Kai Wolter,Felix Leitenberger,Melih Dogru,Patric Grauberger,Tobias Düser,Albert Albers,Sven Matthiesen*

Main category: cs.SE

TL;DR: Data-driven methods are increasingly used in product development, especially machine learning and statistical techniques, but there is uncertainty and fragmentation in their application across different stages. Deep learning is rising but less common. Key challenges remain around interpretability, traceability, and real-world validation. The study recommends more interpretable hybrid models and better guidelines for mapping data methods to engineering problems.


<details>
  <summary>Details</summary>
Motivation: There is increasing adoption of data-driven methods (DDMs) in product development, but their integration remains fragmented due to uncertainty over which DDMs to use, and at what stages. Clarifying this use is essential to support more effective product development.

Method: A PRISMA systematic literature review was conducted, using the V-model framework (simplified into four stages) and searching Scopus, Web of Science, and IEEE Xplore for relevant literature from 2014-2024. Out of 1,689 records, 114 publications were analyzed in full.

Result: Machine learning and statistical methods are predominant in engineering design, while deep learning shows increasing but still limited adoption. Relevant methods (e.g., supervised learning, clustering, regression, surrogate modeling) are mostly applied in design, implementation, and integration, but much less so in validation. Main challenges include limited interpretability, traceability, and real-world validation. The need for interpretable hybrid models and better mapping of algorithms to design problems are highlighted as key opportunities.

Conclusion: The review clarifies current practices of DDMs in product development: ML and statistical approaches are most widely used, DL adoption is rising, and specific methods are concentrated in earlier stages of product development. Significant barriers (interpretability, traceability, validation) exist, but addressing these will improve integration. The study sets the stage for more rigorous guidelines and future mapping between computer science techniques and engineering design needs.

Abstract: The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities.

</details>


### [3] [Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms](https://arxiv.org/abs/2511.20813)
*Simon Hacks*

Main category: cs.SE

TL;DR: This paper analyzes the requirements for military learning platforms that enable 'train while you fight,' identifying seven major technical challenges and mapping them to proven software engineering solutions, illustrated by a German armed forces example.


<details>
  <summary>Details</summary>
Motivation: Continuous learning during military operations (TWYF) demands more than traditional pre/post-training, making it essential to adapt learning platforms to meet real-time operational needs.

Method: The authors use a Design Science Research approach, analyzing NATO/PfPC documentation and practical cases. They derive challenges, define objectives, and systematically map these challenges to software engineering patterns.

Result: Seven technical challenges for ADL platforms supporting TWYF are identified: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. These are matched with appropriate engineering patterns and exemplified with a German armed forces case study.

Conclusion: Existing software engineering patterns can address the demands for TWYF-capable ADL platforms. Mapping these patterns to specific challenges helps guide future development and deployment of robust military learning technologies.

Abstract: "Train While You Fight" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.

</details>


### [4] [Application of machine learning for infrastructure reconstruction programs management](https://arxiv.org/abs/2511.20916)
*Illia Khudiakov,Vladyslav Pliuhin,Sergiy Plankovskyy,Yevgen Tsegelnyk*

Main category: cs.SE

TL;DR: This paper proposes an adaptive decision-making model for managing engineering infrastructure reconstruction, employing machine learning and neural networks to support program architecture and WBS creation. The model improves flexibility and efficiency by predicting outcomes from historical data, and is applicable across utilities such as heat, gas, electricity, and water systems.


<details>
  <summary>Details</summary>
Motivation: Current engineering infrastructure reconstruction programs face inefficiencies in management, particularly in program architecture and work breakdown structure creation. There is a need for adaptive decision support models that leverage modern data-driven and AI tools.

Method: The study analyzes existing adaptive program management tools, justifies the use of infrastructure systems modeling tools, and reviews different models and modeling methods. It selects machine learning and artificial neural networks for the proposed model, defines main components of the model, and implements these using Microsoft Azure Machine Learning Studio.

Result: The developed adaptive model can predict objective function values for program configurations using machine learning on historical engineering system data. It enables modifications based on object types and adapts the decision-making process to specific implementation goals. Evaluation results for neural network parameters are provided, confirming the model’s applicability.

Conclusion: The adaptive decision-making support model, utilizing machine learning and artificial neural networks, improves management efficiency in engineering infrastructure reconstruction programs by supporting flexible, data-driven decisions and providing valuable predictions.

Abstract: The purpose of this article is to describe an adaptive decision-making support model aimed at improving the efficiency of engineering infrastructure reconstruction program management in the context of developing the architecture and work breakdown structure of programs. As part of the study, the existing adaptive program management tools are analyzed, the use of infrastructure systems modelling tools is justified for program architecture and WBS creation. Existing models and modelling methods are viewed, and machine learning and artificial neural networks are selected for the model. The main components of the model are defined, which include a set of decision-maker preferences, decision-making tasks, sets of input data, and applied software components of the model. To support decision-making, the adaptive model applies the method of system modeling and predicting the value of the objective function at a given system configuration. Prediction is done using machine learning methods based on a dataset consisting of historical data related to existing engineering systems. The work describes the components of the redistribution of varied model parameters, which modify the model dataset based on the selected object type, which allows adapting the decision-making process to the existing program implementation goals. The functional composition done in Microsoft Azure Machine Learning Studio is described. The neural network parameters and evaluation results are given. The application of the developed adaptive model is possible in the management of programs for the reconstruction of such engineering systems as systems of heat, gas, electricity supply, water supply, and drainage, etc.

</details>


### [5] [Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code](https://arxiv.org/abs/2511.20933)
*Mootez Saad,Boqi Chen,José Antonio Hernández López,Dániel Varró,Tushar Sharma*

Main category: cs.SE

TL;DR: This paper finds that while LLMs like DeepSeek-R1 can identify code design flaws well in ideal settings, their understanding is fragile—especially for complex coupling scenarios—when faced with noise or less guidance. Improvements are necessary for LLMs to reliably reason about software design in real-world contexts.


<details>
  <summary>Details</summary>
Motivation: Despite increasing use of LLMs in software engineering, their true understanding of key software design principles like cohesion and coupling has not been systematically verified.

Method: The study programmatically generated poorly designed code (with design flaws in cohesion and coupling) and evaluated several DeepSeek-R1 LLM models (14B, 32B, 70B) on tasks ranging from simple verification, guided reasoning, to open-ended generation. Contextual noise was introduced via distractor elements, and the models' reasoning traces were analyzed to identify failure modes.

Result: While LLMs perform well in ideal, guided conditions for both cohesion and coupling, their performance sharply declines in noisy and open-ended scenarios—especially for coupling, with F1 scores dropping by over 50%. Cohesion reasoning is more robust but still vulnerable without guidance. The analysis shows models often use shortcuts for coupling and exhaustive yet insufficient approaches for cohesion.

Conclusion: LLMs are currently reliable for recognizing design flaws in core software engineering concepts but lack robust autonomous reasoning in noisy, real-world scenarios. More scalable and resilient approaches to program understanding must be developed for effective deployment.

Abstract: Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \textit{Verification} to \textit{Guided} and \textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.

</details>


### [6] [SpaceX: Exploring metrics with the SPACE model for developer productivity](https://arxiv.org/abs/2511.20955)
*Sanchit Kaul,Kevin Nhu,Jason Eissayou,Ivan Eser,Victor Borup*

Main category: cs.SE

TL;DR: This paper demonstrates that simplistic productivity metrics miss essential nuances. Using repository mining, advanced statistical analysis, and sentiment classification, the authors find frustration drives commit activity and that mapping developer interactions yields deeper understanding of collaboration. They introduce a Composite Productivity Score (CPS) for more accurate measurement.


<details>
  <summary>Details</summary>
Motivation: The study aims to challenge the oversimplified productivity heuristics often used in developer analysis by adopting the SPACE framework for a more comprehensive measure. It seeks to address the complex and multi-dimensional nature of developer productivity.

Method: The authors employed repository mining on large open-source datasets, utilized Generalized Linear Mixed Models (GLMM) for statistical analysis, and applied RoBERTa-based sentiment classification. Additionally, they investigated contributor interaction topologies as compared to conventional contribution-volume metrics.

Result: Results show a statistically significant positive correlation between negative emotional states (frustration) and increased commit frequency, suggesting developers may iterate more when frustrated. Interaction topology analysis offers more accurate mapping of collaboration than simple volume metrics.

Conclusion: The study proposes a Composite Productivity Score (CPS) that captures the multifaceted nature of developer productivity, demonstrating the inadequacy of single-dimension, volume-based approaches.

Abstract: This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.

</details>


### [7] [Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations](https://arxiv.org/abs/2511.21022)
*Guancheng Lin,Xiao Yu,Jacky Keung,Xing Hu,Xin Xia,Alex X. Liu*

Main category: cs.SE

TL;DR: This study systematically benchmarks model editing techniques for updating deprecated API knowledge in LLMs. While AdaLoRA offers strong results but low specificity, the proposed AdaLoRA-L confines edits to API-specific layers, significantly improving targeted updates without affecting model performance elsewhere.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) excel at code completion, but can suggest deprecated APIs due to outdated training data. Refreshing their knowledge through retraining is costly, and model editing methods' effectiveness for updating API knowledge is unclear.

Method: The study systematically applies 10 state-of-the-art model editing techniques to three LLMs (Qwen2.5-Coder, StarCoder2, DeepSeek-Coder) using EDAPIBench, a benchmark with over 70 deprecated APIs and 3,000 editing instances. Evaluation identifies the best technique and proposes an improved method, AdaLoRA-L, which restricts edits to specific layers storing API-related knowledge.

Result: AdaLoRA, a parameter-efficient fine-tuning method, best enables edited LLMs to generate correct, up-to-date APIs, but suffers from low Specificity. AdaLoRA-L enhances Specificity by restricting edits to layers relevant to the target API, maintaining comparable performance on other metrics.

Conclusion: Targeted model editing—specifically AdaLoRA-L—effectively updates deprecated API knowledge in LLMs, improving their ability for up-to-date code generation while minimizing unintended influence on unrelated knowledge.

Abstract: Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines "Common API Layers" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to "Specific API Layers" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.

</details>


### [8] [Exploring Hidden Geographic Disparities in Android Apps](https://arxiv.org/abs/2511.21151)
*M. Alecci,P. Jiménez,J. Samhi,T. Bissyandé,J. Klein*

Main category: cs.SE

TL;DR: Mobile apps differ more by region than previously realized. Visually identical apps can behave differently depending on where they're downloaded, affecting security, privacy, and reproducibility. The study provides data and insights to help address these geographic disparities.


<details>
  <summary>Details</summary>
Motivation: While much research exists on how mobile apps evolve over time, the impact of geographic differences in app behavior is not thoroughly understood. This paper aims to uncover how apps differ across countries and regions, which can affect security, privacy, and fairness.

Method: The authors built a distributed pipeline that collects Android apps from multiple regions. They analyzed thousands of apps for differences in requested permissions, third-party libraries, privacy disclosures, and file content. A dataset of 81,963 GeoTwins was compiled for further research.

Result: The study discovered significant geographic variation among supposedly identical apps. 'GeoTwins'—functionally similar apps released under different package names per region—showed discrepancies in permissions, libraries, and privacy disclosures. Even the base.apk files, which are assumed to be uniform globally, actually contain regional differences. These discrepancies impact security assessments, user privacy, app functionality, and ethical concerns over transparency.

Conclusion: Systemic regional disparities exist in mobile software, affecting reproducibility, geographic bias, and user consent. The findings have major implications for research, development, and policy. The publicly released dataset enables ongoing analysis.

Abstract: While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.
  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.
  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.

</details>


### [9] [Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools](https://arxiv.org/abs/2511.21197)
*Paolo Buono,Mary Cerullo,Stefano Cirillo,Giuseppe Desolda,Francesco Greco,Emanuela Guglielmi,Grazia Margarella,Giuseppe Polese,Simone Scalabrino,Cesare Tucci*

Main category: cs.SE

TL;DR: This paper studies how developers perceive AI tools for bug detection and code readability, finding that trust and usability depend on clear feedback, control, and personalized guidance, and suggests principles for more human-centered IDE design.


<details>
  <summary>Details</summary>
Motivation: Despite technical advancements, there is limited understanding of developers' mental models of AI tools and how mismatches affect trust, control, and adoption.

Method: Six co-design workshops with 58 developers were conducted to elicit their mental models of AI-assisted bug detection and readability assessment tools.

Result: Developers view bug detection tools as 'bug detectives' that should warn only about critical issues and provide transparency, actionable feedback, and confidence cues. Readability assessment tools are seen as 'quality coaches' that give personalized, progressive guidance. Trust is linked to clarity of explanations, timing, and user control. Design principles for Human-Centered AI in IDEs have been created to balance support, conciseness, and human agency.

Conclusion: Understanding developers' mental models and preferences is vital for designing effective AI-assisted programming tools. Human-centered design principles have been proposed to enhance trust and usability in IDEs.

Abstract: AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.

</details>


### [10] [Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions](https://arxiv.org/abs/2511.21380)
*Jingyi Chen,Xiaoyan Guo,Songqiang Chen,Shing-Chi Cheung,Jiasi Shen*

Main category: cs.SE

TL;DR: This paper empirically evaluates multi-agent LLM systems for automating adaptation of software engineering artifacts across datasets. While current systems can partially adapt code, they rarely achieve fully correct results without feedback and contextual guidance. Providing error messages and reference code can greatly improve output similarity to source implementations. The results highlight both the potential and current limitations of these systems, and suggest the need for self-correcting, feedback-driven agent architectures in future SE research.


<details>
  <summary>Details</summary>
Motivation: Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, but this challenge remains understudied. The recent progress in multi-agent LLM systems could potentially address these gaps.

Method: The study conducts the first empirical evaluation of multi-agent LLM systems, specifically GitHub Copilot using GPT-4.1 and Claude Sonnet 4, on the task of adapting SE research artifacts from benchmark repositories. A five-stage evaluation pipeline is used: file comprehension, code editing, command generation, validation, and final execution. Success rates, failure patterns, and the effect of prompt-based interventions (such as error messages and reference code) are systematically analyzed.

Result: Current systems can locate relevant files and partially adapt code, but struggle to achieve functionally correct implementations on their own. Prompt-level interventions, particularly those supplying error feedback and reference code, sharply increase the structural similarity of outputs to the ground truth (from 7.25% to 67.14%).

Conclusion: While multi-agent LLM systems show promise for automating dataset adaptation in SE research, they have significant limitations, especially regarding functional correctness. Contextual guidance and iterative feedback are critical for improved performance, pointing to future needs for self-correcting, reliable agent designs.

Abstract: Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.

</details>


### [11] [Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead](https://arxiv.org/abs/2511.21382)
*Bei Chu,Yang Feng,Kui Liu,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: This paper surveys and categorizes state-of-the-art research on using LLMs for code unit test generation. While prompt engineering and validation loops have improved practical outcomes, the generated tests still struggle with fault detection and standard evaluation. Future work should focus on autonomous and hybrid systems to enhance robustness and reliability.


<details>
  <summary>Details</summary>
Motivation: Unit testing is important but manually intensive, and classic automated test generation lacks semantic realism. Large Language Models (LLMs) offer the potential to improve test quality with their understanding of code semantics.

Method: The authors conducted a systematic literature review of 115 publications from May 2021 to August 2025, organizing findings under a new taxonomy for LLM-based unit test generation. They analyzed generative strategies, enhancement techniques, and emerging trends in the field.

Result: Prompt engineering is the dominant approach for LLM-powered test generation (accounting for 89% of studies). Iterative validation and repair practices have improved test quality, but issues persist with fault detection and benchmarking standards.

Conclusion: LLMs have advanced automated unit testing, moving towards autonomous agents and hybrid solutions with traditional tools. However, challenges like reliable fault detection and standardized evaluation benchmarks must be addressed for industrial-grade solutions.

Abstract: Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [12] [Optimism in Equality Saturation](https://arxiv.org/abs/2511.20782)
*Russel Arbore,Alvin Cheung,Max Willsey*

Main category: cs.PL

TL;DR: The paper introduces a new algorithm that improves the analysis of cyclic programs in SSA form during equality saturation, using a prototype that surpasses clang and gcc on example tasks.


<details>
  <summary>Details</summary>
Motivation: Existing e-class analysis for equality saturation is pessimistic and struggles with cyclic programs, limiting optimization potential for SSA-form code.

Method: An algorithm combining optimistic analysis and non-destructive rewriting is introduced. A prototype abstract interpreter for SSA programs with new semantics is implemented to demonstrate the approach.

Result: The prototype interpreter successfully analyzes simple SSA programs more precisely than both clang and gcc.

Conclusion: The proposed abstract interpretation algorithm enables precise analysis of cyclic SSA programs within equality saturation, outperforming standard compilers in example cases.

Abstract: Equality saturation is a technique for program optimization based on non-destructive rewriting and a form of program analysis called e-class analysis. The current form of e-class analysis is pessimistic and therefore ineffective at analyzing cyclic programs, such as those in SSA form. We propose an abstract interpretation algorithm that can precisely analyze cycles during equality saturation. This results in a unified algorithm for optimistic analysis and non-destructive rewriting. We instantiate this approach on a prototype abstract interpreter for SSA programs using a new semantics of SSA. Our prototype can analyze simple example programs more precisely than clang and gcc.

</details>


### [13] [Towards Computational UIP in Cubical Agda](https://arxiv.org/abs/2511.21209)
*Yee-Jian Tan,Andreas Nuyts,Dominique Devriese*

Main category: cs.PL

TL;DR: Cubical Type Theory can be simplified to h-Sets with functional extensionality and QITs intact by adopting the UIP axiom and removing Glue Types. The paper analyzes computational formulations of UIP and implements a Glue-less Cubical Agda variant, but notes current UIP approaches are unsatisfactory for routine use.


<details>
  <summary>Details</summary>
Motivation: Cubical Type Theory offers advantages like Quotient Inductive Types (QITs) and provable functional extensionality, which are desirable over traditional Martin-Löf Type Theory. However, the infinite hierarchy of equalities in HoTT complicates formalizations, motivating the investigation of a truncated approach—focusing on h-Set equality levels while retaining key features.

Method: The paper analyzes different formulations of the Uniqueness of Identity Proofs (UIP) axiom within Cubical Agda, detailing their computational rules, and evaluates their practical suitability for implementation. It also implements a variant of Cubical Agda without Glue Types, making it directly compatible with postulated UIP.

Result: They show that functional extensionality and QITs can be preserved even in a version of Cubical Type Theory truncated to h-Sets, where equality is simplified via UIP. They implement a Glue-less variant of Cubical Agda in anticipation of future UIP-adoption, and identify two current but unsatisfactory practical approaches for achieving h-Set Cubical Type Theory.

Conclusion: It is feasible to formulate and implement h-Set Cubical Type Theory in Cubical Agda, retaining critical features such as functional extensionality and QITs, although current approaches for enforcing UIP are unsatisfactory and require further development.

Abstract: Some advantages of Cubical Type Theory, as implemented by Cubical Agda, over intensional Martin-Löf Type Theory include Quotient Inductive Types (QITs), which exist as instances of Higher Inductive Types, and functional extensionality, which is provable in Cubical Type Theory. However, HoTT features an infinite hierarchy of equalities that may become unwieldy in formalisations. Fortunately, QITs and functional extensionality are both preserved even if the equality levels of Cubical Type Theory are truncated to only homotopical Sets (h-Sets). In other words, removing the univalence axiom from Cubical Type Theory and instead postulating a conflicting axiom: the Uniqueness of Identity Proofs (UIP) postulate. Since univalence is proved in Cubical Type Theory from the so-called Glue Types, therefore, it is known that one can first remove the Glue Types (thus removing univalence) and then set-truncate all equalities (essentially assuming UIP), à la XTT. The result is a "h-Set Cubical Type Theory" that retains features such as functional extensionality and QITs.
  However, in Cubical Agda, there are currently only two unsatisfying ways to achieve h-Set Cubical Type Theory. The first is to give up on the canonicity of the theory and simply postulate the UIP axiom, while the second way is to use a standard result stating "type formers preserve h-levels" to manually prove UIP for every defined type. The latter is, however, laborious work best suited for an automatic implementation by the proof assistant. In this project, we analyse formulations of UIP and detail their computation rules for Cubical Agda, and evaluate their suitability for implementation. We also implement a variant of Cubical Agda without Glue, which is already compatible with postulated UIP, in anticipation of a future implementation of UIP in Cubical Agda.

</details>


### [14] [SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks](https://arxiv.org/abs/2511.21509)
*Dirk Beyer,Gidon Ernst,Martin Jonáš,Marian Lingsch-Rosenfeld*

Main category: cs.PL

TL;DR: SV-LIB is a new intermediate language and exchange format for software verification, aiming to unify and simplify verification efforts across different programming languages. It leverages SMT-LIB for easier integration and includes mechanisms for witness validation. The current version defines syntax and informal semantics, with further work planned on formal semantics and concurrency.


<details>
  <summary>Details</summary>
Motivation: Existing verification tools are often tailored to specific programming languages, limiting their applicability and technology transfer. Many methods used are inherently language-agnostic, suggesting an opportunity to create more flexible verification solutions.

Method: The authors propose SV-LIB, an intermediate language and exchange format for software verification tasks. SV-LIB is based on concepts from imperative programming and utilizes SMT-LIB for representing expressions and sorts, facilitating integration with current verification tools.

Result: SV-LIB provides a unified format for representing programs, specifications, and verification witnesses. It also offers a witness format for both correct and incorrect programs, enabling independent validation and reuse of verification components. Version 1.0 includes design goals, syntax, and informal semantics.

Conclusion: SV-LIB lays the groundwork for more universal, interoperable verification tooling by standardizing the representation of verification tasks and witnesses across languages. Future work will focus on formal semantics and supporting concurrency.

Abstract: In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.

</details>
