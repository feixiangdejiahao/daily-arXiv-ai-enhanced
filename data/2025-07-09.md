<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks](https://arxiv.org/abs/2507.05269)
*Danning Xie,Mingwei Zheng,Xuwei Liu,Jiannan Wang,Chengpeng Wang,Lin Tan,Xiangyu Zhang*

Main category: cs.SE

TL;DR: The paper introduces CoRe, a benchmark for evaluating LLMs on static code analysis tasks. Results show LLMs excel at basic code dependencies but falter on deeper semantic reasoning, highlighting gaps in current model capabilities and suggesting directions for future improvement.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluation benchmarks mainly measure end-to-end code correctness but do not adequately assess models' abilities for core program semantic reasoning. There is a need for tasks that probe LLMs’ deeper understanding of code beyond surface-level patterns.

Method: The authors design and introduce CoRe, a benchmark for static code analysis covering data dependency, control dependency, and information flow in C/C++, Java, and Python. They use a semantics-aware sampling strategy to ensure task diversity, and evaluate 10 mainstream LLMs on 12,553 instances, followed by qualitative challenge analyses.

Result: LLMs show competence with basic dependency identification but perform poorly on deeper, multi-step semantic code reasoning tasks, particularly in the presence of complex structures. Qualitative findings point to specific reasoning challenges that future LLM improvements should address.

Conclusion: LLMs perform well on basic dependency identification tasks but struggle with deeper semantic reasoning and multi-step static analysis tasks. Current LLMs have significant limitations in understanding complex control structures and backward dependency patterns.

Abstract: Large language models (LLMs) have been widely adopted across diverse software
engineering domains, such as code generation, program repair, and vulnerability
detection. These applications require understanding beyond surface-level code
patterns: value propagation, control flow, and interdependence between program
elements. However, existing benchmarks primarily evaluate end-to-end outcomes,
such as whether code is correctly repaired or generated, leaving the models
ability for program semantic reasoning underexplored. This work presents CoRe,
a high-quality, human-verified benchmark designed to evaluate LLMs on
fundamental static analysis tasks. CoRe includes 12,553 task instances spanning
data dependency, control dependency, and information flow across programs
written in C/C++, Java, and Python. To ensure semantic diversity and reasoning
complexity, we propose a semantics-aware diverse sampling strategy that selects
targets and task instances based on structural coverage and dependency depth.
We evaluate 10 mainstream LLMs and show that, while they perform well at
identifying dependencies, models still struggle with tasks that require deeper
semantic understanding and multi-step reasoning. We further conduct qualitative
analyses to uncover key challenges, such as complex control structures and
backward dependency patterns, offering insights into improving LLMs code
reasoning capabilities.

</details>


### [2] [Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management](https://arxiv.org/abs/2507.05270)
*Boyuan Li,Chengwei Liu,Lingling Fan,Sen Chen,Zhenlin Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: This paper reviews 80 works on software license risk management, categorizing current solutions, discussing their limitations, and outlining future research needs. It provides insights and recommendations to support better handling of software licensing challenges, especially with emerging OSS and generative AI in software engineering.


<details>
  <summary>Details</summary>
Motivation: Third-party software integration is widespread in software development but brings significant software licensing risks. The evolving landscape of OSS licenses and the rise of technologies like CodeLLMs have increased the complexity and importance of managing these risks systematically.

Method: The authors conducted the first systematic literature review (SLR) on open-source software license-related papers. They reviewed 80 selected papers and classified research into license identification, risk assessment, and risk mitigation. The paper also analyzes challenges and future research opportunities.

Result: The review identifies and classifies existing research into three main categories: license identification, risk assessment, and risk mitigation. It highlights significant limitations in current solutions and the need for improved approaches to software license risk management, especially in light of new technologies and evolving license frameworks.

Conclusion: There remain considerable challenges in managing software license risks. The review outlines opportunities for further research and practical recommendations for software engineers and hopes to foster stronger collaboration between academia and industry while supporting sound license risk governance.

Abstract: Integrating third-party software components is a common practice in modern
software development, offering significant advantages in terms of efficiency
and innovation. However, this practice is fraught with risks related to
software licensing. A lack of understanding may lead to disputes, which can
pose serious legal and operational challenges. To these ends, both academia and
industry have conducted various investigations and proposed solutions and tools
to deal with these challenges. However, significant limitations still remain.
Moreover, the rapid evolution of open-source software (OSS) licenses, as well
as the rapidly incorporated generative software engineering techniques, such as
large language models for code (CodeLLMs), are placing greater demands on the
systematic management of software license risks. To unveil the severe
challenges and explore possible future directions, we conduct the first
systematic literature review (SLR) on 80 carefully selected OSS license-related
papers, classifying existing research into three key categories, i.e., license
identification, license risk assessment, and license risk mitigation. Based on
these, we discuss challenges in existing solutions, conclude the opportunities
to shed light on future research directions and offer practical recommendations
for practitioners. We hope this thorough review will help bridge the gaps
between academia and industry and accelerate the ecosystem-wide governance of
legitimate software risks within the software engineering community.

</details>


### [3] [FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing](https://arxiv.org/abs/2507.05272)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.SE

TL;DR: The paper proposes enhancing weakest precondition generation by combining LLMs with fuzz testing through a method called Fuzzing Guidance, which uses execution feedback to improve results. Experiments on Java array programs show that this approach is effective.


<details>
  <summary>Details</summary>
Motivation: Weakest preconditions (WPs) are crucial for program verification and error checking, but generating them can be challenging. The paper aims to improve WP generation using new techniques.

Method: The authors propose combining Large Language Models (LLMs) with fuzz testing. They introduce a method called Fuzzing Guidance (FG), which provides execution feedback to guide LLMs towards correct WPs. FG uses fuzz testing to evaluate the validity and weakness of candidate WPs and gives this feedback back to the LLM for refining its context.

Result: On benchmarks of deterministic array programs in Java, LLMs generated viable candidate WPs. The Fuzzing Guidance approach further improved the accuracy and practicality of WP generation.

Conclusion: Combining LLMs and fuzz testing using the proposed FG method is effective for generating weakest preconditions, and execution feedback can significantly enhance the LLMs' capabilities in this task.

Abstract: The weakest precondition (WP) of a program describes the largest set of
initial states from which all terminating executions of the program satisfy a
given postcondition. The generation of WPs is an important task with practical
applications in areas ranging from verification to run-time error checking.
  This paper proposes the combination of Large Language Models (LLMs) and fuzz
testing for generating WPs. In pursuit of this goal, we introduce Fuzzing
Guidance (FG); FG acts as a means of directing LLMs towards correct WPs using
program execution feedback. FG utilises fuzz testing for approximately checking
the validity and weakness of candidate WPs, this information is then fed back
to the LLM as a means of context refinement.
  We demonstrate the effectiveness of our approach on a comprehensive benchmark
set of deterministic array programs in Java. Our experiments indicate that LLMs
are capable of producing viable candidate WPs, and that this ability can be
practically enhanced through FG.

</details>


### [4] [ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy](https://arxiv.org/abs/2507.05279)
*Virgile Boraud,Yannis Bendi-Ouis,Paul Bernard,Xavier Hinaut*

Main category: cs.SE

TL;DR: A specialized LLM tool using RAG and knowledge graphs enhances code generation and question answering for ReservoirPy, outperforming major LLMs in coding tasks and improving accuracy over its base model.


<details>
  <summary>Details</summary>
Motivation: The need to improve the reliability and code-assistance capabilities of LLMs, specifically for working with the ReservoirPy library and answering specialized questions in Reservoir Computing, motivates this research.

Method: The authors design a tool that integrates Retrieval-Augmented Generation (RAG) and knowledge graphs to enhance LLMs. This system is made interactive with a ChatGPT-like interface focused on ReservoirPy, allowing users to write, debug, and understand Python code with domain-specific support.

Result: The system enabled better performance on coding tasks related to ReservoirPy compared to proprietary models like ChatGPT-4o and NotebookLM, and it significantly outperformed its own base model (Codestral-22B) in these tasks. Proprietary models still slightly outperformed it on general knowledge questions.

Conclusion: Integrating RAG and knowledge graphs into LLMs improves factual accuracy and reduces hallucinations for domain-specific code development and Q&A, notably surpassing state-of-the-art proprietary models in targeted tasks.

Abstract: We introduce a tool designed to improve the capabilities of Large Language
Models (LLMs) in assisting with code development using the ReservoirPy library,
as well as in answering complex questions in the field of Reservoir Computing.
By incorporating external knowledge through Retrieval-Augmented Generation
(RAG) and knowledge graphs, our approach aims to reduce hallucinations and
increase the factual accuracy of generated responses. The system provides an
interactive experience similar to ChatGPT, tailored specifically for
ReservoirPy, enabling users to write, debug, and understand Python code while
accessing reliable domain-specific insights. In our evaluation, while
proprietary models such as ChatGPT-4o and NotebookLM performed slightly better
on general knowledge questions, our model outperformed them on coding tasks and
showed a significant improvement over its base model, Codestral-22B.

</details>


### [5] [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/abs/2507.05281)
*Lingyue Fu,Hao Guan,Bolun Zhang,Haowei Yuan,Yaoming Zhu,Jun Xu,Zongyu Wang,Lin Qiu,Xunliang Cai,Xuezhi Cao,Weiwen Liu,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: The paper introduces CorePipe and CoreCodeBench—tools for creating and running comprehensive, realistic, and configurable benchmarks to better evaluate LLMs' performance in real-world engineering code contexts, showing existing LLMs have varied strengths but none are universally superior.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating LLMs on code are limited, focusing on single tasks, lacking diversity, and having issues with question control and test reliability. There is a need for more comprehensive and realistic benchmarks to assess LLMs in real engineering contexts.

Method: The authors introduce CorePipe, an automated pipeline that turns code repositories into test cases. They develop CoreCodeBench, a benchmark suite that includes various scenarios and adjustable difficulty, by generating atomic (e.g., Development, BugFix, Test-Driven Development) and composite questions targeting central code regions.

Result: Experiments were conducted with 16 LLMs across different scenarios, showing their varied strengths and weaknesses. CoreCodeBench allowed for multi-dimensional insights into LLM capabilities on engineering-level tasks.

Conclusion: CoreCodeBench, enabled by CorePipe, offers a more realistic, flexible, and comprehensive way to benchmark LLMs on engineering-level repository tasks, better reflecting real-world software development needs.

Abstract: As Large Language Models (LLMs) demonstrate increasingly sophisticated code
processing capabilities, evaluating their performance on engineering-level code
remains challenging. Existing repository-level benchmarks primarily focus on
single scenarios, such as code generation or bug fixing, without adequately
capturing the diversity and complexity of real-world software or project
engineering workflows. Furthermore, these benchmarks suffer from limited
controllability in question positioning and reliability issues in their
generated test cases. To address these limitations, we present CorePipe, a
fully automated pipeline that converts repositories into comprehensive test
cases, and introduce CoreCodeBench, a configurable multi-scenario
repository-level benchmark. To simulate real engineering scenarios, CorePipe
generates three types of atomic questions (Development, BugFix, and Test-Driven
Development) specifically targeting core code segments. These atomic questions
are further combined into three types of composite questions, with difficulty
levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides
a comprehensive and extensive repository-level benchmark to investigate the
applicability of LLMs in real-world engineering projects. Experiments with 16
LLMs across diverse scenarios reveal varying capabilities and offer
multi-dimensional insights into LLM performance in engineering contexts. The
code for CorePipe is available at
https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for
CoreCodeBench can be accessed at
https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.

</details>


### [6] [Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models](https://arxiv.org/abs/2507.05289)
*Igor Regis da Silva Simoes,Elaine Venson*

Main category: cs.SE

TL;DR: This paper investigates how Large Language Models (LLMs) can be used to assess code readability. Through experiments altering code comments, identifier names, and structure, nine LLMs were tested and compared to a traditional model. Results showed LLMs reliably detected readability changes, matched or exceeded the reference tool in sensitivity, and showed potential for robust code quality evaluation.


<details>
  <summary>Details</summary>
Motivation: Code readability is essential for code quality and is influenced by factors like naming, comments, structure, and standard adherence, but is difficult to measure objectively and consistently. Traditional tools and methods have limitations, prompting the exploration of new approaches.

Method: The study performed a quasi-experiment with nine Large Language Models (LLMs) subjected to three types of code interventions: removing comments, obfuscating identifier names, and refactoring to remove code smells. Each scenario was tested with 10 analyses per LLM, and results were compared against a known reference tool and model. Thematic analysis was used to assess LLMs' reasoning.

Result: All LLMs detected the interventions and largely agreed with the reference model in original and refactored scenarios. They showed semantic sensitivity beyond the reference model, and their responses reflected intervention types. Response variability was present but did not consistently affect statistical significance. LLMs demonstrated capability in evaluating semantic aspects like coherence of names, comments, and documentation.

Conclusion: LLMs show promise for standardized, consistent, and semantically rich evaluation of code readability and related quality attributes, offering advantages over traditional tools and manual reviews.

Abstract: Code readability is one of the main aspects of code quality, influenced by
various properties like identifier names, comments, code structure, and
adherence to standards. However, measuring this attribute poses challenges in
both industry and academia. While static analysis tools assess attributes such
as code smells and comment percentage, code reviews introduce an element of
subjectivity. This paper explores using Large Language Models (LLMs) to
evaluate code quality attributes related to its readability in a standardized,
reproducible, and consistent manner. We conducted a quasi-experiment study to
measure the effects of code changes on Large Language Model (LLM)s
interpretation regarding its readability quality attribute. Nine LLMs were
tested, undergoing three interventions: removing comments, replacing identifier
names with obscure names, and refactoring to remove code smells. Each
intervention involved 10 batch analyses per LLM, collecting data on response
variability. We compared the results with a known reference model and tool. The
results showed that all LLMs were sensitive to the interventions, with
agreement with the reference classifier being high for the original and
refactored code scenarios. The LLMs demonstrated a strong semantic sensitivity
that the reference model did not fully capture. A thematic analysis of the LLMs
reasoning confirmed their evaluations directly reflected the nature of each
intervention. The models also exhibited response variability, with 9.37% to
14.58% of executions showing a standard deviation greater than zero, indicating
response oscillation, though this did not always compromise the statistical
significance of the results. LLMs demonstrated potential for evaluating
semantic quality aspects, such as coherence between identifier names, comments,
and documentation with code purpose.

</details>


### [7] [zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection](https://arxiv.org/abs/2507.05294)
*William Law*

Main category: cs.SE

TL;DR: zkSDK streamlines Zero-Knowledge app development by automatically picking the best backend using workload analysis, reducing complexity and improving the developer experience.


<details>
  <summary>Details</summary>
Motivation: Developers in the Zero-Knowledge (ZK) space face a fragmented ecosystem with multiple ZK backend options, resulting in a steep learning curve and a tendency to stick with only one backend. There is a need to simplify and unify the ZK application development experience.

Method: The authors introduce zkSDK, a modular framework with a custom Python-like language called Presto, which profiles and analyzes program workload. Using user-defined criteria, zkSDK implements a dynamic selection algorithm to automatically determine the most appropriate ZK-proving backend for a given workload.

Result: Their analysis and evaluation on real-world workloads show that zkSDK successfully selects the optimal backend from several options, improving the developer experience and backend performance.

Conclusion: zkSDK abstracts the complexity of choosing between multiple ZK backends, enabling developers to build ZK applications more efficiently and user-friendly, regardless of backend fragmentation.

Abstract: The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the
development of numerous tools designed to support developers. Popular options
include being able to write in general-purpose programming languages like Rust
from Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.
However, developers entering the ZK space are faced with many different ZK
backends to choose from, leading to a steep learning curve and a fragmented
developer experience across different platforms. As a result, many developers
tend to select a single ZK backend and remain tied to it. This thesis
introduces zkSDK, a modular framework that streamlines ZK application
development by abstracting the backend complexities. At the core of zkSDK is
Presto, a custom Python-like programming language that enables the profiling
and analysis of a program to assess its computational workload intensity.
Combined with user-defined criteria, zkSDK employs a dynamic selection
algorithm to automatically choose the optimal ZK-proving backend. Through an
in-depth analysis and evaluation of real-world workloads, we demonstrate that
zkSDK effectively selects the best-suited backend from a set of supported ZK
backends, delivering a seamless and user-friendly development experience.

</details>


### [8] [ASSURE: Metamorphic Testing for AI-powered Browser Extensions](https://arxiv.org/abs/2507.05307)
*Xuanqi Gao,Juan Zhai,Shiqing Ma,Siyi Xie,Chao Shen*

Main category: cs.SE

TL;DR: ASSURE is a novel, automated framework for robustly testing LLM-based browser extensions, outperforming manual testing in speed and thoroughness, and uncovering critical vulnerabilities, making it suitable for practical use in development pipelines.


<details>
  <summary>Details</summary>
Motivation: The rapid integration of Large Language Models (LLMs) into browser extensions has enabled advanced features but also introduced new challenges in testing, reliability, and security that existing testing approaches cannot effectively address, particularly due to the complexity and context-sensitivity of LLM-powered extensions.

Method: The paper proposes ASSURE, a modular automated testing framework designed specifically for AI-powered browser extensions. ASSURE consists of a flexible test case generation engine, an execution framework that manages interactions including web and extension behavior, and a validation pipeline focusing on behavioral and security evaluations rather than exact output matching.

Result: The evaluation of ASSURE on six popular AI browser extensions revealed 531 distinct issues, such as security risks and content mismatches. ASSURE achieved a 6.4x increase in testing throughput over manual methods and was able to discover critical vulnerabilities in just 12.4 minutes on average.

Conclusion: ASSURE provides an efficient and comprehensive framework for testing and improving the reliability and security of LLM-powered browser extensions, addressing unique challenges and supporting integration into real-world development workflows.

Abstract: The integration of Large Language Models (LLMs) into browser extensions has
revolutionized web browsing, enabling sophisticated functionalities like
content summarization, intelligent translation, and context-aware writing
assistance. However, these AI-powered extensions introduce unprecedented
challenges in testing and reliability assurance. Traditional browser extension
testing approaches fail to address the non-deterministic behavior,
context-sensitivity, and complex web environment integration inherent to
LLM-powered extensions. Similarly, existing LLM testing methodologies operate
in isolation from browser-specific contexts, creating a critical gap in
effective evaluation frameworks. To bridge this gap, we present ASSURE, a
modular automated testing framework specifically designed for AI-powered
browser extensions. ASSURE comprises three principal components: (1) a modular
test case generation engine that supports plugin-based extension of testing
scenarios, (2) an automated execution framework that orchestrates the complex
interactions between web content, extension processing, and AI model behavior,
and (3) a configurable validation pipeline that systematically evaluates
behavioral consistency and security invariants rather than relying on exact
output matching. Our evaluation across six widely-used AI browser extensions
demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning
security vulnerabilities, metamorphic relation violations, and content
alignment problems. ASSURE achieves 6.4x improved testing throughput compared
to manual approaches, detecting critical security vulnerabilities within 12.4
minutes on average. This efficiency makes ASSURE practical for integration into
development pipelines, offering a comprehensive solution to the unique
challenges of testing AI-powered browser extensions.

</details>


### [9] [OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models](https://arxiv.org/abs/2507.05316)
*Koren Lazar,Matan Vetzler,Kiran Kate,Jason Tsay,David Boaz Himanshu Gupta,Avraham Shinnar,Rohith D Vallam,David Amid Esther Goldbraich,Guy Uziel,Jim Laredo,Ateret Anaby Tavor*

Main category: cs.SE

TL;DR: OASBuilder automates the transformation of unstructured API documentation into machine-readable OpenAPI specs, saving significant manual effort and improving API accessibility for AI tools.


<details>
  <summary>Details</summary>
Motivation: Extracting structured, machine-readable API specifications from online documentation is tedious and inefficient, as most API documentation is unstructured. Standardization is necessary for AI agents and automation tools to interact with APIs efficiently.

Method: The authors designed OASBuilder, a framework that uses a pipeline combining large language models (LLMs) and rule-based algorithms. This system leverages domain knowledge about how documentation webpages are structured to convert unstructured HTML documents into OpenAPI specifications.

Result: OASBuilder was tested on hundreds of APIs and successfully generalized to diverse documentation. It produced valid OpenAPI specifications covering most information from the original documents. Implementation in an enterprise setting significantly reduced manual effort and enabled better accessibility of APIs for LLM tools.

Conclusion: OASBuilder automates the conversion of unstructured API documentation into standardized OpenAPI specs, demonstrating robust generalization, practical enterprise value, and notable time savings.

Abstract: AI agents and business automation tools interacting with external web
services require standardized, machine-readable information about their APIs in
the form of API specifications. However, the information about APIs available
online is often presented as unstructured, free-form HTML documentation,
requiring external users to spend significant time manually converting it into
a structured format. To address this, we introduce OASBuilder, a novel
framework that transforms long and diverse API documentation pages into
consistent, machine-readable API specifications. This is achieved through a
carefully crafted pipeline that integrates large language models and rule-based
algorithms which are guided by domain knowledge of the structure of
documentation webpages. Our experiments demonstrate that OASBuilder generalizes
well across hundreds of APIs, and produces valid OpenAPI specifications that
encapsulate most of the information from the original documentation. OASBuilder
has been successfully implemented in an enterprise environment, saving
thousands of hours of manual effort and making hundreds of complex enterprise
APIs accessible as tools for LLMs.

</details>


### [10] [Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives](https://arxiv.org/abs/2507.05325)
*Lidiany Cerqueira,João Pedro Bastos,Danilo Neves,Glauco Carneiro,Rodrigo Spínola,Sávio Freire,José Amancio Macedo Santos,Manoel Mendonça*

Main category: cs.SE

TL;DR: Empathy is vital yet understudied in software engineering. By analyzing practitioner articles and surveying experts, this study defines empathy in SE, pinpoints barriers, suggests fostering practices, and synthesizes these into a practical framework validated by experts. The work highlights empathy’s benefits and lays groundwork for future research and training integration.


<details>
  <summary>Details</summary>
Motivation: Empathy is crucial for communication and collaboration in software engineering (SE) but is underexplored as a research topic. The authors aim to fill this gap by understanding what empathy means to SE practitioners, the barriers they face, and how empathy can be promoted.

Method: The study used qualitative content analysis on 55 web articles from DEV and Medium, both popular practitioner communities. It also included a follow-up survey with empathy experts to validate and enhance the findings.

Result: The study defined empathy in SE, identified major barriers (such as toxic culture and excessive technical focus), and recommended practices to foster empathy. Outcomes observed were improved collaboration and communication, as well as reduced anxiety, frustration, and stress. These were synthesized into a conceptual framework.

Conclusion: The proposed framework is clear and valuable, raising awareness of empathy and providing actionable suggestions for integration into training. The research encourages addressing empathy barriers and adopting strategies to enhance team dynamics. Future research will further investigate empathy's wider impact in SE.

Abstract: Context. Empathy, a key social skill, is essential for communication and
collaboration in SE but remains an under-researched topic. Aims. This study
investigates empathy in SE from practitioners' perspectives, aiming to
characterize its meaning, identify barriers, discuss practices to overcome
them, and explore its effects. Method. A qualitative content analysis was
conducted on 55 web articles from DEV and Medium, two communities widely used
by practitioners. To strengthen our findings, we conducted a follow-up survey
with empathy experts. Results. The study proposes a definition of empathy in
SE, identifies barriers such as toxic culture and excessive technical focus,
practices to foster empathy in teams, and outcomes, including improved
collaboration, communication, and reduced anxiety, frustration, and stress.
These findings are synthesized into a conceptual framework. Conclusion. Survey
results indicate the framework is clear, valuable, and raises empathy
awareness, with suggestions for improvements and integration into training.
This study paves the way for improving team dynamics by addressing barriers and
offering strategies to cultivate empathy. Future work will explore empathy's
broader implications in SE practice.

</details>


### [11] [Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs](https://arxiv.org/abs/2507.05504)
*Alex Kleijwegt,Sinem Getir Yaman,Radu Calinescu*

Main category: cs.SE

TL;DR: SLEEC-LLM uses large language models to translate complex formal analysis of normative (SLEEC) requirements into understandable natural-language feedback for non-technical stakeholders, improving efficiency and clarity in requirements engineering as shown in two real-world case studies.


<details>
  <summary>Details</summary>
Motivation: Normative requirements involving social, legal, ethical, empathetic, and cultural (SLEEC) norms are complex to define and manage due to their multidisciplinary nature. Non-technical stakeholders often struggle to understand the formal results of consistency analyses, making the requirements elicitation process inefficient.

Method: The paper introduces SLEEC-LLM, a tool that leverages large language models (LLMs) to convert formal model-checking counterexamples related to SLEEC rule inconsistencies into natural-language explanations, making these insights accessible to non-technical stakeholders.

Result: SLEEC-LLM was used in two real-world case studies with non-technical users and demonstrated improved efficiency and explainability in the elicitation and analysis of normative requirements.

Conclusion: SLEEC-LLM supports a more understandable and efficient process for non-technical stakeholders in eliciting and validating SLEEC requirements by generating human-friendly explanations of formal analysis results, thereby improving the overall requirements engineering workflow.

Abstract: Normative requirements specify social, legal, ethical, empathetic, and
cultural (SLEEC) norms that must be observed by a system. To support the
identification of SLEEC requirements, numerous standards and regulations have
been developed. These requirements are typically defined by stakeholders in the
non-technical system with diverse expertise (e.g., ethicists, lawyers, social
scientists). Hence, ensuring their consistency and managing the requirement
elicitation process are complex and error-prone tasks. Recent research has
addressed this challenge using domain-specific languages to specify normative
requirements as rules, whose consistency can then be analyzed with formal
methods. Nevertheless, these approaches often present the results from formal
verification tools in a way that is inaccessible to non-technical users. This
hinders understanding and makes the iterative process of eliciting and
validating these requirements inefficient in terms of both time and effort. To
address this problem, we introduce SLEEC-LLM, a tool that uses large language
models (LLMs) to provide natural-language interpretations for model-checking
counterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves
the efficiency and explainability of normative requirements elicitation and
consistency analysis. To demonstrate its effectiveness, we summarise its use in
two real-world case studies involving non-technical stakeholders.

</details>


### [12] [Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models](https://arxiv.org/abs/2507.05565)
*Sangwon Hyun,Shaukat Ali,M. Ali Babar*

Main category: cs.SE

TL;DR: This paper introduces and evaluates a search-based method, especially using MOEA/D, to optimize the selection of diverse and combinatorial test cases (MRs) for robust and efficient LLM robustness testing, finding 'silver bullet' MRs that reliably reveal model weaknesses.


<details>
  <summary>Details</summary>
Motivation: Current robustness testing of Large Language Models (LLMs) using Metamorphic Relations (MRs) requires selecting a large and diverse set of MRs, which can be inefficient and cost-ineffective. Existing studies mostly focus on generating individual MRs or single perturbations, rather than optimizing MR selection or exploring more complex combinations.

Method: The paper proposes a search-based approach to optimize groups of Metamorphic Relations (MRs) for robustness testing of LLMs. Four search algorithms (Single-GA, NSGA-II, SPEA2, MOEA/D) with novel encoding are implemented to find optimal combinations of MRs. These methods allow the inclusion of combinatorial perturbations, expanding the testing space. Comparative experiments are conducted using these algorithms and a random search on two major LLMs in Text-to-Text tasks.

Result: The empirical study found that the MOEA/D search algorithm outperformed others in optimizing MR selection for robustness testing. Additionally, the study discovered certain 'silver bullet' MRs that were particularly effective at confusing LLMs across various tasks.

Conclusion: The research addresses the key challenge of optimizing metamorphic testing for LLM robustness. It demonstrates the superiority of search-based solutions, especially MOEA/D, for selecting efficient and effective MR sets. The findings provide practical insights for developing scalable and cost-effective LLM testing frameworks.

Abstract: Assessing the trustworthiness of Large Language Models (LLMs), such as
robustness, has garnered significant attention. Recently, metamorphic testing
that defines Metamorphic Relations (MRs) has been widely applied to evaluate
the robustness of LLM executions. However, the MR-based robustness testing
still requires a scalable number of MRs, thereby necessitating the optimization
of selecting MRs. Most extant LLM testing studies are limited to automatically
generating test cases (i.e., MRs) to enhance failure detection. Additionally,
most studies only considered a limited test space of single perturbation MRs in
their evaluation of LLMs. In contrast, our paper proposes a search-based
approach for optimizing the MR groups to maximize failure detection and
minimize the LLM execution cost. Moreover, our approach covers the
combinatorial perturbations in MRs, facilitating the expansion of test space in
the robustness assessment. We have developed a search process and implemented
four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel
encoding to solve the MR selection problem in the LLM robustness testing. We
conducted comparative experiments on the four search algorithms along with a
random search, using two major LLMs with primary Text-to-Text tasks. Our
statistical and empirical investigation revealed two key findings: (1) the
MOEA/D algorithm performed the best in optimizing the MR space for LLM
robustness testing, and (2) we identified silver bullet MRs for the LLM
robustness testing, which demonstrated dominant capabilities in confusing LLMs
across different Text-to-Text tasks. In LLM robustness assessment, our research
sheds light on the fundamental problem for optimized testing and provides
insights into search-based solutions.

</details>


### [13] [TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems](https://arxiv.org/abs/2507.05932)
*You Lu,Dingji Wang,Kaifeng Huang,Bihuan Chen,Xin Peng*

Main category: cs.SE

TL;DR: Manual data collection for testing traffic light detection in autonomous vehicles is limited and laborious. The proposed tool, TigAug, automatically augments labeled images using transformations based on environment and sensor factors. It allows more effective, efficient testing and retraining of detection models, as validated by large-scale experiments.


<details>
  <summary>Details</summary>
Motivation: Ensuring the reliability and robustness of autonomous driving systems (ADSs) is crucial, but automated testing of traffic light detection models in these systems has received little attention. The current practice of manually collecting and labeling traffic light data is labor-intensive and often fails to capture diverse driving environments, making thorough testing impractical.

Method: The authors propose and implement TigAug, a system that automatically augments labeled traffic light images for testing ADS traffic light detection models. TigAug constructs two families of metamorphic relations and three families of image transformations, which consider weather conditions, camera properties, and traffic light characteristics. These augmented images are used for both detecting faults in detection models (via metamorphic testing) and improving them through retraining.

Result: Large-scale experiments with four state-of-the-art traffic light detection models and two datasets show that TigAug is effective in testing detection models, efficient in image synthesis, and produces images with satisfactory naturalness. Furthermore, retraining with TigAug-augmented data improves model performance.

Conclusion: TigAug offers an automated, efficient, and effective approach for generating diverse traffic light images to test and enhance the robustness of traffic light detection models in ADSs. Its use of metamorphic testing and targeted augmentations both improves error detection and model robustness, addressing a significant gap in ADS testing practices.

Abstract: Autonomous vehicle technology has been developed in the last decades with
recent advances in sensing and computing technology. There is an urgent need to
ensure the reliability and robustness of autonomous driving systems (ADSs).
Despite the recent achievements in testing various ADS modules, little
attention has been paid on the automated testing of traffic light detection
models in ADSs. A common practice is to manually collect and label traffic
light data. However, it is labor-intensive, and even impossible to collect
diverse data under different driving environments.
  To address these problems, we propose and implement TigAug to automatically
augment labeled traffic light images for testing traffic light detection models
in ADSs. We construct two families of metamorphic relations and three families
of transformations based on a systematic understanding of weather environments,
camera properties, and traffic light properties. We use augmented images to
detect erroneous behaviors of traffic light detection models by
transformation-specific metamorphic relations, and to improve the performance
of traffic light detection models by retraining. Large-scale experiments with
four state-of-the-art traffic light detection models and two traffic light
datasets have demonstrated that i) TigAug is effective in testing traffic light
detection models, ii) TigAug is efficient in synthesizing traffic light images,
and iii) TigAug generates traffic light images with acceptable naturalness.

</details>


### [14] [Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models](https://arxiv.org/abs/2507.05981)
*Marc Oriol,Quim Motger,Jordi Marco,Xavier Franch*

Main category: cs.SE

TL;DR: The paper explores using Multi-Agent Debate among LLMs to improve performance in Requirements Engineering tasks, finding that collaborative debate strategies are feasible and potentially valuable. They provide a taxonomy of MAD methods and show initial positive results in RE classification.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for improving LLM agents' accuracy in Requirements Engineering (RE) tasks often view models as isolated entities, producing single-pass outputs. This limits their robustness and adaptability, contrasting with how human debate enhances accuracy and reduces bias by incorporating multiple viewpoints.

Method: The authors performed a systematic study of Multi-Agent Debate (MAD) strategies across various domains, categorizing their main characteristics and building a taxonomy. They also implemented and tested a preliminary MAD-based framework for RE classification tasks to assess the approach's feasibility.

Result: The study identified and categorized several MAD strategies and formulated a taxonomy of their core features. Preliminary tests of their MAD framework in RE classification showed the feasibility of the approach.

Conclusion: Multi-Agent Debate (MAD) holds promise for enhancing LLM agent accuracy in RE tasks. The study lays groundwork for understanding MAD strategies and informs future research and improvements in the field.

Abstract: Context: Large Language Model (LLM) agents are becoming widely used for
various Requirements Engineering (RE) tasks. Research on improving their
accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval
augmented generation. However, these methods often treat models as isolated
black boxes - relying on single-pass outputs without iterative refinement or
collaboration, limiting robustness and adaptability. Objective: We propose
that, just as human debates enhance accuracy and reduce bias in RE tasks by
incorporating diverse perspectives, different LLM agents debating and
collaborating may achieve similar improvements. Our goal is to investigate
whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:
We conducted a systematic study of existing MAD strategies across various
domains to identify their key characteristics. To assess their applicability in
RE, we implemented and tested a preliminary MAD-based framework for RE
classification. Results: Our study identified and categorized several MAD
strategies, leading to a taxonomy outlining their core attributes. Our
preliminary evaluation demonstrated the feasibility of applying MAD to RE
classification. Conclusions: MAD presents a promising approach for improving
LLM accuracy in RE tasks. This study provides a foundational understanding of
MAD strategies, offering insights for future research and refinements in RE
applications.

</details>


### [15] [PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning](https://arxiv.org/abs/2507.05995)
*Pengzhou Chen,Tao Chen*

Main category: cs.SE

TL;DR: PromiseTune uses causally purified rules to guide configuration tuning, outperforming 11 other tuners by 42% and providing better system explainability.


<details>
  <summary>Details</summary>
Motivation: Modern software systems offer vast configuration options, making performance tuning both critical and complex. The challenges arise due to the need to balance exploration (searching for new good configurations) and exploitation (refining known good ones), compounded by costly measurements, large configuration spaces, and unpredictable performance landscapes. Lack of understanding about where promising configurations lie also hampers both effectiveness and explainability in tuning.

Method: The authors introduce PromiseTune, a new configuration tuner that leverages rules derived from prior knowledge and purifies them using causal inference. These purified rules highlight promising regions in the configuration landscape, guiding the tuner to focus on these areas while offering explainable results. This approach aims to balance exploration and exploitation more effectively.

Result: PromiseTune was evaluated against 11 state-of-the-art tuners across 12 systems with varying tuning budgets. Results showed that PromiseTune outperformed all competitors, achieving a 42% superior rank compared to the second-best tuner and offering improved explainability of system behaviors.

Conclusion: PromiseTune provides a more effective and explainable method for configuration tuning by utilizing causally purified rules to guide search in software configuration landscapes, yielding improved performance and richer insights into system behavior compared to previous methods.

Abstract: The high configurability of modern software systems has made configuration
tuning a crucial step for assuring system performance, e.g., latency or
throughput. However, given the expensive measurements, large configuration
space, and rugged configuration landscape, existing tuners suffer
ineffectiveness due to the difficult balance of budget utilization between
exploring uncertain regions (for escaping from local optima) and exploiting
guidance of known good configurations (for fast convergence). The root cause is
that we lack knowledge of where the promising regions lay, which also causes
challenges in the explainability of the results.
  In this paper, we propose PromiseTune that tunes configuration guided by
causally purified rules. PromiseTune is unique in the sense that we learn
rules, which reflect certain regions in the configuration landscape, and purify
them with causal inference. The remaining rules serve as approximated
reflections of the promising regions, bounding the tuning to emphasize these
places in the landscape. This, as we demonstrate, can effectively mitigate the
impact of the exploration and exploitation trade-off. Those purified regions
can then be paired with the measured configurations to provide spatial
explainability at the landscape level. Comparing with 11 state-of-the-art
tuners on 12 systems and varying budgets, we show that PromiseTune performs
significantly better than the others with $42\%$ superior rank to the overall
second best while providing richer information to explain the hidden system
characteristics.

</details>


### [16] [Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements](https://arxiv.org/abs/2507.06014)
*Tim Puhlfürß,Julia Butzke,Walid Maalej*

Main category: cs.SE

TL;DR: Most AI model documentation misses out on important ethical principles. The authors analyzed guidelines and existing model cards and developed a taxonomy to help improve documentation to better address ethical concerns.


<details>
  <summary>Details</summary>
Motivation: There is a growing concern that current AI model documentation (model cards) does not sufficiently address ethical requirements mandated by laws, guidelines, and standards. This creates a gap between required ethical practices and real-world documentation.

Method: The authors conducted a thematic analysis of 26 ethics and AI guidelines, three AI documentation frameworks, three quantitative studies of model cards, and ten real model cards to identify ethical requirements and evaluate the current state of model documentation.

Result: The study identified 43 ethical requirements relevant to model documentation and grouped them into a taxonomy with four themes and twelve sub-themes covering various ethical principles. It found that most model cards focus on model capabilities and reliability but neglect important aspects such as explainability, user autonomy, and fairness.

Conclusion: Current model documentation predominantly overlooks several key ethical principles. The taxonomy developed in this research can guide the creation of more comprehensive model card frameworks, helping ensure that ethical requirements are systematically addressed in AI documentation.

Abstract: Model cards are the primary documentation framework for developers of
artificial intelligence (AI) models to communicate critical information to
their users. Those users are often developers themselves looking for relevant
documentation to ensure that their AI systems comply with the ethical
requirements of existing laws, guidelines, and standards. Recent studies
indicate inadequate model documentation practices, suggesting a gap between AI
requirements and current practices in model documentation. To understand this
gap and provide actionable guidance to bridge it, we conducted a thematic
analysis of 26 guidelines on ethics and AI, three AI documentation frameworks,
three quantitative studies of model cards, and ten actual model cards. We
identified a total of 43 ethical requirements relevant to model documentation
and organized them into a taxonomy featuring four themes and twelve sub-themes
representing ethical principles. Our findings indicate that model developers
predominantly emphasize model capabilities and reliability in the documentation
while overlooking other ethical aspects, such as explainability, user autonomy,
and fairness. This underscores the need for enhanced support in documenting
ethical AI considerations. Our taxonomy serves as a foundation for a revised
model card framework that holistically addresses ethical AI requirements.

</details>
