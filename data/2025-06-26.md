<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Can LLMs Replace Humans During Code Chunking?](https://arxiv.org/abs/2506.19897)
*Christopher Glasz,Emily Escamilla,Eric O. Scott,Anand Patel,Jacob Zimmer,Colin Diggs,Michael Doyle,Scott Rosen,Nitin Naik,Justin F. Brunelle,Samruddhi Thaker,Parthav Poudel,Arun Sridharan,Amit Madan,Doug Wendt,William Macke,Thomas Schill*

Main category: cs.SE

TL;DR: LLMs can outperform humans in partitioning and documenting legacy government code (like MUMPS and ALC) when proper chunking strategies are used, making them valuable tools for code modernization.


<details>
  <summary>Details</summary>
Motivation: There is a lack of research on how LLMs handle legacy programming languages, particularly those used in government software, such as MUMPS and ALC. This is a critical gap because these systems are extensive and often exceed the input limitations of current LLMs, which are mostly trained on modern languages.

Method: The paper evaluates different code-chunking strategies to enable large language models to process and generate documentation (module summaries) for legacy code files in ALC and MUMPS. Several LLMs, including GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3, are compared based on the quality of documentation generated via these chunking approaches.

Result: LLMs can create code partitions closely matching expert human partitioning. When LLM-based chunking is used, the produced summary comments are up to 20% more factual and up to 10% more useful than summaries created when humans partition the code.

Conclusion: LLMs can effectively replace humans for partitioning large legacy codebases during modernization efforts, improving the factual quality and usefulness of generated documentation.

Abstract: Large language models (LLMs) have become essential tools in computer science,
especially for tasks involving code understanding and generation. However,
existing work does not address many of the unique challenges presented by code
written for government applications. In particular, government enterprise
software is often written in legacy languages like MUMPS or assembly language
code (ALC) and the overall token lengths of these systems exceed the context
window size for current commercially available LLMs. Additionally, LLMs are
primarily trained on modern software languages and have undergone limited
testing with legacy languages, making their ability to understand legacy
languages unknown and, hence, an area for empirical study. This paper examines
the application of LLMs in the modernization of legacy government code written
in ALC and MUMPS, addressing the challenges of input limitations. We
investigate various code-chunking methods to optimize the generation of summary
module comments for legacy code files, evaluating the impact of code-chunking
methods on the quality of documentation produced by different LLMs, including
GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs
can select partition points closely aligned with human expert partitioning. We
also find that chunking approaches have significant impact on downstream tasks
such as documentation generation. LLM-created partitions produce comments that
are up to 20% more factual and up to 10% more useful than when humans create
partitions. Therefore, we conclude that LLMs can be used as suitable
replacements for human partitioning of large codebases during LLM-aided
modernization.

</details>


### [2] [When Domains Collide: An Activity Theory Exploration of Cross-Disciplinary Collaboration](https://arxiv.org/abs/2506.20063)
*Zixuan Feng,Thomas Zimmermann,Lorenzo Pisani,Christopher Gooley,Jeremiah Wander,Anita Sarma*

Main category: cs.SE

TL;DR: The paper analyzes collaborations between domain experts and software developers, identifies sources of friction through interviews and surveys, and offers insights to improve future cross-disciplinary software development.


<details>
  <summary>Details</summary>
Motivation: Software development is becoming increasingly cross-disciplinary, involving collaboration between domain experts (DEs) and software developers (SDEs). These collaborations introduce different expectations, perspectives, and priorities, which can cause friction and hinder productivity.

Method: The study employs Activity Theory as its analytical framework. It uses a mixed-methods approach: first, 24 interviews split equally between DEs and SDEs; then a large-scale survey with 293 participants (161 DEs and 132 SDEs) to validate findings.

Result: The study identified eight expectations held by SDEs and six by DEs about cross-disciplinary collaboration. Mapping these expectations to Activity Theory components, the study revealed 21 distinct frictions and clarified where and how they develop in practice.

Conclusion: The research provides a comprehensive understanding of the expectations and frictions in cross-disciplinary software development. It offers a new theoretical perspective and practical insights useful for researchers, practitioners, and the design of collaborative tools and infrastructure.

Abstract: Background: Software development teams are increasingly diverse, embedded,
and cross-disciplinary. Domain experts (DEs) from different disciplines
collaborate with professional software developers (SDEs), bringing
complementary expertise in creating and maintaining complex production
software. However, contested expectations, divergent problem-solving
perspectives, and conflicting priorities lead to friction. Aims: This study
aims to investigate the dynamics of emerging collaboration of
cross-disciplinary software development (CDSD) by exploring the expectations
held by DEs and SDEs and understanding how these frictions manifest in
practice. Method: We utilize Activity Theory (AT), a well-established
socio-technical framework, as an analytical lens in a grounded, empirical
investigation, conducted through a mixed-method study involving 24 interviews
(12 DEs and 12 SDEs) and a large-scale validation survey with 293 participants
(161 DEs and 132 SDEs). Results: We conceptualize and empirically ground the
CDSD dynamics. We identified eight expectations held by SDEs and six by DEs. By
mapping these expectations to AT components, we revealed 21 frictions in CDSD
and illustrated where and how they arise. Conclusions: This study offers a
theoretical lens for understanding the dynamics and frictions in CDSD and
provides actionable insights for future research, practitioners, and
infrastructure design.

</details>


### [3] [AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary](https://arxiv.org/abs/2506.20159)
*Tomas Herda,Victoria Pichler,Zheying Zhang,Pekka Abrahamsson,Geir K. Hanssen*

Main category: cs.SE

TL;DR: This workshop brought together researchers and practitioners to tackle challenges in merging AI with Agile software development, culminating in a prioritized roadmap for research and joint action.


<details>
  <summary>Details</summary>
Motivation: The motivation for this workshop is the increasing need and complexity of integrating Artificial Intelligence into Agile software development practices, which poses various practical challenges.

Method: The workshop used interactive sessions to identify shared frustrations, systematically prioritize and analyze these issues to uncover root causes, and collaboratively develop a research roadmap.

Result: Participants developed a structured, prioritized research roadmap highlighting both immediate and long-term actionable directions for integrating AI into Agile. This agenda aims to promote collaboration between industry and academia.

Conclusion: The main conclusion is the formulation of a collaborative, actionable research agenda to address current challenges and guide successful AI integration in Agile practices.

Abstract: The full-day workshop on AI and Agile at XP 2025 convened a diverse group of
researchers and industry practitioners to address the practical challenges and
opportunities of integrating Artificial Intelligence into Agile software
development. Through interactive sessions, participants identified shared
frustrations related to integrating AI into Agile Software Development
practices, including challenges with tooling, governance, data quality, and
critical skill gaps. These challenges were systematically prioritized and
analyzed to uncover root causes. The workshop culminated in the collaborative
development of a research roadmap that pinpoints actionable directions for
future work, including both immediate solutions and ambitious long-term goals.
The key outcome is a structured agenda designed to foster joint
industry-academic efforts to move from identified frustrations to successful
implementation.

</details>


### [4] [Ten simple rules for PIs to integrate Research Software Engineering into their research group](https://arxiv.org/abs/2506.20217)
*Stuart M. Allen,Neil Chue Hong,Stephan Druskat,Toby Hodges,Daniel S. Katz,Jan Linxweiler,Frank Löffler,Lars Grunske,Heidi Seibold,Jan Philipp Thiele,Samantha Wittke*

Main category: cs.SE

TL;DR: This paper offers ten easy-to-follow rules to help research leaders understand and adopt Research Software Engineering, making their research software more reliable and reproducible.


<details>
  <summary>Details</summary>
Motivation: Research software plays a crucial role in research quality and outcomes, but many research leaders lack awareness or practical guidance about Research Software Engineering (RSEng) and how to utilize it effectively. Additionally, RSEng can be technically complex and inaccessible to some researchers.

Method: The paper presents ten simple, practical rules designed to help principal investigators and research group leaders adopt and integrate RSEng into their workflows.

Result: The rules and guidance provided can increase accessibility to RSEng, enabling group leaders to implement best practices in software development within their research teams.

Conclusion: By following the recommended rules, research leaders can improve the quality, reproducibility, and trustworthiness of their research software, resulting in more robust and credible research outcomes.

Abstract: Research Software Engineering (RSEng) is a key success factor in producing
high-quality research software, which in turn enables and improves research
outcomes. However, as a principal investigator or leader of a research group
you may not know what RSEng is, where to get started with it, or how to use it
to maximize its benefit for your research. RSEng also often comes with
technical complexity, and therefore reduced accessibility to some researchers.
The ten simple rules presented in this paper aim to improve the accessibility
of RSEng, and provide practical and actionable advice to PIs and leaders for
integrating RSEng into their research group. By following these rules, readers
can improve the quality, reproducibility, and trustworthiness of their research
software, ultimately leading to better, more reproducible and more trustworthy
research outcomes.

</details>


### [5] [The Composition of Digital Twins for Systems-of-Systems: a Systematic Literature Review](https://arxiv.org/abs/2506.20435)
*Mennatullah T. Khedr,John S. Fitzgerald*

Main category: cs.SE

TL;DR: This review highlights the lack of formal, standardized methodologies for composing and validating Digital Twins in complex systems. Most current verification relies on informal approaches, and the field needs scalable, rigorous solutions to manage integration, uncertainty, and cyber-physical consistency.


<details>
  <summary>Details</summary>
Motivation: Digital Twins are becoming vital for modeling and managing complex systems, but their integration in CPS and SoS faces challenges, particularly in model composition and ensuring their correctness through verification and validation (V&V). The paper aims to address the gap in understanding how to effectively compose and validate DTs, especially given the lack of formal and standardized V&V methods.

Method: The paper conducts a systematic literature review, analyzing 21 studies published between 2022 and 2024. The review focuses on mechanisms for DT composition, System-of-Systems characteristics, and the formality, scope, and challenges of existing V&V methodologies.

Result: The review finds that while DT composition is often discussed, its formalization is rare. Most V&V methods are semi-formal or simulation-based; formal V&V is underused. Major technical challenges revolve around model uncertainty and complex integrations, while methodological issues include the absence of standard, DT-specific V&V frameworks. The study contributes by classifying V&V approaches and underlining the need for standardized and scalable methods for complex DTs.

Conclusion: There is an urgent need to move beyond current V&V practices, which mostly focus on model validation, and address the consistency and integration challenges unique to cyber-physical systems and SoS. The field must develop standardized, scalable V&V and rigorous composition methodologies for complex DT implementations.

Abstract: Digital Twins (DTs) are increasingly used to model complex systems,
especially in Cyber-Physical Systems (CPS) and System-of-Systems (SoS), where
effective integration is key. This systematic literature review investigates DT
composition and verification and validation (V&V) methodologies. Analyzing 21
studies from 2022-2024, we examined composition mechanisms, SoS
characteristics, and V&V formality, scope, and challenges. While composition is
discussed, formalization is limited. V&V approaches vary, with semi-formal
methods and simulations dominating; formal verification is underutilized. Key
technical challenges include model uncertainty and integration complexity.
Methodological challenges highlight the lack of standardized DT-specific V&V
frameworks. There is a need to move beyond model validation to address
integration and cyber-physical consistency. This review contributes a
structured classification of V&V approaches and emphasizes the need for
standardized, scalable V&V and rigorous composition methodologies for complex
DT implementations.

</details>


### [6] [Smart Cuts: Enhance Active Learning for Vulnerability Detection by Pruning Bad Seeds](https://arxiv.org/abs/2506.20444)
*Xiang Lan,Tim Menzies,Bowen Xu*

Main category: cs.SE

TL;DR: The paper introduces a novel method for improving vulnerability detection by filtering out bad training samples and leveraging dataset maps, resulting in much better model performance and more reliable machine learning outcomes.


<details>
  <summary>Details</summary>
Motivation: Machine learning models for vulnerability detection often struggle due to poor-quality datasets, containing noise, mislabeled, or imbalanced samples, which negatively impact model effectiveness.

Method: The authors introduce a 'dataset maps-empowered' method that identifies and filters out hard-to-learn outliers ('bad seeds') from the training data. This approach categorizes samples by learning difficulty and integrates with an active learning framework to prioritize dataset quality by excluding harmful samples and focusing on informative ones, rather than relying on uncertainty-based sampling.

Result: The proposed method achieves significant F1 score improvements over random selection and standard active learning approaches, with gains up to 61.46% in some settings using CodeBERT on the Big-Vul dataset. The approach also enhances model robustness and stabilizes active learning performance.

Conclusion: Systematically filtering out performance-harmful samples while prioritizing informative ones improves both the robustness and effectiveness of machine learning-based vulnerability detection. Analyzing and mitigating dataset outliers is crucial for constructing higher-quality datasets and more reliable systems.

Abstract: Vulnerability detection is crucial for identifying security weaknesses in
software systems. However, the effectiveness of machine learning models in this
domain is often hindered by low-quality training datasets, which contain noisy,
mislabeled, or imbalanced samples. This paper proposes a novel dataset
maps-empowered approach that systematically identifies and mitigates
hard-to-learn outliers, referred to as "bad seeds", to improve model training
efficiency. Our approach can categorize training examples based on learning
difficulty and integrate this information into an active learning framework.
Unlike traditional methods that focus on uncertainty-based sampling, our
strategy prioritizes dataset quality by filtering out performance-harmful
samples while emphasizing informative ones. Our experimental results show that
our approach can improve F1 score over random selection by 45.36% (DeepGini)
and 45.91% (K-Means) and outperforms standard active learning by 61.46%
(DeepGini) and 32.65% (K-Means) for CodeBERT on the Big-Vul dataset,
demonstrating the effectiveness of integrating dataset maps for optimizing
sample selection in vulnerability detection. Furthermore, our approach also
enhances model robustness, improves sample selection by filtering bad seeds,
and stabilizes active learning performance across iterations. By analyzing the
characteristics of these outliers, we provide insights for future improvements
in dataset construction, making vulnerability detection more reliable and
cost-effective.

</details>


### [7] [Large Language Model-Driven Code Compliance Checking in Building Information Modeling](https://arxiv.org/abs/2506.20551)
*Soumya Madireddy,Lu Gao,Zia Din,Kinam Kim,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Main category: cs.SE

TL;DR: By using large language models in conjunction with BIM software, the proposed system semi-automates code compliance checking, reducing time and errors and offering adaptable, reliable compliance solutions for construction projects.


<details>
  <summary>Details</summary>
Motivation: Manual code compliance checking in BIM is time-consuming, error-prone, and inefficient. There is a need for a more efficient, accurate, and automated approach to streamline this critical process in construction projects.

Method: The paper develops a system that integrates Large Language Models (LLMs) like GPT, Claude, Gemini, and Llama with Revit software. The LLMs interpret building codes, generate Python scripts, and conduct semi-automated compliance checks within the BIM environment. Case studies on residential and office projects were used to validate the approach.

Result: The LLM-driven system reduced the time and effort for compliance checks, improved accuracy, and streamlined the identification of code violations. It eliminated repetitive tasks, simplified complex regulatory analysis, and produced actionable reports. Compared to purely manual methods, it demonstrated greater efficiency and reliability.

Conclusion: Integrating LLMs with BIM presents a significant improvement in code compliance checking, making the process faster, more accurate, and cost-effective. The approach is adaptable to various construction projects and regulatory contexts, providing a promising direction for future development in the industry.

Abstract: This research addresses the time-consuming and error-prone nature of manual
code compliance checking in Building Information Modeling (BIM) by introducing
a Large Language Model (LLM)-driven approach to semi-automate this critical
process. The developed system integrates LLMs such as GPT, Claude, Gemini, and
Llama, with Revit software to interpret building codes, generate Python
scripts, and perform semi-automated compliance checks within the BIM
environment. Case studies on a single-family residential project and an office
building project demonstrated the system's ability to reduce the time and
effort required for compliance checks while improving accuracy. It streamlined
the identification of violations, such as non-compliant room dimensions,
material usage, and object placements, by automatically assessing relationships
and generating actionable reports. Compared to manual methods, the system
eliminated repetitive tasks, simplified complex regulations, and ensured
reliable adherence to standards. By offering a comprehensive, adaptable, and
cost-effective solution, this proposed approach offers a promising advancement
in BIM-based compliance checking, with potential applications across diverse
regulatory documents in construction projects.

</details>


### [8] [CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency](https://arxiv.org/abs/2506.20558)
*Renyi Zhong,Yintong Huo,Wenwei Gu,Jinxi Kuang,Zhihan Jiang,Guangba Yu,Yichen Li,David Lo,Michael R. Lyu*

Main category: cs.SE

TL;DR: This paper introduces a new high-quality dataset (CCIBench) and a powerful LLM-based system (CCISolver) for detecting and fixing inconsistencies between code and comments. CCISolver outperforms previous methods in accuracy, fixing rate, and speed, making it highly valuable for real-world software development.


<details>
  <summary>Details</summary>
Motivation: Code comments are essential for software documentation and developer communication. However, inconsistencies between code and comments (CCI) can hinder software development and maintenance. Current solutions are hampered by inaccurate datasets and ineffective methods, limiting their real-world value.

Method: The authors quantitatively analyze existing code-comment inconsistency datasets, exposing high rates of mislabeling. They introduce CCIBench, a curated and higher-quality dataset for training and evaluating CCI methods. Additionally, they propose CCISolver, an end-to-end framework based on large language models (LLM) to detect and fix CCIs.

Result: CCISolver achieves a new state-of-the-art F1-score of 89.54% for CCI detection and an 18.84% improvement in GLEU for code-comment fixing compared to the best previous method. Human evaluation indicates a fixing success rate of 0.6533, and CCISolver is 36% faster in inference than competing models.

Conclusion: The CCIBench dataset improves reliability in CCI research, and the CCISolver framework achieves significant advancements in both accuracy and speed for code-comment inconsistency detection and correction, suggesting strong practical applicability for software engineering tasks.

Abstract: Comments within code serve as a crucial foundation for software
documentation, facilitating developers to communicate and understand the code
effectively. However, code-comment inconsistency (CCI) can negatively affect
software development, testing, and maintenance. Recent efforts to mitigate this
issue have emerged, but existing studies often suffer from inaccurate datasets
and inadequate solutions, weakening their practical effectiveness. In this
study, we first conduct a quantitative analysis of existing datasets, revealing
a substantial portion of sampled data are mislabeled. To address these data
limitations, we introduce CCIBench, a refined dataset comprising high-quality
data, to support the training and evaluation of method-level CCI methods.
Furthermore, we present an innovative end-to-end LLM-based framework,
CCISolver, designed to improve code quality by identifying and rectifying CCIs.
Comprehensive evaluations demonstrate CCISolver's superior performance. For
detection, it establishes a new state-of-the-art with an F1-score of 89.54%. In
fixing task, it achieves a remarkable 18.84% relative improvement in GLEU score
over the strongest baseline. This superiority is confirmed by human evaluation,
where CCISolver's fixing success rate of 0.6533 significantly surpasses
existing methods. Critically, in a practical end-to-end setting, CCISolver's
innovative architecture is approximately 36% faster for inference than the
baseline model, underscoring its scalability and real-world applicability.

</details>


### [9] [Define-ML: An Approach to Ideate Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.20621)
*Silvio Alonso,Antonio Pedro Santos Alves,Lucas Romao,Hélio Lopes,Marcos Kalinowski*

Main category: cs.SE

TL;DR: Define-ML is a new framework designed to enhance early-stage ideation for ML products by addressing data and technical challenges that traditional methods overlook. It improves clarity, alignment, and collaboration in ML projects and has been validated for effectiveness, with unanimous intent for adoption among users.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unique challenges present in machine learning (ML) product ideation, such as data dependencies, technical feasibility, and the need to align probabilistic system behavior with business objectives. Traditional ideation methods, like Lean Inception, do not sufficiently support these ML-specific concerns, often leading to misaligned visions and unrealistic expectations.

Method: The authors developed the Define-ML framework by extending Lean Inception with ML-focused activities: Data Source Mapping, Feature-to-Data Source Mapping, and ML Mapping. They validated the framework using the Technology Transfer Model, including static validation with a toy problem and dynamic validation within a real-world industrial case study. Methods included quantitative surveys and qualitative feedback to evaluate utility, ease of use, and adoption intent.

Result: Participants found Define-ML effective in clarifying data concerns, aligning ML capabilities with business goals, and fostering cross-functional collaboration. The structured framework reduced ambiguity in the ideation process, though a learning curve was noted for the ML-specific aspects. This learning curve can be mitigated through expert facilitation. All participants intended to adopt the framework.

Conclusion: Define-ML is an open, validated framework for ML product ideation that extends Lean Inception with activities specifically addressing data and technical constraints. It supports agility, improves alignment with available data, and raises awareness of technical feasibility within ML projects.

Abstract: [Context] The increasing adoption of machine learning (ML) in software
systems demands specialized ideation approaches that address ML-specific
challenges, including data dependencies, technical feasibility, and alignment
between business objectives and probabilistic system behavior. Traditional
ideation methods like Lean Inception lack structured support for these ML
considerations, which can result in misaligned product visions and unrealistic
expectations. [Goal] This paper presents Define-ML, a framework that extends
Lean Inception with tailored activities - Data Source Mapping, Feature-to-Data
Source Mapping, and ML Mapping - to systematically integrate data and technical
constraints into early-stage ML product ideation. [Method] We developed and
validated Define-ML following the Technology Transfer Model, conducting both
static validation (with a toy problem) and dynamic validation (in a real-world
industrial case study). The analysis combined quantitative surveys with
qualitative feedback, assessing utility, ease of use, and intent of adoption.
[Results] Participants found Define-ML effective for clarifying data concerns,
aligning ML capabilities with business goals, and fostering cross-functional
collaboration. The approach's structured activities reduced ideation ambiguity,
though some noted a learning curve for ML-specific components, which can be
mitigated by expert facilitation. All participants expressed the intention to
adopt Define-ML. [Conclusion] Define-ML provides an openly available, validated
approach for ML product ideation, building on Lean Inception's agility while
aligning features with available data and increasing awareness of technical
feasibility.

</details>
