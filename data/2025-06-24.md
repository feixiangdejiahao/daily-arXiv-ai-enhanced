<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 29]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners](https://arxiv.org/abs/2506.17306)
*Jake Zappin,Trevor Stalnaker,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: Quantum software developers mostly use classical, manual testing and debugging practices due to a lack of quantum-specific tools. Major bugs are classical, stemming from libraries, coding mistakes, and poor SDK abstraction. The field urgently needs tailored tools and workflows for quantum development, and this paper suggests detailed, actionable improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the unique challenges faced in quantum software engineering, particularly in testing and debugging, as the field transitions from theory to practical implementation. Issues such as probabilistic execution, limited observability, and a lack of quantum-specific tools present significant obstacles that have not been systematically studied from the perspective of current developers' practices.

Method: The authors conducted a survey of 26 quantum software developers from both academia and industry, followed by in-depth interviews, focusing on testing, debugging, and recurring challenges in quantum software development.

Result: The study found that although all participants engage in software testing, the majority rely on manual or classical methods, such as unit, regression, and acceptance testing, rather than quantum-specific tools. Only 31% reported using quantum-specific testing tools. Debugging also predominantly used traditional methods that do not scale well for quantum systems. The main sources of bugs in quantum software were classical in nature, such as library updates, developer mistakes, and compatibility issues, which are compounded by limited abstraction in current SDKs.

Conclusion: There is an urgent need for better-aligned and more seamlessly integrated quantum software testing and debugging tools to support developers. The paper provides detailed findings from the study and offers actionable recommendations to address the real-world needs of quantum software practitioners.

Abstract: Quantum software engineering is an emerging discipline with distinct
challenges, particularly in testing and debugging. As quantum computing
transitions from theory to implementation, developers face issues not present
in classical software development, such as probabilistic execution, limited
observability, shallow abstractions, and low awareness of quantum-specific
tools. To better understand current practices, we surveyed 26 quantum software
developers from academia and industry and conducted follow-up interviews
focused on testing, debugging, and recurring challenges. All participants
reported engaging in testing, with unit testing (88%), regression testing
(54%), and acceptance testing (54%) being the most common. However, only 31%
reported using quantum-specific testing tools, relying instead on manual
methods. Debugging practices were similarly grounded in classical strategies,
such as print statements, circuit visualizations, and simulators, which
respondents noted do not scale well. The most frequently cited sources of bugs
were classical in nature-library updates (81%), developer mistakes (68%), and
compatibility issues (62%)-often worsened by limited abstraction in existing
SDKs. These findings highlight the urgent need for better-aligned testing and
debugging tools, integrated more seamlessly into the workflows of quantum
developers. We present these results in detail and offer actionable
recommendations grounded in the real-world needs of practitioners.

</details>


### [2] [An Expert Survey on Models and Digital Twins](https://arxiv.org/abs/2506.17313)
*Jonathan Reif,Daniel Dittler,Milapji Singh Gill,Tamás Farkas,Valentin Stegmaier,Felix Gehlhoff,Tobias Kleinert,Michael Weyrich*

Main category: cs.SE

TL;DR: An expert survey across industries revealed major challenges in integrating digital models within Digital Twins: lack of standard interfaces, high manual effort, and limited model reuse. The study highlights the need for automation and better interoperability methods.


<details>
  <summary>Details</summary>
Motivation: Digital Twins are increasingly used in industry for monitoring, control, and optimization, but integrating various digital models within DTs poses practical challenges. Industry views on these integration issues and related research needs are not well studied.

Method: The study conducted an expert survey across multiple industrial application domains to collect insights about the challenges of utilizing diverse digital models within digital twins.

Result: Key challenges identified include the lack of standardized interfaces, high manual effort required for model adaptation, and insufficient support for model reuse throughout different lifecycle phases. The study also identifies research needs in areas such as automated model composition and improved semantics-based interoperability.

Conclusion: Standardization gaps, manual integration burdens, and lifecycle reuse limitations hinder effective digital model integration in Digital Twins. Advancements in automation and semantics-driven interoperability are necessary to address these challenges.

Abstract: Digital Twins (DTs) are becoming increasingly vital for future industrial
applications, enhancing monitoring, control, and optimization of physical
assets. This enhancement is made possible by integrating various Digital Models
(DMs) within DTs, which must interoperate to represent different system aspects
and fulfill diverse application purposes. However, industry perspectives on the
challenges and research needs for integrating these models are rarely obtained.
Thus, this study conducts an expert survey across multiple application domains
to identify and analyze the challenges in utilizing diverse DMs within DTs. The
results reveal missing standardized interfaces, high manual adaptation effort,
and limited support for model reuse across lifecycle phases, highlighting
future research needs in automated model composition and semantics-based
interoperability.

</details>


### [3] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: LLMs do well with basic spreadsheet tasks but struggle with complex ones. The new FLARE benchmark shows these weaknesses and emphasizes the need for better reasoning abilities in LLMs for spreadsheet-related tasks.


<details>
  <summary>Details</summary>
Motivation: Although LLMs show strong performance in many domains, their capabilities with spreadsheet-specific tasks like formula generation and data manipulation are not well understood. There is a need to assess and improve how LLMs handle such tasks, especially complex ones requiring logical reasoning.

Method: The authors proposed and developed a comprehensive benchmark framework, called FLARE (Formula Logic, Auditing, Reasoning and Evaluation). This framework evaluates state-of-the-art LLMs on a wide range of spreadsheet tasks, from basic to complex, real-world scenarios.

Result: The study found that while LLMs perform well on simple spreadsheet tasks, they often make mistakes in complex, multi-step tasks, sometimes producing answers that sound plausible but are actually incorrect. This suggests current LLMs lack the necessary logical reasoning for complicated spreadsheet work.

Conclusion: Current LLMs are limited in their ability to handle advanced spreadsheet tasks. There is a clear need to integrate symbolic reasoning capabilities into LLMs to improve accuracy and reliability in such scenarios. The FLARE benchmark provides a foundation for further research and evaluation in this area.

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities
across various domains; however, their effectiveness in spreadsheet related
tasks remains underexplored. This study introduces a foundation for a
comprehensive benchmark framework to evaluate the performance of leading LLMs
in executing spreadsheet functions, formula generation and data manipulation
tasks. The benchmark encompasses tasks ranging from basic formula creation to
complex, real world spreadsheet scenarios. Our findings reveal that while LLMs
exhibit proficiency in straightforward tasks, they often falter in complex,
multi step operations, frequently producing plausible yet incorrect outputs.
These results underscore the limitations of current LLMs in handling
spreadsheet tasks that require precise logical reasoning and highlight the need
for integrating symbolic reasoning capabilities into LLM architectures. To
support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and
Evaluation) a new benchmark for evaluating LLM performance on real-world
spreadsheet logic, auditing, and reasoning tasks.

</details>


### [4] [LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research](https://arxiv.org/abs/2506.17335)
*Shuo Yan,Ruochen Li,Ziming Luo,Zimu Wang,Daoyang Li,Liqiang Jing,Kaiyu He,Peilin Wu,George Michalopoulos,Yue Zhang,Ziyang Zhang,Mian Zhang,Zhiyu Chen,Xinya Du*

Main category: cs.SE

TL;DR: A new benchmark (LMR-BENCH) reveals that state-of-the-art LLMs struggle significantly with reproducing code from top NLP research papers, exposing persistent shortcomings in reasoning and code synthesis necessary for scientific research reproducibility.


<details>
  <summary>Details</summary>
Motivation: While large language models (LLMs) have shown significant promise in advancing scientific discovery, their ability to reproduce code from research papers, particularly in NLP, has not been thoroughly examined. Reproducing such code involves complex reasoning and understanding of both abstract concepts and interdependent code structures. Addressing this gap is important for verifying and building upon scientific work.

Method: The authors introduce LMR-BENCH, a benchmark containing 28 code reproduction tasks based on 23 recent NLP research papers. The benchmark assesses how well LLM agents can reproduce masked functions from provided code repositories and paper instructions. The evaluation uses standard prompting and agent-based approaches with state-of-the-art LLMs, measuring performance via unit test accuracy and LLM-based code correctness checks.

Result: Experimental results indicate that current leading LLMs face significant challenges in scientific reasoning and accurate code synthesis when tasked with reproducing research code. These models still have notable deficiencies, as revealed by persistent errors even under optimal prompting and agent strategies.

Conclusion: Despite their progress, LLM agents are still limited in their ability to autonomously reproduce scientific research code, especially within the NLP domain. The findings underscore a gap between the current capabilities of LLMs and the nuanced, complex requirements of research code reproduction.

Abstract: Large language model (LLM) agents have demonstrated remarkable potential in
advancing scientific discovery. However, their capability in the fundamental
yet crucial task of reproducing code from research papers, especially in the
NLP domain, remains underexplored. This task includes unique complex reasoning
challenges in the intellectual synthesis of abstract concepts and the
comprehension of code repositories with interdependent files. Motivated by this
gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the
capability of LLM agents on code reproduction from Language Modeling Research.
It consists of 28 code reproduction tasks derived from 23 research papers
published in top-tier NLP venues over the past five years, spanning nine
fundamental categories. Models are provided with a research paper, a code
repository containing one or more masked functions, and instructions for
implementing these functions. We conduct extensive experiments in standard
prompting and LLM agent settings with state-of-the-art LLMs, evaluating the
accuracy of unit tests and performing LLM-based evaluation of code correctness.
Experimental results reveal that even the most advanced models still exhibit
persistent limitations in scientific reasoning and code synthesis, highlighting
critical gaps in LLM agents' ability to autonomously reproduce scientific
research

</details>


### [5] [Re-Evaluating Code LLM Benchmarks Under Semantic Mutation](https://arxiv.org/abs/2506.17369)
*Zhiyuan Pan,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: Changes to prompt templates, even if subtle, can greatly affect how LLMs perform on code-related tasks. This makes current code benchmarks unreliable, and future designs must take prompt sensitivity into account for trustworthy evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing code benchmarks often use only one prompt per task, making evaluations sensitive to minor prompt changes, which results in unreliable model comparisons. Previous research on prompt sensitivity has mostly focused on standard NLP tasks, not code-related tasks.

Method: The authors propose a general framework to systematically modify code benchmark prompt templates while preserving their meaning and structure. They conduct experiments using this framework on eight code tasks with 10 open-source LLMs, each tested with 100 semantically similar prompts. They use various statistical metrics for evaluation.

Result: Slight prompt modifications can cause significant changes in LLM performance and can even alter the relative ranking among models. This demonstrates high prompt sensitivity for code benchmarks.

Conclusion: Prompt sensitivity is a significant issue in code benchmarks. More robust benchmark design is needed to ensure reliable and accurate LLM assessment.

Abstract: In the era of large language models (LLMs), code benchmarks have become an
important research area in software engineering and are widely used by
practitioners. These benchmarks evaluate the performance of LLMs on specific
code-related tasks, such as code understanding and generation. A critical step
in constructing code benchmarks is the design of prompts. However, as existing
code benchmarks typically rely on a single prompt template per task, they are
prone to the issue of prompt sensitivity, where minor prompt variations could
result in substantial performance variations, leading to unreliable evaluations
of model capabilities.
  While previous studies have explored prompt sensitivity, their experimental
designs and findings are limited to traditional natural language processing
(NLP) tasks. In this paper, we present an empirical study to investigate prompt
sensitivity in code benchmarks. We first propose a general framework that
modifies prompt templates in a manner that preserves both their semantics and
their structure as much as possible. Based on the framework, we conduct
extensive experiments across eight code benchmark tasks on 10 representative
open-source LLMs, with each task featuring 100 semantically similar prompt
templates. We then analyze the evaluation results using various statistical
metrics, focusing on both absolute and relative model performance. Our findings
suggest that even slight prompt variations can lead to significant shifts in
performance. Additionally, we observe that such variations can introduce
inconsistencies in the performance rankings across different models. These
insights highlight the need for considering prompt sensitivity when designing
future code benchmarks, to ensure more reliable and accurate evaluation of LLM
capabilities.

</details>


### [6] [Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing](https://arxiv.org/abs/2506.17539)
*Sidong Feng,Changhao Du,Huaxiao Liu,Qingnan Wang,Zhengwei Lv,Mengfei Wang,Chunyang Chen*

Main category: cs.SE

TL;DR: MAdroid, a multi-agent testing framework powered by LLMs, effectively automates multi-user interactive feature testing for mobile apps, demonstrating high success rates, action similarity, and bug finding ability beyond current tools.


<details>
  <summary>Details</summary>
Motivation: Modern mobile apps rely heavily on multi-user interactive features (e.g., chats, streaming, conferencing), which are challenging to test automatically due to their need for coordinated, dynamic, and collaborative user actions. Existing automated testing tools do not adequately handle these requirements.

Method: The proposed method, MAdroid, leverages a multi-agent system run by Large Language Models (LLMs). It utilizes user agents (Operators) to simulate user actions, and supervisor agents (Coordinators and Observers) to manage and review the test processes. Each agent performs a specific role in orchestrating and simulating multi-user interactions for thorough app feature testing.

Result: MAdroid was evaluated over 41 multi-user interactive tasks and was able to accomplish 82.9% of them with 96.8% similarity to real user actions, surpassing prior work and ablation study baselines. It also identified 11 bugs during regression testing in real app scenarios.

Conclusion: MAdroid demonstrates significant promise in automating multi-user interactive feature testing for mobile apps, improving coverage, and helping uncover bugs that are hard to detect with traditional automation methods. The multi-agent, LLM-powered approach is effective and practical for real-world app development.

Abstract: The growing dependence on mobile phones and their apps has made multi-user
interactive features, like chat calls, live streaming, and video conferencing,
indispensable for bridging the gaps in social connectivity caused by physical
and situational barriers. However, automating these interactive features for
testing is fraught with challenges, owing to their inherent need for timely,
dynamic, and collaborative user interactions, which current automated testing
methods inadequately address. Inspired by the concept of agents designed to
autonomously and collaboratively tackle problems, we propose MAdroid, a novel
multi-agent approach powered by the Large Language Models (LLMs) to automate
the multi-user interactive task for app feature testing. Specifically, MAdroid
employs two functional types of multi-agents: user agents (Operator) and
supervisor agents (Coordinator and Observer). Each agent takes a specific role:
the Coordinator directs the interactive task; the Operator mimics user
interactions on the device; and the Observer monitors and reviews the task
automation process. Our evaluation, which included 41 multi-user interactive
tasks, demonstrates the effectiveness of our approach, achieving 82.9% of the
tasks with 96.8% action similarity, outperforming the ablation studies and
state-of-the-art baselines. Additionally, a preliminary investigation
underscores MAdroid's practicality by helping identify 11 multi-user
interactive bugs during regression app testing, confirming its potential value
in real-world software development contexts.

</details>


### [7] [CodeMorph: Mitigating Data Leakage in Large Language Model Assessment](https://arxiv.org/abs/2506.17627)
*Hongzhou Rao,Yanjie Zhao,Wenjie Zhu,Ling Xiao,Meizhen Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: CodeMorph is a new method for generating harder, contamination-resistant code evaluation datasets for LLMs. It uses advanced perturbations and a genetic algorithm to lower code similarity, supporting multiple languages and real program structures, leading to more realistic LLM evaluation.


<details>
  <summary>Details</summary>
Motivation: There are growing concerns about benchmark leakage in code-focused large language models (Code LLMs) due to accidental overlap between training and evaluation data, leading to inflated performance metrics. Current approaches to avoiding data contamination are not foolproof due to inaccessible and diverse datasets.

Method: The authors introduce CodeMorph, an approach that generates new code datasets via semantic-preserving code perturbations. It has two main components: (1) a set of 26 iterative transformation methods to create diverse, compilable code variations, and (2) PESO, a genetic algorithm-based selection algorithm that optimizes for lower similarity between original and perturbed code. CodeMorph supports multiple languages and maintains cross-file dependencies.

Result: Applying CodeMorph to code completion benchmarks for five programming languages led to an average 24.67% drop in LLM accuracy, with up to 45% for Python. The code perturbed using PESO had, on average, a 7.01% lower similarity score compared to random perturbations, with a maximum reduction of 42.86%.

Conclusion: CodeMorph effectively mitigates data leakage concerns by generating diverse, challenging benchmark datasets for code LLMs, thus providing more accurate evaluation. It supports multiple languages and maintains code dependencies, outperforming existing perturbation methods.

Abstract: Concerns about benchmark leakage in large language models for code (Code
LLMs) have raised issues of data contamination and inflated evaluation metrics.
The diversity and inaccessibility of many training datasets make it difficult
to prevent data leakage entirely, even with time lag strategies. Consequently,
generating new datasets through code perturbation has become essential.
However, existing methods often fail to produce complex and diverse variations,
struggle with complex cross-file dependencies, and lack support for multiple
programming languages, which limits their effectiveness in enhancing LLM
evaluations for coding tasks. To fill this gap, we propose CodeMorph, an
approach designed to support multiple programming languages while preserving
cross-file dependencies to mitigate data leakage. CodeMorph consists of two
main components that work together to enhance the perturbation process. The
first component employs 26 semantic-preserving transformation methods to
iteratively perturb code, generating diverse variations while ensuring that the
modified code remains compilable. The second component introduces a genetic
algorithm-based selection algorithm, PESO, to identify the more effective
perturbation method for each iteration by targeting lower similarity scores
between the perturbed and original code, thereby enhancing overall perturbation
effectiveness. Experimental results demonstrate that after applying CodeMorph,
the accuracy of the LLM on code completion tasks across five programming
languages decreased by an average of 24.67%, with Python showing the most
significant reduction at 45%. The similarity score of code optimized by PESO
is, on average, 7.01% lower than that of randomly perturbed code, peaking at a
reduction of 42.86%.

</details>


### [8] [Deep Learning Framework Testing via Model Mutation: How Far Are We?](https://arxiv.org/abs/2506.17638)
*Yanzhou Mu,Rong Wang,Juan Zhai,Chunrong Fang,Xiang Chen,Zhiyuan Peng,Peiran Yang,Ruixiang Qian,Shaoyu Yang,Zhenyu Chen*

Main category: cs.SE

TL;DR: Current mutation-based testing for deep learning frameworks is hampered by ineffectiveness and high false positives. This study analyzes and optimizes these methods, resulting in improved detection and resolution of real framework defects.


<details>
  <summary>Details</summary>
Motivation: Deep Learning frameworks are essential for DL development, but detecting their defects remains both crucial and challenging. Current mutation-based testing techniques often lack customization for framework testing and suffer from high false positive rates, as well as limited attention from developers. There is a need to reassess the effectiveness of these techniques and propose optimizations.

Method: The authors reviewed existing mutation-based defect detection methods, collected defect reports from three popular DL frameworks, and classified the reports using developer validation. They performed an analysis to understand the effectiveness and shortcomings of current techniques, leading to the proposal of new optimization strategies.

Result: The optimized approach led to the identification of seven new defects, with four confirmed as high-priority and three already resolved by developers. In total, 39 unique defects were found in 23 models; 31 confirmed by developers, and eight fixed.

Conclusion: Existing mutation-based methods for DL framework testing have significant limitations, especially in terms of customization and false positives. By optimizing these methods, significant improvements in defect detection and resolution can be achieved, benefiting both framework reliability and developer workflow.

Abstract: Deep Learning (DL) frameworks are a fundamental component of DL development.
Therefore, the detection of DL framework defects is important and challenging.
As one of the most widely adopted DL testing techniques, model mutation has
recently gained significant attention. In this study, we revisit the defect
detection ability of existing mutation-based testing methods and investigate
the factors that influence their effectiveness. To begin with, we reviewed
existing methods and observed that many of them mutate DL models (e.g.,
changing their parameters) without any customization, ignoring the unique
challenges in framework testing. Another issue with these methods is their
limited effectiveness, characterized by a high rate of false positives caused
by illegal mutations arising from the use of generic, non-customized mutation
operators. Moreover, we tracked the defects identified by these methods and
discovered that most of them were ignored by developers. Motivated by these
observations, we investigate the effectiveness of existing mutation-based
testing methods in detecting important defects that have been authenticated by
framework developers. We begin by collecting defect reports from three popular
frameworks and classifying them based on framework developers' ratings to build
a comprehensive dataset. We then perform an in-depth analysis to uncover
valuable insights. Based on our findings, we propose optimization strategies to
address the shortcomings of existing approaches. Following these optimizations,
we identified seven new defects, four of which were confirmed by developers as
high-priority issues, with three resolved. In summary, we identified 39 unique
defects across just 23 models, of which 31 were confirmed by developers, and
eight have been fixed.

</details>


### [9] [May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs](https://arxiv.org/abs/2506.17642)
*Shaoyu Yang,Chunrong Fang,Haifeng Lin,Xiang Chen,Zhenyu Chen*

Main category: cs.SE

TL;DR: FUEL, a novel fuzzing framework using LLMs for both feedback analysis and test generation, effectively found numerous and impactful bugs in major DL frameworks, showing that deep, multi-type feedback integration is key to advancing AI infrastructure reliability.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is the critical necessity to reliably detect bugs in Deep Learning (DL) frameworks, as such bugs can have catastrophic effects in high-stakes domains like healthcare and autonomous driving. Existing fuzzing techniques lack comprehensive, multi-type feedback usage and perform feedback analysis in a coarse-grained way. Moreover, current LLM-based fuzzing only utilizes LLMs for test case generation, overlooking feedback analysis.

Method: The authors propose FUEL, a fuzzing framework that integrates two Large Language Model (LLM) agents: an analysis LLM to infer summaries from feedback information, and a generation LLM to create test cases based on these summaries. This method leverages diverse feedback for more effective and varied test case generation.

Result: FUEL identified 104 bugs in PyTorch and TensorFlow. Among these, 93 were confirmed as new bugs, 47 have already been fixed, and 5 have been assigned CVE IDs, demonstrating significant impact and effectiveness.

Conclusion: Leveraging LLMs for both feedback analysis and test case generation, and considering multiple feedback types, enhances fuzzing efficacy for DL frameworks. FUEL’s results suggest this is a promising path for future research and practical application.

Abstract: Artificial Intelligence (AI) Infrastructures, represented by Deep Learning
(DL) frameworks, have served as fundamental DL systems over the last decade.
However, the bugs in DL frameworks could lead to catastrophic consequences in
some critical scenarios (e.g., healthcare and autonomous driving). A simple yet
effective way to find bugs in DL frameworks is fuzz testing (Fuzzing).
Unfortunately, existing fuzzing techniques have not comprehensively considered
multiple types of feedback. Additionally, they analyze feedback in a
coarse-grained manner, such as mutating the test cases only according to
whether the coverage increases. Recently, researchers introduced Large Language
Models (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only
focus on using LLMs to generate test cases while overlooking their potential to
analyze feedback information, failing to create more valid and diverse test
cases. To fill this gap, we propose FUEL to break the seal of Feedback-driven
fuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents,
namely analysis LLM and generation LLM. Analysis LLM agent infers analysis
summaries from feedback information, while the generation LLM agent creates
tests guided by these analysis summaries. So far, FUEL has detected 104 bugs
for PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed,
and 5 assigned with CVE IDs. Our work indicates that considering multiple types
of feedback is beneficial to fuzzing performance, and leveraging LLMs to
analyze feedback information is a promising direction. Our artifact is
available at https://github.com/NJU-iSE/FUEL

</details>


### [10] [Improving Compiler Bug Isolation by Leveraging Large Language Models](https://arxiv.org/abs/2506.17647)
*Yixian Qi,Jiajun Jiang,Fengjie Li,Bowen Chen,Hongyu Zhang,Junjie Chen*

Main category: cs.SE

TL;DR: AutoCBI uses large language models to summarize and rank suspicious files in compilers, significantly improving bug isolation over current methods in real-world GCC and LLVM bugs.


<details>
  <summary>Details</summary>
Motivation: Bugs in compilers can have severe consequences, but current automated bug localization techniques struggle with the scale and complexity of modern compilers. Existing methods have limitations in test mutability and efficiency. There is a need for more scalable and effective approaches.

Method: The authors introduce AutoCBI, an innovative method that leverages large language models (LLMs) to summarize the functions of compiler source files and uses specialized prompting to guide the LLM in reordering suspicious file rankings. AutoCBI integrates four key sources of information: failing test programs, summaries of source file functions, lists of suspicious files from test coverage analysis, and compilation-related configurations/output messages, to refine the ranking of suspected buggy files.

Result: AutoCBI was evaluated against leading approaches (DiWi, RecBi, and FuseFL) using 120 real bugs from GCC and LLVM compilers. AutoCBI outperformed the others, isolating 66.67%/69.23% (GCC/LLVM), 300%/340%, and 100%/57.14% more bugs in the Top-1 ranked files compared to RecBi, DiWi, and FuseFL respectively. An ablation study confirmed the importance of each component of AutoCBI.

Conclusion: By leveraging LLMs and integrating multiple sources of information, AutoCBI provides a more scalable and effective solution for compiler bug isolation, surpassing current state-of-the-art approaches in accuracy and efficiency.

Abstract: Compilers play a foundational role in building reliable software systems, and
bugs within them can lead to catastrophic consequences. The compilation process
typically involves hundreds of files, making traditional automated bug
isolation techniques inapplicable due to scalability or effectiveness issues.
Current mainstream compiler bug localization techniques have limitations in
test program mutation and resource consumption. Inspired by the recent advances
of pre-trained Large Language Models (LLMs), we propose an innovative approach
named AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2)
employs specialized prompts to guide LLM in reordering suspicious file
rankings. This approach leverages four types of information: the failing test
program, source file function summaries, lists of suspicious files identified
through analyzing test coverage, as well as compilation configurations with
related output messages, resulting in a refined ranking of suspicious files.
Our evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and
FuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers
demonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%,
300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL,
respectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the
ablation study underscores the significance of each component in our approach.

</details>


### [11] [PAGENT: Learning to Patch Software Engineering Agents](https://arxiv.org/abs/2506.17772)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.SE

TL;DR: The paper investigates why LLM code agents often generate faulty patches, identifying key failure types—including type errors. PAGENT, a tool combining static analysis and LLMs, can fix a notable portion of such errors, advancing automated code repair.


<details>
  <summary>Details</summary>
Motivation: Large Language Model (LLM) agents auto-generate code patches but often create inaccurate ones. The root causes of these failures are poorly understood, and solutions to improve patch accuracy are needed.

Method: The authors conducted an empirical study of failed patches generated by seven top LLM code agents using 114 unresolved issues from the SWE-bench Lite dataset. They analyzed 769 failed patches (using GPT-4o and manual review), created a taxonomy of failure reasons, and focused on type-related errors. To address these, they developed PAGENT, which combines traditional program analysis (static code analysis and control flow graph exploration) with LLM-based inference to improve patch type information.

Result: A taxonomy with six main categories of failure reasons was established. Among these, type inference errors were common. The new tool, PAGENT, was able to fix 29 out of 127 type-related failed patches produced by the top agents, showing promise in addressing a common cause of failure.

Conclusion: LLM agents often fail to generate correct patches due to several reasons, with type inference being a major problem. Addressing this, PAGENT demonstrates that combining static code analysis with LLM inference can repair a notable subset of type-related patch failures. Further improvements can likely increase this success rate and help improve automated patching.

Abstract: LLM Agents produce patches automatically to resolve an issue. However, they
can generate inaccurate patches. Little is known about the root causes behind
those failed patches or how those could be fixed. This paper reports an
empirical study of the failed patches generated by seven top LLM code agents.
We collected 114 issues from the SWE-bench Lite dataset that remained
unresolved across the agents. The seven agents produced a total of 769 failed
patches for those issues, which we checked with a combination of GPT-4o and
manual analysis. We present a taxonomy of the failure reasons across the
patches. The taxonomy contains six categories, with several sub-categories
under each category. For example, a frequently observed category is the
inability of an LLM to correctly infer/produce the appropriate variable type in
the produced patch. As a first step towards addressing such type-related
errors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis
techniques like CFG creation and exploration to infer the type of information
of a patch. PAGENT does this by applying repository-level static code analysis
techniques. Then, PAGENT refines the inferred type by further utilizing an
LLM-based inference technique. We tested PAGENT on all 127 type-related failed
patches from the top three agents in our study. PAGENT could fix 29 of the 127
failed patches.

</details>


### [12] [SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis](https://arxiv.org/abs/2506.17798)
*Wang Lingxiang,Quanzhi Fu,Wenjia Song,Gelei Deng,Yi Liu,Dan Williams,Ying Zhang*

Main category: cs.SE

TL;DR: SAVANT uses LLMs and semantic code analysis to better detect true vulnerable API usages in Java dependencies, reducing false alerts and improving security response, with evaluation showing higher performance than leading tools.


<details>
  <summary>Details</summary>
Motivation: Java projects increasingly rely on open-source third-party libraries, but when these libraries have vulnerabilities, they pose significant security risks. Current SCA tools are not effective enough, leading to many false alerts and missed critical vulnerabilities due to poor understanding of API usage semantics and difficulty analyzing complex code.

Method: The authors propose SAVANT, a method that (1) semantically pre-processes source code to segment it into meaningful, semantically-related blocks, and (2) applies Large Language Models (LLMs) for context-aware reflection on how APIs from potentially vulnerable libraries are actually used, determining the real risk.

Result: SAVANT was evaluated on 55 real-world Java applications and achieved an 83.8% precision, 73.8% recall, 69.0% accuracy, and 78.5% F1-score—showing superior performance compared to leading SCA tools.

Conclusion: Leveraging LLMs and semantic pre-processing enables more accurate detection of vulnerable API usage in third-party libraries, significantly reducing the burden of inaccurate alerts and accelerating necessary security fixes.

Abstract: The integration of open-source third-party library dependencies in Java
development introduces significant security risks when these libraries contain
known vulnerabilities. Existing Software Composition Analysis (SCA) tools
struggle to effectively detect vulnerable API usage from these libraries due to
limitations in understanding API usage semantics and computational challenges
in analyzing complex codebases, leading to inaccurate vulnerability alerts that
burden development teams and delay critical security fixes.
  To address these challenges, we proposed SAVANT by leveraging two insights:
proof-of-vulnerability test cases demonstrate how vulnerabilities can be
triggered in specific contexts, and Large Language Models (LLMs) can understand
code semantics. SAVANT combines semantic preprocessing with LLM-powered context
analysis for accurate vulnerability detection. SAVANT first segments source
code into meaningful blocks while preserving semantic relationships, then
leverages LLM-based reflection to analyze API usage context and determine
actual vulnerability impacts. Our evaluation on 55 real-world applications
shows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and
78.5% F1-score, outperforming state-of-the-art SCA tools.

</details>


### [13] [Is Your Automated Software Engineer Trustworthy?](https://arxiv.org/abs/2506.17812)
*Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: BouncerBench is a new benchmark for testing if LLMs can refuse to answer when bug reports are too vague or suggested code patches are wrong. Most current models still answer even when uncertain, showing a key shortcoming. This benchmark aims to push LLMs to be more cautious before being used reliably in real-world software development.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are increasingly used in software engineering, especially for bug report resolution. However, current LLM-based systems always generate responses, even when input is unclear or outputs are potentially wrong, leading to unreliable and untrustworthy behavior. There's a need for mechanisms enabling models to abstain in the face of uncertainty.

Method: The authors introduce BouncerBench, a benchmark designed to evaluate whether LLM-based software agents can refuse to act on ill-defined inputs or abstain from providing outputs when their answers are likely incorrect. BouncerBench focuses on two failure points: vague issue reports and functionally incorrect code patches, and includes tools to test and assess model abstention.

Result: Experiments show that most current LLMs consistently fail to abstain when presented with vague inputs or when their outputs are likely incorrect, often generating responses or patches regardless of confidence or quality.

Conclusion: Significant improvements are needed before LLMs can be reliably trusted in software engineering tasks. BouncerBench establishes an important framework for evaluating and fostering the development of more cautious, trustworthy code-generation agents.

Abstract: Large Language Models (LLMs) are being increasingly used in software
engineering tasks, with an increased focus on bug report resolution over the
past year. However, most proposed systems fail to properly handle uncertain or
incorrect inputs and outputs. Existing LLM-based tools and coding agents
respond to every issue and generate a patch for every case, even when the input
is vague or their own output is incorrect. There are no mechanisms in place to
abstain when confidence is low. This leads to unreliable behaviour, such as
hallucinated code changes or responses based on vague issue reports. We
introduce BouncerBench, a benchmark that evaluates whether LLM-based software
agents can refuse to act when inputs are ill-defined or refuse to respond when
their own outputs are likely to be incorrect. Unlike prior benchmarks that
implicitly incentivize models to generate responses even when uncertain,
BouncerBench aims to improve precision by targeting two overlooked failure
points: (1) vague or underspecified issue descriptions in tickets and (2)
logically or functionally incorrect code patches created by the system. It
measures whether proposed systems can distinguish actionable issues from vague
tickets and valid patches from untrustworthy ones. We also implement a basic
input and output bouncer, evaluating how well current LLMs can abstain when
needed. Our results show that most models fail to abstain from underspecified
inputs or incorrect outputs. Hence, we conclude that there is significant room
for improvement before LLMs can be trusted to make correct decisions and
recommendations in real-world software engineering workflows. BouncerBench
provides a first step toward evaluating and building more cautious, trustworthy
code agents. The replication package, dataset, and leaderboard can be found at
bouncerbench.com

</details>


### [14] [The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study](https://arxiv.org/abs/2506.17833)
*Giorgio Amasanti,Jasmin Jahic*

Main category: cs.SE

TL;DR: AI tools help software engineers work faster, especially on smaller, simpler tasks, without harming code quality. However, for complex problems, their effectiveness drops and solution quality suffers, requiring human oversight.


<details>
  <summary>Details</summary>
Motivation: There is widespread use of AI-powered tools in software engineering, but the actual productivity benefits and the long-term effects on software quality are not fully understood.

Method: A survey was conducted among software practitioners who actively use AI tools during software development.

Result: AI tools significantly boost software engineers' productivity, but the benefit declines as project complexity increases. AI-generated solutions do not significantly harm software quality for small code snippets, but for larger and more complex tasks, the quality drops, suggesting the need for human architectural oversight.

Conclusion: AI tools are useful for boosting productivity and do not negatively affect software quality for simple tasks. However, for complex software engineering tasks, human intervention remains vital.

Abstract: AI-powered software tools are widely used to assist software engineers.
However, there is still a need to understand the productivity benefits of such
tools for software engineers. In addition to short-term benefits, there is a
question of how adopting AI-generated solutions affects the quality of software
over time (e.g., maintainability and extendability).
  To provide some insight on these questions, we conducted a survey among
software practitioners who use AI tools. Based on the data collected from our
survey, we conclude that AI tools significantly increase the productivity of
software engineers. However, the productivity benefits of using AI tools reduce
as projects become more complex. The results also show that there are no
significant negative influences of adopting AI-generated solutions on software
quality, as long as those solutions are limited to smaller code snippets.
However, when solving larger and more complex problems, AI tools generate
solutions of a lower quality, indicating the need for architects to perform
problem decomposition and solution integration.

</details>


### [15] [Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering](https://arxiv.org/abs/2506.17937)
*Tommi Mikkonen,Antero Taivalsaari*

Main category: cs.SE

TL;DR: AI-powered software reuse is reshaping software engineering, bringing both opportunities and risks. The paper calls for urgent research to address issues similar to 'cargo cult development' and guides the community towards informed and responsible adoption of these new practices.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the rapidly changing landscape of software development, where AI-assisted and generative software reuse is replacing traditional software reuse practices. The paper aims to analyze the implications of this paradigm shift.

Method: The paper employs a discussion-based method, critically assessing the impact of AI-assisted generative software reuse and raising key questions for the emerging 'AI native' software engineering context. It also outlines a preliminary research agenda.

Result: The paper presents the potential issues and challenges arising from AI-driven software reuse, likening it to cargo cult development, and suggests focal points for future research.

Conclusion: The paper concludes that there are significant unanswered questions and risks associated with AI-assisted software reuse, necessitating comprehensive research and community action to better understand and address these challenges.

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Consequently, earlier software reuse practices and methods
are rapidly being replaced by AI-assisted approaches in which developers place
their trust on code that has been generated by artificial intelligence. This is
leading to a new form of software reuse that is conceptually not all that
different from cargo cult development. In this paper we discuss the
implications of AI-assisted generative software reuse in the context of
emerging "AI native" software engineering, bring forth relevant questions, and
define a tentative research agenda and call to action for tackling some of the
central issues associated with this approach.

</details>


### [16] [Build It Clean: Large-Scale Detection of Code Smells in Build Scripts](https://arxiv.org/abs/2506.17948)
*Mahzabin Tamanna,Yash Chandrani,Matthew Burrows,Brandon Wroblewski,Laurie Williams,Dominik Wermke*

Main category: cs.SE

TL;DR: By analyzing thousands of GitHub build scripts and issues, the authors identified common code smells across Maven, Gradle, CMake, and Makefiles, and developed a tool (Sniffer) for detection. Their findings inform practical strategies for writing cleaner, more robust build scripts.


<details>
  <summary>Details</summary>
Motivation: Build scripts are widely used in modern software pipelines but are susceptible to code smells—patterns of poor practices that affect reliability and maintainability. There is a need to systematically study and mitigate these issues to improve software quality.

Method: A mixed-methods approach was used: first, a qualitative analysis of 2000 build-script-related GitHub issues; then, development of a static analysis tool (Sniffer) that scanned 5882 build scripts from 4877 open-source repositories using Maven, Gradle, CMake, and Make.

Result: 13 code smell categories were identified with nearly 11,000 occurrences. The most common smells varied between build systems: Insecure URLs in Maven, Hardcoded Paths/URLs in Gradle and CMake, and Wildcard Usage in Makefiles. Specific smell pairs were found to co-occur, highlighting deeper structural or maintenance issues.

Conclusion: This study provides insights into the prevalence and types of code smells in build scripts across different tools. It offers recommendations and strategies for practitioners to avoid these pitfalls, thereby enhancing the efficiency, reliability, and maintainability of software projects.

Abstract: Build scripts are files that automate the process of compiling source code,
managing dependencies, running tests, and packaging software into deployable
artifacts. These scripts are ubiquitous in modern software development
pipelines for streamlining testing and delivery. While developing build
scripts, practitioners may inadvertently introduce code smells. Code smells are
recurring patterns of poor coding practices that may lead to build failures or
increase risk and technical debt. The goal of this study is to aid
practitioners in avoiding code smells in build scripts through an empirical
study of build scripts and issues on GitHub. We employed a mixed-methods
approach, combining qualitative and quantitative analysis. We conducted a
qualitative analysis of 2000 build-script-related GitHub issues. Next, we
developed a static analysis tool, Sniffer, to identify code smells in 5882
build scripts of Maven, Gradle, CMake, and Make files, collected from 4877
open-source GitHub repositories. We identified 13 code smell categories, with a
total of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,
337 in CMake, and 6160 in Makefiles.
  Our analysis revealed that Insecure URLs were the most prevalent code smell
in Maven build scripts, while Hardcoded Paths/URLs were commonly observed in
both Gradle and CMake scripts. Wildcard Usage emerged as the most frequent
smell in Makefiles. The co-occurrence analysis revealed strong associations
between specific smell pairs of Hardcoded Paths/URLs with Duplicates, and
Inconsistent Dependency Management with Empty or Incomplete Tags, indicating
potential underlying issues in the build script structure and maintenance
practices. Based on our findings, we recommend strategies to mitigate the
existence of code smells in build scripts to improve the efficiency,
reliability, and maintainability of software projects.

</details>


### [17] [VFArchē: A Dual-Mode Framework for Locating Vulnerable Functions in Open-Source Software](https://arxiv.org/abs/2506.18050)
*Lyuye Zhang,Jian Zhang,Kaixuan Li,Chong Wang,Chengwei Liu,Jiahui Wu,Sen Chen,Yaowen Zheng,Yang Liu*

Main category: cs.SE

TL;DR: The paper presents VFArch=e, a method for more accurately and efficiently finding vulnerable functions in open source software, regardless of whether patches are available. It shows strong results in both benchmark tests and real-world usage, outperforming previous methods and reducing false positives.


<details>
  <summary>Details</summary>
Motivation: Software Composition Analysis (SCA) is important for identifying vulnerabilities in dependencies of open-source software projects. However, existing vulnerability databases often lack crucial information about vulnerable functions (VF), making it challenging to perform reachability analysis that identifies exploitable risks via call graphs. Both the absence of patches and the fact that not all VFs exist in modified functions further complicate the automatic localization of VFs.

Method: The paper introduces VFArch=e, a dual-mode approach for automatically localizing vulnerable functions (VF) for disclosed vulnerabilities. This method is applicable both when patch links are available and when they are absent. The approach is evaluated on a constructed benchmark dataset and on real-world vulnerabilities.

Result: VFArch=e achieved significant improvements in locating vulnerable functions, with 1.3x and 1.9x Mean Reciprocal Rank over the best existing baselines for scenarios with and without patches, respectively. In real-world tests, VFArch=e successfully located VFs for 43 out of 50 recent vulnerabilities, while significantly reducing false positives (by 78-89%) compared to existing SCA tools.

Conclusion: VFArch=e effectively and automatically localizes vulnerable functions both with and without the availability of vulnerability patches. It outperforms existing approaches in accuracy and reduces the noise in identifying true vulnerabilities, proving applicable and beneficial in practical software security scenarios.

Abstract: Software Composition Analysis (SCA) has become pivotal in addressing
vulnerabilities inherent in software project dependencies. In particular,
reachability analysis is increasingly used in Open-Source Software (OSS)
projects to identify reachable vulnerabilities (e.g., CVEs) through call
graphs, enabling a focus on exploitable risks. Performing reachability analysis
typically requires the vulnerable function (VF) to track the call chains from
downstream applications. However, such crucial information is usually
unavailable in modern vulnerability databases like NVD. While directly
extracting VF from modified functions in vulnerability patches is intuitive,
patches are not always available. Moreover, our preliminary study shows that
over 26% of VF do not exist in the modified functions. Meanwhile, simply
ignoring patches to search vulnerable functions suffers from overwhelming
noises and lexical gaps between descriptions and source code. Given that almost
half of the vulnerabilities are equipped with patches, a holistic solution that
handles both scenarios with and without patches is required. To meet real-world
needs and automatically localize VF, we present VFArch\=e, a dual-mode approach
designed for disclosed vulnerabilities, applicable in scenarios with or without
available patch links. The experimental results of VFArch\=e on our constructed
benchmark dataset demonstrate significant efficacy regarding three metrics,
achieving 1.3x and 1.9x Mean Reciprocal Rank over the best baselines for
Patch-present and Patch-absent modes, respectively. Moreover, VFArch\=e has
proven its applicability in real-world scenarios by successfully locating VF
for 43 out of 50 latest vulnerabilities with reasonable efforts and
significantly reducing 78-89% false positives of SCA tools.

</details>


### [18] [Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks](https://arxiv.org/abs/2506.18191)
*Masudul Hasan Masud Bhuiyan,Gianluca De Stefano,Giancarlo Pellegrino,Cristian-Alexandru Staicu*

Main category: cs.SE

TL;DR: GRAPHIA uses graph neural networks to predict and recover missed call edges in JavaScript program call graphs, improving the accuracy and recall of static analysis tools. Evaluated on major libraries, it identified top candidate matches in over 42% of unresolved cases, representing the first GNN-based interprocedural link prediction for full JavaScript programs.


<details>
  <summary>Details</summary>
Motivation: Existing call graph construction algorithms for JavaScript are neither sound nor complete due to hard-to-analyze language features. Even advanced solutions miss valid call edges or introduce false ones. This hinders the accuracy and utility of static analysis, which is central to finding bugs and security issues.

Method: The paper presents GRAPHIA, a method that frames the problem of finding missed call edges as a link prediction task on full program graphs. It leverages graph neural networks (GNNs) and represents JavaScript programs using both syntactic and semantic edges. GRAPHIA learns from imperfect labels (from static analysis tools and dynamic tests) and ranks function definitions for each unresolved call site.

Result: In a large-scale evaluation on 50 popular JavaScript libraries with 163K call edges, GRAPHIA achieved ranking the correct target as the top candidate in over 42% of unresolved cases and within the top 5 in 72% of cases. It reduces manual effort and shows that learning-based methods improve recall in JavaScript call graph construction.

Conclusion: GRAPHIA, by applying GNN-based link prediction to multi-file program graphs, enhances interprocedural call graph construction and recall for JavaScript programs. This innovative learning-based approach can supplement existing static analysis tools, reducing missed call edges and the associated manual effort.

Abstract: Static analysis plays a key role in finding bugs, including security issues.
A critical step in static analysis is building accurate call graphs that model
function calls in a program. However, due to hard-to-analyze language features,
existing call graph construction algorithms for JavaScript are neither sound
nor complete. Prior work shows that even advanced solutions produce false edges
and miss valid ones. In this work, we assist these tools by identifying missed
call edges. Our main idea is to frame the problem as link prediction on full
program graphs, using a rich representation with multiple edge types. Our
approach, GRAPHIA, leverages recent advances in graph neural networks to model
non-local relationships between code elements. Concretely, we propose
representing JavaScript programs using a combination of syntactic- and
semantic-based edges. GRAPHIA can learn from imperfect labels, including static
call edges from existing tools and dynamic edges from tests, either from the
same or different projects. Because call graphs are sparse, standard machine
learning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by
ranking function definitions for each unresolved call site. We conduct a
large-scale evaluation on 50 popular JavaScript libraries with 163K call edges
(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M
structural and 386K semantic edges. It ranks the correct target as the top
candidate in over 42% of unresolved cases and within the top 5 in 72% of cases,
reducing the manual effort needed for analysis. Our results show that
learning-based methods can improve the recall of JavaScript call graph
construction. To our knowledge, this is the first work to apply GNN-based link
prediction to full multi-file program graphs for interprocedural analysis.

</details>


### [19] [Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study](https://arxiv.org/abs/2506.18219)
*Ulrike M. Graetsch,Rashina Hoda,Hourieh Khalazjadeh,Mojtaba Shahin,John Grundy*

Main category: cs.SE

TL;DR: This paper investigates how multidisciplinary teams managing data-intensive systems handle growing technical debt, using case study and grounded theory methods. It identifies specific types of debt, team management practices, and highlights the need for better tools and new patterns tailored to such teams.


<details>
  <summary>Details</summary>
Motivation: There is a rise in investment and development of data-intensive (DI) solutions, which manage large data volumes. However, such growth can also escalate technical debt (TD) if not managed properly. Despite the multidisciplinary nature of teams delivering DI solutions, little is known about how they handle technical debt.

Method: The study uses an exploratory observation case study, leveraging socio-technical grounded theory (STGT) to analyze data and develop concepts and categories describing technical debt (TD) and its management within a multidisciplinary DI team.

Result: The researchers identify the types of technical debt confronting the DI team—particularly in technical data components and pipeline debt. They detail the team's approaches to managing, assessing, and treating TD, including strategies for implementing debt treatments within sprint capacity limitations.

Conclusion: The study aligns its findings with current technical debt (TD) and technical debt management (TDM) taxonomies, discusses practical implications, and underscores the need for new implementation patterns and tool support for multidisciplinary data-intensive teams.

Abstract: Context: There is an increase in the investment and development of
data-intensive (DI) solutions, systems that manage large amounts of data.
Without careful management, this growing investment will also grow associated
technical debt (TD). Delivery of DI solutions requires a multidisciplinary
skill set, but there is limited knowledge about how multidisciplinary teams
develop DI systems and manage TD.
  Objective: This research contributes empirical, practice based insights about
multidisciplinary DI team TD management practices.
  Method: This research was conducted as an exploratory observation case study.
We used socio-technical grounded theory (STGT) for data analysis to develop
concepts and categories that articulate TD and TDs debt management practices.
  Results: We identify TD that the DI team deals with, in particular technical
data components debt and pipeline debt. We explain how the team manages the TD,
assesses TD, what TD treatments they consider and how they implement TD
treatments to fit sprint capacity constraints.
  Conclusion: We align our findings to existing TD and TDM taxonomies, discuss
their implications and highlight the need for new implementation patterns and
tool support for multidisciplinary DI teams.

</details>


### [20] [Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations](https://arxiv.org/abs/2506.18289)
*Saurabhsingh Rajput,Mootez Saad,Tushar Sharma*

Main category: cs.SE

TL;DR: Treating energy efficiency as a first-class goal across the whole AI pipeline achieves dramatic energy savings (up to 94.6%) with little loss in accuracy, and the paper presents actionable frameworks for sustainable AI design.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of AI leads to increased computational demand and significant energy consumption. Current optimization techniques for AI efficiency are used reactively and individually, often missing the comprehensive impact on energy and model performance.

Method: The paper proposes treating energy efficiency as a primary objective throughout all five phases of the AI pipeline: data, model, training, system, and inference. It encourages a holistic strategy by strategically combining efficiency measures across these phases, instead of applying them in isolation.

Result: Experimental results indicate that selectively combining optimizations throughout the pipeline can reduce energy consumption by up to 94.6%, while maintaining 95.95% of the original F1 score.

Conclusion: By proactively incorporating energy efficiency into fundamental design decisions and optimizing the entire AI pipeline, practitioners can achieve substantial sustainability benefits without significant performance loss. The paper offers practical frameworks to enable more sustainable and efficient AI development.

Abstract: AI's exponential growth intensifies computational demands and energy
challenges. While practitioners employ various optimization techniques, that we
refer as "knobs" in this paper, to tune model efficiency, these are typically
afterthoughts and reactive ad-hoc changes applied in isolation without
understanding their combinatorial effects on energy efficiency. This paper
emphasizes on treating energy efficiency as the first-class citizen and as a
fundamental design consideration for a compute-intensive pipeline. We show that
strategic selection across five AI pipeline phases (data, model, training,
system, inference) creates cascading efficiency. Experimental validation shows
orthogonal combinations reduce energy consumption by up to $94.6$% while
preserving $95.95$% of the original F1 score of non-optimized pipelines. This
curated approach provides actionable frameworks for informed sustainable AI
that balance efficiency, performance, and environmental responsibility.

</details>


### [21] [Use Property-Based Testing to Bridge LLM Code Generation and Validation](https://arxiv.org/abs/2506.18315)
*Lehan He,Zeren Chen,Zhe Zhang,Jing Shao,Xiang Gao,Lu Sheng*

Main category: cs.SE

TL;DR: A novel LLM-driven code synthesis approach using Property-Based Testing achieves significantly better correctness than conventional TDD, making LLM output more reliable for complex programming tasks.


<details>
  <summary>Details</summary>
Motivation: Ensuring that code generated by Large Language Models (LLMs) is functionally correct is a major challenge, especially for complex programming tasks. Traditional Test-Driven Development (TDD) methods either suffer from a lack of quality test cases or from pitfalls in automated test generation. This motivates the search for more reliable means to validate and refine LLM-generated code.

Method: The paper introduces the Property-Generated Solver framework, which uses Property-Based Testing (PBT) instead of standard test cases. This framework involves two LLM-based agents: a Generator, which generates and iteratively refines code, and a Tester, which controls the PBT process and provides detailed feedback based on property violations. The Generator uses this feedback to improve the code iteratively.

Result: Experiments on various code generation benchmarks show that the Property-Generated Solver significantly improves functional correctness of generated code, achieving pass@1 improvements between 23.1% and 37.3% compared to traditional TDD approaches.

Conclusion: Property-Generated Solver, which leverages Property-Based Testing in an iterative refinement loop using LLM agents, is an effective mechanism for producing more correct and generalizable code from LLMs. It outperforms established TDD-based code synthesis methods by a substantial margin in benchmark tests.

Abstract: Large Language Models (LLMs) excel at code generation, but ensuring their
outputs to be functionally correct, especially in complex programming tasks, is
a persistent challenge. While traditional Test-Driven Development (TDD) offers
a path for code refinement, its efficacy with LLMs is often undermined by the
scarcity of high-quality test cases or the pitfalls of automated test
generation, including biased tests or inaccurate output predictions that can
misdirect the correction process. This paper introduces Property-Generated
Solver, a novel framework that leverages Property-Based Testing (PBT) to
validate high-level program properties or invariants, instead of relying on
specific input-output examples. These properties are often simpler to define
and verify than directly predicting exhaustive test oracles, breaking the
"cycle of self-deception" where tests might share flaws with the code they are
meant to validate. Property-Generated Solver employs two collaborative
LLM-based agents: a Generator dedicated to code generation and iterative
refinement, and a Tester that manages the PBT life-cycle and formulate
semantically rich feedback from property violations. The resulting
comprehensive and actionable feedback then guides the Generator in its
refinement efforts. By establishing PBT as the core validation engine within
this iterative, closed-loop paradigm, Property-Generated Solver provides a
robust mechanism for steering LLMs towards more correct and generalizable code.
Extensive experimental results on multiple code generation benchmarks
demonstrate that Property-Generated Solver achieves substantial pass@1
improvements, ranging from 23.1% to 37.3% relative gains over established TDD
methods.

</details>


### [22] [Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow](https://arxiv.org/abs/2506.18329)
*Elijah Zolduoarrati,Sherlock A. Licorish,Nigel Stanger*

Main category: cs.SE

TL;DR: Benchmarked 21 algorithms for Stack Overflow predictive tasks. Found best-performing algorithms and preprocessing methods for each task, offering valuable insights for future model selection and optimization.


<details>
  <summary>Details</summary>
Motivation: Previous studies using Stack Overflow data for predictive modeling relied on few models (3-5) or arbitrary selection, potentially overlooking better-performing algorithms; this paper aims to give a comprehensive benchmark across more algorithms.

Method: Benchmarked 21 algorithms on three prediction tasks (questions answered, code quality violations, dropout) using data transforms (normalisation, standardisation, logarithmic, power), Bayesian hyperparameter optimization, genetic algorithms, and fine-tuned CodeBERT for dropout classification.

Result: Standardised Bagging ensemble models had the highest R2 (0.821) for answer prediction. Stochastic Gradient Descent regressor, Bagging, and Epsilon SVM outperformed others for code quality prediction. For dropout prediction, Extreme Gradient Boosting with log-transform had F1=0.825; CodeBERT F1=0.809, confirming similar performance from numerical and textual features.

Conclusion: Comprehensive benchmarking reveals optimal models for different Stack Overflow prediction tasks and optimal hyperparameters, helping future researchers and practitioners select better models and streamline hyperparameter tuning.

Abstract: Previous studies that used data from Stack Overflow to develop predictive
models often employed limited benchmarks of 3-5 models or adopted arbitrary
selection methods. Despite being insightful, their limited scope suggests the
need to benchmark more models to avoid overlooking untested algorithms. Our
study evaluates 21 algorithms across three tasks: predicting the number of
question a user is likely to answer, their code quality violations, and their
dropout status. We employed normalisation, standardisation, as well as
logarithmic and power transformations paired with Bayesian hyperparameter
optimisation and genetic algorithms. CodeBERT, a pre-trained language model for
both natural and programming languages, was fine-tuned to classify user dropout
given their posts (questions and answers) and code snippets. We found Bagging
ensemble models combined with standardisation achieved the highest R2 value
(0.821) in predicting user answers. The Stochastic Gradient Descent regressor,
followed by Bagging and Epsilon Support Vector Machine models, consistently
demonstrated superior performance to other benchmarked algorithms in predicting
user code quality across multiple quality dimensions and languages. Extreme
Gradient Boosting paired with log-transformation exhibited the highest F1-score
(0.825) in predicting user dropout. CodeBERT was able to classify user dropout
with a final F1-score of 0.809, validating the performance of Extreme Gradient
Boosting that was solely based on numerical data. Overall, our benchmarking of
21 algorithms provides multiple insights. Researchers can leverage findings
regarding the most suitable models for specific target variables, and
practitioners can utilise the identified optimal hyperparameters to reduce the
initial search space during their own hyperparameter tuning processes.

</details>


### [23] [Recipe for Discovery: A Framework for Systematic Open Source Project Identification](https://arxiv.org/abs/2506.18359)
*Juanita Gomez,Emily Lovell,Stephanie Lieggi,Alvaro A. Cardenas,James Davis*

Main category: cs.SE

TL;DR: The paper introduces a scalable framework utilizing GitHub data and machine learning to discover and classify open source projects affiliated with the University of California. It finds over 52,000 relevant repositories and achieves high affiliation prediction accuracy, addressing the recognition gap for academic software work.


<details>
  <summary>Details</summary>
Motivation: Open source software projects in universities and labs have major impact but are hard to recognize or track due to decentralization and poor institutional awareness. There is a lack of methods to systematically discover and analyze these contributions.

Method: The paper presents a systematic framework to identify and analyze open source projects affiliated with institutions, specifically taking the University of California as a case study. The approach uses GitHub's REST API to discover repositories and extract metadata. Several classification strategies are tested, including traditional machine learning models and large language models (LLMs), to determine project affiliation and generate insights.

Result: The proposed framework successfully discovers over 52,000 repositories and predicts institutional affiliation with high accuracy, demonstrating scalability and effectiveness.

Conclusion: Systematic discovery and classification of institution-affiliated open source projects is feasible and accurate at scale, offering a replicable approach for increasing visibility and recognition of academic software contributions.

Abstract: Open source software development, particularly within institutions such as
universities and research laboratories, is often decentralized and difficult to
track. Despite producing highly impactful tools in science, these efforts often
go unrecognized due to a lack of visibility and institutional awareness. This
paper addresses the challenge of discovering, classifying, and analyzing open
source software projects developed across distributed institutional systems. We
present a framework for systematically identifying institutional affiliated
repositories, using the University of California (UC) system as a case study.
  Using GitHub's REST API, we build a pipeline to discover relevant
repositories and extract meaningful metadata. We then propose and evaluate
multiple classification strategies, including both traditional machine learning
models and large language models (LLMs), to distinguish affiliated projects
from unrelated repositories and generate accurate insights into the academic
open source landscape. Our results show that the framework is effective at
scale, discovering over 52,000 repositories and predicting institutional
affiliation with high accuracy.

</details>


### [24] [Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval](https://arxiv.org/abs/2506.18394)
*Xiao Cheng,Zhihao Guo,Huan Huo,Yulei Sui*

Main category: cs.SE

TL;DR: Memory errors in C are hard to fix and existing automated repair tools have drawbacks. This paper introduces LTFix, which uses a typestate automaton to provide summary information for LLMs, helping them fix complex, repository-wide memory errors more effectively despite limited context windows.


<details>
  <summary>Details</summary>
Motivation: C programming is prone to memory-related errors due to complex manual memory management, leading to software vulnerabilities and requiring deep expertise for effective repairs. Traditional automated program repair (APR) methods rely on manual, expert-defined approaches, while deep learning-based solutions face issues with data requirements and interpretability. There is a need for more effective, scalable, and interpretable automated solutions.

Method: The paper proposes LTFix, a novel approach leveraging Large Language Models (LLMs) for automated repair of memory errors in C programs—especially for errors spanning multiple functions and files. The core method addresses LLMs’ limitations in understanding interprocedural memory management and context-window size by employing a finite typestate automaton. This automaton tracks error propagation paths and context traces, capturing key spatial and temporal aspects of error behavior, and generates concise, semantically rich context for the LLM.

Result: LTFix provides a typestate-guided context retrieval mechanism that assists LLMs in effective diagnosis and repair of complex memory errors. By delivering crucial and condensed semantic information to the LLM, the approach overcomes token limitations and enables efficient repair across repository-level code bases.

Conclusion: LTFix demonstrates that combining typestate-based program analysis with LLMs can significantly improve automated memory error repair in C programs. This hybrid approach offers scalability, greater accuracy, and better applicability than traditional template-based or purely deep learning-driven methods.

Abstract: Memory-related errors in C programming continue to pose significant
challenges in software development, primarily due to the complexities of manual
memory management inherent in the language. These errors frequently serve as
vectors for severe vulnerabilities, while their repair requires extensive
knowledge of program logic and C's memory model. Automated Program Repair (APR)
has emerged as a critical research area to address these challenges.
Traditional APR approaches rely on expert-designed strategies and predefined
templates, which are labor-intensive and constrained by the effectiveness of
manual specifications. Deep learning techniques offer a promising alternative
by automatically extracting repair patterns, but they require substantial
training datasets and often lack interpretability.
  This paper introduces LTFix, a novel approach that harnesses the potential of
Large Language Models (LLMs) for automated memory error repair, especially for
complex repository-level errors that span multiple functions and files. We
address two fundamental challenges in LLM-based memory error repair: a limited
understanding of interprocedural memory management patterns and context window
limitations for repository-wide analysis. Our approach utilizes a finite
typestate automaton to guide the tracking of error-propagation paths and
context trace, capturing both spatial (memory states) and temporal (execution
history) dimensions of error behavior. This typestate-guided context retrieval
strategy provides the LLM with concise yet semantically rich information
relevant to erroneous memory management, effectively addressing the token
limitation of LLMs.

</details>


### [25] [Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis](https://arxiv.org/abs/2506.18398)
*Hao Wu,Haijun Wang,Shangwang Li,Yin Wu,Ming Fan,Wuxia Jin,Yitao Zhao,Ting Liu*

Main category: cs.SE

TL;DR: RPhunter is a novel Rug Pull scam detection system that fuses contract code and transaction behavior analysis using graph neural networks, yielding higher detection accuracy than previous approaches.


<details>
  <summary>Details</summary>
Motivation: Rug pull scams are a serious and persistent problem in cryptocurrency markets, leading to substantial financial losses for investors. Existing detection methods are insufficient because they only analyze code or transaction data separately, missing the complex interaction between the two in real attacks.

Method: The paper introduces RPhunter, a comprehensive detection technique that combines both smart contract code analysis and token transaction behavior analysis. RPhunter creates two graph models: a semantic risk code graph (SRCG) for code features and a token flow behavior graph (TFBG) for transaction features. It then uses graph neural networks and an attention fusion model to integrate these features for more effective Rug Pull detection.

Result: RPhunter was tested on a manually curated dataset of 645 Rug Pull incidents as well as real-world data. It achieved a precision of 95.3%, recall of 93.8%, and F1 score of 94.5%—outperforming existing methods. In real-world applications, RPhunter discovered 4801 Rug Pull tokens with a precision of 91%.

Conclusion: RPhunter significantly improves Rug Pull scam detection by integrating code and transaction analysis through advanced graph-based techniques and neural networks, outperforming state-of-the-art methods in both experimental and real-world settings.

Abstract: Rug pull scams have emerged as a persistent threat to cryptocurrency, causing
significant financial losses. A typical scenario involves scammers deploying
honeypot contracts to attract investments, restricting token sales, and
draining the funds, which leaves investors with worthless tokens. Current
methods either rely on predefined patterns to detect code risks or utilize
statistical transaction data to train detection models. However, real-world Rug
Pull schemes often involve a complex interplay between malicious code and
suspicious transaction behaviors. These methods, which solely focus on one
aspect, fall short in detecting such schemes effectively.
  In this paper, we propose RPhunter, a novel technique that integrates code
and transaction for Rug Pull detection. First, RPhunter establishes declarative
rules and performs flow analysis to extract code risk information, further
constructing a semantic risk code graph (SRCG). Meanwhile, to leverage
transaction information, RPhunter formulates dynamic token transaction
activities as a token flow behavior graph (TFBG) in which nodes and edges are
characterized from network structure and market manipulation perspectives.
Finally, RPhunter employs graph neural networks to extract complementary
features from SRCG and TFBG, integrating them through an attention fusion model
to enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull
incidents from code and transaction aspects and constructed a ground-truth
dataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%,
a recall of 93.8% and an F1 score of 94.5%, which highlights superior
performance compared to existing state-of-the-art methods. Furthermore, when
applied to the real-world scenarios, RPhunter has identified 4801 Rug Pull
tokens, achieving a precision of 91%.

</details>


### [26] [The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs](https://arxiv.org/abs/2506.18403)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: AI code debugging rapidly becomes ineffective after a few tries. The proposed DDI framework allows developers to quantify and strategically counteract this decay, markedly improving debugging outcomes.


<details>
  <summary>Details</summary>
Motivation: Iterative debugging is vital for practical code generation with AI, but its effectiveness sharply declines with repeated attempts. A framework is needed to quantify this decay and optimize intervention strategies.

Method: The authors propose the Debugging Decay Index (DDI), a mathematical framework that measures and predicts the effectiveness of AI debugging over multiple attempts. They also implement a 'strategic fresh start' approach to empirical debugging sequences.

Result: Most AI models lose 60-80% of their debugging capability within 2-3 attempts. Using DDI, timing fresh interventions optimizes debugging effectiveness and provides a strategy to rescue degraded performance.

Conclusion: The study concludes that AI debugging capability decays rapidly during iterative attempts, and introduces the Debugging Decay Index (DDI) to quantify and predict this phenomenon. Well-timed fresh interventions significantly improve debugging outcomes.

Abstract: The effectiveness of AI debugging follows a predictable exponential decay
pattern; most models lose 60-80% of their debugging capability within just 2-3
attempts, despite iterative debugging being a critical capability for practical
code generation systems. We introduce the Debugging Decay Index (DDI), a
mathematical framework that quantifies when debugging becomes ineffective and
predicts intervention points. Our strategic fresh start approach shifts from
exploitation to exploration at strategic points in the debugging process,
demonstrating that well-timed interventions can rescue the effectiveness of
debugging. DDI reveals a fundamental limitation in current AI debugging and
provides the first quantitative framework for optimising iterative code
generation strategies.

</details>


### [27] [ModeliHub: A Web-based, Federated Analytics Platform for Modelica-centric, Model-based Systems Engineering](https://arxiv.org/abs/2506.18790)
*Mohamad Omar Nachawati*

Main category: cs.SE

TL;DR: ModeliHub is an innovative, web-based platform for model-based systems engineering that leverages a federated hub-and-spoke Modelica architecture to unify and simulate heterogeneous engineering models, enabling seamless, interactive deployment of digital twins for collaborative systems development.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to support model-based systems engineering (MBSE) by addressing the challenge of integrating and analyzing heterogeneous engineering artifacts using Modelica, and to provide a real-time and interactive simulation environment for evolving digital twins throughout the systems engineering life cycle.

Method: The paper presents ModeliHub, a web-based federated analytics platform with a Modelica-centric, hub-and-spoke federation architecture. The platform features an extensible Modelica compiler frontend developed in TypeScript, capable of running on browsers, desktops, and servers, and supports seamless integration of various engineering artifacts.

Result: ModeliHub enables systems engineers to access a unified Modelica-based system model from distributed and heterogeneous repositories. Its Virtual Twin engine allows real-time, interactive deployment of Modelica simulation models, effectively facilitating the use of digital twins for virtual prototyping at different life cycle stages.

Conclusion: ModeliHub successfully balances rigor and agility in MBSE by providing a unified, scalable, and interactive simulation platform that integrates diverse engineering domains, streamlining the deployment of digital twins and the collaborative development of complex systems.

Abstract: This paper introduces ModeliHub, a Web-based, federated analytics platform
designed specifically for model-based systems engineering with Modelica.
ModeliHub's key innovation lies in its Modelica-centric, hub-and-spoke
federation architecture that provides systems engineers with a Modelica-based,
unified system model of repositories containing heterogeneous engineering
artifacts. From this unified system model, ModeliHub's Virtual Twin engine
provides a real-time, interactive simulation environment for deploying Modelica
simulation models that represent digital twins of the virtual prototype of the
system under development at a particular iteration of the iterative systems
engineering life cycle. The implementation of ModeliHub is centered around its
extensible, Modelica compiler frontend developed in Isomorphic TypeScript that
can run seamlessly across browser, desktop and server environments. This
architecture aims to strike a balance between rigor and agility, enabling
seamless integration and analysis across various engineering domains.

</details>


### [28] [Context-Aware CodeLLM Eviction for AI-assisted Coding](https://arxiv.org/abs/2506.18796)
*Kishanthan Thangarajah,Boyuan Chen,Shi Chang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: CACE is a new model eviction strategy for self-hosted AI coding tools that uses multiple context-aware factors to improve performance and resource use, outperforming traditional methods like LRU and making it easier to deploy efficient coding assistants at scale.


<details>
  <summary>Details</summary>
Motivation: The increasing adoption of AI-assisted coding tools using Code Large Language Models (CodeLLMs) brings challenges when self-hosting these models due to limited accelerator memory and the need for efficient model management. Enterprises require solutions that address privacy, latency, and model customization, necessitating smarter resource management.

Method: The paper introduces CACE, a novel context-aware model eviction strategy tailored for self-hosted CodeLLM environments. Unlike conventional eviction policies such as Least Recently Used (LRU), CACE incorporates multiple context-aware parameters—model load time, task-specific latency requirements, expected output length, and patterns of recent or anticipated usage via a sliding window—to optimize which models are kept in memory and which are evicted.

Result: Empirical evaluations using hybrid workloads (latency-sensitive code completion and throughput-oriented code reasoning tasks) demonstrate that CACE achieves improved Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while reducing the frequency of model evictions compared to state-of-the-art strategies. Ablation studies confirm the benefits and necessity of multi-factor considerations in model eviction.

Conclusion: CACE provides a practical, effective context-aware model eviction strategy that enables scalable and low-latency deployment of AI coding assistants under constrained resources, offering a significant contribution for AI-assisted software engineering in real-world production settings.

Abstract: AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are
increasingly integrated into modern software development workflows. To address
concerns around privacy, latency, and model customization, many enterprises opt
to self-host these models. However, the diversity and growing number of
CodeLLMs, coupled with limited accelerator memory, introduce practical
challenges in model management and serving efficiency. This paper presents
CACE, a novel context-aware model eviction strategy designed specifically to
optimize self-hosted CodeLLM serving under resource constraints. Unlike
traditional eviction strategies based solely on recency (e.g., Least Recently
Used), CACE leverages multiple context-aware factors, including model load
time, task-specific latency sensitivity, expected output length, and recent
usage and future demand tracked through a sliding window. We evaluate CACE
using realistic workloads that include both latency-sensitive code completion
and throughput-intensive code reasoning tasks. Our experiments show that CACE
reduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while
significantly lowering the number of model evictions compared to
state-of-the-art systems. Ablation studies further demonstrate the importance
of multi-factor eviction in balancing responsiveness and resource efficiency.
This work contributes practical strategies for deploying scalable, low-latency
AI coding assistants in real-world software engineering environments.

</details>


### [29] [Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories](https://arxiv.org/abs/2506.18824)
*Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: The paper empirically analyzes three LLM-based software engineering agents, revealing key behavioral patterns and anti-patterns that correlate with success or failure. Their insights inform improved agent design, and they release a dataset and framework for community use.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents are widely used for automating complex software engineering tasks, but their internal reasoning and decision-making processes are poorly understood. This gap hinders the improvement and reliability of such systems.

Method: The authors conducted a large-scale empirical study by unifying the interaction logs of three advanced LLM-based agents (	extsc{RepairAgent}, 	extsc{AutoCodeRover}, and 	extsc{OpenHands}) into a common format. They analyzed 120 problem-solving trajectories and 2822 LLM-agent interactions, combining quantitative metrics (iteration count, token usage, action sequences) and qualitative assessments (reasoning and feedback coherence).

Result: The study identified recurring behavioral patterns and anti-patterns in the agents' operation—such as how thoughts, actions, and results are interconnected, and how iteration and token usage varies between successful and failed trajectories. They also highlight behavioral motifs and anti-patterns that help differentiate successes from failures.

Conclusion: The findings provide actionable insights to improve LLM-based agent design, specifically in prompting strategies, diagnosing failures, and detecting failure-prone behavior. The authors also released their dataset and annotation framework to promote further research in this area.

Abstract: Large Language Model (LLM)-based agents are increasingly employed to automate
complex software engineering tasks such as program repair and issue resolution.
These agents operate by autonomously generating natural language thoughts,
invoking external tools, and iteratively refining their solutions. Despite
their widespread adoption, the internal decision-making processes of these
agents remain largely unexplored, limiting our understanding of their
operational dynamics and failure modes. In this paper, we present a large-scale
empirical study of the thought-action-result trajectories of three
state-of-the-art LLM-based agents: \textsc{RepairAgent},
\textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs
into a common format, capturing 120 trajectories and 2822 LLM interactions
focused on program repair and issue resolution. Our study combines quantitative
analyses of structural properties, action patterns, and token usage with
qualitative assessments of reasoning coherence and feedback integration. We
identify key trajectory characteristics such as iteration counts and token
consumption, recurring action sequences, and the semantic coherence linking
thoughts, actions, and their results. Our findings reveal behavioral motifs and
anti-patterns that distinguish successful from failed executions, providing
actionable insights for improving agent design, including prompting strategies,
failure diagnosis, and anti-pattern detection. We release our dataset and
annotation framework to support further research on transparent and robust
autonomous software engineering agents.

</details>
