{"id": "2510.21031", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.21031", "abs": "https://arxiv.org/abs/2510.21031", "authors": ["Qinghua Lu", "Dehai Zhao", "Yue Liu", "Hao Zhang", "Liming Zhu", "Xiwei Xu", "Angela Shi", "Tristan Tan", "Rick Kazman"], "title": "AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents", "comment": null, "summary": "The emergence of foundation models (FMs) has enabled the development of\nhighly capable and autonomous agents, unlocking new application opportunities\nacross a wide range of domains. Evaluating the architecture of agents is\nparticularly important as the architectural decisions significantly impact the\nquality attributes of agents given their unique characteristics, including\ncompound architecture, autonomous and non-deterministic behaviour, and\ncontinuous evolution. However, these traditional methods fall short in\naddressing the evaluation needs of agent architecture due to the unique\ncharacteristics of these agents. Therefore, in this paper, we present\nAgentArcEval, a novel agent architecture evaluation method designed specially\nto address the complexities of FM-based agent architecture and its evaluation.\nMoreover, we present a catalogue of agent-specific general scenarios, which\nserves as a guide for generating concrete scenarios to design and evaluate the\nagent architecture. We demonstrate the usefulness of AgentArcEval and the\ncatalogue through a case study on the architecture evaluation of a real-world\ntax copilot, named Luna.", "AI": {"tldr": "Existing evaluation methods for agent architectures fall short for advanced agents leveraging foundation models. The paper proposes AgentArcEval\u2014a specialized evaluation method\u2014plus a scenario catalogue, and validates them with a real-world tax copilot agent.", "motivation": "Traditional architecture evaluation methods do not sufficiently address the complexities and unique requirements of agents built on foundation models, which exhibit compound architecture, autonomy, non-determinism, and continuous evolution.", "method": "The paper presents AgentArcEval, a new method for evaluating agent architectures, specifically tailored for foundation model agents, and introduces a scenario catalogue for guiding evaluation. The utility is demonstrated via a case study.", "result": "AgentArcEval, alongside the scenario catalogue, provides a more suitable and effective framework for agent architecture evaluation. Its real-world applicability is affirmed through the Luna tax copilot case study.", "conclusion": "AgentArcEval and the catalogue of general scenarios effectively address the unique architectural evaluation needs for foundation-model-based agents, as demonstrated in the Luna tax copilot case study."}}
{"id": "2510.21094", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21094", "abs": "https://arxiv.org/abs/2510.21094", "authors": ["Yao Lu", "Wanwei Liu", "Tanghaoran Zhang", "Kang Yang", "Yang Zhang", "Wenyu Xu", "Longfei Sun", "Xinjun Mao", "Shuzheng Gao", "Michael R. Lyu"], "title": "BDiff: Block-aware and Accurate Text-based Code Differencing", "comment": null, "summary": "Code differencing is a fundamental technique in software engineering practice\nand research. While researchers have proposed text-based differencing\ntechniques capable of identifying line changes over the past decade, existing\nmethods exhibit a notable limitation in identifying edit actions (EAs) that\noperate on text blocks spanning multiple lines. Such EAs are common in\ndevelopers' practice, such as moving a code block for conditional branching or\nduplicating a method definition block for overloading. Existing tools represent\nsuch block-level operations as discrete sequences of line-level EAs, compelling\ndevelopers to manually correlate them and thereby substantially impeding the\nefficiency of change comprehension. To address this issue, we propose BDiff, a\ntext-based differencing algorithm capable of identifying two types of\nblock-level EAs and five types of line-level EAs. Building on traditional\ndifferencing algorithms, we first construct a candidate set containing all\npossible line mappings and block mappings. Leveraging the Kuhn-Munkres\nalgorithm, we then compute the optimal mapping set that can minimize the size\nof the edit script (ES) while closely aligning with the original developer's\nintent. To validate the effectiveness of BDiff, we selected five\nstate-of-the-art tools, including large language models (LLMs), as baselines\nand adopted a combined qualitative and quantitative approach to evaluate their\nperformance in terms of ES size, result quality, and running time. Experimental\nresults show that BDiff produces higher-quality differencing results than\nbaseline tools while maintaining competitive runtime performance. Our\nexperiments also show the unreliability of LLMs in code differencing tasks\nregarding result quality and their infeasibility in terms of runtime\nefficiency. We have implemented a web-based visual differencing tool.", "AI": {"tldr": "BDiff is a new code differencing algorithm that accurately recognizes both block-level and line-level code edits to help developers understand changes efficiently. It uses the Kuhn-Munkres algorithm for optimal edit script generation and outperforms current tools, including LLMs, in quality and speed. A web-based tool is available for visualization.", "motivation": "Existing code differencing techniques mainly focus on line-level edits, which makes them inadequate for identifying block-level edit actions often performed by developers (e.g., moving or duplicating code blocks). This limitation hinders efficient code change comprehension and forces manual correlation of line-level edits.", "method": "The authors propose BDiff, a text-based differencing algorithm that can identify both block-level and line-level edit actions. BDiff constructs a candidate set of possible line and block mappings, and uses the Kuhn-Munkres algorithm to determine the optimal mapping set that minimizes the edit script size and aligns with developer intent. The algorithm's performance is evaluated against five state-of-the-art tools (including LLMs) using qualitative and quantitative approaches.", "result": "BDiff consistently delivers higher-quality differencing results compared to baseline tools, while maintaining competitive runtime performance. Experiments reveal that LLMs are unreliable for code differencing tasks due to poor result quality and impractical runtime efficiency.", "conclusion": "BDiff surpasses existing code differencing tools in identifying block-level and line-level edit actions, effectively improving change comprehension for developers. The provided web-based tool enhances usability."}}
{"id": "2510.21106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21106", "abs": "https://arxiv.org/abs/2510.21106", "authors": ["Zhen Yang", "Hongyi Lin", "Xiao Yu", "Jacky Wai Keung", "Shuo Liu", "Pak Yuen Patrick Chan", "Yicheng Sun", "Fengji Zhang"], "title": "R2ComSync: Improving Code-Comment Synchronization with In-Context Learning and Reranking", "comment": null, "summary": "Code-Comment Synchronization (CCS) aims to synchronize the comments with code\nchanges in an automated fashion, thereby significantly reducing the workload of\ndevelopers during software maintenance and evolution. While previous studies\nhave proposed various solutions that have shown success, they often exhibit\nlimitations, such as a lack of generalization ability or the need for extensive\ntask-specific learning resources. This motivates us to investigate the\npotential of Large Language Models (LLMs) in this area. However, a pilot\nanalysis proves that LLMs fall short of State-Of-The-Art (SOTA) CCS approaches\nbecause (1) they lack instructive demonstrations for In-Context Learning (ICL)\nand (2) many correct-prone candidates are not prioritized.To tackle the above\nchallenges, we propose R2ComSync, an ICL-based code-Comment Synchronization\napproach enhanced with Retrieval and Re-ranking. Specifically, R2ComSync\ncarries corresponding two novelties: (1) Ensemble hybrid retrieval. It equally\nconsiders the similarity in both code-comment semantics and change patterns\nwhen retrieval, thereby creating ICL prompts with effective examples. (2)\nMulti-turn re-ranking strategy. We derived three significant rules through\nlarge-scale CCS sample analysis. Given the inference results of LLMs, it\nprogressively exploits three re-ranking rules to prioritize relatively\ncorrect-prone candidates. We evaluate R2ComSync using five recent LLMs on three\nCCS datasets covering both Java and Python programming languages, and make\ncomparisons with five SOTA approaches. Extensive experiments demonstrate the\nsuperior performance of R2ComSync against other approaches. Moreover, both\nquantitative and qualitative analyses provide compelling evidence that the\ncomments synchronized by our proposal exhibit significantly higher quality.}", "AI": {"tldr": "R2ComSync leverages advanced prompt retrieval and re-ranking techniques to significantly improve code-comment synchronization, outperforming state-of-the-art methods and enhancing the quality of synchronized comments.", "motivation": "Previous approaches to Code-Comment Synchronization (CCS) have limitations like insufficient generalization and high dependence on task-specific resources. There is a need to leverage Large Language Models (LLMs) for this task, but pilot analysis shows LLMs currently underperform SOTA methods due to lack of instructive demonstrations and ineffective candidate prioritization.", "method": "The paper proposes R2ComSync, an In-Context Learning (ICL)-based CCS approach, augmented with hybrid retrieval and multi-turn re-ranking. Hybrid retrieval balances code-comment semantic and change pattern similarities to construct effective ICL prompts, while multi-turn re-ranking applies three derived rules to prioritize likely correct outputs from the LLMs.", "result": "R2ComSync was evaluated with five recent LLMs on three CCS datasets (Java and Python). It was compared with five SOTA CCS approaches. The experiments show that R2ComSync substantially outperforms the baselines in both quantitative metrics and comment quality.", "conclusion": "R2ComSync achieves superior performance in code-comment synchronization compared to SOTA approaches, delivering higher-quality synchronized comments by exploiting hybrid retrieval for prompt construction and a novel multi-turn re-ranking strategy. The method boosts both the effectiveness and output quality of LLM-based CCS systems."}}
{"id": "2510.21405", "categories": ["cs.SE", "cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.21405", "abs": "https://arxiv.org/abs/2510.21405", "authors": ["Aidan Dakhama", "W. B. Langdon", "Hector D. Menendez", "Karine Even-Mendoza"], "title": "GreenMalloc: Allocator Optimisation for Industrial Workloads", "comment": null, "summary": "We present GreenMalloc, a multi objective search-based framework for\nautomatically configuring memory allocators. Our approach uses NSGA II and\nrand_malloc as a lightweight proxy benchmarking tool. We efficiently explore\nallocator parameters from execution traces and transfer the best configurations\nto gem5, a large system simulator, in a case study on two allocators: the GNU\nC/CPP compiler's glibc malloc and Google's TCMalloc. Across diverse workloads,\nour empirical results show up to 4.1 percantage reduction in average heap usage\nwithout loss of runtime efficiency; indeed, we get a 0.25 percantage reduction.", "AI": {"tldr": "GreenMalloc is an automated framework for tuning memory allocator parameters using a genetic algorithm and lightweight benchmarking. Applied to glibc malloc and TCMalloc, it reduces heap usage by up to 4.1% without runtime penalties, suggesting search-based configuration is effective for allocator optimization.", "motivation": "Memory allocators have tunable parameters that significantly impact their performance, but manual configuration is time-consuming and may not yield optimal results. The authors are motivated to automate and improve the configuration process to balance multiple objectives such as heap usage and runtime efficiency.", "method": "The paper introduces GreenMalloc, a search-based framework that automatically configures memory allocators. It leverages NSGA II, a multi-objective genetic algorithm, along with a lightweight proxy benchmarking tool (rand_malloc). Execution trace analysis is used to efficiently explore allocator parameter space. The best configurations are then transferred and validated in gem5 system simulator for two different allocators: glibc malloc and TCMalloc.", "result": "GreenMalloc demonstrated up to 4.1% reduction in average heap usage without sacrificing runtime efficiency and observed an overall average reduction of 0.25%. The results were consistent across various workloads for both glibc malloc and TCMalloc.", "conclusion": "Automatic, multi-objective parameter tuning using GreenMalloc can effectively optimize memory allocator behavior, achieving meaningful reductions in heap usage while maintaining runtime performance. This highlights the potential of search-based configuration for widely-used allocator implementations."}}
{"id": "2510.21413", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21413", "abs": "https://arxiv.org/abs/2510.21413", "authors": ["Seyedmoein Mohsenimofidi", "Matthias Galster", "Christoph Treude", "Sebastian Baltes"], "title": "Context Engineering for AI Agents in Open-Source Software", "comment": "6 pages, 1 figure, 2 tables", "summary": "GenAI-based coding assistants have disrupted software development. Their next\ngeneration is agent-based, operating with more autonomy and potentially without\nhuman oversight. One challenge is to provide AI agents with sufficient context\nabout the software projects they operate in. Like humans, AI agents require\ncontextual information to develop solutions that are in line with the target\narchitecture, interface specifications, coding guidelines, standard workflows,\nand other project-specific policies. Popular AI agents for software development\n(e.g., Claude Code) advocate for maintaining tool-specific version-controlled\nMarkdown files that cover aspects such as the project structure, building and\ntesting, or code style. The content of these files is automatically added to\neach prompt. AGENTS.md has emerged as a potential standard that consolidates\ntool-specific formats. However, little is known about whether and how\ndevelopers adopt this format. Therefore, in this paper, we present the results\nof a preliminary study investigating the adoption of AI configuration files in\n466 open-source software projects, what information developers provide in these\nfiles, how they present that information, and how they evolve over time. Our\nfindings indicate that there is no established structure yet, and that there is\na lot of variation in terms of how context is provided (descriptive,\nprescriptive, prohibitive, explanatory, conditional). We see great potential in\nstudying which modifications in structure or presentation can positively affect\nthe quality of the generated content. Finally, our analysis of commits that\nhave modified AGENTS.md files provides first insights into how projects\ncontinuously extend and maintain these files. We conclude the paper by\noutlining how the adoption of AI configuration files in provides a unique\nopportunity to study real-world prompt and context engineering.", "AI": {"tldr": "This paper investigates how 466 open-source projects adopt and structure AI configuration files (like AGENTS.md) for GenAI coding assistants, finding a lack of standardization and significant variation, with recommendations for future research on optimizing prompt and context engineering.", "motivation": "The motivation behind this paper is the rise of agent-based GenAI coding assistants, which require rich, project-specific context to be effective. There is no clear understanding of how developers create and maintain contextual configuration files (e.g., AGENTS.md) necessary for such assistants.", "method": "The authors conducted a preliminary study analyzing AGENTS.md and similar configuration files from 466 open-source software projects. They examined what information these files contain, how it is presented, and how these files evolve over time through commit analysis.", "result": "The study found significant variation in the structure and presentation of configuration files. There is no standard structure, and the manner in which context is provided includes descriptive, prescriptive, prohibitive, explanatory, and conditional forms. The data also showed how these files are continuously extended and maintained.", "conclusion": "AI configuration files adoption is still in its infancy, with no established practices. Their evolution and diverse formats offer a unique opportunity to study prompt and context engineering in real-world settings. The field can benefit from research on how to structure these files to improve the output of AI coding agents."}}
{"id": "2510.21443", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.21443", "abs": "https://arxiv.org/abs/2510.21443", "authors": ["Mohammad Amin Zadenoori", "Vincenzo De Martino", "Jacek Dabrowski", "Xavier Franch", "Alessio Ferrari"], "title": "Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification", "comment": null, "summary": "[Context and motivation] Large language models (LLMs) show notable results in\nnatural language processing (NLP) tasks for requirements engineering (RE).\nHowever, their use is compromised by high computational cost, data sharing\nrisks, and dependence on external services. In contrast, small language models\n(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]\nIt remains unclear how well SLMs perform compared to LLMs in RE tasks in terms\nof accuracy. [Results] Our preliminary study compares eight models, including\nthree LLMs and five SLMs, on requirements classification tasks using the\nPROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although\nLLMs achieve an average F1 score of 2% higher than SLMs, this difference is not\nstatistically significant. SLMs almost reach LLMs performance across all\ndatasets and even outperform them in recall on the PROMISE Reclass dataset,\ndespite being up to 300 times smaller. We also found that dataset\ncharacteristics play a more significant role in performance than model size.\n[Contribution] Our study contributes with evidence that SLMs are a valid\nalternative to LLMs for requirements classification, offering advantages in\nprivacy, cost, and local deployability.", "AI": {"tldr": "Small language models perform almost as well as large language models on requirements classification tasks, offering significant advantages in privacy and efficiency, with minimal loss in accuracy. Dataset properties affect model performance more than model size.", "motivation": "While large language models (LLMs) are effective in NLP tasks related to requirements engineering (RE), their practical adoption is limited by high computational resource requirements, privacy concerns, and a reliance on external services. Small language models (SLMs), which can be run locally with fewer resources, could address these issues but it is unclear how their performance compares to LLMs on RE tasks.", "method": "The authors conducted a comparative experiment using eight language models (three LLMs and five SLMs) for requirements classification tasks. They evaluated these models on three datasets (PROMISE, PROMISE Reclass, and SecReq), focusing on accuracy metrics such as F1 score and recall.", "result": "LLMs achieved only a marginally better average F1 score (about 2% higher) than SLMs, but this difference was not statistically significant. SLMs performed almost as well as LLMs on all datasets and even outperformed them in recall on the PROMISE Reclass dataset. Model size was less important for performance than the properties of the datasets themselves.", "conclusion": "SLMs represent a strong alternative to LLMs in requirements classification tasks in RE, providing comparable accuracy with additional benefits in terms of privacy, cost, and local deployment. Dataset characteristics should also be considered a key factor in task performance."}}
{"id": "2510.21451", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21451", "abs": "https://arxiv.org/abs/2510.21451", "authors": ["Yinglong Zou", "Juan Zhai", "Chunrong Fang", "An Guo", "Jiawei Liu", "Zhenyu Chen"], "title": "Scalpel: Automotive Deep Learning Framework Testing via Assembling Model Components", "comment": null, "summary": "Deep learning (DL) plays a key role in autonomous driving systems. DL models\nsupport perception modules, equipped with tasks such as object detection and\nsensor fusion. These DL models enable vehicles to process multi-sensor inputs\nto understand complex surroundings. Deploying DL models in autonomous driving\nsystems faces stringent challenges, including real-time processing, limited\ncomputational resources, and strict power constraints. To address these\nchallenges, automotive DL frameworks (e.g., PaddleInference) have emerged to\noptimize inference efficiency. However, these frameworks encounter unique\nquality issues due to their more complex deployment environments, such as\ncrashes stemming from limited scheduled memory and incorrect memory allocation.\nUnfortunately, existing DL framework testing methods fail to detect these\nquality issues due to the failure in deploying generated test input models, as\nthese models lack three essential capabilities: (1) multi-input/output tensor\nprocessing, (2) multi-modal data processing, and (3) multi-level data feature\nextraction. These capabilities necessitate specialized model components, which\nexisting testing methods neglect during model generation. To bridge this gap,\nwe propose Scalpel, an automotive DL frameworks testing method that generates\ntest input models at the model component level. Scalpel generates models by\nassembling model components (heads, necks, backbones) to support capabilities\nrequired by autonomous driving systems. Specifically, Scalpel maintains and\nupdates a repository of model components, generating test inputs by selecting,\nmutating, and assembling them. Successfully generated models are added back to\nenrich the repository. Newly generated models are then deployed within the\nautonomous driving system to test automotive DL frameworks via differential\ntesting.", "AI": {"tldr": "The paper proposes Scalpel, a new approach for testing deep learning frameworks in autonomous driving systems. Unlike existing methods, Scalpel creates test models reflecting real-world needs by assembling model components, enabling it to detect deployment issues in automotive DL frameworks that previous techniques miss.", "motivation": "Autonomous driving systems heavily rely on deep learning (DL) models for tasks like object detection and sensor fusion. However, deploying DL models in these systems is challenging due to requirements for real-time inference, limited computational resources, and power constraints. Existing DL framework testing methods are inadequate because they fail to generate test models that reflect the specialized capabilities needed for autonomous driving.", "method": "The paper introduces Scalpel, a novel testing approach for automotive DL frameworks. Scalpel generates test input models at the component level by assembling specialized model components (heads, necks, backbones) to fulfill multi-input/output tensor processing, multi-modal data processing, and multi-level feature extraction. The approach involves maintaining a repository of model components, selecting and mutating them, assembling new models, and deploying them for differential testing in real autonomous driving systems.", "result": "Scalpel can successfully generate and deploy test models that reflect the real requirements and complexities of autonomous driving DL models. It enriches the repository with new, functional models and systematically tests automotive DL frameworks, identifying deployment issues such as memory crashes and incorrect allocations that are missed by previous testing approaches.", "conclusion": "Scalpel addresses key gaps in DL framework testing for autonomous driving by focusing on component-level model generation. This enables the detection of quality issues that existing testing methods overlook, improving the reliability of automotive DL frameworks under real-world deployment constraints."}}
{"id": "2510.21452", "categories": ["cs.SE", "cs.CR", "cs.SI", "J.4; K.4.2; K.6.5; D.2.9; D.4.6"], "pdf": "https://arxiv.org/pdf/2510.21452", "abs": "https://arxiv.org/abs/2510.21452", "authors": ["Thomas Welsh", "Krist\u00f3fer Finnsson", "Brynj\u00f3lfur Stef\u00e1nsson", "Helmut Neukirchen"], "title": "Towards Socio-Technical Topology-Aware Adaptive Threat Detection in Software Supply Chains", "comment": "to be published in: The 12th International Conference on Social\n  Networks Analysis, Management and Security (SNAMS), IEEE", "summary": "Software supply chains (SSCs) are complex systems composed of dynamic,\nheterogeneous technical and social components which collectively achieve the\nproduction and maintenance of software artefacts. Attacks on SSCs are\nincreasing, yet pervasive vulnerability analysis is challenging due to their\ncomplexity. Therefore, threat detection must be targeted, to account for the\nlarge and dynamic structure, and adaptive, to account for its change and\ndiversity. While current work focuses on technical approaches for monitoring\nsupply chain dependencies and establishing component controls, approaches which\ninform threat detection through understanding the socio-technical dynamics are\nlacking. We outline a position and research vision to develop and investigate\nthe use of socio-technical models to support adaptive threat detection of SSCs.\nWe motivate this approach through an analysis of the XZ Utils attack whereby\nmalicious actors undermined the maintainers' trust via the project's GitHub and\nmailing lists. We highlight that monitoring technical and social data can\nidentify trends which indicate suspicious behaviour to then inform targeted and\nintensive vulnerability assessment. We identify challenges and research\ndirections to achieve this vision considering techniques for developer and\nsoftware analysis, decentralised adaptation and the need for a test bed for\nsoftware supply chain security research.", "AI": {"tldr": "Current supply chain security focuses too much on software components, missing social aspects that attackers exploit; by analyzing both technical and social signals, threat detection becomes adaptive and targeted.", "motivation": "Frequent and sophisticated attacks on software supply chains exploit not just technical vulnerabilities but also social dynamics, such as the manipulation of trust within developer communities. Existing vulnerability analysis is insufficiently adaptive and overlooks these social factors, necessitating more holistic and dynamic threat detection models.", "method": "The authors present a research vision based on position paper methodology, using a motivating example (the XZ Utils attack) and reviewing literature. They propose leveraging socio-technical network analysis and joint monitoring of technical and social signals to inform adaptive threat detection in SSCs.", "result": "The paper emphasizes the importance of integrating socio-technical models into software supply chain (SSC) security. It uncovers that current approaches overly focus on technical controls, neglecting the social dynamics that attackers exploit. Using the XZ Utils attack as a case study, it demonstrates how monitoring both social and technical data can improve threat detection and vulnerability assessment.", "conclusion": "Socio-technical modeling and analysis can substantially enhance threat detection in SSCs. Incorporating social dynamics with technical monitoring allows for more adaptive and targeted vulnerability assessment, helping preempt sophisticated attacks. The paper calls for further research into developer/social analysis methods, decentralization, and test beds for supply chain security."}}
{"id": "2510.21460", "categories": ["cs.SE", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21460", "abs": "https://arxiv.org/abs/2510.21460", "authors": ["Sean McGregor", "Victor Lu", "Vassil Tashev", "Armstrong Foundjem", "Aishwarya Ramasethu", "Sadegh AlMahdi Kazemi Zarkouei", "Chris Knotz", "Kongtao Chen", "Alicia Parrish", "Anka Reuel", "Heather Frase"], "title": "Risk Management for Mitigating Benchmark Failure Modes: BenchRisk", "comment": "19 pages, 7 figures, to be published in the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)", "summary": "Large language model (LLM) benchmarks inform LLM use decisions (e.g., \"is\nthis LLM safe to deploy for my use case and context?\"). However, benchmarks may\nbe rendered unreliable by various failure modes that impact benchmark bias,\nvariance, coverage, or people's capacity to understand benchmark evidence.\nUsing the National Institute of Standards and Technology's risk management\nprocess as a foundation, this research iteratively analyzed 26 popular\nbenchmarks, identifying 57 potential failure modes and 196 corresponding\nmitigation strategies. The mitigations reduce failure likelihood and/or\nseverity, providing a frame for evaluating \"benchmark risk,\" which is scored to\nprovide a metaevaluation benchmark: BenchRisk. Higher scores indicate that\nbenchmark users are less likely to reach an incorrect or unsupported conclusion\nabout an LLM. All 26 scored benchmarks present significant risk within one or\nmore of the five scored dimensions (comprehensiveness, intelligibility,\nconsistency, correctness, and longevity), which points to important open\nresearch directions for the field of LLM benchmarking. The BenchRisk workflow\nallows for comparison between benchmarks; as an open-source tool, it also\nfacilitates the identification and sharing of risks and their mitigations.", "AI": {"tldr": "This research analyzed 26 popular LLM benchmarks for risk using a standardized framework, identified 57 failure modes and 196 mitigations, and introduced a scoring tool (BenchRisk) to quantify and compare benchmark reliability. All benchmarks showed substantial risk, suggesting the need for further research and better risk management in LLM evaluations.", "motivation": "Benchmarks are critical for informing decisions about LLM deployment, but are often unreliable due to bias, lack of coverage, or difficulty in interpreting results. Therefore, understanding and mitigating benchmark risks is essential to ensure safe and accurate use of LLMs.", "method": "An iterative analysis based on the NIST risk management process was applied to 26 popular LLM benchmarks, identifying 57 failure modes and 196 mitigation strategies. Benchmarks were scored across five risk dimensions, and the results were compiled to create the BenchRisk tool.", "result": "Mitigation strategies were cataloged, and a scoring system (BenchRisk) was developed to quantify benchmark risk. BenchRisk enables comparison between benchmarks, identification of their risks, and sharing mitigations. All benchmarks exhibited notable risks, highlighting areas for improvement in LLM benchmarking.", "conclusion": "All 26 popular LLM benchmarks evaluated present significant risks in at least one of five dimensions: comprehensiveness, intelligibility, consistency, correctness, and longevity. The study introduces 'BenchRisk' for scoring and metaevaluation."}}
{"id": "2510.21513", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.21513", "abs": "https://arxiv.org/abs/2510.21513", "authors": ["Fernando Vallecillos Ruiz", "Max Hort", "Leon Moonen"], "title": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair", "comment": null, "summary": "Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems.\n  To address this gap, we empirically compare ten individual LLMs from five\nfamilies, and three ensembles of these LLMs across three software engineering\nbenchmarks covering code generation and program repair. We assess the\ncomplementarity between models and the performance gap between the best\nindividual model and the ensembles. Next, we evaluate various selection\nheuristics to identify correct solutions from an ensemble's candidate pool.\n  We find that the theoretical upperbound for an ensemble's performance can be\n83% above the best single model. Our results show that consensus-based\nstrategies for selecting solutions fall into a \"popularity trap,\" amplifying\ncommon but incorrect outputs. In contrast, a diversity-based strategy realizes\nup to 95% of this theoretical potential, and proves effective even in small\ntwo-model ensembles, enabling a cost-efficient way to enhance performance by\nleveraging multiple LLMs.", "AI": {"tldr": "Instead of relying on a single LLM for tasks like code generation or program repair, combining diverse models through ensemble methods\u2014particularly using diversity-based selection\u2014yields much better results efficiently, avoiding the pitfalls of consensus-based selection.", "motivation": "The current trend in software engineering is to use a single Large Language Model (LLM) for all tasks. This approach is resource-intensive and possibly overlooks the benefits of combining models with complementary strengths. However, it remains unclear how much different coding LLMs complement each other and the best methodologies for maximizing ensemble model performance.", "method": "The authors empirically compare ten individual LLMs from five families and three ensembles made from these models. They test these models on three software engineering benchmarks that involve code generation and program repair. They then assess the complementarity between models, the performance gap between the best model and ensembles, and evaluate different selection heuristics for picking correct outputs from ensemble candidates.", "result": "A theoretical ensemble can outperform the best single model by up to 83%. Consensus-based strategies often amplify shared but incorrect answers, falling into a 'popularity trap.' Meanwhile, diversity-based selection heuristics can achieve up to 95% of the maximum possible ensemble advantage, working well even with only two models in the ensemble.", "conclusion": "Utilizing diversity-based strategies in model ensembles delivers significant, cost-efficient performance improvements over single-model systems\u2014up to 95% of the theoretical ensemble potential\u2014suggesting practitioners should move beyond single-model approaches."}}
{"id": "2510.21516", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21516", "abs": "https://arxiv.org/abs/2510.21516", "authors": ["Marvin B\u00f6cker", "Ralph Biggins", "Michael Schmeing"], "title": "Lights-Out: An Automated Ground Segment for unstaffed Satellite Operations", "comment": null, "summary": "We present our approach for a periodically unstaffed, fully automated ground\nsegment. The concept is in use for the first time on the German satellite\ncommunications mission Heinrich Hertz on behalf of the German Space Agency at\nDLR. Heinrich Hertz was launched in July 2023 and offers access to scientific\nand technical experiments to its users. The mission utilizes major automation\nconcepts for the satellite platform operations, allowing fully automated\noperations outside of office hours. The concept includes tracking, telemetry\nand commanding (TTC) of the satellite. Pre-planned and automatically executed\nschedules enable commanding without human interaction. The user mission\nschedule is planned separately from the main mission schedule and is\nautomatically de-conflicted. The automatic monitoring concept monitors the\nsystems of the satellite and all assets in the ground segment and triggers\nreactions in operator-configurable ways depending on the mission needs, for\nexample emergency notifications or automated execution of flight operation\nprocedures. Additionally, the concept also puts special emphasis on a\nself-service user portal that provides flexible access 24/7, even when the\ncontrol center is not staffed. The portal allows external users of the payload\nto schedule pre-defined experiments, monitor the live execution of the\nexperiment with browser-based displays and access ground station telemetry and\ndedicated RF test equipment during the time of their scheduled experiment.\nTasks can be planned long in advance as well as with a short reaction time\n(less than 1 minute), which allows, for example, the reconfiguration of the\npayload during a running experiment.", "AI": {"tldr": "The paper presents a fully automated ground segment for the Heinrich Hertz satellite, enabling 24/7 autonomous operation, user experiment scheduling, and system monitoring, greatly reducing the need for human staff and increasing operational flexibility.", "motivation": "The motivation behind this work is to enable satellite operations that are independent of continuous human presence, addressing the need for flexibility, efficiency, and 24/7 user access to satellite resources. This is important for maximizing mission potential and reducing operational costs.", "method": "The authors designed a fully automated ground segment for the Heinrich Hertz satellite mission. The method includes automation of satellite operations such as tracking, telemetry, and commanding; pre-planned automated schedules; automated schedule de-confliction; automated system monitoring with configurable responses; and a self-service user portal for experiment scheduling and data access.", "result": "The resulting system provides fully automated satellite operations outside office hours, enabling users to schedule and monitor experiments in near real-time and access mission resources anytime. It supports both long-term and rapid-response planning and allows reconfiguration within a minute during ongoing experiments.", "conclusion": "A periodically unstaffed, fully automated ground segment is feasible and operationally beneficial. The approach supports continuous and flexible user access, reliable mission operations, and reduced need for constant human intervention, setting a precedent for future satellite missions."}}
{"id": "2510.21591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.21591", "abs": "https://arxiv.org/abs/2510.21591", "authors": ["Oleksandr Kosenkov", "Ehsan Zabardast", "Davide Fucci", "Daniel Mendez", "Michael Unterkalmsteiner"], "title": "Privacy by Design: Aligning GDPR and Software Engineering Specifications with a Requirements Engineering Approach", "comment": null, "summary": "Context: Consistent requirements and system specifications are essential for\nthe compliance of software systems towards the General Data Protection\nRegulation (GDPR). Both artefacts need to be grounded in the original text and\nconjointly assure the achievement of privacy by design (PbD). Objectives: There\nis little understanding of the perspectives of practitioners on specification\nobjectives and goals to address PbD. Existing approaches do not account for the\ncomplex intersection between problem and solution space expressed in GDPR. In\nthis study we explore the demand for conjoint requirements and system\nspecification for PbD and suggest an approach to address this demand. Methods:\nWe reviewed secondary and related primary studies and conducted interviews with\npractitioners to (1) investigate the state-of-practice and (2) understand the\nunderlying specification objectives and goals (e.g., traceability). We\ndeveloped and evaluated an approach for requirements and systems specification\nfor PbD, and evaluated it against the specification objectives. Results: The\nrelationship between problem and solution space, as expressed in GDPR, is\ninstrumental in supporting PbD. We demonstrate how our approach, based on the\nmodeling GDPR content with original legal concepts, contributes to\nspecification objectives of capturing legal knowledge, supporting specification\ntransparency, and traceability. Conclusion: GDPR demands need to be addressed\nthroughout different levels of abstraction in the engineering lifecycle to\nachieve PbD. Legal knowledge specified in the GDPR text should be captured in\nspecifications to address the demands of different stakeholders and ensure\ncompliance. While our results confirm the suitability of our approach to\naddress practical needs, we also revealed specific needs for the future\neffective operationalization of the approach.", "AI": {"tldr": "This paper develops and evaluates a requirements and system specification approach that models GDPR content for improved privacy by design (PbD) in software systems, highlighting the need for traceability, transparency, and stakeholder compliance, while outlining future operational challenges.", "motivation": "There is limited understanding of practitioners' perspectives on specification objectives and goals to address privacy by design (PbD) in the context of GDPR. Existing approaches fail to capture the complex relationship between problem and solution space as required by GDPR.", "method": "The authors conducted a review of secondary and related primary studies along with interviews with practitioners to explore the state-of-practice and understand specification objectives. They then developed and evaluated an approach for requirements and system specification tailored to PbD and assessed it against key specification goals such as traceability.", "result": "The proposed approach, which models GDPR content with original legal concepts, supports the specification objectives of capturing legal knowledge, enhancing specification transparency, and improving traceability. The connection between problem and solution space defined in GDPR is vital for effective PbD.", "conclusion": "GDPR requirements need to be addressed at multiple levels of abstraction in the engineering lifecycle to realize PbD. Specifications should capture legal knowledge directly from the GDPR text to satisfy stakeholder demands and ensure compliance. The proposed approach is suitable for practical needs but also indicates areas for future improvement in operationalization."}}
