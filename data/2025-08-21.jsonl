{"id": "2508.14104", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14104", "abs": "https://arxiv.org/abs/2508.14104", "authors": ["Yutong Bian", "Xianhao Lin", "Yupeng Xie", "Tianyang Liu", "Mingchen Zhuge", "Siyuan Lu", "Haoming Tang", "Jinlin Wang", "Jiayi Zhang", "Jiaqi Chen", "Xiangru Tang", "Yongxin Ni", "Sirui Hong", "Chenglin Wu"], "title": "You Don't Know Until You Click:Automated GUI Testing for Production-Ready Software Evaluation", "comment": null, "summary": "Large Language Models (LLMs) and code agents in software development are\nrapidly evolving from generating isolated code snippets to producing\nfull-fledged software applications with graphical interfaces, interactive\nlogic, and dynamic behaviors. However, current benchmarks fall short in\nevaluating such production-ready software, as they often rely on static checks\nor binary pass/fail scripts, failing to capture the interactive behaviors and\nruntime dynamics that define real-world usability - qualities that only emerge\nwhen an application is actively used. This is the blind spot of current\nevaluation: you don't know if an app works until you click through it, interact\nwith it, and observe how it responds. To bridge this gap, we introduce\nRealDevWorld, a novel evaluation framework for automated end-to-end assessment\nof LLMs' ability to generate production-ready repositories from scratch. It\nfeatures two key components: (1) RealDevBench, a diverse collection of 194\nopen-ended software engineering tasks across multiple domains, incorporating\nmultimodal elements to reflect real-world complexity; and (2) AppEvalPilot, a\nnew agent-as-a-judge evaluation system that simulates realistic, GUI-based user\ninteractions to automatically and holistically assess software functional\ncorrectness, visual fidelity, and runtime behavior. The framework delivers\nfine-grained, task-specific diagnostic feedback, supporting nuanced evaluation\nbeyond simple success/failure judgments. Empirical results show that\nRealDevWorld delivers effective, automatic, and human-aligned evaluations,\nachieving an accuracy of 0.92 and a correlation of 0.85 with expert human\nassessments, while significantly reducing the reliance on manual review. This\nenables scalable, human-aligned assessment of production-level software\ngenerated by LLMs. Our code is available on GitHub."}
{"id": "2508.14114", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14114", "abs": "https://arxiv.org/abs/2508.14114", "authors": ["Aditey Nandan", "Viraj Kumar"], "title": "Ambiguity Resolution with Human Feedback for Code Writing Tasks", "comment": "Accepted at the Proceedings of the 33rd International Conference on\n  Computers in Education (ICCE 2025), Asia-Pacific Society for Computers in\n  Education (APSCE)", "summary": "Specifications for code writing tasks are usually expressed in natural\nlanguage and may be ambiguous. Programmers must therefore develop the ability\nto recognize ambiguities in task specifications and resolve them by asking\nclarifying questions. We present and evaluate a prototype system, based on a\nnovel technique (ARHF: Ambiguity Resolution with Human Feedback), that (1)\nsuggests specific inputs on which a given task specification may be ambiguous,\n(2) seeks limited human feedback about the code's desired behavior on those\ninputs, and (3) uses this feedback to generate code that resolves these\nambiguities. We evaluate the efficacy of our prototype, and we discuss the\nimplications of such assistive systems on Computer Science education."}
{"id": "2508.14288", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14288", "abs": "https://arxiv.org/abs/2508.14288", "authors": ["Yewei Song", "Tiezhu Sun", "Xunzhu Tang", "Prateek Rajput", "Tegawende F. Bissyande", "Jacques Klein"], "title": "Measuring LLM Code Generation Stability via Structural Entropy", "comment": "ASE-NIER", "summary": "Assessing the stability of code generation from large language models (LLMs)\nis essential for judging their reliability in real-world development. We extend\nprior \"structural-entropy concepts\" to the program domain by pairing entropy\nwith abstract syntax tree (AST) analysis. For any fixed prompt, we collect the\nmultiset of depth-bounded subtrees of AST in each generated program and treat\ntheir relative frequencies as a probability distribution. We then measure\nstability in two complementary ways: (i) Jensen-Shannon divergence, a\nsymmetric, bounded indicator of structural overlap, and (ii) a Structural\nCross-Entropy ratio that highlights missing high-probability patterns. Both\nmetrics admit structural-only and token-aware variants, enabling separate views\non control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or\nCodeBLEU, our metrics are reference-free, language-agnostic, and\nexecution-independent. We benchmark several leading LLMs on standard code\ngeneration tasks, demonstrating that AST-driven structural entropy reveals\nnuances in model consistency and robustness. The method runs in O(n,d) time\nwith no external tests, providing a lightweight addition to the code-generation\nevaluation toolkit."}
{"id": "2508.14419", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.14419", "abs": "https://arxiv.org/abs/2508.14419", "authors": ["Scott Blyth", "Sherlock A. Licorish", "Christoph Treude", "Markus Wagner"], "title": "Static Analysis as a Feedback Loop: Enhancing LLM-Generated Code Beyond Correctness", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ncode generation, achieving high scores on benchmarks such as HumanEval and\nMBPP. However, these benchmarks primarily assess functional correctness and\nneglect broader dimensions of code quality, including security, reliability,\nreadability, and maintainability. In this work, we systematically evaluate the\nability of LLMs to generate high-quality code across multiple dimensions using\nthe PythonSecurityEval benchmark. We introduce an iterative static\nanalysis-driven prompting algorithm that leverages Bandit and Pylint to\nidentify and resolve code quality issues. Our experiments with GPT-4o show\nsubstantial improvements: security issues reduced from >40% to 13%, readability\nviolations from >80% to 11%, and reliability warnings from >50% to 11% within\nten iterations. These results demonstrate that LLMs, when guided by static\nanalysis feedback, can significantly enhance code quality beyond functional\ncorrectness."}
{"id": "2508.14394", "categories": ["cs.PL", "cs.SE", "D.3; D.2.5; G.3"], "pdf": "https://arxiv.org/pdf/2508.14394", "abs": "https://arxiv.org/abs/2508.14394", "authors": ["Ryan Tjoa", "Poorva Garg", "Harrison Goldstein", "Todd Millstein", "Benjamin Pierce", "Guy Van den Broeck"], "title": "Tuning Random Generators: Property-Based Testing as Probabilistic Programming", "comment": "Extended version of OOPSLA '25 paper", "summary": "Property-based testing validates software against an executable specification\nby evaluating it on randomly generated inputs. The standard way that PBT users\ngenerate test inputs is via generators that describe how to sample test inputs\nthrough random choices. To achieve a good distribution over test inputs, users\nmust tune their generators, i.e., decide on the weights of these individual\nrandom choices. Unfortunately, it is very difficult to understand how to choose\nindividual generator weights in order to achieve a desired distribution, so\ntoday this process is tedious and limits the distributions that can be\npractically achieved.\n  In this paper, we develop techniques for the automatic and offline tuning of\ngenerators. Given a generator with undetermined symbolic weights and an\nobjective function, our approach automatically learns values for these weights\nthat optimize for the objective. We describe useful objective functions that\nallow users to (1) target desired distributions and (2) improve the diversity\nand validity of their test cases. We have implemented our approach in a novel\ndiscrete probabilistic programming system, Loaded Dice, that supports\ndifferentiation and parameter learning, and use it as a language for\ngenerators. We empirically demonstrate that our approach is effective at\noptimizing generator distributions according to the specified objective\nfunctions. We also perform a thorough evaluation on PBT benchmarks,\ndemonstrating that, when automatically tuned for diversity and validity, the\ngenerators exhibit a 3.1-7.4x speedup in bug finding."}
{"id": "2508.14451", "categories": ["cs.SE", "K.6.3; E.0"], "pdf": "https://arxiv.org/pdf/2508.14451", "abs": "https://arxiv.org/abs/2508.14451", "authors": ["Richard Sserujongi", "Daniel Ogenrwot", "Nicholas Niwamanya", "Noah Nsimbe", "Martin Bbaale", "Benjamin Ssempala", "Noble Mutabazi", "Raja Fidel Wabinyai", "Deo Okure", "Engineer Bainomugisha"], "title": "Design and Evaluation of a Scalable Data Pipeline for AI-Driven Air Quality Monitoring in Low-Resource Settings", "comment": "15 pages, 11 figures, 34th International Conference on Software\n  Engineering and Data Engineering", "summary": "The increasing adoption of low-cost environmental sensors and AI-enabled\napplications has accelerated the demand for scalable and resilient data\ninfrastructures, particularly in data-scarce and resource-constrained regions.\nThis paper presents the design, implementation, and evaluation of the AirQo\ndata pipeline: a modular, cloud-native Extract-Transform-Load (ETL) system\nengineered to support both real-time and batch processing of heterogeneous air\nquality data across urban deployments in Africa. It is Built using open-source\ntechnologies such as Apache Airflow, Apache Kafka, and Google BigQuery. The\npipeline integrates diverse data streams from low-cost sensors, third-party\nweather APIs, and reference-grade monitors to enable automated calibration,\nforecasting, and accessible analytics. We demonstrate the pipeline's ability to\ningest, transform, and distribute millions of air quality measurements monthly\nfrom over 400 monitoring devices while achieving low latency, high throughput,\nand robust data availability, even under constrained power and connectivity\nconditions. The paper details key architectural features, including workflow\norchestration, decoupled ingestion layers, machine learning-driven sensor\ncalibration, and observability frameworks. Performance is evaluated across\noperational metrics such as resource utilization, ingestion throughput,\ncalibration accuracy, and data availability, offering practical insights into\nbuilding sustainable environmental data platforms. By open-sourcing the\nplatform and documenting deployment experiences, this work contributes a\nreusable blueprint for similar initiatives seeking to advance environmental\nintelligence through data engineering in low-resource settings."}
{"id": "2508.14614", "categories": ["cs.PL", "D.3.0; D.3.1"], "pdf": "https://arxiv.org/pdf/2508.14614", "abs": "https://arxiv.org/abs/2508.14614", "authors": ["Ashish Mishra", "Suresh Jagannathan"], "title": "Close is Good Enough: Component-Based Synthesis Modulo Logical Similarity", "comment": null, "summary": "Component-based synthesis (CBS) aims to generate loop-free programs from a\nset of libraries whose methods are annotated with specifications and whose\noutput must satisfy a set of logical constraints, expressed as a query. The\neffectiveness of a CBS algorithm critically depends on the severity of the\nconstraints imposed by the query. The more exact these constraints are, the\nsparser the space of feasible solutions. This maxim also applies when we enrich\nthe expressiveness of the specifications affixed to library methods. In both\ncases, the search must now contend with constraints that may only hold over a\nsmall number of the possible execution paths that can be enumerated by a CBS\nprocedure.\n  In this paper, we address this challenge by equipping CBS search with the\nability to reason about logical similarities among the paths it explores. Our\nsetting considers library methods equipped with refinement-type specifications\nthat enrich ordinary base types with a set of rich logical qualifiers to\nconstrain the set of values accepted by that type. We perform a search over a\ntree automata variant called Qualified Tree Automata that intelligently records\ninformation about enumerated terms, leveraging subtyping constraints over the\nrefinement types associated with these terms to enable reasoning about\nsimilarity among candidate solutions as search proceeds, thereby avoiding\nexploration of semantically similar paths.\n  We present an implementation of this idea in a tool called \\name, and provide\na comprehensive evaluation that demonstrates \\name's ability to synthesize\nsolutions to complex CBS queries that go well-beyond the capabilities of the\nexisting state-of-the-art."}
{"id": "2508.14511", "categories": ["cs.SE", "D.2.11"], "pdf": "https://arxiv.org/pdf/2508.14511", "abs": "https://arxiv.org/abs/2508.14511", "authors": ["Eagon Meng", "Daniel Jackson"], "title": "What You See Is What It Does: A Structural Pattern for Legible Software", "comment": "16 pages. Appearing in Onward! at SPLASH 2025", "summary": "The opportunities offered by LLM coders (and their current limitations)\ndemand a reevaluation of how software is structured. Software today is often\n\"illegible\" - lacking a direct correspondence between code and observed\nbehavior - and insufficiently modular, leading to a failure of three key\nrequirements of robust coding: incrementality (the ability to deliver small\nincrements by making localized changes), integrity (avoiding breaking prior\nincrements) and transparency (making clear what has changed at build time, and\nwhat actions have happened at runtime).\n  A new structural pattern offers improved legibility and modularity. Its\nelements are concepts and synchronizations: fully independent services and\nevent-based rules that mediate between them. A domain-specific language for\nsynchronizations allows behavioral features to be expressed in a granular and\ndeclarative way (and thus readily generated by an LLM). A case study of the\nRealWorld benchmark is used to illustrate and evaluate the approach."}
{"id": "2508.14532", "categories": ["cs.SE", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.14532", "abs": "https://arxiv.org/abs/2508.14532", "authors": ["Zhongyi Wang", "Tengjie Lin", "Mingshuai Chen", "Mingqi Yang", "Haokun Li", "Xiao Yi", "Shengchao Qin", "Jianwei Yin"], "title": "Preguss: It Analyzes, It Specifies, It Verifies", "comment": "Position paper to appear in the 1st International Workshop on\n  Language Models and Programming Languages (LMPL '25)", "summary": "Fully automated verification of large-scale software and hardware systems is\narguably the holy grail of formal methods. Large language models (LLMs) have\nrecently demonstrated their potential for enhancing the degree of automation in\nformal verification by, e.g., generating formal specifications as essential to\ndeductive verification, yet exhibit poor scalability due to context-length\nlimitations and, more importantly, the difficulty of inferring complex,\ninterprocedural specifications. This paper outlines Preguss - a modular,\nfine-grained framework for automating the generation and refinement of formal\nspecifications. Preguss synergizes between static analysis and deductive\nverification by orchestrating two components: (i) potential runtime error\n(RTE)-guided construction and prioritization of verification units, and (ii)\nLLM-aided synthesis of interprocedural specifications at the unit level. We\nenvisage that Preguss paves a compelling path towards the automated\nverification of large-scale programs."}
{"id": "2508.14540", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14540", "abs": "https://arxiv.org/abs/2508.14540", "authors": ["Dennis Schiese", "Andreas Both"], "title": "Post-hoc LLM-Supported Debugging of Distributed Processes", "comment": "Presented at ICWE 2025, Delft (30 June - 03 July 2025)", "summary": "In this paper, we address the problem of manual debugging, which nowadays\nremains resource-intensive and in some parts archaic. This problem is\nespecially evident in increasingly complex and distributed software systems.\nTherefore, our objective of this work is to introduce an approach that can\npossibly be applied to any system, at both the macro- and micro-level, to ease\nthis debugging process. This approach utilizes a system's process data, in\nconjunction with generative AI, to generate natural-language explanations.\nThese explanations are generated from the actual process data, interface\ninformation, and documentation to guide the developers more efficiently to\nunderstand the behavior and possible errors of a process and its sub-processes.\nHere, we present a demonstrator that employs this approach on a component-based\nJava system. However, our approach is language-agnostic. Ideally, the generated\nexplanations will provide a good understanding of the process, even if\ndevelopers are not familiar with all the details of the considered system. Our\ndemonstrator is provided as an open-source web application that is freely\naccessible to all users."}
{"id": "2508.14553", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14553", "abs": "https://arxiv.org/abs/2508.14553", "authors": ["Dennis Schiese", "Aleksandr Perevalov", "Andreas Both"], "title": "Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems", "comment": "Presented at ICWI 2024, Zagreb. Released with ISBN:\n  978-989-8704-62-7. Data source:\n  https://figshare.com/articles/dataset/Towards_LLM-generated_explanations_for_component-based_knowledge_graph_question_answering_systems/27079687", "summary": "Over time, software systems have reached a level of complexity that makes it\ndifficult for their developers and users to explain particular decisions made\nby them. In this paper, we focus on the explainability of component-based\nsystems for Question Answering (QA). These components often conduct processes\ndriven by AI methods, in which behavior and decisions cannot be clearly\nexplained or justified, s.t., even for QA experts interpreting the executed\nprocess and its results is hard. To address this challenge, we present an\napproach that considers the components' input and output data flows as a source\nfor representing the behavior and provide explanations for the components,\nenabling users to comprehend what happened. In the QA framework used here, the\ndata flows of the components are represented as SPARQL queries (inputs) and RDF\ntriples (outputs). Hence, we are also providing valuable insights on\nverbalization regarding these data types. In our experiments, the approach\ngenerates explanations while following template-based settings (baseline) or\nvia the use of Large Language Models (LLMs) with different configurations\n(automatic generation). Our evaluation shows that the explanations generated\nvia LLMs achieve high quality and mostly outperform template-based approaches\naccording to the users' ratings. Therefore, it enables us to automatically\nexplain the behavior and decisions of QA components to humans while using RDF\nand SPARQL as a context for explanations."}
{"id": "2508.14631", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.14631", "abs": "https://arxiv.org/abs/2508.14631", "authors": ["Marcos Gomez-Vazquez", "Jordi Cabot"], "title": "Towards a DSL to Formalize Multimodal Requirements", "comment": null, "summary": "Multimodal systems, which process multiple input types such as text, audio,\nand images, are becoming increasingly prevalent in software systems, enabled by\nthe huge advancements in Machine Learning. This triggers the need to easily\ndefine the requirements linked to these new types of user interactions,\npotentially involving more than one modality at the same time. This remains an\nopen challenge due to the lack of languages and methods adapted to the diverse\nnature of multimodal interactions, with the risk of implementing AI-enhanced\nsystems that do not properly satisfy the user needs.\n  In this sense, this paper presents MERLAN, a Domain-Specific Language (DSL)\nto specify the requirements for these new types of multimodal interfaces. We\npresent the metamodel for such language together with a textual syntax\nimplemented as an ANTLR grammar. A prototype tool enabling requirements\nengineers to write such requirements and automatically generate a possible\nimplementation of a system compliant with them on top of an agentic framework\nis also provided."}
{"id": "2508.14727", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14727", "abs": "https://arxiv.org/abs/2508.14727", "authors": ["Abbas Sabra", "Olivier Schmitt", "Joseph Tyler"], "title": "Assessing the Quality and Security of AI-Generated Code: A Quantitative Analysis", "comment": null, "summary": "This study presents a quantitative evaluation of the code quality and\nsecurity of five prominent Large Language Models (LLMs): Claude Sonnet 4,\nClaude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior\nresearch has assessed the functional performance of LLM-generated code, this\nresearch tested LLM output from 4,442 Java coding assignments through\ncomprehensive static analysis using SonarQube. The findings suggest that\nalthough LLMs can generate functional code, they also introduce a range of\nsoftware defects, including bugs, security vulnerabilities, and code smells.\nThese defects do not appear to be isolated; rather, they may represent shared\nweaknesses stemming from systemic limitations within current LLM code\ngeneration methods. In particular, critically severe issues, such as hard-coded\npasswords and path traversal vulnerabilities, were observed across multiple\nmodels. These results indicate that LLM-generated code requires verification in\norder to be considered production-ready. This study found no direct correlation\nbetween a model's functional performance (measured by Pass@1 rate of unit\ntests) and the overall quality and security of its generated code, measured by\nthe number of SonarQube issues in benchmark solutions that passed the\nfunctional tests. This suggests that functional benchmark performance score is\nnot a good indicator of overall code quality and security. The goal of this\nstudy is not to rank LLM performance but to highlight that all evaluated models\nappear to share certain weaknesses. Consequently, these findings support the\nview that static analysis can be a valuable instrument for detecting latent\ndefects and an important safeguard for organizations that deploy AI in software\ndevelopment."}
{"id": "2508.14747", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.14747", "abs": "https://arxiv.org/abs/2508.14747", "authors": ["Beatriz Cabrero-Daniel", "Mazen Mohamad"], "title": "Challenges of Virtual Validation and Verification for Automotive Functions", "comment": "This work is supported by Sweden's innovation agency, Vinnova, under\n  Grant No. 2021-05043 entitled \"Enabling Virtual Validation and Verification\n  for ADAS and AD Features (EVIDENT).\"", "summary": "Verification and validation of vehicles is a complex yet critical process,\nparticularly for ensuring safety and coverage through simulations. However,\nachieving realistic and useful simulations comes with significant challenges.\nTo explore these challenges, we conducted a workshop with experts in the field,\nallowing them to brainstorm key obstacles. Following this, we distributed a\nsurvey to consolidate findings and gain further insights into potential\nsolutions. The experts identified 17 key challenges, along with proposed\nsolutions, an assessment of whether they represent next steps for research, and\nthe roadblocks to their implementation. While a lack of resources was not\ninitially highlighted as a major challenge, utilizing more resources emerged as\na critical necessity when experts discussed solutions. Interestingly, we\nexpected some of these challenges to have already been addressed or to have\nsystematic solutions readily available, given the collective expertise in the\nfield. Many of the identified problems already have known solutions, allowing\nus to shift focus towards unresolved challenges and share the next steps with\nthe broader community."}
{"id": "2508.14394", "categories": ["cs.PL", "cs.SE", "D.3; D.2.5; G.3"], "pdf": "https://arxiv.org/pdf/2508.14394", "abs": "https://arxiv.org/abs/2508.14394", "authors": ["Ryan Tjoa", "Poorva Garg", "Harrison Goldstein", "Todd Millstein", "Benjamin Pierce", "Guy Van den Broeck"], "title": "Tuning Random Generators: Property-Based Testing as Probabilistic Programming", "comment": "Extended version of OOPSLA '25 paper", "summary": "Property-based testing validates software against an executable specification\nby evaluating it on randomly generated inputs. The standard way that PBT users\ngenerate test inputs is via generators that describe how to sample test inputs\nthrough random choices. To achieve a good distribution over test inputs, users\nmust tune their generators, i.e., decide on the weights of these individual\nrandom choices. Unfortunately, it is very difficult to understand how to choose\nindividual generator weights in order to achieve a desired distribution, so\ntoday this process is tedious and limits the distributions that can be\npractically achieved.\n  In this paper, we develop techniques for the automatic and offline tuning of\ngenerators. Given a generator with undetermined symbolic weights and an\nobjective function, our approach automatically learns values for these weights\nthat optimize for the objective. We describe useful objective functions that\nallow users to (1) target desired distributions and (2) improve the diversity\nand validity of their test cases. We have implemented our approach in a novel\ndiscrete probabilistic programming system, Loaded Dice, that supports\ndifferentiation and parameter learning, and use it as a language for\ngenerators. We empirically demonstrate that our approach is effective at\noptimizing generator distributions according to the specified objective\nfunctions. We also perform a thorough evaluation on PBT benchmarks,\ndemonstrating that, when automatically tuned for diversity and validity, the\ngenerators exhibit a 3.1-7.4x speedup in bug finding."}
