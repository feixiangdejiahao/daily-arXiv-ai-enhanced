{"id": "2510.09721", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09721", "abs": "https://arxiv.org/abs/2510.09721", "authors": ["Jiale Guo", "Suizhi Huang", "Mei Li", "Dong Huang", "Xingsheng Chen", "Regina Zhang", "Zhijiang Guo", "Han Yu", "Siu-Ming Yiu", "Christian Jensen", "Pietro Lio", "Kwok-Yan Lam"], "title": "A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System", "comment": "21 pages", "summary": "The integration of LLMs into software engineering has catalyzed a paradigm\nshift from traditional rule-based systems to sophisticated agentic systems\ncapable of autonomous problem-solving. Despite this transformation, the field\nlacks a comprehensive understanding of how benchmarks and solutions\ninterconnect, hindering systematic progress and evaluation. This survey\npresents the first holistic analysis of LLM-empowered software engineering,\nbridging the critical gap between evaluation and solution approaches. We\nanalyze 150+ recent papers and organize them into a comprehensive taxonomy\nspanning two major dimensions: (1) Solutions, categorized into prompt-based,\nfine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code\ngeneration, translation, repair, and other tasks. Our analysis reveals how the\nfield has evolved from simple prompt engineering to complex agentic systems\nincorporating planning and decomposition, reasoning and self-refinement, memory\nmechanisms, and tool augmentation. We present a unified pipeline that\nillustrates the complete workflow from task specification to final\ndeliverables, demonstrating how different solution paradigms address varying\ncomplexity levels across software engineering tasks. Unlike existing surveys\nthat focus on isolated aspects, we provide full-spectrum coverage connecting\n50+ benchmarks with their corresponding solution strategies, enabling\nresearchers to identify optimal approaches for specific evaluation criteria.\nFurthermore, we identify critical research gaps and propose actionable future\ndirections, including multi-agent collaboration frameworks, self-evolving code\ngeneration systems, and integration of formal verification with LLM-based\nmethods. This survey serves as a foundational resource for researchers and\npractitioners seeking to understand, evaluate, and advance LLM-empowered\nsoftware engineering systems.", "AI": {"tldr": "This survey reviews and analyzes 150+ papers on LLMs in software engineering, organizing solution methods and benchmarks into a clear taxonomy. It connects over 50 benchmarks to relevant solution strategies, charts the field's evolution to advanced agentic systems, and provides a unified workflow. The paper identifies research gaps and recommends future work, making it a key resource for researchers and practitioners in LLM-enabled software engineering.", "motivation": "The rapid integration of LLMs (Large Language Models) into software engineering has transformed the landscape from traditional rule-based to autonomous agentic systems. However, the field lacks a comprehensive perspective on how benchmarks (evaluations) and solution paradigms relate, limiting systematic development and assessment.", "method": "Conducted a large-scale survey and analysis of more than 150 recent research papers on LLM-powered software engineering. Developed a comprehensive taxonomy based on two key axes: solution approaches (prompt-based, fine-tuning-based, agent-based) and benchmark types (code generation, translation, repair, etc.). Connected 50+ benchmarks with associated solution strategies, and presented a unified workflow pipeline to illustrate the software engineering lifecycle using LLMs. Identified research gaps and proposed actionable future research directions.", "result": "Mapped the evolution of the field from basic prompt-based methods to advanced agent-based systems with planning, reasoning, memory, and tool augmentation. Provided a comprehensive overview linking evaluation benchmarks and solution methods. Identified current shortcomings\u2014such as lack of multi-agent frameworks and integration with formal verification\u2014and outlined future opportunities (multi-agent collaboration, self-evolving systems, etc.).", "conclusion": "This work establishes a holistic framework for understanding and advancing LLM-powered software engineering by thoroughly connecting evaluation benchmarks with solution strategies. It serves as a foundational guide for ongoing and future research, highlighting both the spectrum of current capabilities and the gaps to be addressed moving forward."}}
{"id": "2510.09724", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09724", "abs": "https://arxiv.org/abs/2510.09724", "authors": ["Qiaosheng Chen", "Yang Liu", "Lei Li", "Kai Chen", "Qipeng Guo", "Gong Cheng", "Fei Yuan"], "title": "InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation", "comment": "27 pages, 17 figures", "summary": "Large Language Models (LLMs) are increasingly capable of generating complete\napplications from natural language instructions, creating new opportunities in\nscience and education. In these domains, interactive scientific demonstrations\nare particularly valuable for explaining concepts, supporting new teaching\nmethods, and presenting research findings. Generating such demonstrations\nrequires models to combine accurate scientific knowledge with the ability to\nimplement interactive front-end code that behaves correctly and responds to\nuser actions. This capability goes beyond the scope of existing benchmarks,\nwhich typically evaluate either knowledge question answering without grounding\nin code or static web code generation without scientific interactivity. To\nevaluate this integrated ability, we design a hybrid framework that combines\nprogrammatic functional testing to rigorously verify interaction logic with\nvisually-grounded qualitative testing to assess rendered outputs against\nreference snapshots. Building on this framework, we present InteractScience, a\nbenchmark consisting of a substantial set of carefully designed questions\nacross five scientific domains, each paired with unit tests, reference\nsnapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs\nand report results that highlight ongoing weaknesses in integrating domain\nknowledge with interactive front-end coding. Our work positions InteractScience\nas the first benchmark to automatically measure this combined capability with\nrealistic interactive operations, providing a foundation for advancing reliable\nand educationally useful scientific demonstration code generation. All code and\ndata are publicly available at https://github.com/open-compass/InteractScience.", "AI": {"tldr": "This paper presents InteractScience, a new benchmark that tests LLMs' ability to generate interactive scientific demonstrations by integrating domain knowledge with front-end coding. Current LLMs struggle with this task, highlighting an important area for further research and development.", "motivation": "Large Language Models (LLMs) are capable of generating software from natural language, opening opportunities in science and education for interactive demonstrations. However, current benchmarks do not evaluate LLMs\u2019 ability to integrate scientific knowledge with interactive code generation.", "method": "The paper introduces a hybrid evaluation framework combining programmatic functional testing for interaction logic and visually-grounded qualitative testing against reference outputs. Using this, they build the InteractScience benchmark: a large set of questions from 5 scientific domains, each with unit tests, snapshots, and checklists.", "result": "30 open- and closed-source LLMs were tested with InteractScience. Results show significant weaknesses in LLMs when integrating scientific domain knowledge with interactive front-end coding.", "conclusion": "InteractScience is the first benchmark for systematically and automatically evaluating LLMs\u2019 ability to generate realistic, educational, and interactive scientific demonstration code, laying the foundation for future improvements."}}
{"id": "2510.09907", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09907", "abs": "https://arxiv.org/abs/2510.09907", "authors": ["Muhammad Maaz", "Liam DeVoe", "Zac Hatfield-Dodds", "Nicholas Carlini"], "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem", "comment": "4 pages (main), NeurIPS 2025, The 4th Deep Learning for Code Workshop", "summary": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt.", "AI": {"tldr": "LLMs can automate property-based testing for Python packages by synthesizing tests, analyzing outputs, and generating useful bug reports. Applied to 100 packages, the approach surfaced many valid and actionable bugs\u2014including in major libraries\u2014with several resulting patches merged. This method offers scalable, rigorous automated software testing.", "motivation": "Property-based testing (PBT) is effective but requires manual specification of test properties and input domains. The motivation is to automate and scale PBT by leveraging the capabilities of large language models (LLMs) to analyze code, infer properties, and execute robust test generation and bug reporting.", "method": "The authors developed an LLM-based agent that automatically analyzes Python modules, infers properties from both code and documentation, synthesizes and runs property-based tests, interprets the results to confirm bugs, and generates actionable bug reports. The agent was evaluated on 100 popular Python packages.", "result": "Manual review found that 56% of bug reports were valid bugs and 32% were valid bugs worth reporting. A ranking rubric was created to prioritize bugs, and among the top 21, 86% were valid and 81% were worth reporting. The team reported 5 bugs (4 with patches), including issues in major libraries like NumPy, and successfully merged 3 patches.", "conclusion": "The study demonstrates that combining LLMs with PBT forms an effective and scalable method for autonomous software testing, capable of identifying a broad range of real-world bugs and producing actionable reports and patches. The approach shows promise for improving software robustness with minimal manual effort."}}
{"id": "2510.09938", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09938", "abs": "https://arxiv.org/abs/2510.09938", "authors": ["Youshuai Tan", "Zishuo Ding", "Jinfu Chen", "Weiyi Shang"], "title": "OFP-Repair: Repairing Floating-point Errors via Original-Precision Arithmetic", "comment": null, "summary": "Errors in floating-point programs can lead to severe consequences,\nparticularly in critical domains such as military, aerospace, and financial\nsystems, making their repair a crucial research problem. In practice, some\nerrors can be fixed using original-precision arithmetic, while others require\nhigh-precision computation. Developers often avoid addressing the latter due to\nexcessive computational resources required. However, they sometimes struggle to\ndistinguish between these two types of errors, and existing repair tools fail\nto assist in this differentiation. Most current repair tools rely on\nhigh-precision implementations, which are time-consuming to develop and demand\nspecialized expertise. Although a few tools do not require high-precision\nprograms, they can only fix a limited subset of errors or produce suboptimal\nresults.\n  To address these challenges, we propose a novel method, named OFP-Repair.On\nACESO's dataset, our patches achieve improvements of three, seven, three, and\neight orders of magnitude across four accuracy metrics. In real-world cases,\nour method successfully detects all five original-precision-repairable errors\nand fixes three, whereas ACESO only repairs one. Notably, these results are\nbased on verified data and do not fully capture the potential of OFP-Repair. To\nfurther validate our method, we deploy it on a decade-old open bug report from\nGNU Scientific Library (GSL), successfully repairing five out of 15 bugs. The\ndevelopers have expressed interest in our method and are considering\nintegrating our tool into their development workflow. We are currently working\non applying our patches to GSL. The results are highly encouraging,\ndemonstrating the practical applicability of our technique.", "AI": {"tldr": "Floating-point bugs can be hard to fix, especially if high-precision computation is required. Most tools either fail to identify which errors need high-precision or struggle to fix a wide range of bugs. This paper introduces OFP-Repair, a new method that efficiently repairs floating-point errors, achieving significant improvements over prior tools on benchmarks and real-world cases and being considered by real developers for practical use.", "motivation": "Floating-point errors can have severe consequences in high-stakes domains (military, aerospace, finance), and current repair tools are limited\u2014being either too reliant on high-precision (expensive and difficult) methods or not effective enough in fixing many errors. Distinguishing which errors need high-precision remains a practical challenge, leaving a gap in effective repair strategies.", "method": "The paper proposes a new repair method called OFP-Repair, designed to better distinguish and repair errors that can be fixed with original-precision arithmetic versus those requiring high-precision. The approach aims to be practical and efficient, able to fix more errors without always requiring high-precision implementations.", "result": "On the ACESO dataset, OFP-Repair shows improvements of 3, 7, 3, and 8 orders of magnitude across four accuracy metrics. In real-world trials, it detects all five original-precision-repairable errors and fixes three (whereas the competing ACESO tool fixes only one). When tested on historical bugs in the GNU Scientific Library, OFP-Repair repairs 5 out of 15, and the maintainers show interest in adoption.", "conclusion": "OFP-Repair significantly advances the state of floating-point error repair by fixing more bugs efficiently, often without resorting to high-precision computation, and demonstrates promising results both in benchmark datasets and real-world libraries. Its adoption is being considered in practice, indicating strong practical relevance."}}
{"id": "2510.09726", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09726", "abs": "https://arxiv.org/abs/2510.09726", "authors": ["Tilman Hinnerichs", "Reuben Gardos Reid", "Jaap de Jong", "Bart Swinkels", "Pamela Wochner", "Nicolae Filat", "Tudor Magurescu", "Issa Hanou", "Sebastijan Dumancic"], "title": "Herb.jl: A Unifying Program Synthesis Library", "comment": null, "summary": "Program synthesis -- the automatic generation of code given a specification\n-- is one of the most fundamental tasks in artificial intelligence (AI) and\nmany programmers' dream. Numerous synthesizers have been developed to tackle\nprogram synthesis, manifesting different ideas to approach the exponentially\ngrowing program space. While numerous smart program synthesis tools exist,\nreusing and remixing previously developed methods is tedious and\ntime-consuming. We propose Herb.jl, a unifying program synthesis library\nwritten in the Julia programming language, to address these issues. Since\ncurrent methods rely on similar building blocks, we aim to modularize the\nunderlying synthesis algorithm into communicating and fully extendable\nsub-compartments, allowing for straightforward reapplication of these modules.\nTo demonstrate the benefits of using Herb.jl, we show three common use cases:\n1. how to implement a simple problem and grammar, and how to solve it, 2. how\nto implement a previously developed synthesizer with just a few lines of code,\nand 3. how to run a synthesizer against a benchmark.", "AI": {"tldr": "Herb.jl is a modular Julia library designed to unify and simplify program synthesis. It allows easier reuse, remixing, and benchmarking of synthesis methods, addressing inefficiencies in existing tools and demonstrating practical benefits through key use cases.", "motivation": "Existing program synthesis tools are difficult to reuse and remix due to their monolithic structures, leading to tedious and time-consuming development. Since many synthesis methods rely on similar algorithmic blocks, there is a need for a unified, modular library to accelerate and simplify tool development.", "method": "Herb.jl is implemented in Julia, comprising modular and extendable compartments for synthesis algorithms. The paper demonstrates its utility through use cases: implementing simple problems and grammars, recreating synthesizers with minimal code, and benchmarking synthesizer performance.", "result": "Herb.jl enables users to quickly reimplement synthesizers, solve specification problems with minimal setup, and conduct benchmarking studies, showing improved usability and extensibility over previous approaches.", "conclusion": "Herb.jl streamlines and unifies program synthesis tasks by modularizing core synthesis components, making reuse and extension easier and more efficient."}}
{"id": "2510.09968", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09968", "abs": "https://arxiv.org/abs/2510.09968", "authors": ["Stefan Pasch"], "title": "Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context", "comment": null, "summary": "Organizational efforts to utilize and operationalize artificial intelligence\n(AI) are often accompanied by substantial challenges, including scalability,\nmaintenance, and coordination across teams. In response, the concept of Machine\nLearning Operations (MLOps) has emerged as a set of best practices that\nintegrate software engineering principles with the unique demands of managing\nthe ML lifecycle. Yet, empirical evidence on whether and how these practices\nsupport users in developing and operationalizing AI applications remains\nlimited. To address this gap, this study analyzes over 8,000 user reviews of AI\ndevelopment platforms from G2.com. Using zero-shot classification, we measure\nreview sentiment toward nine established MLOps practices, including continuous\nintegration and delivery (CI/CD), workflow orchestration, reproducibility,\nversioning, collaboration, and monitoring. Seven of the nine practices show a\nsignificant positive relationship with user satisfaction, suggesting that\neffective MLOps implementation contributes tangible value to AI development.\nHowever, organizational context also matters: reviewers from small firms\ndiscuss certain MLOps practices less frequently, suggesting that organizational\ncontext influences the prevalence and salience of MLOps, though firm size does\nnot moderate the MLOps-satisfaction link. This indicates that once applied,\nMLOps practices are perceived as universally beneficial across organizational\nsettings.", "AI": {"tldr": "MLOps best practices significantly improve user satisfaction in AI development across organizations of all sizes, even though smaller firms talk about them less.", "motivation": "Organizations face significant challenges in scaling, maintaining, and coordinating AI initiatives. The effectiveness and real-world impact of MLOps practices in overcoming these challenges and improving AI application development are not well understood.", "method": "The study analyzed more than 8,000 user reviews from G2.com for various AI development platforms. Zero-shot classification was used to gauge review sentiment toward nine specific MLOps practices, such as CI/CD, orchestration, reproducibility, versioning, collaboration, and monitoring.", "result": "Seven out of nine MLOps practices showed a significant positive relationship with user satisfaction, indicating their valuable contribution to AI development. However, smaller firms discuss these practices less often, though firm size does not affect the positive impact of MLOps practices on satisfaction.", "conclusion": "MLOps practices deliver tangible benefits for AI development across organizations, regardless of firm size. Their implementation is universally valued by users, even if the emphasis and discussion of these practices vary by organizational context."}}
{"id": "2510.09932", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.09932", "abs": "https://arxiv.org/abs/2510.09932", "authors": ["Devansh Jain", "Akash Pardeshi", "Marco Frigo", "Krut Patel", "Kaustubh Khulbe", "Jai Arora", "Charith Mendis"], "title": "ACT: Automatically Generating Compiler Backends from Tensor Accelerator ISA Descriptions", "comment": null, "summary": "Tensor compilers play a key role in enabling high-performance implementations\nof deep learning workloads. These compilers rely on existing CPU and GPU code\ngeneration backends to generate device-specific code. Recently, many tensor\naccelerators (neural processing units) have been proposed to further accelerate\nthese workloads. Compared to commodity hardware, however, most of the proposed\ntensor accelerators do not have compiler backends with code generation support.\nMoreover, the accelerator designs are subject to fast iteration cycles, making\nit difficult to manually develop compiler backends similar to commodity\nhardware platforms. Therefore, to increase adoption and enable faster software\ndevelopment cycles for novel tensor accelerator designs, we need to make the\ncompiler backend construction process more agile.\n  To address this gap, we introduce ACT, a compiler backend generator that\nautomatically generates compiler backends for tensor accelerators, given just\nthe instruction set architecture (ISA) descriptions. We first formally specify\nthe compiler backend generation problem that introduces a novel specification\nfor describing tensor accelerator ISAs. Next, we design ACT such that it\nsupports user-programmable memories and complex parameterized instructions that\nare prevalent in tensor accelerators. ACT uses a novel parameterized equality\nsaturation-based instruction selection phase and a constraint programming-based\nmemory allocation phase. We prove that compiler backends generated by ACT are\nsound and complete. Finally, we generate compiler backends for three\naccelerator platforms from industry and academia, and show that they match or\noutperform code written using hand-optimized kernel libraries while maintaining\nlow compilation overheads.", "AI": {"tldr": "The paper presents ACT, an automated tool for generating high-quality compiler backends for tensor accelerators from just their ISA descriptions. ACT's generated backends are both correct and performant, matching or surpassing hand-tuned code and greatly streamlining software development for emerging accelerator designs.", "motivation": "Most new tensor accelerators lack dedicated compiler backend support, making software development for them challenging and slow. Manual development of these backends is labor-intensive and can't keep up with rapid hardware iteration cycles. There's a need for an automated, agile solution to support fast and efficient code generation for novel accelerators.", "method": "The authors introduce ACT, a compiler backend generator that automatically produces compiler backends for tensor accelerators using only their ISA descriptions. ACT introduces a novel ISA specification, employs parameterized equality saturation for instruction selection, and utilizes constraint programming for memory allocation. The generated backends are formally proven to be sound and complete.", "result": "ACT successfully generated compiler backends for three different accelerator platforms from both industry and academia. The resulting backends achieved performance on par with or better than hand-optimized kernel libraries, while keeping compilation overheads low.", "conclusion": "ACT automates the generation of high-performance and correct compiler backends for tensor accelerators, significantly accelerating software development for new hardware and closing the gap between hardware innovation and usable software ecosystems."}}
{"id": "2510.10010", "categories": ["cs.SE", "cs.AI", "68N01, 68T05, 68T07", "D.2.5; D.2.7; I.2.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.10010", "abs": "https://arxiv.org/abs/2510.10010", "authors": ["Matheus J. T. Vargas"], "title": "SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study", "comment": "14 pages, 4 figures, 6 tables, link to code repo", "summary": "We present SLEAN (Simple Lightweight Ensemble Analysis Network), a\ndeterministic framework for coordinating multiple LLM providers through\ntext-based prompt orchestration. Unlike complex multi-agent systems requiring\nspecialized infrastructure, SLEAN operates as a simple prompt bridge between\nLLMs using .txt templates, requiring no deep technical knowledge for\ndeployment. The three-phase protocol formed by independent analysis,\ncross-critique, and arbitration, filters harmful AI-generated code suggestions\nbefore production deployment, addressing how AI-assisted debugging increasingly\nproduces modifications that introduce unnecessary complexity, break existing\nfunctionality, or address problems. Evaluating 15 software bugs, we analyzed 69\nAI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95%\nCI 20.9-42.9%) while rejecting 47 that would have been harmful if applied\nverbatim. The arbitration process reduced code change surface by 83-90%\nrelative to raw AI outputs, enforcing minimal causal edits over scope-expanding\nmodifications. Minimal Type 2 inputs proved more efficient than detailed Type 1\ninputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus\n28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems\nshowed weak correlation with fix quality: high convergence (at least 80%)\noccurred in 4 of 15 cases and improved acceptance by only 2.4% points;\narbitration appeared only at exactly 10% convergence in 2 of 15 cases, although\nlow convergence alone did not necessitate arbitration. The file-driven,\nprovider-agnostic architecture enables deployment without specialized coding\nexpertise, making it applicable to security auditing, code review, document\nverification, and other domains requiring reliable multi-provider synthesis\nwith end-to-end auditability.", "AI": {"tldr": "SLEAN is a simple, file-based framework for orchestrating multiple LLMs to safely filter and arbitrate AI-generated code fixes. It efficiently reduces harmful changes, requires minimal expertise, and is broadly applicable to reliable, auditable multi-provider code and document review workflows.", "motivation": "AI-assisted debugging can introduce harmful, unnecessary, or overly complex changes to software code. There is a need for a simple framework to coordinate multiple large language model (LLM) providers for safer and more reliable code fixes without deep technical barriers.", "method": "SLEAN is a deterministic text-based orchestration framework that coordinates multiple LLMs using .txt templates. It uses a three-phase protocol: independent analysis, cross-critique, and arbitration, to filter and select appropriate code changes. The framework avoids complex infrastructure and enables a file-driven deployment.", "result": "On 15 software bugs and 69 AI-generated fix proposals, SLEAN accepted 22 fixes and rejected 47 potentially harmful ones. It reduced code change surface by 83-90%, prioritized minimal edits, and showed about 20% efficiency gain with minimal inputs. The agreement rate between LLMs was weakly correlated with fix quality, and arbitration occurred only under specific convergence scenarios.", "conclusion": "SLEAN is an efficient, provider-agnostic framework for filtering and managing code fixes from multiple LLMs. It requires little technical expertise to deploy and improves safety and minimalism in code changes. It has potential applications in security auditing, code review, and document verification beyond software debugging."}}
{"id": "2510.10015", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.10015", "abs": "https://arxiv.org/abs/2510.10015", "authors": ["Jinhua Wu", "Yuting Wang", "Liukun Yu", "Linglong Meng"], "title": "End-to-end Compositional Verification of Program Safety through Verified and Verifying Compilation", "comment": null, "summary": "Program safety (i.e., absence of undefined behaviors) is critical for correct\noperation of computer systems. It is usually verified at the source level\n(e.g., by separation logics) and preserved to the target by verified compilers\n(e.g., CompCert), thereby achieving end-to-end verification of safety. However,\nmodern safe programming languages like Rust pose new problems in achieving\nend-to-end safety. Because not all functionalities can be implemented in the\nsafe language, mixing safe and unsafe modules is needed. Therefore, verified\ncompilation must preserve a modular notion of safety which can be composed at\nthe target level. Furthermore, certain classes of errors (e.g., memory errors)\nare automatically excluded by verifying compilation (e.g., borrow checking) for\nmodules written in safe languages. As a result, verified compilation needs to\ncooperate with verifying compilation to ensure end-to-end safety.\n  To address the above problems, we propose a modular and generic definition of\nsafety called open safety based on program semantics described as open labeled\ntransition systems (LTS). Open safety is composable at the boundary of modules\nand can be modularly preserved by verified compositional compilation. Those\nproperties enable separate verification of safety for heterogeneous modules and\ncomposition of the safety results at the target level. Open safety can be\ngeneralized to partial safety (i.e., only a certain class of errors can occur).\nBy this we formalized the correctness of verifying compilation as derivation of\ntotal safety from partial safety. We demonstrate how our framework can combine\nverified and verifying compilation by developing a verified compiler for an\nownership language (called Owlang) inspired by Rust. We evaluate our approach\non the compositional safety verification using a hash map implemented by Owlang\nand C.", "AI": {"tldr": "The paper tackles end-to-end safety verification in modern safe languages like Rust, proposing a new modular safety concept ('open safety') that supports compositional verification across safe and unsafe modules, and demonstrates its practicality with a prototype language and real-world example.", "motivation": "Modern safe programming languages like Rust require mixing safe and unsafe modules for full functionality, but current verification and compilation approaches struggle to ensure end-to-end safety in such modular, heterogeneous environments.", "method": "The authors introduce a modular and generic definition of safety\u2014called open safety\u2014framed by open labeled transition systems (LTS) for program semantics. This approach enables safety properties to be composable and modularly preserved through verified compositional compilation. The framework is demonstrated by developing a verified compiler for an ownership-based language (Owlang) and is evaluated using a hash map implemented in Owlang and C.", "result": "The proposed framework enables the separate verification of safety for heterogeneous modules and composes the safety results at the target (compiled) level. Open safety also allows for generalization to partial safety. The verified compiler for Owlang shows the practical applicability of their approach in compositional safety verification.", "conclusion": "The paper introduces open safety to support compositional, modular, and end-to-end safety verification for programs mixing safe and unsafe modules, bridging gaps between verified and verifying compilation approaches. The approach has been validated with a prototype compiler and use case involving Owlang and C."}}
{"id": "2510.10066", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.10066", "abs": "https://arxiv.org/abs/2510.10066", "authors": ["Shan Jiang", "Chenguang Zhu", "Sarfraz Khurshid"], "title": "OBsmith: Testing JavaScript Obfuscator using LLM-powered sketching", "comment": null, "summary": "JavaScript obfuscators are widely deployed to protect intellectual property\nand resist reverse engineering, yet their correctness has been largely\noverlooked compared to performance and resilience. Existing evaluations\ntypically measure resistance to deobfuscation, leaving the critical question of\nwhether obfuscators preserve program semantics unanswered. Incorrect\ntransformations can silently alter functionality, compromise reliability, and\nerode security-undermining the very purpose of obfuscation. To address this\ngap, we present OBsmith, a novel framework to systematically test JavaScript\nobfuscators using large language models (LLMs). OBsmith leverages LLMs to\ngenerate program sketches abstract templates capturing diverse language\nconstructs, idioms, and corner cases-which are instantiated into executable\nprograms and subjected to obfuscation under different configurations. Besides\nLLM-powered sketching, OBsmith also employs a second source: automatic\nextraction of sketches from real programs. This extraction path enables more\nfocused testing of project specific features and lets developers inject domain\nknowledge into the resulting test cases. OBsmith uncovers 11 previously unknown\ncorrectness bugs. Under an equal program budget, five general purpose\nstate-of-the-art JavaScript fuzzers (FuzzJIT, Jsfunfuzz, Superion, DIE,\nFuzzilli) failed to detect these issues, highlighting OBsmith's complementary\nfocus on obfuscation induced misbehavior. An ablation shows that all components\nexcept our generic MRs contribute to at least one bug class; the negative MR\nresult suggests the need for obfuscator-specific metamorphic relations. Our\nresults also seed discussion on how to balance obfuscation presets and\nperformance cost. We envision OBsmith as an important step towards automated\ntesting and quality assurance of obfuscators and other semantic-preserving\ntoolchains.", "AI": {"tldr": "OBsmith is a new LLM-powered framework to test if JavaScript obfuscators preserve code correctness. It found bugs that existing fuzzers missed, showing the need for obfuscator-focused correctness testing to ensure reliable protection of intellectual property.", "motivation": "JavaScript obfuscators are commonly used to protect code and intellectual property, but the correctness of these tools\u2014whether they preserve the meaning and function of the original code\u2014remains largely overlooked. Most evaluations focus on how well obfuscators resist deobfuscation rather than whether they change program behavior, which can undermine functionality and security.", "method": "The authors introduce OBsmith, a new framework that leverages large language models (LLMs) to automatically generate a wide variety of JavaScript program sketches that cover different constructs, idioms, and edge cases. These sketches are instantiated into executable programs for systematic testing of obfuscators. OBsmith also extracts real-world sketches from existing projects to enable more domain-focused testing. Programs are then obfuscated and analyzed across different configurations to detect semantic changes.", "result": "OBsmith identified 11 previously unknown correctness bugs in JavaScript obfuscators. State-of-the-art JavaScript fuzzers failed to detect these bugs, indicating that OBsmith offers complementary capabilities by targeting obfuscation-specific issues. An ablation study found that all system components (except one generic testing strategy) contributed to the detection of unique bug classes, but pointed out the need for more tailored metamorphic relations for different obfuscators.", "conclusion": "OBsmith demonstrates that current JavaScript obfuscators may introduce semantic errors, and its systematic approach using LLMs and extracted sketching provides a powerful testing methodology. The results emphasize the importance of correctness testing alongside resilience to deobfuscation and endorse OBsmith as a valuable tool for quality assurance of obfuscators and potentially other transformation tools."}}
{"id": "2510.10209", "categories": ["cs.PL", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10209", "abs": "https://arxiv.org/abs/2510.10209", "authors": ["Massinissa Merouani", "Afif Boudaoud", "Riyadh Baghdadi"], "title": "LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization", "comment": null, "summary": "The advancement of machine learning for compiler optimization, particularly\nwithin the polyhedral model, is constrained by the scarcity of large-scale,\npublic performance datasets. This data bottleneck forces researchers to\nundertake costly data generation campaigns, slowing down innovation and\nhindering reproducible research learned code optimization. To address this gap,\nwe introduce LOOPerSet, a new public dataset containing 28 million labeled data\npoints derived from 220,000 unique, synthetically generated polyhedral\nprograms. Each data point maps a program and a complex sequence of\nsemantics-preserving transformations (such as fusion, skewing, tiling, and\nparallelism)to a ground truth performance measurement (execution time). The\nscale and diversity of LOOPerSet make it a valuable resource for training and\nevaluating learned cost models, benchmarking new model architectures, and\nexploring the frontiers of automated polyhedral scheduling. The dataset is\nreleased under a permissive license to foster reproducible research and lower\nthe barrier to entry for data-driven compiler optimization.", "AI": {"tldr": "The paper introduces LOOPerSet, a massive and publicly available dataset for polyhedral compiler optimization research, enabling easier, reproducible, and data-driven advancements in the field.", "motivation": "The motivation for this paper is to address the lack of large-scale, public performance datasets in the area of machine learning-driven compiler optimization, especially within the polyhedral model. This scarcity hampers innovation and reproducibility in research.", "method": "The authors create and release LOOPerSet, a new public dataset that consists of 28 million labeled data points from 220,000 unique, synthetically generated polyhedral programs. For each program, a set of semantics-preserving transformation sequences is mapped to ground truth execution times.", "result": "LOOPerSet provides extensive, diverse data for training and evaluating cost models, benchmarking new architectures, and exploring automated polyhedral scheduling in compilers.", "conclusion": "LOOPerSet significantly contributes to the field of data-driven compiler optimization by providing a large, permissively licensed dataset, supporting reproducible research and lowering barriers for further innovations in polyhedral scheduling and optimization."}}
{"id": "2510.10081", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10081", "abs": "https://arxiv.org/abs/2510.10081", "authors": ["Youshuai Tan", "Zhanwei Zhang", "Zishuo Ding", "Lianyu Zheng", "Jinfu Chen", "Weiyi Shang"], "title": "A Mathematics-Guided Approach to Floating-Point Error Detection", "comment": null, "summary": "Floating-point program errors can lead to severe consequences, particularly\nin critical domains such as military applications. Only a small subset of\ninputs may induce substantial floating-point errors, prompting researchers to\ndevelop methods for identifying these error-inducing inputs. Although existing\napproaches have achieved some success, they still suffer from two major\nlimitations: (1) High computational cost: The evaluation of error magnitude for\ncandidate inputs relies on high-precision programs, which are prohibitively\ntime-consuming. (2) Limited long-range convergence capability: Current methods\nexhibit inefficiency in search, making the process akin to finding a needle in\na haystack.\n  To address these two limitations, we propose a novel method, named MGDE, to\ndetect error-inducing inputs based on mathematical guidance. By employing the\nNewton-Raphson method, which exhibits quadratic convergence properties, we\nachieve highly effective and efficient results. Since the goal of identifying\nerror-inducing inputs is to uncover the underlying bugs, we use the number of\nbugs detected in floating-point programs as the primary evaluation metric in\nour experiments. As FPCC represents the most effective state-of-the-art\napproach to date, we use it as the baseline for comparison. The dataset of FPCC\nconsists of 88 single-input floating-point programs. FPCC is able to detect 48\nbugs across 29 programs, whereas our method successfully identifies 89 bugs\nacross 44 programs. Moreover, FPCC takes 6.4096 times as long as our proposed\nmethod. We also deploy our method to multi-input programs, identifying a total\nof nine bugs with an average detection time of 0.6443 seconds per program. In\ncontrast, FPCC fails to detect any bugs while requiring an average computation\ntime of 100 seconds per program.", "AI": {"tldr": "A new method, MGDE, uses mathematical insight to detect floating-point software bugs far faster and more effectively than the existing best approach, identifying twice as many bugs in a fraction of the time.", "motivation": "Floating-point errors can cause critical failures, especially in high-stakes applications like military systems. Existing tools for finding such errors are slow and ineffective at finding hard-to-locate, error-inducing inputs.", "method": "The authors propose MGDE, a detection method based on mathematical guidance using the Newton-Raphson method for more efficient and effective identification of potentially disastrous floating-point errors.", "result": "MGDE outperforms the current best method, FPCC, by discovering almost twice as many bugs (89 vs. 48), affecting more programs (44 vs. 29), and operates over six times faster. On multi-input programs, MGDE found nine bugs in under a second each, while FPCC detected none and took 100 seconds per program.", "conclusion": "MGDE dramatically improves both the speed and efficacy of finding floating-point bugs, surpassing the state-of-the-art and demonstrating scalability to more complex, multi-input cases."}}
{"id": "2510.10216", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10216", "abs": "https://arxiv.org/abs/2510.10216", "authors": ["Zhechong Huang", "Zhao Zhang", "Ruyi Ji", "Tingxuan Xia", "Qihao Zhu", "Qinxiang Cao", "Zeyu Sun", "Yingfei Xiong"], "title": "Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis", "comment": null, "summary": "Language models have shown remarkable proficiency in code generation;\nnevertheless, ensuring type correctness remains a challenge. Although\ntraditional methods, such as constrained decoding, alleviate this problem by\nexternally rejecting untypable code, the model itself does not effectively\nlearn type reasoning internally, which ultimately limits its overall\nperformance. This paper introduces TyFlow, a novel system that internalizes\ntype reasoning within code generation to guide the model to learn the type\nsystem. The core of our approach is a novel type-guided program synthesis\nsystem that maintains an isomorphism between type derivation trees and\nsynthesis derivation trees, enabling a new code representation based on\nsynthesis decision sequences rather than traditional text-based token\nsequences. By offloading the complexity of type system learning to the\nrepresentation itself, models can redirect their computational resources toward\nhigher-level program semantics. Our evaluation shows that TyFlow not only\neliminates type errors but also significantly improves functional correctness,\nhighlighting the importance of aligning LMs with type systems internally.", "AI": {"tldr": "TyFlow helps language models generate code with fewer type errors by teaching them to reason about types internally, resulting in better and more correct code.", "motivation": "Despite strong code generation abilities of language models, ensuring type correctness is still challenging. Existing approaches only externally filter out type errors, and do not help the model learn type reasoning natively.", "method": "The paper introduces TyFlow, a system that integrates type reasoning directly into code generation. TyFlow uses a novel type-guided program synthesis technique, establishing an isomorphism between type derivation trees and synthesis derivation trees. This results in a new code representation guided by synthesis decisions instead of regular token sequences.", "result": "TyFlow eliminates type errors and significantly enhances the functional correctness of code generated by language models.", "conclusion": "Aligning language models with type systems internally, as with TyFlow, leads to better type correctness and functional quality in code generation. This approach allows language models to learn and use type reasoning as part of their generation process."}}
{"id": "2510.10119", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10119", "abs": "https://arxiv.org/abs/2510.10119", "authors": ["Liutong Han", "Zhiyuan Tan", "Hongbin Zhang", "Pengcheng Wang", "Chu Kang", "Mingjie Xing", "Yanjun Wu"], "title": "IntrinTrans: LLM-based Intrinsic Code Translator for RISC-V Vector", "comment": "9 pages", "summary": "The use of intrinsic functions to exploit hardware-specific capabilities is\nan important approach for optimizing library performance. Many mainstream\nlibraries implement a large number of vectorized algorithms on Arm or x86 SIMD\nintrinsic functions. With the rapid expansion of the RISC-V hardware-software\necosystem, there is a growing demand for support of the RISC-V Vector (RVV)\nextension. Translating existing vectorized intrinsic code onto RVV intrinsics\nis a practical and effective approach. However, current cross-architecture\ntranslation largely relies on manual rewriting, which is time-consuming and\nerror-prone. Furthermore, while some rule-based methods can reduce the need for\nmanual intervention, their translation success rate is limited by incomplete\nrule coverage and syntactic constraints, and the performance suffers from\ninadequate utilization of RVV-specific features. We present IntrinTrans, a\nLLM-based multi-agent approach that utilizes compile-and-test feedback to\ntranslate intrinsic code across architectures automatically, and further\noptimizes the generated RVV intrinsics using register-usage information derived\nfrom liveness analysis. To evaluate the effectiveness of our approach, we\ncollected 34 vectorized algorithm cases from open-source libraries. Each case\nincludes an Arm Neon intrinsics implementation and a RVV intrinsics\nimplementation contributed by the open-source community, together with\ncorrectness and performance tests. Our experiments show that advanced LLMs\nproduce semantically correct RISC-V Vector intrinsics in most cases within a\nlimited number of iterations, and in some cases achieve up to 5.93x the\nperformance of the native implementation from the open-source community.", "AI": {"tldr": "Manual translation of vectorized code to RISC-V is slow and limited. This paper presents IntrinTrans, an automated LLM-based system that translates and optimizes intrinsic code for RVV, achieving high correctness and up to 5.93x speedup over open-source versions.", "motivation": "The motivation is to address the growing need to support RISC-V Vector (RVV) extensions in software libraries as the RISC-V ecosystem expands. Current reliance on manual translation from Arm or x86 SIMD intrinsic code to RVV is time-consuming and error-prone, and rule-based methods are limited. Efficient and accurate automatic translation is in demand to leverage RVV hardware features and improve library performance.", "method": "The authors propose IntrinTrans, a multi-agent system based on large language models (LLMs) that automatically translates vectorized intrinsic code from one architecture to another (e.g., from Arm Neon to RVV). The approach incorporates compile-and-test feedback and optimizes RVV intrinsics by analyzing register usage through liveness analysis.", "result": "In experiments with 34 open-source vectorized algorithm cases, the system produced semantically correct RVV intrinsic code in most scenarios after a few iterations. Furthermore, the translated code in some cases achieved up to 5.93x higher performance compared to the native RVV implementations from the open-source community.", "conclusion": "IntrinTrans successfully automates the translation of intrinsic code across architectures and significantly optimizes RVV-specific performance, reducing reliance on manual rewriting and surpassing community implementations in both correctness and speed."}}
{"id": "2510.10219", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.10219", "abs": "https://arxiv.org/abs/2510.10219", "authors": ["Ruihao Li", "Lizy K. John", "Neeraja J. Yadwadkar"], "title": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc", "comment": null, "summary": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively.", "AI": {"tldr": "Exgen-Malloc is a new memory allocator tailored for single-threaded applications, delivering significant speed and memory savings compared to legacy allocators by cutting unnecessary complexity and leveraging modern design principles.", "motivation": "Memory allocators are crucial for application performance, but their inefficiencies at datacenter scale can result in significant costs and energy consumption. Most allocators are designed for multi-threaded environments and introduce complexity that may be unnecessary for single-threaded applications, which remain common.", "method": "The paper introduces Exgen-Malloc, a memory allocator specialized for single-threaded applications. The design eliminates unnecessary metadata, uses a centralized heap with one free-block list, and balances memory commitment and relocation. It also borrows principles from modern multi-threaded allocators to improve upon legacy single-threaded designs. The authors evaluate Exgen-Malloc on Intel Xeon platforms using SPEC CPU2017, redis-benchmark, and mimalloc-bench.", "result": "Exgen-Malloc achieved speedups of 1.17x, 1.10x, and 1.93x over dlmalloc and memory savings of 6.2%, 0.1%, and 25.2% over mimalloc, across various benchmarks.", "conclusion": "Exgen-Malloc provides a more efficient memory allocation solution for single-threaded applications, reducing both performance overhead and memory usage compared to existing allocators."}}
{"id": "2510.10148", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10148", "abs": "https://arxiv.org/abs/2510.10148", "authors": ["Mengyao Zhao", "Kaixuan Li", "Lyuye Zhang", "Wenjing Dang", "Chenggong Ding", "Sen Chen", "Zheli Liu"], "title": "A Systematic Study on Generating Web Vulnerability Proof-of-Concepts Using Large Language Models", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have brought remarkable\nprogress in code understanding and reasoning, creating new opportunities and\nraising new concerns for software security. Among many downstream tasks,\ngenerating Proof-of-Concept (PoC) exploits plays a central role in\nvulnerability reproduction, comprehension, and mitigation. While previous\nresearch has focused primarily on zero-day exploitation, the growing\navailability of rich public information accompanying disclosed CVEs leads to a\nnatural question: can LLMs effectively use this information to automatically\ngenerate valid PoCs? In this paper, we present the first empirical study of\nLLM-based PoC generation for web application vulnerabilities, focusing on the\npractical feasibility of leveraging publicly disclosed information. We evaluate\nGPT-4o and DeepSeek-R1 on 100 real-world and reproducible CVEs across three\nstages of vulnerability disclosure: (1) newly disclosed vulnerabilities with\nonly descriptions, (2) 1-day vulnerabilities with patches, and (3) N-day\nvulnerabilities with full contextual code. Our results show that LLMs can\nautomatically generate working PoCs in 8%-34% of cases using only public data,\nwith DeepSeek-R1 consistently outperforming GPT-4o. Further analysis shows that\nsupplementing code context improves success rates by 17%-20%, with\nfunction-level providing 9%-13% improvement than file-level ones. Further\nintegrating adaptive reasoning strategies to prompt refinement significantly\nimproves success rates to 68%-72%. Our findings suggest that LLMs could reshape\nvulnerability exploitation dynamics. To date, 23 newly generated PoCs have been\naccepted by NVD and Exploit DB.", "AI": {"tldr": "This paper studies how well LLMs can automatically create working exploit PoCs for web vulnerabilities using publicly disclosed data. With more code context and better prompting, success rates rise sharply, indicating LLMs could impact security dynamics. DeepSeek-R1 performs better than GPT-4o, and dozens of AI-generated PoCs have already been recognized by major databases.", "motivation": "Advances in LLMs have opened new possibilities for automated code exploitation, especially in generating PoC exploits, which are crucial for vulnerability reproduction and mitigation. With rich public information available for disclosed CVEs, the study investigates whether LLMs can leverage this data to automatically generate valid PoCs.", "method": "The paper empirically evaluates two LLMs (GPT-4o and DeepSeek-R1) on 100 real-world CVEs at different disclosure stages (description-only, patch-available, full code context). It measures PoC generation rates and analyzes the impact of supplied context (function-level, file-level) and adaptive prompt refinement.", "result": "LLMs can generate valid PoCs in 8%-34% of cases using only public information, with DeepSeek-R1 outperforming GPT-4o. Including code context increases success by 17%-20%, and prompt refinement strategy boosts rates up to 68%-72%. 23 generated PoCs were accepted by NVD/Exploit DB.", "conclusion": "LLM-based PoC generation is feasible with public CVE data and improves with more code context and advanced prompting. This capability could alter the landscape of vulnerability exploitation."}}
{"id": "2510.10410", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10410", "abs": "https://arxiv.org/abs/2510.10410", "authors": ["Hui Xu"], "title": "A Trace-based Approach for Code Safety Analysis", "comment": null, "summary": "Rust is a memory-safe programming language that disallows undefined behavior.\nIts safety guarantees have been extensively examined by the community through\nempirical studies, which has led to its remarkable success. However, unsafe\ncode remains a critical concern in Rust. By reviewing the safety design of Rust\nand analyzing real-world Rust projects, this paper establishes a systematic\nframework for understanding unsafe code and undefined behavior, and summarizes\nthe soundness criteria for Rust code. It further derives actionable guidance\nfor achieving sound encapsulation.", "AI": {"tldr": "This paper analyzes real-world Rust projects to build a framework for understanding unsafe code and undefined behavior, summarizes soundness criteria, and offers practical guidance for safely using and encapsulating unsafe code in Rust.", "motivation": "Rust offers strong memory safety guarantees by preventing undefined behavior, which has led to widespread adoption and success. However, the use of 'unsafe' code sections remains a concern, as they can potentially violate Rust's safety promises. This paper is motivated by the need to systematically understand and address the challenges posed by unsafe code in Rust.", "method": "The authors review Rust's safety design and conduct an analysis of real-world Rust projects. They establish a structured framework for understanding the concepts of unsafe code and undefined behavior within Rust, and synthesize criteria for code soundness based on this analysis.", "result": "The paper provides a systematic framework for interpreting unsafe code and undefined behavior in Rust. It also summarizes the soundness criteria that Rust code should meet, and offers practical guidance for safely encapsulating unsafe code.", "conclusion": "Unsafe code is a persistent issue in Rust that requires thorough scrutiny. The proposed framework and derived guidelines assist developers in achieving safe and robust encapsulation of unsafe code, contributing to overall program soundness."}}
{"id": "2510.10179", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10179", "abs": "https://arxiv.org/abs/2510.10179", "authors": ["Linghan Huang", "Peizhou Zhao", "Huaming Chen"], "title": "LLMs are All You Need? Improving Fuzz Testing for MOJO with Large Language Models", "comment": null, "summary": "The rapid development of large language models (LLMs) has revolutionized\nsoftware testing, particularly fuzz testing, by automating the generation of\ndiverse and effective test inputs. This advancement holds great promise for\nimproving software reliability. Meanwhile, the introduction of MOJO, a\nhigh-performance AI programming language blending Python's usability with the\nefficiency of C and C++, presents new opportunities to enhance AI model\nscalability and programmability. However, as a new language, MOJO lacks\ncomprehensive testing frameworks and a sufficient corpus for LLM-based testing,\nwhich exacerbates model hallucination. In this case, LLMs will generate\nsyntactically valid but semantically incorrect code, significantly reducing the\neffectiveness of fuzz testing. To address this challenge, we propose\nMOJOFuzzer, the first adaptive LLM-based fuzzing framework designed for\nzero-shot learning environments of emerging programming languages. MOJOFuzzer\nintegrates a mutil-phase framework that systematically eliminates low-quality\ngenerated inputs before execution, significantly improving test case validity.\nFurthermore, MOJOFuzzer dynamically adapts LLM prompts based on runtime\nfeedback for test case mutation, enabling an iterative learning process that\ncontinuously enhances fuzzing efficiency and bug detection performance. Our\nexperimental results demonstrate that MOJOFuzzer significantly enhances test\nvalidity, API coverage, and bug detection performance, outperforming\ntraditional fuzz testing and state-of-the-art LLM-based fuzzing approaches.\nUsing MOJOFuzzer, we have conducted a first large-scale fuzz testing evaluation\nof MOJO, uncorvering 13 previous unknown bugs. This study not only advances the\nfield of LLM-driven software testing but also establishes a foundational\nmethodology for leveraging LLMs in the testing of emerging programming\nlanguages.", "AI": {"tldr": "MOJOFuzzer, an adaptive LLM-based fuzzing framework, addresses challenges in testing the new MOJO language by filtering poor test inputs and using runtime feedback to improve LLM generation. It enhances test validity and bug detection, finding 13 new MOJO bugs, and sets the stage for robust LLM-driven testing in emerging programming languages.", "motivation": "Large language models (LLMs) have improved software testing automation, especially fuzz testing. However, the new MOJO programming language lacks adequate testing frameworks and a comprehensive code corpus. This deficiency causes model hallucination when LLMs generate MOJO code, resulting in semantically incorrect test cases that reduce fuzzing effectiveness.", "method": "The paper introduces MOJOFuzzer, an adaptive LLM-based fuzzing framework tailored for MOJO and similar emerging languages where zero-shot learning is required. MOJOFuzzer uses a multi-phase approach to filter out low-quality test inputs before execution and adaptively refines prompts using runtime feedback, enabling iterative learning.", "result": "Experiments show that MOJOFuzzer increases the validity of test cases, API coverage, and bug detection performance relative to both traditional fuzz testing and existing LLM-based fuzzers. It was used for a large-scale evaluation of MOJO, discovering 13 previously unknown bugs.", "conclusion": "MOJOFuzzer marks a significant advance in applying LLMs to software testing for new languages. It improves fuzzing efficiency and reliability and provides a foundational approach for LLM-based testing of future programming languages."}}
{"id": "2510.10517", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10517", "abs": "https://arxiv.org/abs/2510.10517", "authors": ["Su-Hyeon Kim", "Joonghyuk Hahn", "Sooyoung Cha", "Yo-Sub Han"], "title": "ECO: Enhanced Code Optimization via Performance-Aware Prompting for Code-LLMs", "comment": null, "summary": "Code runtime optimization-the task of rewriting a given code to a faster\none-remains challenging, as it requires reasoning about performance trade-offs\ninvolving algorithmic and structural choices. Recent approaches employ\ncode-LLMs with slow-fast code pairs provided as optimization guidance, but such\npair-based methods obscure the causal factors of performance gains and often\nlead to superficial pattern imitation rather than genuine performance\nreasoning. We introduce ECO, a performance-aware prompting framework for code\noptimization. ECO first distills runtime optimization instructions (ROIs) from\nreference slow-fast code pairs; Each ROI describes root causes of inefficiency\nand the rationales that drive performance improvements. For a given input code,\nECO in parallel employs (i) a symbolic advisor to produce a bottleneck\ndiagnosis tailored to the code, and (ii) an ROI retriever to return related\nROIs. These two outputs are then composed into a performance-aware prompt,\nproviding actionable guidance for code-LLMs. ECO's prompts are model-agnostic,\nrequire no fine-tuning, and can be easily prepended to any code-LLM prompt. Our\nempirical studies highlight that ECO prompting significantly improves\ncode-LLMs' ability to generate efficient code, achieving speedups of up to\n7.81x while minimizing correctness loss.", "AI": {"tldr": "ECO is a new performance-aware prompting framework for code optimization that guides code-LLMs to generate faster, efficient code by focusing on root causes of inefficiency, showing significant improvements without model fine-tuning.", "motivation": "Code runtime optimization is challenging because it requires understanding complex trade-offs and reasoning about performance rather than simply copying patterns from faster code versions. Existing code-LLM optimization methods often miss the underlying reasons for performance improvements.", "method": "ECO is introduced as a performance-aware prompting framework. It works by (1) distilling Runtime Optimization Instructions (ROIs) from slow-fast code pairs, explaining root causes of inefficiency and rationales for improvement; (2) using a symbolic advisor to diagnose code bottlenecks; (3) retrieving relevant ROIs; and (4) composing a model-agnostic, actionable prompt for code-LLMs without fine-tuning.", "result": "ECO prompting enables code-LLMs to generate more efficient code in empirical studies, yielding speedups up to 7.81x with low correctness loss.", "conclusion": "ECO provides a practical, model-agnostic, and effective approach to boost code-LLMs in generating optimized code, outperforming traditional pair-based optimization prompts by fostering real performance reasoning."}}
{"id": "2510.10290", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10290", "abs": "https://arxiv.org/abs/2510.10290", "authors": ["Sayan Mandal", "Hua Jiang"], "title": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in Enterprise Pipelines", "comment": "Submitted to MLSys 2026", "summary": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching.", "AI": {"tldr": "A modular, context-aware code review system using efficient LLM deployment achieves fast, accurate, and auditable remediation and explanation, reducing triage effort in compliance-driven environments.", "motivation": "Automated code review tools are underutilized in compliance-heavy contexts due to overwhelming output from static analyzers and the shortcomings of naive LLM implementations, such as hallucinations and high costs. There is a need for more accurate, grounded, and efficient automated review systems.", "method": "The paper introduces a production code review system that integrates static-analysis findings with context derived from AST (Abstract Syntax Tree) guided extraction. It features a lightweight on-demand serving stack (using quantized open-weight LLMs and multi-tier caching) to provide concise explanations and remediation advice. The architecture is modular, enabling independent adoption of prompting/grounding or serving layers.", "result": "On safety-focused C/C++ standards, the system delivers sub-minute median first feedback (59.8s for build and LLM) with competitive or improved violation reduction when compared to larger proprietary models. Surveyed internal users (n=8) noted reduced triage effort, moderate perceived grounding, and fewer review cycles.", "conclusion": "The proposed architecture effectively streamlines code review in compliance-oriented settings, balancing speed, cost, and output quality while offering modularity for incremental adoption. The authors detail operational lessons, stress reproducibility and auditability, and highlight remaining limitations and future directions for broader standard coverage and automated patching."}}
{"id": "2510.10531", "categories": ["cs.PL", "cs.DC", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10531", "abs": "https://arxiv.org/abs/2510.10531", "authors": ["Guillaume Ambal", "George Hodgkins", "Mark Madler", "Gregory Chockler", "Brijesh Dongol", "Joseph Izraelevitz", "Azalea Raad", "Viktor Vafeiadis"], "title": "A Verified High-Performance Composable Object Library for Remote Direct Memory Access (Extended Version)", "comment": null, "summary": "Remote Direct Memory Access (RDMA) is a memory technology that allows remote\ndevices to directly write to and read from each other's memory, bypassing\ncomponents such as the CPU and operating system. This enables low-latency\nhigh-throughput networking, as required for many modern data centres, HPC\napplications and AI/ML workloads. However, baseline RDMA comprises a highly\npermissive weak memory model that is difficult to use in practice and has only\nrecently been formalised. In this paper, we introduce the Library of Composable\nObjects (LOCO), a formally verified library for building multi-node objects on\nRDMA, filling the gap between shared memory and distributed system programming.\nLOCO objects are well-encapsulated and take advantage of the strong locality\nand the weak consistency characteristics of RDMA. They have performance\ncomparable to custom RDMA systems (e.g. distributed maps), but with a far\nsimpler programming model amenable to formal proofs of correctness. To support\nverification, we develop a novel modular declarative verification framework,\ncalled Mowgli, that is flexible enough to model multinode objects and is\nindependent of a memory consistency model. We instantiate Mowgli with the RDMA\nmemory model, and use it to verify correctness of LOCO libraries.", "AI": {"tldr": "The paper introduces LOCO, a formally verified library for building efficient, multi-node abstractions on RDMA. It leverages a new verification framework, Mowgli, to ensure correctness even with RDMA's challenging memory model, achieving simplicity and high performance.", "motivation": "RDMA offers high performance for distributed computing but suffers from a weak, permissive memory model that is difficult to use and has only recently been formalized. There is a gap in practical, verified tools to build reliable, high-level abstractions (objects) over RDMA, bridging shared memory and distributed system programming paradigms.", "method": "The paper introduces LOCO, a formally verified library for building multi-node objects on RDMA, and a novel modular declarative verification framework called Mowgli. Mowgli can model multi-node objects and is memory-consistency-model independent. It is instantiated with the RDMA memory model and used to verify LOCO.", "result": "LOCO provides well-encapsulated RDMA objects that leverage both locality and the weak consistency of RDMA. LOCO achieves performance comparable to custom RDMA systems, such as distributed maps, while being easier to program and formally verify for correctness. Mowgli successfully enables verification of LOCO even under RDMA\u2019s weak memory model.", "conclusion": "The paper demonstrates that formally verified, high-level, composable abstractions for RDMA are possible without sacrificing performance. Their approach fills the gap between shared memory and distributed system programming and provides a usable, modular, and verifiable foundation for RDMA-based applications."}}
{"id": "2510.10320", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10320", "abs": "https://arxiv.org/abs/2510.10320", "authors": ["Lorena Poenaru-Olaru", "Wouter van 't Hof", "Adrian Stando", "Arkadiusz P. Trawinski", "Eileen Kapel", "Jan S. Rellermeyer", "Luis Cruz", "Arie van Deursen"], "title": "Prepared for the Unknown: Adapting AIOps Capacity Forecasting Models to Data Changes", "comment": null, "summary": "Capacity management is critical for software organizations to allocate\nresources effectively and meet operational demands. An important step in\ncapacity management is predicting future resource needs often relies on\ndata-driven analytics and machine learning (ML) forecasting models, which\nrequire frequent retraining to stay relevant as data evolves. Continuously\nretraining the forecasting models can be expensive and difficult to scale,\nposing a challenge for engineering teams tasked with balancing accuracy and\nefficiency. Retraining only when the data changes appears to be a more\ncomputationally efficient alternative, but its impact on accuracy requires\nfurther investigation. In this work, we investigate the effects of retraining\ncapacity forecasting models for time series based on detected changes in the\ndata compared to periodic retraining. Our results show that drift-based\nretraining achieves comparable forecasting accuracy to periodic retraining in\nmost cases, making it a cost-effective strategy. However, in cases where data\nis changing rapidly, periodic retraining is still preferred to maximize the\nforecasting accuracy. These findings offer actionable insights for software\nteams to enhance forecasting systems, reducing retraining overhead while\nmaintaining robust performance.", "AI": {"tldr": "Retrain forecasting models based on data changes (drift-based) to save computational resources without losing much accuracy, except when data shifts rapidly\u2014then use regular (periodic) retraining for best results.", "motivation": "Software organizations need to predict future resource needs for effective capacity management. Traditional approaches require frequent model retraining, which is costly and hard to scale. There is a need to identify when retraining is necessary without sacrificing accuracy.", "method": "The paper compares two retraining strategies for time series forecasting models: drift-based retraining (triggered by data changes) versus periodic retraining (triggered at regular intervals). The impact of both methods on forecasting accuracy and computational cost is investigated empirically.", "result": "Drift-based retraining achieves forecasting accuracy comparable to periodic retraining in most cases, offering significant computational savings. In scenarios with rapidly changing data, periodic retraining remains superior in accuracy.", "conclusion": "Drift-based retraining is a cost-effective alternative without significant loss in accuracy for most cases, but periodic retraining should be used when data changes quickly. The findings can help software teams make more efficient retraining decisions for forecasting models."}}
{"id": "2510.11007", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.11007", "abs": "https://arxiv.org/abs/2510.11007", "authors": ["Antonina Nepeivoda", "Ilya Afanasyev"], "title": "Abstract String Domain Defined with Word Equations as a Reduced Product (Extended Version)", "comment": null, "summary": "We introduce a string-interval abstract domain, where string intervals are\ncharacterized by systems of word equations (encoding lower bounds on string\nvalues) and word disequalities (encoding upper bounds). Building upon the\nlattice structure of string intervals, we define an abstract string object as a\nreduced product on a string property semilattice, determined by\nlength-non-increasing morphisms. We consider several reduction strategies for\nabstract string objects and show that upon these strategies the string object\ndomain forms a lattice. We define basic abstract string operations on the\ndomain, aiming to minimize computational overheads on the reduction, and show\nhow the domain can be used to analyse properties of JavaScript string\nmanipulating programs.", "AI": {"tldr": "The paper presents a new abstract domain for static analysis of string-manipulating programs by modeling string values via intervals defined with word equations and disequalities. It defines a novel lattice-based structure and efficient operations, showing practical use in analyzing JavaScript string manipulations.", "motivation": "There is a need for precise and efficient static analysis methods for reasoning about string values in programs, particularly for languages like JavaScript where string manipulation is pervasive and complex.", "method": "The paper introduces a string-interval abstract domain, representing string intervals using word equations (for lower bounds) and word disequalities (for upper bounds). The authors define a reduced product construction on a string property semilattice using length-non-increasing morphisms. They explore several reduction strategies for these abstract string objects, analyze the lattice structure, and define abstract operations to support the analysis with minimal computational overhead.", "result": "They show that the proposed abstract string object domain, under their reduction strategies, forms a lattice. The basic abstract string operations are defined effectively to keep the reduction process computationally efficient. They demonstrate that the domain is applicable for analyzing properties of JavaScript string manipulating programs.", "conclusion": "The string-interval abstract domain provides a theoretically sound and computationally efficient framework for static analysis of programs with complex string operations, with demonstrated applicability to JavaScript."}}
{"id": "2510.10321", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.10321", "abs": "https://arxiv.org/abs/2510.10321", "authors": ["Jugal Gajjar", "Kaustik Ranaware", "Kamalasankari Subramaniakuppusamy"], "title": "Bridging Semantics & Structure for Software Vulnerability Detection using Hybrid Network Models", "comment": "13 pages, 3 figures, 5 tables, 14 equations, accepted at the 14th\n  International Conference on Complex Networks and Their Applications (COMPLEX\n  NETWORKS 2025) and the conference proceedings will be published by Springer\n  in the Studies in Computational Intelligence series", "summary": "Software vulnerabilities remain a persistent risk, yet static and dynamic\nanalyses often overlook structural dependencies that shape insecure behaviors.\nViewing programs as heterogeneous graphs, we capture control- and data-flow\nrelations as complex interaction networks. Our hybrid framework combines these\ngraph representations with light-weight (<4B) local LLMs, uniting topological\nfeatures with semantic reasoning while avoiding the cost and privacy concerns\nof large cloud models. Evaluated on Java vulnerability detection (binary\nclassification), our method achieves 93.57% accuracy-an 8.36% gain over Graph\nAttention Network-based embeddings and 17.81% over pretrained LLM baselines\nsuch as Qwen2.5 Coder 3B. Beyond accuracy, the approach extracts salient\nsubgraphs and generates natural language explanations, improving\ninterpretability for developers. These results pave the way for scalable,\nexplainable, and locally deployable tools that can shift vulnerability analysis\nfrom purely syntactic checks to deeper structural and semantic insights,\nfacilitating broader adoption in real-world secure software development.", "AI": {"tldr": "The paper introduces a method that models code as graphs and uses efficient, local LLMs to jointly analyze structure and meaning for vulnerability detection. This approach achieves high accuracy, surpasses existing methods, and explains its findings, making it both practical and interpretable for secure software development.", "motivation": "Traditional static and dynamic analysis methods for software vulnerability detection frequently miss the deeper structural dependencies and interactions in code that underpin insecure behavior. There is a need for techniques that can combine structural and semantic reasoning while being lightweight and privacy-preserving without relying on large cloud-based models.", "method": "The authors propose a hybrid framework that models software as heterogeneous graphs to capture control- and data-flow dependencies. This representation is combined with small, locally run LLMs (less than 4 billion parameters) to integrate topological graph features with semantic analysis, allowing accurate and private vulnerability detection without expensive cloud resources.", "result": "The proposed method achieves 93.57% accuracy in Java vulnerability detection, outperforming existing baselines by 8.36% over Graph Attention Network embeddings and 17.81% over pretrained large language models (LLMs) like Qwen2.5 Coder 3B. Additionally, the approach enhances interpretability by extracting significant subgraphs and producing natural language explanations for developers.", "conclusion": "This novel framework demonstrates that combining graph-based structural analysis with efficient, local LLMs enables scalable, explainable, and privacy-preserving tools for software vulnerability detection. It paves the way for moving beyond simple syntactic checks toward deeper semantic understanding, fostering broader adoption in secure software engineering."}}
{"id": "2510.11420", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11420", "abs": "https://arxiv.org/abs/2510.11420", "authors": ["Mark Koch", "Agust\u00edn Borgna", "Seyon Sivarajah", "Alan Lawrence", "Alec Edgington", "Douglas Wilson", "Craig Roy", "Luca Mondada", "Lukas Heidemann", "Ross Duncan"], "title": "HUGR: A Quantum-Classical Intermediate Representation", "comment": "8 pages, extended abstract submitted to PlanQC25", "summary": "We introduce the Hierarchical Unified Graph Representation (HUGR): a novel\ngraph based intermediate representation for mixed quantum-classical programs.\nHUGR's design features high expressivity and extensibility to capture the\ncapabilities of near-term and forthcoming quantum computing devices, as well as\nnew and evolving abstractions from novel quantum programming paradigms. The\ngraph based structure is machine-friendly and supports powerful pattern\nmatching based compilation techniques. Inspired by MLIR, HUGR's extensibility\nfurther allows compilation tooling to reason about programs at multiple levels\nof abstraction, lowering smoothly between them. Safety guarantees in the\nstructure including strict, static typing and linear quantum types allow rapid\ndevelopment of compilation tooling without fear of program invalidation. A full\nspecification of HUGR and reference implementation are open-source and\navailable online.", "AI": {"tldr": "HUGR is a new, open graph-based IR for quantum-classical programming, designed for extensibility, safety, and powerful compilation, ensuring adaptability to evolving quantum technologies and paradigms.", "motivation": "The motivation is to provide a new intermediate representation (IR) capable of supporting modern quantum-classical programming needs, accommodating the evolving abstractions and requirements of current and future quantum hardware.", "method": "The authors introduce Hierarchical Unified Graph Representation (HUGR), a graph-based IR designed for mixed quantum-classical programs. It borrows ideas from MLIR, focusing on extensibility, safety, and powerful compilation through pattern matching. The design includes strict, static typing and linear quantum types to ensure program safety.", "result": "HUGR is shown to be highly expressive and extensible, supporting various abstraction levels and easing the development of compilation tools. Safety is enhanced through typing rules, and a reference implementation is openly available.", "conclusion": "HUGR provides a promising solution for the intermediate representation of quantum-classical programs, bridging the gap between current compilation needs and the challenges posed by new quantum programming paradigms."}}
{"id": "2510.10460", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10460", "abs": "https://arxiv.org/abs/2510.10460", "authors": ["Zongyi Lyu", "Songqiang Chen", "Zhenlan Ji", "Liwen Wang", "Shuai Wang", "Daoyuan Wu", "Wenxuan Wang", "Shing-Chi Cheung"], "title": "Testing and Enhancing Multi-Agent Systems for Robust Code Generation", "comment": "19pages, 5 figures", "summary": "Multi-agent systems (MASs) have emerged as a promising paradigm for automated\ncode generation, demonstrating impressive performance on established benchmarks\nby decomposing complex coding tasks across specialized agents with different\nroles. Despite their prosperous development and adoption, their robustness\nremains pressingly under-explored, raising critical concerns for real-world\ndeployment. This paper presents the first comprehensive study examining the\nrobustness of MASs for code generation through a fuzzing-based testing\napproach. By designing a fuzzing pipeline incorporating semantic-preserving\nmutation operators and a novel fitness function, we assess mainstream MASs\nacross multiple datasets and LLMs. Our findings reveal substantial robustness\nflaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they\ninitially resolved successfully after applying the semantic-preserving\nmutations. Through comprehensive failure analysis, we identify a common yet\nlargely overlooked cause of the robustness issue: miscommunications between\nplanning and coding agents, where plans lack sufficient detail and coding\nagents misinterpret intricate logic, aligning with the challenges inherent in a\nmulti-stage information transformation process. Accordingly, we also propose a\nrepairing method that encompasses multi-prompt generation and introduces a new\nmonitor agent to address this issue. Evaluation shows that our repairing method\neffectively enhances the robustness of MASs by solving 40.0%-88.9% of\nidentified failures. Our work uncovers critical robustness flaws in MASs and\nprovides effective mitigation strategies, contributing essential insights for\ndeveloping more reliable MASs for code generation.", "AI": {"tldr": "Despite strong performance, multi-agent systems for code generation have critical robustness issues, particularly due to miscommunication between agents. Using fuzzing-based testing, the authors show these systems often fail on mutated tasks. They propose an effective repair method that adds a monitor agent and multi-prompt generation, improving reliability. This research highlights overlooked weaknesses and proposes improvements for trustworthy MAS-based code generation.", "motivation": "While multi-agent systems (MASs) have shown significant progress in automated code generation, their robustness\u2014important for real-world applications\u2014remains insufficiently studied. This paper is motivated by the need to understand and improve the reliability of MASs when deployed in practical coding tasks.", "method": "The study uses a fuzzing-based testing approach employing semantic-preserving mutation operators and a novel fitness function. This methodology is applied across several mainstream MASs, different datasets, and language models. Additionally, the authors propose a repairing method that uses multi-prompt generation and a new monitor agent to address identified failures.", "result": "The results demonstrate that popular MASs display substantial robustness flaws, as they fail to solve 7.9%-83.3% of previously solved problems after semantic-preserving mutations are applied. Comprehensive analysis points to miscommunication between planning and coding agents as a major cause. The proposed repairing method improves system robustness, resolving 40.0%-88.9% of previously identified failures.", "conclusion": "This work identifies significant robustness gaps in current MASs for code generation and offers an effective mitigation strategy. Introducing a new monitor agent and multi-prompt generation substantially enhances MAS robustness, providing valuable directions for future development of reliable code generation systems."}}
{"id": "2510.11573", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.11573", "abs": "https://arxiv.org/abs/2510.11573", "authors": ["Santiago Arranz-Olmos", "Gilles Barthe", "Lionel Blatter", "Xingyu Xie", "Zhiyuan Zhang"], "title": "(Dis)Proving Spectre Security with Speculation-Passing Style", "comment": null, "summary": "Constant-time (CT) verification tools are commonly used for detecting\npotential side-channel vulnerabilities in cryptographic libraries. Recently, a\nnew class of tools, called speculative constant-time (SCT) tools, has also been\nused for detecting potential Spectre vulnerabilities. In many cases, these SCT\ntools have emerged as liftings of CT tools. However, these liftings are seldom\ndefined precisely and are almost never analyzed formally. The goal of this\npaper is to address this gap, by developing formal foundations for these\nliftings, and to demonstrate that these foundations can yield practical\nbenefits.\n  Concretely, we introduce a program transformation, coined Speculation-Passing\nStyle (SPS), for reducing SCT verification to CT verification. Essentially, the\ntransformation instruments the program with a new input that corresponds to\nattacker-controlled predictions and modifies the program to follow them. This\napproach is sound and complete, in the sense that a program is SCT if and only\nif its SPS transform is CT. Thus, we can leverage existing CT verification\ntools to prove SCT; we illustrate this by combining SPS with three standard\nmethodologies for CT verification, namely reducing it to non-interference,\nassertion safety and dynamic taint analysis. We realize these combinations with\nthree existing tools, EasyCrypt, BINSEC, and ctgrind, and we evaluate them on\nKocher's benchmarks for Spectre-v1. Our results focus on Spectre-v1 in the\nstandard CT leakage model; however, we also discuss applications of our method\nto other variants of Spectre and other leakage models.", "AI": {"tldr": "The paper provides a formal and practical method (Speculation-Passing Style) for verifying speculative constant-time security using existing constant-time verification tools, showing soundness and completeness, and demonstrating effectiveness on Spectre benchmarks.", "motivation": "Speculative constant-time (SCT) analysis is crucial for detecting Spectre vulnerabilities in cryptographic software, but existing SCT tools are mostly informal extensions of constant-time (CT) verification tools with little formal foundation or analysis.", "method": "The paper introduces the Speculation-Passing Style (SPS) program transformation that converts SCT verification problems into CT verification problems. SPS instruments programs to model attacker-controlled predictions, allowing existing CT verification methodologies to cover SCT. The approach is combined with tools such as EasyCrypt, BINSEC, and ctgrind and tested on Spectre-v1 benchmarks.", "result": "The SPS transformation is shown to be both sound and complete: SCT in the original program exactly matches CT in the transformed program. This allows existing CT verification tools to be repurposed for SCT. The integrations are demonstrated using EasyCrypt, BINSEC, and ctgrind on Spectre-v1 benchmarks, with discussions of broader applicability to other Spectre variants and leakage models.", "conclusion": "Formalizing the lifting from CT to SCT tool verification via SPS enables practical, precise, and reusable analyses of speculative vulnerabilities, using established CT methods and tools. This advances both the theory and practice of cryptographic side-channel security."}}
