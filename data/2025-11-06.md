<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation](https://arxiv.org/abs/2511.02854)
*Yixiang Chen,Tianshi Zheng,Shijue Huang,Zhitao He,Yi R. Fung*

Main category: cs.SE

TL;DR: SELF-REDRAFT improves LLM code generation by balancing solution refinement and exploration at test time, but further progress depends on better feedback and judgment abilities in LLMs.


<details>
  <summary>Details</summary>
Motivation: Real-world code generation often lacks interpreter feedback, making test-time scaling crucial. Existing methods either exploit known solutions or explore new ones, but rarely balance both.

Method: The paper proposes SELF-REDRAFT, a framework building on Self-Refine, which encourages LLMs to generate new solutions when current drafts are clearly flawed. This investigates how well LLMs can balance between exploiting known good solutions and exploring new possibilities.

Result: SELF-REDRAFT outperforms Self-Refine in performance when limited to the same iteration count. However, it is limited by the LLMs’ ability to generate useful feedback and distinguish between good and bad solutions. The strategy for balancing exploration and exploitation differs among various LLM models.

Conclusion: SELF-REDRAFT establishes a baseline for LLMs' intrinsic balance between exploration and exploitation in test-time scaling, but improving feedback and judgment abilities is needed for further progress.

Abstract: Test-time scaling without interpreter feedback is essential for real-world
code generation scenarios where test cases are not readily available. While
existing paradigms often rely on either greedy exploitation (i.e., iterative
refinement) or stochastic exploration (i.e., relying on sample-based voting or
reranking mechanisms), the balance between these two dimensions remains
underexplored. To investigate the LLM's intrinsic ability to balance
exploitation and exploration, we introduce SELF-REDRAFT, a framework built upon
Self-Refine that encourages the model to propose new drafts for solutions that
are fundamentally flawed. Our results show that SELF-REDRAFT consistently
achieves better performance than Self-Refine when converged under the same
maximum number of iterations. Still, we observe that significant room for
improvement remains, largely due to two core aspects of current self-redraft
capabilities: constrained capacity for generating instructive feedback and
fragile discriminative judgment. We also find that balancing strategies vary
notably across different LLMs, reflecting distinct, model-specific behaviors.
Overall, our study establishes a baseline for intrinsic
exploration-exploitation balancing in test-time scaling and identifies feedback
and discrimination as key areas with potential for future advances.

</details>


### [2] [The Evolution of Agile and Hybrid Project Management Methodologies: A Systematic Literature Review](https://arxiv.org/abs/2511.02859)
*Bianca Leech,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: This review analyzes how IT project management has shifted from agile to hybrid models, identifies why hybrids are used, outlines implementation challenges, and gives practical advice for organizations transitioning to hybrid approaches.


<details>
  <summary>Details</summary>
Motivation: With the fast-changing nature of IT projects, there is a need to understand how project management approaches have evolved from traditional to agile and now to hybrid models, and the factors driving these changes.

Method: A systematic literature review was conducted, using PRISMA guidelines, to analyze recent (last 8 years) peer-reviewed studies regarding the evolution, challenges, and success factors of agile-to-hybrid project management methodologies.

Result: Hybrid project management methodologies have developed primarily because of the limitations of pure agile approaches in large-scale or regulated settings. Success with hybrids depends on leadership support, context-driven process customization, and mechanisms for continuous improvement.

Conclusion: Organizations should focus on adapting project methodologies to their specific contexts rather than adhering strictly to a single framework. Practical guidance is provided for managing the transition to hybrid methods, emphasizing flexibility and tailored integration over rigidity.

Abstract: The rapid evolution of IT projects has driven the transformation of project
management methodologies, from traditional waterfall approaches to agile
frameworks and, more recently, hybrid models. This systematic literature review
investigates the evolution of agile methodologies into hybrid frameworks,
analysing their implementation challenges and success factors. We identify key
trends through PRISMA-guided analysis of peer-reviewed studies from the last 8
years. Hybrid methodologies emerge from agile limitations in large-scale and
regulated environments, combining iterative flexibility with structured
governance. Agile has several implementation challenges, leading to hybrid
methods, and the success hinges on leadership support, tailored process
integration, and continuous improvement mechanisms. The study explores the need
for contextual adaptation over rigid frameworks, offering practical insights
for organisations navigating hybrid transitions.

</details>


### [3] [LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models](https://arxiv.org/abs/2511.02866)
*Ahmad Tahmasivand,Noureldin Zahran,Saba Al-Sayouri,Mohammed Fouda,Khaled N. Khasawneh*

Main category: cs.SE

TL;DR: LM-Fix is a fast, lightweight method to detect and quickly fix faults in large language models, detecting most bit-flip errors with minimal overhead and vastly accelerating recovery compared to full reloads.


<details>
  <summary>Details</summary>
Motivation: Ensuring the reliability and integrity of large language models (LLMs) in production is a significant challenge due to their size and complexity. Traditional fault detection and recovery methods are often too resource-intensive or slow for modern LLMs, necessitating more efficient solutions.

Method: The paper introduces LM-Fix, a lightweight framework for fault detection and rapid recovery in LLMs. LM-Fix employs a short test-vector pass and hash-guided checks to efficiently identify bit-flip faults, and offers localized repair strategies without the need for a complete model reload.

Result: LM-Fix is able to detect more than 94% of single-bit flip faults and nearly 100% of multi-bit flip faults in various models using a test vector length (TVL) of 200. The overhead on runtime is low, ranging from approximately 1% to 7.7%. Recovery through LM-Fix is more than 100 times faster than a full model reload.

Conclusion: LM-Fix provides a practical, low-overhead solution for maintaining the reliability of LLMs in production by enabling efficient fault detection and rapid recovery, significantly reducing downtime and resource consumption compared to traditional approaches.

Abstract: This paper presents LM-Fix, a lightweight detection and rapid recovery
framework for faults in large language models (LLMs). Existing integrity
approaches are often heavy or slow for modern LLMs. LM-Fix runs a short
test-vector pass and uses hash-guided checks to detect bit-flip faults, then
repairs them locally without a full reload. Across multiple models, it detects
over 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with
approximately 1% to 7.7% runtime overhead; recovery is more than 100x faster
than reloading. These results show a practical, low-overhead solution to keep
LLMs reliable in production

</details>


### [4] [Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models](https://arxiv.org/abs/2511.02869)
*Amirreza Esmaeili,Fahd Seddik,Yongyi Ji,Fatemeh Fard,Fuxiang Chen*

Main category: cs.SE

TL;DR: This paper analyzes the performance of a novel fine-tuning method, AdvFusion, on large code language models for various programming tasks. Results show that its effectiveness depends on the specific task and model, with no one-size-fits-all solution for parameter-efficient fine-tuning in code-related applications.


<details>
  <summary>Details</summary>
Motivation: Programming languages have varying strengths, and language models can transfer knowledge across languages for software engineering tasks. There is a need to further evaluate and improve methods for multilingual adaptation on a wider range of software engineering problems.

Method: The authors previously proposed AdvFusion, a Parameter Efficient Fine-Tuning (PEFT) method that learns from multiple programming languages before focusing on the target task. In this study, they extend AdvFusion to Code Large Language Models (Code-LLMs) and evaluate its effectiveness on three new tasks: code generation, code translation, and commit message generation, comparing it with AdapterFusion and other PEFT approaches like LoRA, Compacter, and TaskAdapter.

Result: AdvFusion was superior to AdapterFusion for code generation but not as strong as LoRA, Compacter, and TaskAdapter. For commit message generation, AdapterFusion outperformed AdvFusion, and other PEFT methods did not surpass their performance. In code translation, AdapterFusion did better than AdvFusion, with the gap increasing for larger models, but other PEFT methods again outperformed them.

Conclusion: The effectiveness of AdvFusion and other PEFT architectures varies by task and model type in Code-LLMs. There is no universally superior method; instead, the optimal approach depends on the specific software engineering task and model.

Abstract: Programming languages can benefit from one another by utilizing a language
model for software engineering tasks. Full fine-tuning and Parameter Efficient
Fine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for
multilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims
to enhance task performance by leveraging information from multiple programming
languages, but primarily focuses on the target programming language.
  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that
effectively learns from other programming languages before adapting to the
target task. Though previous experiments showed that AdvFusion outperformed
AdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited
to only two tasks, code summarization and method name prediction. In this
study, we expanded our work and investigated AdvFusion on Code Large Language
Models (Code-LLMs), considering three new tasks: code generation, code
translation, and commit message generation. We observed that different
Code-LLMs/tasks exhibit different characteristics. In code generation,
AdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,
Compacter, and TaskAdapter). In commit message generation, AdapterFusion
performed better than AdvFusion, and contrary to code generation, we found that
the other PEFT methods do not have better performance. In code translation,
AdvFusion performed worse than AdapterFusion overall, with the performance gap
marginally widening as the model size increases. However, consistent with code
generation, other PEFT methods showed better performance.

</details>


### [5] [An Analysis of Early-Stage Functional Safety Analysis Methods and Their Integration into Model-Based Systems Engineering](https://arxiv.org/abs/2511.02874)
*Jannatul Shefa,Taylan G. Topcu*

Main category: cs.SE

TL;DR: This paper compares major safety analysis techniques (FMEA, FHA, FFIP), highlighting FFIP's strengths for complex systems and reviewing how these techniques are incorporated into MBSE. Integration efforts mainly focus on FMEA and lack standardization, revealing the need for a unified MBSE safety analysis framework.


<details>
  <summary>Details</summary>
Motivation: With system complexity growing, early and effective safety analysis is needed to identify and address risks before they escalate, particularly within the context of Model-Based Systems Engineering (MBSE).

Method: The paper adopts a two-phase approach: (1) a comparative analysis of FMEA, FHA, and FFIP safety analysis techniques with documentation of their strengths and limitations; (2) a literature review examining efforts to integrate these techniques into MBSE, with categorization of FMEA integration methods.

Result: FFIP is found to be particularly capable in identifying emergent behaviors, second-order effects, and fault propagation—making it well-suited to modern complex systems. Most MBSE integration efforts focus on FMEA, with FHA and FFIP integrations still nascent. FMEA-MBSE integration is organized into four categories (model transformation, external algorithms, built-in packages, manual diagramming).

Conclusion: There is no universally accepted framework for integrating safety analysis techniques into MBSE, indicating an opportunity for developing a more unified approach that supports digital engineering and lifecycle safety management.

Abstract: As systems become increasingly complex, conducting effective safety analysis
in the earlier phases of a system's lifecycle is essential to identify and
mitigate risks before they escalate. To that end, this paper investigates the
capabilities of key safety analysis techniques, namely: Failure Mode and
Effects Analysis (FMEA), Functional Hazard Analysis (FHA), and Functional
Failure Identification and Propagation (FFIP), along with the current state of
the literature in terms of their integration into Model-Based Systems
Engineering (MBSE). A two-phase approach is adopted. The first phase is focused
on contrasting FMEA, FHA, and FFIP techniques, examining their procedures,
along with a documentation of their relative strengths and limitations. Our
analysis highlights FFIP's capability in identifying emergent system behaviors,
second-order effects, and fault propagation; thus, suggesting it is better
suited for the safety needs of modern interconnected systems. Second, we review
the existing research on the efforts to integrate each of these methods into
MBSE. We find that MBSE integration efforts primarily focus on FMEA, and
integration of FHA and FFIP is nascent. Additionally, FMEA-MBSE integration
efforts could be organized into four categories: model-to-model transformation,
use of external customized algorithms, built-in MBSE packages, and manual use
of standard MBSE diagrams. While our findings indicate a variety of MBSE
integration approaches, there is no universally established framework or
standard. This leaves room for an integration approach that could support the
ongoing Digital Engineering transformation efforts by enabling a more
synergistic lifecycle safety management methods and tools.

</details>


### [6] [CS Educator challenges and their solutions : A systematic mapping study](https://arxiv.org/abs/2511.02876)
*Anjali Chouhan,Sruti Srinivasa Ragavan,Amey Karkare*

Main category: cs.SE

TL;DR: A structured review of recent literature reveals ongoing challenges and strategies in CS education across ten themes, pointing out both well-addressed and neglected areas, with practical insights for improving teacher support and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Computer Science (CS) education is growing fast, yet educators still encounter persistent obstacles. There's a lack of systematic research categorizing these challenges and the remedies used, making it difficult to know which issues have been addressed and which remain under-explored.

Method: The authors conducted a structured literature review, analyzing peer-reviewed research papers from the last five years. They focused on challenges and remedies within ten categories, such as pedagogical, emotional, technological, and institutional aspects.

Result: The study found recurring challenges in assessment practices, teacher training, classroom management, and emotional well-being. It also identified strategies like professional development programs and policy interventions, while highlighting several under-researched areas.

Conclusion: This review synthesizes recent research, offering a comprehensive overview of issues and responses in CS education, and serves as a resource for researchers, curriculum designers, and policymakers to enhance teaching and support for educators.

Abstract: Computer Science (CS) education is expanding rapidly, but educators continue
to face persistent challenges in teaching and learning environments.Despite
growing interest, limited systematic work exists to categorize and synthesize
the specific challenges faced by CS educators and the remedies adopted in
response.This is problematic because it remains unclear which areas have been
thoroughly addressed and which still lack sufficient scholarly attention. In
this study, we conducted a structured literature review of peer-reviewed
research papers published over the last five years, focusing on challenges and
remedies across ten categorized themes, including pedagogical, emotional,
technological, and institutional dimensions.Our analysis revealed recurring
issues in areas such as assessment practices, teacher training, classroom
management, and emotional well-being, along with various strategies such as
professional development programs and policy interventions adopted to mitigate
them while also revealing several areas that have received insufficient
attention.This review offers a consolidated understanding of the CS education
landscape, providing valuable insights for researchers, curriculum designers,
and policymakers aiming to improve teaching effectiveness and educator support.

</details>


### [7] [AgentSLA : Towards a Service Level Agreement for AI Agents](https://arxiv.org/abs/2511.02885)
*Gwendal Jouneaux,Jordi Cabot*

Main category: cs.SE

TL;DR: This paper tackles the lack of standardized ways to define quality and SLAs for AI agents. It presents a new quality model based on ISO/IEC 25010 and a domain-specific language to help specify service agreements, aiming to improve quality assurance in systems using AI agents.


<details>
  <summary>Details</summary>
Motivation: AI agents are becoming essential for smart software systems, transitioning from simple LLM integrations to more autonomous agents. However, defining and assuring quality (Quality of Service and Service Level Agreements) for these agents is difficult and lacks standardized approaches, creating challenges for software developers.

Method: The paper proposes a quality model for AI agents grounded in the ISO/IEC 25010 standard. Additionally, it introduces a domain-specific language (DSL) to help specify SLAs for the services these AI agents provide.

Result: A structured quality model tailored to AI agents and a newly developed DSL for SLA specification are presented, aiming to improve quality assurance in systems involving AI agents.

Conclusion: The proposed quality model and DSL provide a foundation for better defining and managing the quality and SLAs of AI agent-based services, addressing a major gap in current software engineering practices for intelligent systems.

Abstract: AI components are increasingly becoming a key element of all types of
software systems to enhance their functionality. These AI components are often
implemented as AI Agents, offering more autonomy than a plain integration of
Large Language Models (LLMs), moving from a Model-as-a-Service paradigm to an
Agent-as-a-Service one, bringing new challenges to the development of smart
software systems. Indeed, while support for the design, implementation, and
deployment of those agents exist, the specification of Quality of Service (QoS)
and definition of Service Level Agreements (SLAs) aspects for those agents,
important to ensure the quality of the resulting systems, remains an open
challenge. Part of this is due to the difficulty to clearly define quality in
the context of AI components, resulting in a lack of consensus on how to best
approach Quality Assurance (QA) for these types of systems. To address this
challenge, this paper proposes both a quality model for AI agents based on the
ISO/IEC 25010 standard, and a domain specific language to support the
definition of SLAs for the services provided by these AI agents.

</details>


### [8] [Comprehension-Performance Gap in GenAI-Assisted Brownfield Programming: A Replication and Extension](https://arxiv.org/abs/2511.02922)
*Yunhan Qiao,Christopher Hundhausen,Summit Haque,Md Istiak Hossain Shihab*

Main category: cs.SE

TL;DR: Using AI coding assistants like Copilot helps developers work faster and pass more tests on legacy code, but doesn't actually help them understand the code better. This suggests a gap between doing more tasks and really comprehending the code.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to understand how generative AI coding assistants, like GitHub Copilot, affect code comprehension during brownfield programming tasks, i.e., working with and modifying legacy code.

Method: A within-subjects experimental study was conducted with 18 computer science graduate students. Each participant completed feature implementation tasks on legacy code with and without the assistance of Copilot. Performance and comprehension were both measured.

Result: Results showed that using Copilot significantly reduced the time spent on tasks and increased the number of test cases passed. However, comprehension scores did not improve when using Copilot, and there was no correlation found between task performance and comprehension.

Conclusion: GenAI tools like Copilot can speed up programming in legacy code by increasing productivity and performance. However, this progress does not translate into better comprehension of legacy code. This points to a comprehension-performance gap, which has implications for programming education and tool design.

Abstract: Code comprehension is essential for brownfield programming tasks, in which
developers maintain and enhance legacy code bases. Generative AI (GenAI) coding
assistants such as GitHub Copilot have been shown to improve developer
productivity, but their impact on code understanding is less clear. We
replicate and extend a previous study by exploring both performance and
comprehension in GenAI-assisted brownfield programming tasks. In a
within-subjects experimental study, 18 computer science graduate students
completed feature implementation tasks with and without Copilot. Results show
that Copilot significantly reduced task time and increased the number of test
cases passed. However, comprehension scores did not differ across conditions,
revealing a comprehension-performance gap: participants passed more test cases
with Copilot, but did not demonstrate greater understanding of the legacy
codebase. Moreover, we failed to find a correlation between comprehension and
task performance. These findings suggest that while GenAI tools can accelerate
programming progress in a legacy codebase, such progress may come without an
improved understanding of that codebase. We consider the implications of these
findings for programming education and GenAI tool design.

</details>


### [9] [Risk Estimation in Differential Fuzzing via Extreme Value Theory](https://arxiv.org/abs/2511.02927)
*Rafael Baez,Alejandro Olivas,Nathan K. Diamond,Marcelo Frias,Yannic Noller,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: This paper shows that using Extreme Value Theory (EVT) helps better assess and manage the risk of undetected bugs in differential fuzzing, outperforming traditional statistical methods and leading to significant efficiency improvements in real-world software analysis.


<details>
  <summary>Details</summary>
Motivation: Differential fuzzing is effective in discovering bugs, but traditional dynamic analysis methods offer no assurance about bugs that may go undetected in a fuzzing campaign, creating risk in software assurance.

Method: The paper applies Extreme Value Theory (EVT), a statistical approach for analyzing extreme values, to model the distribution of maximum observed differences in differential fuzzing. Through experiments on real-world Java libraries, the study tests the feasibility of EVT, tunes hyperparameters, and compares EVT-based extrapolation to other statistical techniques such as Markov's and Chebyshev's inequalities and the Bayes factor.

Result: EVT-based extrapolation outperforms baseline statistical techniques in 14.3% of cases and ties in 64.2% of cases. The EVT-enabled differential fuzzing approach achieves notable efficiency: saving tens of millions of bytecode executions via earlier stopping.

Conclusion: EVT provides a promising framework for quantifying and minimizing the risk of missing bugs in differential fuzzing campaigns, while enabling substantial computational savings.

Abstract: Differential testing is a highly effective technique for automatically
detecting software bugs and vulnerabilities when the specifications involve an
analysis over multiple executions simultaneously. Differential fuzzing, in
particular, operates as a guided randomized search, aiming to find (similar)
inputs that lead to a maximum difference in software outputs or their
behaviors. However, fuzzing, as a dynamic analysis, lacks any guarantees on the
absence of bugs: from a differential fuzzing campaign that has observed no bugs
(or a minimal difference), what is the risk of observing a bug (or a larger
difference) if we run the fuzzer for one or more steps?
  This paper investigates the application of Extreme Value Theory (EVT) to
address the risk of missing or underestimating bugs in differential fuzzing.
The key observation is that differential fuzzing as a random process resembles
the maximum distribution of observed differences. Hence, EVT, a branch of
statistics dealing with extreme values, is an ideal framework to analyze the
tail of the differential fuzzing campaign to contain the risk. We perform
experiments on a set of real-world Java libraries and use differential fuzzing
to find information leaks via side channels in these libraries. We first
explore the feasibility of EVT for this task and the optimal hyperparameters
for EVT distributions. We then compare EVT-based extrapolation against baseline
statistical methods like Markov's as well as Chebyshev's inequalities, and the
Bayes factor. EVT-based extrapolations outperform the baseline techniques in
14.3% of cases and tie with the baseline in 64.2% of cases. Finally, we
evaluate the accuracy and performance gains of EVT-enabled differential fuzzing
in real-world Java libraries, where we reported an average saving of tens of
millions of bytecode executions by an early stop.

</details>


### [10] [Assurance Case Development for Evolving Software Product Lines: A Formal Approach](https://arxiv.org/abs/2511.03026)
*Logan Murphy,Torin Viger,Alessio Di Sandro,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: The paper introduces a formal, tool-supported approach that enables scalable, efficient development and regression analysis of assurance cases across entire software product lines, demonstrated with a medical device product line example.


<details>
  <summary>Details</summary>
Motivation: Creating rigorous assurance cases (ACs) for critical software is essential but particularly challenging in software product lines (SPLs), which include many products with shared and differing features. Managing evidence for each product individually is infeasible, especially as the product line evolves.

Method: The paper formalizes a language for variability-aware assurance cases tailored to SPLs and studies 'lifting' template-based assurance case development. It also develops a regression analysis to understand the effects of SPL changes on these assurance cases. A model-based management tool implementing these techniques is also described and demonstrated on a medical device product line.

Result: The authors successfully formalized an approach and implemented a tool for developing and maintaining ACs for entire SPLs, demonstrated through a medical device example, enabling more efficient and manageable assurance for evolving SPLs.

Conclusion: Lifting AC development to the product line level and enabling variability-aware regression analysis makes it feasible to assure large and evolving software product lines more rigorously and efficiently.

Abstract: In critical software engineering, structured assurance cases (ACs) are used
to demonstrate how key system properties are supported by evidence (e.g., test
results, proofs). Creating rigorous ACs is particularly challenging in the
context of software product lines (SPLs), i.e, sets of software products with
overlapping but distinct features and behaviours. Since SPLs can encompass very
large numbers of products, developing a rigorous AC for each product
individually is infeasible. Moreover, if the SPL evolves, e.g., by the
modification or introduction of features, it can be infeasible to assess the
impact of this change. Instead, the development and maintenance of ACs ought to
be lifted such that a single AC can be developed for the entire SPL
simultaneously, and be analyzed for regression in a variability-aware fashion.
In this article, we describe a formal approach to lifted AC development and
regression analysis. We formalize a language of variability-aware ACs for SPLs
and study the lifting of template-based AC development. We also define a
regression analysis to determine the effects of SPL evolutions on
variability-aware ACs. We describe a model-based assurance management tool
which implements these techniques, and illustrate our contributions by
developing an AC for a product line of medical devices.

</details>


### [11] [Adaptive Detection of Software Aging under Workload Shift](https://arxiv.org/abs/2511.03103)
*Rafael José Moura,Maria Gizele Nascimento,Fumio Machida,Ermeson Andrade*

Main category: cs.SE

TL;DR: Adaptive machine learning models, especially with ADWIN, reliably detect software aging despite varied workloads, surpassing static approaches and keeping performance high (F1 > 0.93).


<details>
  <summary>Details</summary>
Motivation: Software aging leads to performance degradation and failures in long-running systems, especially under dynamic workload conditions. Current static detection models are insufficient to address such variations.

Method: The paper proposes an adaptive approach using machine learning for detecting software aging. It evaluates static models against adaptive models that use Drift Detection Method (DDM) and Adaptive Windowing (ADWIN), which are designed to handle concept drift and are here applied to shift in workload profiles. Experiments are conducted with simulated workloads exhibiting sudden, gradual, and recurring transitions.

Result: Static models experience significant performance drops when exposed to new workload profiles. In contrast, the adaptive model using ADWIN consistently maintains high accuracy, achieving an F1-Score above 0.93 across all tested scenarios.

Conclusion: Adaptive models, particularly those incorporating ADWIN, are highly effective at detecting software aging under changing workload conditions, outperforming static models and maintaining robust performance.

Abstract: Software aging is a phenomenon that affects long-running systems, leading to
progressive performance degradation and increasing the risk of failures. To
mitigate this problem, this work proposes an adaptive approach based on machine
learning for software aging detection in environments subject to dynamic
workload conditions. We evaluate and compare a static model with adaptive
models that incorporate adaptive detectors, specifically the Drift Detection
Method (DDM) and Adaptive Windowing (ADWIN), originally developed for concept
drift scenarios and applied in this work to handle workload shifts. Experiments
with simulated sudden, gradual, and recurring workload transitions show that
static models suffer a notable performance drop when applied to unseen workload
profiles, whereas the adaptive model with ADWIN maintains high accuracy,
achieving an F1-Score above 0.93 in all analyzed scenarios.

</details>


### [12] [Automated Prompt Generation for Code Intelligence: An Empirical study and Experience in WeChat](https://arxiv.org/abs/2511.03136)
*Kexing Ji,Shiyun Fu,Cuiyun Gao,Yujia Chen,Zezhou Yang,Chaozheng Wang,Yuetang Deng*

Main category: cs.SE

TL;DR: Manual prompt engineering for code models is tedious and task-specific. This work empirically evaluates and combines existing automated prompt generation methods—Instruction Generation and Multi-Step Reasoning—for code intelligence tasks. The combined approach dramatically improves performance over basic prompts (up to 148% in industrial API recommendation) for open-source and real-world data.


<details>
  <summary>Details</summary>
Motivation: Large Code Models (LCMs) are powerful but rely heavily on well-crafted prompts to perform effectively in code intelligence tasks. Manual prompt design is labor-intensive and often tailored to specific models or tasks. Automated prompt generation (APG) is established in NLP but insufficiently explored for code intelligence, creating a need for automated solutions to boost LCM performance on diverse and opaque tasks.

Method: The study empirically investigates APG by focusing on two key components: Instruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides task-specific context to guide LCMs, while MSR encourages logical step-by-step reasoning. They evaluate popular APG methods for IG and MSR across four open-source LCMs and three code intelligence tasks—code translation, summarization, and API recommendation. Then, they propose a combined APG approach using the best-performing IG and MSR methods.

Result: Both IG and MSR markedly improve LCM performance compared to basic prompting. The new combined APG approach delivers significant average metric gains: 28.38% in CodeBLEU for code translation, 58.11% in ROUGE-L for summarization, and 84.53% in SuccessRate@1 for API recommendation. In a real-world industrial test (WeChat-Bench), there is an impressive 148.89% increase in average MRR for API recommendation.

Conclusion: Automated prompt generation methods, especially combining Instruction Generation and Multi-Step Reasoning, substantially boost the effectiveness of Large Code Models in varied code intelligence tasks, both in open-source and real-world scenarios. The proposed approach significantly beats manual or basic prompts.

Abstract: Large Code Models (LCMs) show potential in code intelligence, but their
effectiveness is greatly influenced by prompt quality. Current prompt design is
mostly manual, which is time-consuming and highly dependent on specific LCMs
and tasks. While automated prompt generation (APG) exists in NLP, it is
underexplored for code intelligence. This creates a gap, as automating the
prompt process is essential for developers facing diverse tasks and black-box
LCMs.
  To mitigate this, we empirically investigate two important parts of APG:
Instruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides a
task-related description to instruct LCMs, while MSR guides them to produce
logical steps before the final answer. We evaluate widely-used APG methods for
each part on four open-source LCMs and three code intelligence tasks: code
translation (PL-PL), code summarization (PL-NL), and API recommendation
(NL-PL).Experimental results indicate that both IG and MSR dramatically enhance
performance compared to basic prompts. Based on these results, we propose a
novel APG approach combining the best methods of the two parts. Experiments
show our approach achieves average improvements of 28.38% in CodeBLEU (code
translation), 58.11% in ROUGE-L (code summarization), and 84.53% in
SuccessRate@1 (API recommendation) over basic prompts. To validate its
effectiveness in an industrial scenario, we evaluate our approach on
WeChat-Bench, a proprietary dataset, achieving an average MRR improvement of
148.89% for API recommendation.

</details>


### [13] [RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring](https://arxiv.org/abs/2511.03153)
*Khouloud Oueslati,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: RefAgent, a multi-agent LLM framework for software refactoring, outperforms traditional and single-agent methods in code quality, reliability, and refactoring opportunity identification, showing strong potential for automated code improvement.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs are limited in software refactoring tasks due to reliance on static instructions. LLM-based agents offer dynamic adaptation and autonomy, motivating the investigation of their effectiveness and the proposal of a flexible, multi-agent system.

Method: The authors designed RefAgent, a multi-agent LLM-based framework with specialized agents (planning, execution, testing, refinement) utilizing self-reflection and tool-calling. Performance was empirically evaluated on eight open-source Java projects and compared to single-agent, search-based, and historical developer approaches, using metrics such as unit test pass rate, code smell reduction, and F1-score.

Result: RefAgent achieved a median unit test pass rate of 90%, median code smell reduction of 52.5%, and improved key quality attributes by 8.6%. It closely matches developers and tools in identifying refactoring opportunities (median F1-score: 79.15%/72.7%). It outperformed single-agent approaches in test pass rate (by 64.7%) and compilation success (by 40.1%).

Conclusion: Multi-agent LLM-based frameworks like RefAgent significantly advance automated software refactoring, achieving high code quality, reliability, and alignment with human and tool-based refactoring strategies.

Abstract: Large Language Models (LLMs) have substantially influenced various software
engineering tasks. Indeed, in the case of software refactoring, traditional
LLMs have shown the ability to reduce development time and enhance code
quality. However, these LLMs often rely on static, detailed instructions for
specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving
contexts and autonomously make decisions by interacting with software tools and
executing workflows. In this paper, we explore the potential of LLM-based
agents in supporting refactoring activities. Specifically, we introduce
RefAgent, a multi-agent LLM-based framework for end-to-end software
refactoring. RefAgent consists of specialized agents responsible for planning,
executing, testing, and iteratively refining refactorings using self-reflection
and tool-calling capabilities. We evaluate RefAgent on eight open-source Java
projects, comparing its effectiveness against a single-agent approach, a
search-based refactoring tool, and historical developer refactorings. Our
assessment focuses on: (1) the impact of generated refactorings on software
quality, (2) the ability to identify refactoring opportunities, and (3) the
contribution of each LLM agent through an ablation study. Our results show that
RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a
median of 52.5%, and improves key quality attributes (e.g., reusability) by a
median of 8.6%. Additionally, it closely aligns with developer refactorings and
the search-based tool in identifying refactoring opportunities, attaining a
median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent
approaches, RefAgent improves the median unit test pass rate by 64.7% and the
median compilation success rate by 40.1%. These findings highlight the promise
of multi-agent architectures in advancing automated software refactoring.

</details>


### [14] [Understanding Robustness of Model Editing in Code LLMs: An Empirical Study](https://arxiv.org/abs/2511.03182)
*Vinaik Chhetri,A. B Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: Updating code LLMs using model editing methods to reflect API changes often fails: performance drops significantly, most outputs use workarounds instead of correct fixes, and only a small fraction (<6%) of changes are properly adopted. Existing editing approaches are insufficient for reliable code evolution.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in programming, but since they don't adapt after pretraining, they may produce outdated or incompatible code as languages and APIs change. Retraining is expensive, so lightweight model editing is proposed, but its true effectiveness is unclear.

Method: A systematic study of five state-of-the-art model editing methods (Constrained FT, GRACE, MEMIT, PMET, ROME) was conducted on three leading code LLMs (CodeLlama, CodeQwen1.5, DeepSeek-Coder) using controlled API deprecation scenarios. Evaluation assessed reliability, generalization, and specificity through three different metrics: compilation success, partial test pass, and full test pass.

Result: Instant model edits degrade syntactic validity (up to −86 points) and functional correctness (−45 points at best). Sequential edits worsen these effects; sometimes, models fail completely. Most successful outputs use workarounds rather than correct API changes. Faulty or incorrect adoptions are common, while correct integrations of the intended change occur only ~6% of the time.

Conclusion: Current model editing methods struggle to effectively update code LLMs for API changes; edits often weaken models and rarely produce genuinely correct and reliable code. Reliance on workarounds highlights the need for better techniques.

Abstract: Large language models (LLMs) are increasingly used in software development.
However, while LLMs remain static after pretraining, programming languages and
APIs continue to evolve, leading to the generation of deprecated or
incompatible code that undermines reliability. Retraining LLMs from scratch to
reflect such changes is computationally expensive, making model editing a
promising lightweight alternative that updates only a small subset of
parameters. Despite its potential, it remains unclear whether model editing
yields genuine syntactic and semantic adaptations or merely superficial fixes.
In this work, we present a systematic study of five state-of-the-art model
editing methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We
apply these methods to three leading open-source code LLMs, CodeLlama,
CodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios.
Our evaluation covers both instant and sequential editing settings, using three
disjoint evaluation sets designed to assess reliability, generalization, and
specificity. We measure model correctness at three levels: successful
compilation, partial test case pass, and full test pass. Our findings show that
instant edits consistently degrade model performance, with syntactic validity
dropping by up to 86 percentage points and functional correctness declining by
45 points even in the best-performing setting. Sequential edits further amplify
this degradation, and in some cases, model performance collapses entirely.
Across all models, most passing generations relied on workarounds rather than
correctly adopting the intended changes, while faulty adoptions that result in
test failures or compilation errors were significantly more frequent. Correct
adoptions, where the model correctly integrates the intended change, occurred
in only about 6% of cases.

</details>


### [15] [Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling](https://arxiv.org/abs/2511.03404)
*Qianhui Zhao,Li Zhang,Fang Liu,Junhang Cheng,Chengru Wu,Junchen Ai,Qiaoyuanhe Meng,Lichen Zhang,Xiaoli Lian,Shubin Song,Yuanping Guo*

Main category: cs.SE

TL;DR: This paper introduces CodeProjectEval, a realistic project-level code generation dataset, and ProjectGen, a multi-agent LLM-driven framework featuring semantic architecture mapping and iterative generation. ProjectGen achieves substantial performance gains over baselines, greatly improving real-world automatic software project generation.


<details>
  <summary>Details</summary>
Motivation: Existing project-level code generation methods suffer from unrealistic datasets, unreliable metrics, and difficulty mapping requirements to code and managing complexity. There is a pressing need to enable LLMs to generate complete projects directly from real user requirements for practical software engineering use.

Method: The study introduces CodeProjectEval, a new dataset for project-level code generation, and proposes ProjectGen—a framework that decomposes the generation task into architecture design, skeleton generation, and code filling, supplemented by iterative refinement and memory-based context management. The framework uses SSAT, a structured semantic representation, to bridge requirements and implementation.

Result: ProjectGen outperformed baseline methods, passing 52/124 test cases on DevBench (57% improvement) and 310 test cases on CodeProjectEval (about ten times more than baselines), indicating state-of-the-art performance.

Conclusion: ProjectGen, with its multi-agent framework and SSAT, significantly advances the capability of LLMs for project-level code generation, demonstrating substantial improvements over prior baselines on real-world datasets.

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
progress in automated code generation. In real-world software engineering, the
growing demand for rapid iteration and continuous delivery underscores the
importance of project-level code generation, where LLMs are expected to
generate complete software projects directly from complex user requirements.
Although existing studies have made initial explorations, they still face key
limitations, including unrealistic datasets and unreliable evaluation metrics
that fail to reflect real-world complexity, the semantic gap between
human-written requirements and machine-interpretable structures, and
difficulties in managing hierarchical dependencies and maintaining quality
throughout the generation process. To address these limitations, we first
introduce CodeProjectEval, a project-level code generation dataset built from
18 real-world repositories with 12.7 files and 2,388.6 lines of code per task
on average, supplemented with documentation and executable test cases for
automatic evaluation. We further propose ProjectGen, a multi-agent framework
that decomposes projects into architecture design, skeleton generation, and
code filling stages with iterative refinement and memory-based context
management. Within this framework, we introduce the Semantic Software
Architecture Tree (SSAT), a structured and semantically rich representation
that effectively bridges user requirements and source code implementation.
Experiments show that ProjectGen achieves state-of-the-art performance, passing
52/124 test cases on the small-scale project-level code generation dataset
DevBench, a 57% improvement over the baseline approaches, and 310 test cases on
CodeProjectEval, representing an improvement of roughly tenfold compared to the
baselines.

</details>


### [16] [Light over Heavy: Automated Performance Requirements Quantification with Linguistic Inducement](https://arxiv.org/abs/2511.03421)
*Shihai Wang,Tao Chen*

Main category: cs.SE

TL;DR: LQPR is a fast and accurate automated method for quantifying performance requirements, outperforming general LLM approaches and existing learning-based solutions while being much less resource intensive.


<details>
  <summary>Details</summary>
Motivation: Quantifying performance requirements is essential for engineering tasks but current methods are manual, costly, and prone to errors. There is a need for a more efficient and accurate automated approach.

Method: The authors present LQPR, an automatic approach that treats quantification as a classification problem, using a lightweight linguistically induced matching mechanism instead of typical LLM-based methods.

Result: LQPR outperformed nine state-of-the-art learning-based methods in varied datasets, achieving best ranking in 75% or more cases and operating at significantly lower computational cost.

Conclusion: Specialized approaches like LQPR can be more effective and efficient than general LLM-driven techniques for quantifying performance requirements.

Abstract: Elicited performance requirements need to be quantified for compliance in
different engineering tasks, e.g., configuration tuning and performance
testing. Much existing work has relied on manual quantification, which is
expensive and error-prone due to the imprecision. In this paper, we present
LQPR, a highly efficient automatic approach for performance requirements
quantification.LQPR relies on a new theoretical framework that converts
quantification as a classification problem. Despite the prevalent applications
of Large Language Models (LLMs) for requirement analytics, LQPR takes a
different perspective to address the classification: we observed that
performance requirements can exhibit strong patterns and are often
short/concise, therefore we design a lightweight linguistically induced
matching mechanism. We compare LQPR against nine state-of-the-art
learning-based approaches over diverse datasets, demonstrating that it is
ranked as the sole best for 75% or more cases with two orders less cost. Our
work proves that, at least for performance requirement quantification,
specialized methods can be more suitable than the general LLM-driven
approaches.

</details>


### [17] [U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility](https://arxiv.org/abs/2511.03517)
*Wencheng Ye,Yan Liu*

Main category: cs.SE

TL;DR: U2F is a new cognitive-inspired framework that helps software agents find novel and feasible solutions beyond conventional approaches. By embracing uncertainty and using analogical reasoning, reverse thinking, and external validation, U2F significantly increases the novelty of solutions in real-world software tasks.


<details>
  <summary>Details</summary>
Motivation: Most existing LLM-based software engineering (SWE) agents focus on well-defined problems and use conventional methods, which limits their ability to find innovative or alternative solutions. This is especially problematic in open-world environments where new challenges can't be addressed by established paradigms. Thus, there is a need for approaches that can systematically explore and surface novel solutions.

Method: The authors propose U2F (Unknown Unknowns to Functional solutions), a cognitive-inspired, multi-agent framework. U2F comprises two main components: (1) a Discovery-Exploration-Integration agent system that uncovers and synthesizes new solution pathways, and (2) cognitive enhancement mechanisms (cross-domain analogical reasoning, reverse thinking, and external validation) to expand conventional solution boundaries.

Result: When tested on 218 real-world software enabler stories, U2F led to a 14% increase in overall solution novelty, a 51% improvement in semantic novelty, and maintained a high feasibility score (4.02/5.0), as judged by human experts and validated by an LLM-based evaluator.

Conclusion: The study demonstrates that U2F's approach of embracing and systematically surfacing uncertainty results in significantly more innovative solutions for software engineering tasks, suggesting this method catalyzes creativity and expands solution spaces beyond conventional frameworks.

Abstract: Large language models (LLMs) have shown strong capabilities in software
engineering tasks, yet most existing LLM-based SWE-Agents mainly tackle
well-defined problems using conventional methods, often overlooking alternative
or innovative solutions beyond their predefined frameworks. This limitation is
evident in open-world software environments, where emerging challenges
transcend established paradigms.
  We propose U2F (Unknown Unknowns to Functional solutions), a
cognitive-inspired, uncertainty-embracing multi-agent framework that
systematically surfaces "Unknown Unknowns" - novel solution pathways absent
from initial formulations but holding innovative potential. U2F consists of two
key components: (1) a Discovery-Exploration-Integration agent system for
uncovering and synthesizing potential solutions, and (2) cognitive enhancement
mechanisms across three dimensions: cross-domain analogical reasoning, reverse
thinking, and external validation, which strategically reframe and extend
conventional solution boundaries.
  Applied to 218 real-world software enabler stories curated from authentic
engineering tasks, U2F achieved notable improvements: human experts reported a
14 percent increase in overall novelty, 51 percent improvement in semantic
novelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based
evaluator. These results highlight the potential of embracing uncertainty as a
catalyst for innovation in software engineering.

</details>


### [18] [Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code Understanding](https://arxiv.org/abs/2511.03549)
*Ziv Nevo,Orna Raz,Karen Yorav*

Main category: cs.SE

TL;DR: The paper presents a tool that uses natural language context from GitHub to boost LLMs' code explanations. Evaluated via a user study, it shows improved, meaningful, and reliable insights for developers.


<details>
  <summary>Details</summary>
Motivation: LLMs can generate code explanations but often lack broader software engineering context, limiting their usefulness for tasks like maintenance and onboarding. Leveraging natural language artifacts from software repositories can potentially ground and enhance code understanding.

Method: The proposed system consists of three modules: extracting relevant GitHub artifacts, generating code purpose explanations using this context, and validating the explanations. The system is implemented as both a standalone tool and a server within the Model Context Protocol (MCP). Evaluation is done via a user study with developers from open and proprietary projects.

Result: The user study found that the generated insights are generally helpful, non-trivial, and do not exhibit hallucinations, suggesting the system improves upon standard LLM-based code explanations.

Conclusion: The study concludes that enriching LLM-based code explanations with contextual insights from GitHub artifacts provides developers with more helpful, non-trivial, and hallucination-free explanations.

Abstract: Understanding the purpose of source code is a critical task in software
maintenance, onboarding, and modernization. While large language models (LLMs)
have shown promise in generating code explanations, they often lack grounding
in the broader software engineering context. We propose a novel approach that
leverages natural language artifacts from GitHub -- such as pull request
descriptions, issue descriptions and discussions, and commit messages -- to
enhance LLM-based code understanding. Our system consists of three components:
one that extracts and structures relevant GitHub context, another that uses
this context to generate high-level explanations of the code's purpose, and a
third that validates the explanation. We implemented this as a standalone tool,
as well as a server within the Model Context Protocol (MCP), enabling
integration with other AI-assisted development tools. Our main use case is that
of enhancing a standard LLM-based code explanation with code insights that our
system generates. To evaluate explanations' quality, we conducted a small scale
user study, with developers of several open projects, as well as developers of
proprietary projects. Our user study indicates that when insights are generated
they often are helpful and non trivial, and are free from hallucinations.

</details>


### [19] [The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents](https://arxiv.org/abs/2511.03690)
*Xingyao Wang,Simon Rosenberg,Juan Michelini,Calvin Smith,Hoang Tran,Engel Nyst,Rohit Malhotra,Xuhui Zhou,Valerie Chen,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: OpenHands Software Agent SDK offers a comprehensive, secure, and flexible toolkit for building and deploying software engineering agents, with strong benchmark results and features surpassing leading alternatives.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the complexity and challenges in building, deploying, and interacting with production-ready software engineering agents, including flexibility, reliability, security, and user interaction.

Method: The paper introduces the OpenHands Software Agent SDK, which is an architectural redesign of the agent component from OpenHands framework. The SDK features a simple extensible interface, supports custom tools and memory, enables seamless execution locally/remotely, integrates communication protocols (REST/WebSocket), supports multiple user interfaces (visual, CLI, APIs), and includes unique offerings like sandboxed execution, lifecycle control, multi-LLM routing, and built-in security analysis. The performance is evaluated empirically using SWE-Bench Verified and GAIA benchmarks.

Result: The empirical results on benchmarks demonstrate strong performance for the OpenHands SDK. It is able to flexibly, securely, and reliably enable software engineering agent deployment with features not present in comparable SDKs.

Conclusion: The OpenHands Software Agent SDK provides a practical and scalable solution for developing, prototyping, and deploying robust software development agents, unlocking new applications and improving deployment reliability.

Abstract: Agents are now used widely in the process of software development, but
building production-ready software engineering agents is a complex task.
Deploying software agents effectively requires flexibility in implementation
and experimentation, reliable and secure execution, and interfaces for users to
interact with agents. In this paper, we present the OpenHands Software Agent
SDK, a toolkit for implementing software development agents that satisfy these
desiderata. This toolkit is a complete architectural redesign of the agent
components of the popular OpenHands framework for software development agents,
which has 64k+ GitHub stars. To achieve flexibility, we design a simple
interface for implementing agents that requires only a few lines of code in the
default case, but is easily extensible to more complex, full-featured agents
with features such as custom tools, memory management, and more. For security
and reliability, it delivers seamless local-to-remote execution portability,
integrated REST/WebSocket services. For interaction with human users, it can
connect directly to a variety of interfaces, such as visual workspaces (VS
Code, VNC, browser), command-line interfaces, and APIs. Compared with existing
SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native
sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and
built-in security analysis. Empirical results on SWE-Bench Verified and GAIA
benchmarks demonstrate strong performance. Put together, these elements allow
the OpenHands Software Agent SDK to provide a practical foundation for
prototyping, unlocking new classes of custom applications, and reliably
deploying agents at scale.

</details>
