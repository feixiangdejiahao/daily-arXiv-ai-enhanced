<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 26]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments](https://arxiv.org/abs/2510.21902)
*Timothé Boulet,Xavier Hinaut,Clément Moulin-Frier*

Main category: cs.SE

TL;DR: This paper extends SWE-Agent evaluation to embodied tasks using the Minigrid environment, analyzing how code access and interactive exploration affect agent success, and sets a benchmark for future research in reasoning systems for controller generation.


<details>
  <summary>Details</summary>
Motivation: Traditional software engineering agents perform well on tasks with accessible code, but their effectiveness on embodied tasks—requiring discovery and interaction with environments—has not been investigated.

Method: This study adapts the Mini-SWE-Agent (MSWEA) to address 20 varied embodied tasks in the Minigrid environment. It systematically evaluates agent performance under different information access scenarios: having or lacking environment source code, and with varying abilities for interactive exploration.

Result: The research quantifies the impact of different sources of information (code access and exploration capabilities) on agent performance. It also examines the comparative roles of static code analysis and dynamic exploration in solving embodied tasks.

Conclusion: Controller generation for embodied tasks is identified as an important benchmark for SWE-Agents. The paper presents baseline quantitative results that will guide future research on reasoning systems for these tasks.

Abstract: Software Engineering Agents (SWE-Agents) have proven effective for
traditional software engineering tasks with accessible codebases, but their
performance for embodied tasks requiring well-designed information discovery
remains unexplored. We present the first extended evaluation of SWE-Agents on
controller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to
solve 20 diverse embodied tasks from the Minigrid environment. Our experiments
compare agent performance across different information access conditions: with
and without environment source code access, and with varying capabilities for
interactive exploration. We quantify how different information access levels
affect SWE-Agent performance for embodied tasks and analyze the relative
importance of static code analysis versus dynamic exploration for task solving.
This work establishes controller generation for embodied tasks as a crucial
evaluation domain for SWE-Agents and provides baseline results for future
research in efficient reasoning systems.

</details>


### [2] [TOM-SWE: User Mental Modeling For Software Engineering Agents](https://arxiv.org/abs/2510.21903)
*Xuhui Zhou,Valerie Chen,Zora Zhiruo Wang,Graham Neubig,Maarten Sap,Xingyao Wang*

Main category: cs.SE

TL;DR: ToM-SWE, a dual-agent coding system featuring a theory-of-mind component to track user goals and context, significantly improves task outcomes and user satisfaction over prior methods, underscoring the importance of advanced user modeling in coding agents.


<details>
  <summary>Details</summary>
Motivation: Despite advances in coding agents, these systems still have difficulty understanding and maintaining user intent, particularly when user instructions lack detail or depend heavily on context. There is a need to enhance coding agents' ability to model and track user goals for improved performance.

Method: The paper introduces ToM-SWE, a dual-agent system composed of a primary software-engineering (SWE) agent and a complementary theory-of-mind (ToM) agent. The ToM agent models the user's mental state by inferring their goals, constraints, and preferences from their instructions and interaction history. It maintains persistent memory of the user and supplies relevant cues to the SWE agent. The framework is evaluated on two benchmarks (ambiguous SWE-bench and stateful SWE-bench) and through a user study with professional developers.

Result: ToM-SWE outperforms the baselines in both benchmarks. In the stateful SWE-bench evaluation, ToM-SWE achieved a task success rate of 59.7% versus 18.1% for OpenHands, a state-of-the-art SWE agent. In a three-week real-world study, developers rated ToM-SWE as useful 86% of the time.

Conclusion: Stateful user modeling, as enabled by the ToM agent, significantly enhances the effectiveness and practical usefulness of coding agents. Persistent memory and context-aware reasoning about user intentions lead to higher task success rates and improved developer satisfaction.

Abstract: Recent advances in coding agents have made them capable of planning, editing,
running, and testing complex code bases. Despite their growing ability in
coding tasks, these systems still struggle to infer and track user intent,
especially when instructions are underspecified or context-dependent. To bridge
this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary
software-engineering (SWE) agent with a lightweight theory-of-mind (ToM)
partner agent dedicated to modeling the user's mental state. The ToM agent
infers user goals, constraints, and preferences from instructions and
interaction history, maintains a \textbf{persistent memory} of the user, and
provides user-related suggestions to the SWE agent. In two software engineering
benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task
success rates and user satisfaction. Notably, on the stateful SWE benchmark, a
newly introduced evaluation that provides agents with a user simulator along
with previous interaction histories, ToM-SWE achieves a substantially higher
task success rate of 59.7\% compared to 18.1\% for OpenHands, a
state-of-the-art SWE agent. Furthermore, in a three-week study with
professional developers using ToM-SWE in their daily work, participants found
it useful 86\% of the time, underscoring the value of stateful user modeling
for practical coding agents.

</details>


### [3] [A Comparison of Conversational Models and Humans in Answering Technical Questions: the Firefox Case](https://arxiv.org/abs/2510.21933)
*Joao Correia,Daniel Coutinho,Marco Castelluccio,Caio Barbosa,Rafael de Mello,Anita Sarma,Alessandro Garcia,Marco Gerosa,Igor Steinmacher*

Main category: cs.SE

TL;DR: The paper finds Retrieval-Augmented Generation tools offer more comprehensive and almost equally helpful answers as humans in OSS developer chats, but tend to be verbose. RAG could lessen maintainers' workload, especially for projects like Firefox, if future improvements focus on conciseness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve developer support in open source projects by using advanced AI models, particularly focusing on helping developers address questions and reduce the workload for core maintainers without compromising answer quality.

Method: The study conducts an empirical comparison of response quality from human developers, a standard GPT model, and a GPT model enhanced with Retrieval-Augmented Generation (RAG), using actual queries from Mozilla Firefox developer chat rooms. Responses are evaluated by Mozilla experts on helpfulness, comprehensiveness, and conciseness.

Result: RAG-augmented responses are more comprehensive than those from human developers, nearly as helpful, but less concise due to verbosity. This demonstrates that RAG can support developers effectively, though improvements in making responses shorter may be desirable.

Conclusion: RAG-based language models can provide significant assistance in OSS projects by offering high-quality, comprehensive responses and reducing the burden on maintainers. However, optimizing for brevity can further improve their utility in large-scale projects.

Abstract: The use of Large Language Models (LLMs) to support tasks in software
development has steadily increased over recent years. From assisting developers
in coding activities to providing conversational agents that answer newcomers'
questions. In collaboration with the Mozilla Foundation, this study evaluates
the effectiveness of Retrieval-Augmented Generation (RAG) in assisting
developers within the Mozilla Firefox project. We conducted an empirical
analysis comparing responses from human developers, a standard GPT model, and a
GPT model enhanced with RAG, using real queries from Mozilla's developer chat
rooms. To ensure a rigorous evaluation, Mozilla experts assessed the responses
based on helpfulness, comprehensiveness, and conciseness. The results show that
RAG-assisted responses were more comprehensive than human developers (62.50% to
54.17%) and almost as helpful (75.00% to 79.17%), suggesting RAG's potential to
enhance developer assistance. However, the RAG responses were not as concise
and often verbose. The results show the potential to apply RAG-based tools to
Open Source Software (OSS) to minimize the load to core maintainers without
losing answer quality. Toning down retrieval mechanisms and making responses
even shorter in the future would enhance developer assistance in massive
projects like Mozilla Firefox.

</details>


### [4] [ArchISMiner: A Framework for Automatic Mining of Architectural Issue-Solution Pairs from Online Developer Communities](https://arxiv.org/abs/2510.21966)
*Musengamana Jean de Dieu,Ruiyin Li,Peng Liang,Mojtaba Shahin,Muhammad Waseem,Arif Ali Khan,Bangchao Wang,Mst Shamima Aktar*

Main category: cs.SE

TL;DR: ArchISMiner enables automatic, accurate mining of architectural issue-solution pairs from Stack Overflow and other forums, outperforming existing methods and releasing a valuable dataset to aid software architects and developers.


<details>
  <summary>Details</summary>
Motivation: Locating architectural knowledge such as solutions from Stack Overflow is difficult due to the vast volume of unstructured and fragmented discussions, making manual searching time-consuming and prone to errors.

Method: The study presents ArchISMiner, a mining framework consisting of two components: ArchPI, which selects the best ML/DL/PLM/LLM model to detect architecture-related posts, and ArchISPE, which uses BERT embeddings and local TextCNN features to automatically extract issue-solution pairs through indirect supervised learning.

Result: ArchPI achieves a high F1-score of 0.960 for detecting architecture-related posts. ArchISPE outperforms baseline methods in Software Engineering and NLP, with F1-scores of 0.883 for issues and 0.894 for solutions. ArchISMiner was further applied to other forums, resulting in a released dataset of over 18,000 architectural issue-solution pairs.

Conclusion: ArchISMiner significantly improves the efficiency and accuracy of discovering and extracting architectural insights from developer forums, benefiting architects and developers in accessing relevant and useful knowledge.

Abstract: Stack Overflow (SO), a leading online community forum, is a rich source of
software development knowledge. However, locating architectural knowledge, such
as architectural solutions remains challenging due to the overwhelming volume
of unstructured content and fragmented discussions. Developers must manually
sift through posts to find relevant architectural insights, which is
time-consuming and error-prone. This study introduces ArchISMiner, a framework
for mining architectural knowledge from SO. The framework comprises two
complementary components: ArchPI and ArchISPE. ArchPI trains and evaluates
multiple models, including conventional ML/DL models, Pre-trained Language
Models (PLMs), and Large Language Models (LLMs), and selects the
best-performing model to automatically identify Architecture-Related Posts
(ARPs) among programming-related discussions. ArchISPE employs an indirect
supervised approach that leverages diverse features, including BERT embeddings
and local TextCNN features, to extract architectural issue-solution pairs. Our
evaluation shows that the best model in ArchPI achieves an F1-score of 0.960 in
ARP detection, and ArchISPE outperforms baselines in both SE and NLP fields,
achieving F1-scores of 0.883 for architectural issues and 0.894 for solutions.
A user study further validated the quality (e.g., relevance and usefulness) of
the identified ARPs and the extracted issue-solution pairs. Moreover, we
applied ArchISMiner to three additional forums, releasing a dataset of over 18K
architectural issue-solution pairs. Overall, ArchISMiner can help architects
and developers identify ARPs and extract succinct, relevant, and useful
architectural knowledge from developer communities more accurately and
efficiently. The replication package of this study has been provided at
https://github.com/JeanMusenga/ArchISPE

</details>


### [5] [FeaGPT: an End-to-End agentic-AI for Finite Element Analysis](https://arxiv.org/abs/2510.21993)
*Yupeng Qi,Ran Xu,Xu Chu*

Main category: cs.SE

TL;DR: FeaGPT uses natural language commands to fully automate geometry, mesh generation, FEA simulation, and analysis for engineering tasks, validated on industrial and parametric cases. It shows LLM-powered conversational interfaces can make advanced engineering tools much more accessible and user-friendly.


<details>
  <summary>Details</summary>
Motivation: Traditional finite element analysis (FEA) tools require manual intervention and complex workflows, slowing down engineering design and limiting accessibility. The motivation is to make advanced computational engineering more user-friendly and accessible by leveraging natural language interfaces that automate complete FEA processes.

Method: The paper introduces FeaGPT, a framework that enables conversational, natural language-based control of the full geometry-mesh-simulation-analysis (GMSA) pipeline. The system interprets engineering intent, automatically generates adaptive meshes, configures simulations with proper boundary conditions, and performs iterative multi-objective analysis. It uses large language models to automate the workflow end-to-end.

Result: FeaGPT is validated with industrial turbocharger cases (7-blade compressor and 12-blade turbine at 110,000 rpm), demonstrating the successful transformation of natural language specifications into accurate CalculiX simulations. Further validation with 432 NACA airfoil cases shows scalability for parametric design. The results confirm that the system produces physically realistic solutions and supports advanced engineering tasks.

Conclusion: FeaGPT achieves full automation of complex engineering simulation workflows via conversational interfaces, demonstrating the ability of LLMs to democratize and streamline computational engineering for both industrial and parametric design tasks, while maintaining analytical rigor and scalability.

Abstract: Large language models (LLMs) are establishing new paradigms for engineering
applications by enabling natural language control of complex computational
workflows. This paper introduces FeaGPT, the first framework to achieve
complete geometry-mesh-simulation workflows through conversational interfaces.
Unlike existing tools that automate individual FEA components, FeaGPT
implements a fully integrated Geometry-Mesh-Simulation-Analysis (GMSA) pipeline
that transforms engineering specifications into validated computational results
without manual intervention. The system interprets engineering intent,
automatically generates physics-aware adaptive meshes, configures complete FEA
simulations with proper boundary condition inference, and performs
multi-objective analysis through closed-loop iteration.
  Experimental validation confirms complete end-to-end automation capability.
Industrial turbocharger cases (7-blade compressor and 12-blade turbine at
\SI{110000}{rpm}) demonstrate the system successfully transforms natural
language specifications into validated CalculiX simulations, producing
physically realistic results for rotating machinery analysis. Additional
validation through 432 NACA airfoil configurations confirms scalability for
parametric design exploration. These results demonstrate that natural language
interfaces can effectively democratize access to advanced computational
engineering tools while preserving analytical rigor.

</details>


### [6] [Impact and Implications of Generative AI for Enterprise Architects in Agile Environments: A Systematic Literature Review](https://arxiv.org/abs/2510.22003)
*Stefan Julian Kooy,Jean Paul Sebastian Piest,Rob Henk Bemthuis*

Main category: cs.SE

TL;DR: GenAI is transforming agile enterprise architecture work by supporting creativity, automation, and knowledge retrieval, but also introduces risks related to bias, errors, and privacy. The study maps use cases, risks, and identifies skills and governance needs for responsible adoption.


<details>
  <summary>Details</summary>
Motivation: Generative AI is rapidly influencing enterprise architecture in agile software organizations, but there is fragmented knowledge on its real-world effects and implications.

Method: A systematic literature review (SLR) guided by Kitchenham and PRISMA protocols, analyzing 1,697 records to synthesize findings from 33 studies across architectural roles.

Result: Through a systematic literature review of 1,697 records, the study identifies 33 relevant studies that detail GenAI's key benefits, such as enhanced design ideation, accelerated artifact creation, and improved decision support, as well as significant risks like bias, errors, privacy concerns, and reduced social effort.

Conclusion: The responsible adoption of GenAI can accelerate digital transformation in organizations, provided architectural integrity is safeguarded through new skills, adaptive governance, and awareness of associated risks.

Abstract: Generative AI (GenAI) is reshaping enterprise architecture work in agile
software organizations, yet evidence on its effects remains scattered. We
report a systematic literature review (SLR), following established SLR
protocols of Kitchenham and PRISMA, of 1,697 records, yielding 33 studies
across enterprise, solution, domain, business, and IT architect roles. GenAI
most consistently supports (i) design ideation and trade-off exploration; (ii)
rapid creation and refinement of artifacts (e.g., code, models, documentation);
and (iii) architectural decision support and knowledge retrieval. Reported
risks include opacity and bias, contextually incorrect outputs leading to
rework, privacy and compliance concerns, and social loafing. We also identify
emerging skills and competencies, including prompt engineering, model
evaluation, and professional oversight, and organizational enablers around
readiness and adaptive governance. The review contributes with (1) a mapping of
GenAI use cases and risks in agile architecting, (2) implications for
capability building and governance, and (3) an initial research agenda on
human-AI collaboration in architecture. Overall, the findings inform
responsible adoption of GenAI that accelerates digital transformation while
safeguarding architectural integrity.

</details>


### [7] [LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation](https://arxiv.org/abs/2510.22210)
*Gwihwan Go,Quan Zhang,Chijin Zhou,Zhao Wei,Yu Jiang*

Main category: cs.SE

TL;DR: LSPRAG is a framework that uses Language Server Protocols to provide real-time, language-agnostic context to LLMs for unit test generation. It outperforms existing methods in code coverage across several programming languages, requiring minimal additional engineering.


<details>
  <summary>Details</summary>
Motivation: Automated unit test generation is crucial for software robustness, but existing methods are limited in generalizing across programming languages and providing real-time support. Techniques like Retrieval-Augmented Generation are hindered by imprecise searches and high engineering costs for language-specific solutions.

Method: The paper introduces LSPRAG, a framework that retrieves precise context for real-time, language-agnostic unit test generation by leveraging Language Server Protocol (LSP) backends. LSPRAG uses LSP servers to supply Large Language Models (LLMs) with real-time, language-aware symbol definitions and references for concise and effective context during unit test generation, requiring minimal effort per language.

Result: LSPRAG was evaluated on open-source projects written in Java, Go, and Python. It outperformed baselines, improving line coverage by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.

Conclusion: LSPRAG enables scalable, effective, and efficient language-agnostic automated unit test generation using LLMs. By leveraging LSP backends, it reduces the costs and engineering efforts associated with language-specific pipelines, and significantly increases test coverage across multiple languages.

Abstract: Automated unit test generation is essential for robust software development,
yet existing approaches struggle to generalize across multiple programming
languages and operate within real-time development. While Large Language Models
(LLMs) offer a promising solution, their ability to generate high coverage test
code depends on prompting a concise context of the focal method. Current
solutions, such as Retrieval-Augmented Generation, either rely on imprecise
similarity-based searches or demand the creation of costly, language-specific
static analysis pipelines. To address this gap, we present LSPRAG, a framework
for concise-context retrieval tailored for real-time, language-agnostic unit
test generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)
back-ends to supply LLMs with precise symbol definitions and references in real
time. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware
context retrieval, requiring minimal per-language engineering effort. We
evaluated LSPRAG on open-source projects spanning Java, Go, and Python.
Compared to the best performance of baselines, LSPRAG increased line coverage
by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.

</details>


### [8] [Taming Silent Failures: A Framework for Verifiable AI Reliability](https://arxiv.org/abs/2510.22224)
*Guan-Yan Yang,Farn Wang*

Main category: cs.SE

TL;DR: AI introduces silent failure risks in critical systems. FAME integrates formal synthesis and runtime monitoring to detect these failures, catching 93.5% of safety violations in autonomous vehicles and providing a certifiable path to trustworthy AI.


<details>
  <summary>Details</summary>
Motivation: AI systems in safety-critical domains (like autonomous vehicles) can fail silently, producing confident but wrong outputs with potentially dangerous consequences. Existing approaches lack practical, certifiable methods to reliably detect these failures and assure safety.

Method: The authors introduced FAME (Formal Assurance and Monitoring Environment), a framework that integrates mathematical offline formal synthesis with online runtime monitoring, designed to create a verifiable safety net for AI components. They demonstrated FAME using an autonomous vehicle perception system and aligned it with industry safety standards (ISO 26262 and ISO/PAS 8800).

Result: FAME detected 93.5% of otherwise silent critical safety violations in the tested autonomous vehicle perception system. It provided reliability engineers with a practical, certifiable method for trustworthy AI deployment.

Conclusion: FAME shifts the reliability paradigm from merely probabilistic assurance to provable safety for AI in safety-critical systems, facilitating certified deployment aligned with industry safety standards.

Abstract: The integration of Artificial Intelligence (AI) into safety-critical systems
introduces a new reliability paradigm: silent failures, where AI produces
confident but incorrect outputs that can be dangerous. This paper introduces
the Formal Assurance and Monitoring Environment (FAME), a novel framework that
confronts this challenge. FAME synergizes the mathematical rigor of offline
formal synthesis with the vigilance of online runtime monitoring to create a
verifiable safety net around opaque AI components. We demonstrate its efficacy
in an autonomous vehicle perception system, where FAME successfully detected
93.5% of critical safety violations that were otherwise silent. By
contextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards,
we provide reliability engineers with a practical, certifiable pathway for
deploying trustworthy AI. FAME represents a crucial shift from accepting
probabilistic performance to enforcing provable safety in next-generation
systems.

</details>


### [9] [Understanding Self-Admitted Technical Debt in Test Code: An Empirical Study](https://arxiv.org/abs/2510.22249)
*Ibuki Nakamura,Yutaro Kashiwa,Bin Lin,Hajimu Iida*

Main category: cs.SE

TL;DR: This study reveals the prevalence and distinct nature of self-admitted technical debt (SATD) in test code, uncovers its categories, and shows that it is not linked to test smells. Machine learning models, especially CodeBERT, can automatically classify SATD comments, supporting efficient management.


<details>
  <summary>Details</summary>
Motivation: Previous research on Self-Admitted Technical Debt (SATD) has largely focused on production code, neglecting test code, even though test code also contains significant SATD with unique characteristics. This paper aims to analyze SATD specifically in test code, explore its distribution and types, and examine its impact on test quality.

Method: The authors conducted an empirical study by collecting 17,766 SATD comments from 50 repositories, categorizing them into production and test code. They developed comprehensive SATD type categories for test code and built machine learning models (including a CodeBERT-based model) to automatically classify SATD comments.

Result: SATD is prevalent in test code but does not have a direct association with test smells. The study provides detailed category types of SATD in test code. The CodeBERT-based model achieved superior recall and F1-score compared to other models when classifying SATD comments, although its performance varied across different SATD types.

Conclusion: SATD exists widely in test code and differs from SATD in production code. Machine learning models can effectively aid in categorizing SATD comments for better management, with CodeBERT demonstrating strong overall results but varying effectiveness according to SATD types.

Abstract: Developers often opt for easier but non-optimal implementation to meet
deadlines or create rapid prototypes, leading to additional effort known as
technical debt to improve the code later. Oftentimes, developers explicitly
document the technical debt in code comments, referred to as Self-Admitted
Technical Debt (SATD). Numerous researchers have investigated the impact of
SATD on different aspects of software quality and development processes.
However, most of these studies focus on SATD in production code, often
overlooking SATD in the test code or assuming that it shares similar
characteristics with SATD in production code. In fact, a significant amount of
SATD is also present in the test code, with many instances not fitting into
existing categories for the production code. This study aims to fill this gap
and disclose the nature of SATD in the test code by examining its distribution
and types. Moreover, the relation between its presence and test quality is also
analyzed. Our empirical study, involving 17,766 SATD comments (14,987 from
production code, 2,779 from test code) collected from 50 repositories,
demonstrates that while SATD widely exists in test code, it is not directly
associated with test smells. Our study also presents comprehensive categories
of SATD types in the test code, and machine learning models are developed to
automatically classify SATD comments based on their types for easier
management. Our results show that the CodeBERT-based model outperforms other
machine learning models in terms of recall and F1-score. However, the
performance varies on different types of SATD.

</details>


### [10] [Ten Simple Rules for AI-Assisted Coding in Science](https://arxiv.org/abs/2510.22254)
*Eric W. Bridgeford,Iain Campbell,Zijao Chen,Zhicheng Lin,Harrison Ritz,Joachim Vandekerckhove,Russell A. Poldrack*

Main category: cs.SE

TL;DR: This paper offers ten practical rules to help researchers use AI coding tools for scientific computing while maintaining code quality, scientific validity, and research integrity. The rules advocate for balanced, strategic use of AI, robust validation, and continued human oversight.


<details>
  <summary>Details</summary>
Motivation: As AI coding tools become more prevalent in scientific computing, concerns arise about maintaining code quality and scientific validity. Researchers need practical guidelines to safely and effectively integrate AI into their workflows.

Method: This paper proposes ten practical rules for AI-assisted coding, organized around four themes: problem preparation and understanding, managing context and interaction, testing and validation, and code quality assurance and iterative improvement. The approach is principled, focusing on balancing AI capabilities with scientific rigor.

Result: The paper presents detailed rules to guide researchers in using AI tools responsibly, emphasizing human oversight, robust validation, and preservation of domain expertise. These rules promote reliable, reproducible, and scientifically valid code.

Conclusion: Researchers can accelerate software development with AI tools if they follow structured guidelines that maintain scientific rigor and research integrity. The ten rules provide a roadmap for responsible and effective AI-assisted coding within scientific computing.

Abstract: While AI coding tools have demonstrated potential to accelerate software
development, their use in scientific computing raises critical questions about
code quality and scientific validity. In this paper, we provide ten practical
rules for AI-assisted coding that balance leveraging capabilities of AI with
maintaining scientific and methodological rigor. We address how AI can be
leveraged strategically throughout the development cycle with four key themes:
problem preparation and understanding, managing context and interaction,
testing and validation, and code quality assurance and iterative improvement.
These principles serve to emphasize maintaining human agency in coding
decisions, establishing robust validation procedures, and preserving the domain
expertise essential for methodologically sound research. These rules are
intended to help researchers harness AI's transformative potential for faster
software development while ensuring that their code meets the standards of
reliability, reproducibility, and scientific validity that research integrity
demands.

</details>


### [11] [Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus](https://arxiv.org/abs/2510.22318)
*Tuan-Phong Ngo,Bao-Ngoc Duong,Tuan-Anh Hoang,Joshua Dwight,Ushik Shrestha Khwakhali*

Main category: cs.SE

TL;DR: The paper assesses how LLMs can improve ISTQB-based software testing education, presenting a decade-long exam dataset, optimized prompts for LLM accuracy, systematic LLM evaluation, and practical educational recommendations, concluding that LLMs can significantly support ISTQB preparation and broader engineering instruction.


<details>
  <summary>Details</summary>
Motivation: Software testing is crucial to software engineering and education, but educational methods need updating to reflect industry standards. The ISTQB framework is widely used, but its integration with recent AI advancements, especially LLMs, is largely unexplored.

Method: The paper creates an extensive ISTQB-aligned dataset of sample exams and questions, designs a domain-optimized prompt for LLMs, evaluates state-of-the-art LLMs on these tasks, and derives actionable recommendations for educational integration.

Result: The study produced a comprehensive dataset, enhanced LLM precision and explanations for ISTQB topics, systematically evaluated LLM performance, and offered recommendations for implementing LLMs in software testing education.

Conclusion: LLMs show great potential in assisting ISTQB certification preparation and can serve as a foundation for wider application in software engineering education.

Abstract: Software testing is a critical component in the software engineering field
and is important for software engineering education. Thus, it is vital for
academia to continuously improve and update educational methods to reflect the
current state of the field. The International Software Testing Qualifications
Board (ISTQB) certification framework is globally recognized and widely adopted
in industry and academia. However, ISTQB-based learning has been rarely applied
with recent generative artificial intelligence advances. Despite the growing
capabilities of large language models (LLMs), ISTQB-based learning and
instruction with LLMs have not been thoroughly explored. This paper explores
and evaluates how LLMs can complement the ISTQB framework for higher education.
The findings present four key contributions: (i) the creation of a
comprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28
sample exams and 1,145 questions; (ii) the development of a domain-optimized
prompt that enhances LLM precision and explanation quality on ISTQB tasks;
(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and
(iv) actionable insights and recommendations for integrating LLMs into software
testing education. These findings highlight the promise of LLMs in supporting
ISTQB certification preparation and offer a foundation for their broader use in
software engineering at higher education.

</details>


### [12] [Operationalizing Large Language Models with Design-Aware Contexts for Code Comment Generation](https://arxiv.org/abs/2510.22338)
*Aritra Mitra,Srijoni Majumdar,Anamitra Mukhopadhyay,Partha Pratim Das,Paul D Clough,Partha Pratim Chakrabarti*

Main category: cs.SE

TL;DR: Novice coder comments are often unhelpful for maintenance. This study explores using LLMs, with design documents as context, to automatically generate better code comments.


<details>
  <summary>Details</summary>
Motivation: Novice coders increasingly create codebases with poor or useless comments due to lacking standards, hindering code maintenance. The study aims to address this by exploring automated comment generation using LLMs.

Method: The research investigates the feasibility of leveraging design documents as context for LLMs to automatically generate code comments, comparing them against those written by novices.

Result: The study assesses the potential of LLMs, using design documents, to enhance comment quality and code maintainability compared to traditional novice-written comments.

Conclusion: The study evaluates whether large language models can improve the usefulness of code comments, especially for novice coders, by utilizing design documents as additional context.

Abstract: Comments are very useful to the flow of code development. With the increasing
commonality of code, novice coders have been creating a significant amount of
codebases. Due to lack of commenting standards, their comments are often
useless, and increase the time taken to further maintain codes. This study
intends to find the usefulness of large language models (LLMs) in these cases
to generate potentially better comments. This study focuses on the feasibility
of design documents as a context for the LLMs to generate more useful comments,
as design documents are often used by maintainers to understand code when
comments do not suffice.

</details>


### [13] [A First Look at the Self-Admitted Technical Debt in Test Code: Taxonomy and Detection](https://arxiv.org/abs/2510.22409)
*Shahidul Islam,Md Nahidul Islam Opu,Shaowei Wang,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: SATD in test code is underexplored. This study manually analyzed test comments, built the first taxonomy for test code SATD, and found that existing tools and LLMs cannot reliably detect it. More research is needed to develop effective detection methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address a research gap: while self-admitted technical debt (SATD) is well-studied in source code, its presence and impact in test code have not been explored, leaving an incomplete understanding of SATD in software development.

Method: The method involved manually analyzing 50,000 comments, randomly sampled from 1.6 million comments across 1,000 open-source Java projects. SATD comments in test code were identified and categorized into a taxonomy of 15 types. Existing SATD detection tools and large language models (LLMs) were evaluated for their ability to detect test code SATD.

Result: The study identified 615 SATD comments in test code, classified into 15 distinct categories. The best-performing tool (MAT) had only moderate recall, and both open-source and proprietary LLMs performed poorly, especially in precision. Neither traditional tools nor current LLMs reliably detect SATD in test code.

Conclusion: This work presents the first large-scale study of SATD in test code, introduces a taxonomy of SATD types, and shows that current SATD detection approaches are inadequate for test code. It highlights substantial limitations in both traditional tools and modern LLMs, setting the stage for future research focused on improving SATD detection in test code.

Abstract: Self-admitted technical debt (SATD) refers to comments in which developers
explicitly acknowledge code issues, workarounds, or suboptimal solutions. SATD
is known to significantly increase software maintenance effort. While extensive
research has examined SATD in source code, its presence and impact in test code
have received no focused attention, leaving a significant gap in our
understanding of how SATD manifests in testing contexts.
  This study, the first of its kind, investigates SATD in test code by manually
analyzing 50,000 comments randomly sampled from 1.6 million comments across
1,000 open-source Java projects. From this sample, after manual analysis and
filtering, we identified 615 SATD comments and classified them into 15 distinct
categories, building a taxonomy of test code SATD. To investigate whether test
code SATD can be detected automatically, we evaluated existing SATD detection
tools, as well as both open-source and proprietary LLMs. Among the existing
tools, MAT performed the best, albeit with moderate recall. To our surprise,
both open-source and proprietary LLMs exhibited poor detection accuracy,
primarily due to low precision. These results indicate that neither existing
approaches nor current LLMs can reliably detect SATD in test code.
  Overall, this work provides the first large-scale analysis of SATD in test
code, a nuanced understanding of its types, and the limitations of current SATD
detection methods. Our findings lay the groundwork for future research on test
code-specific SATD.

</details>


### [14] [A Multifaceted View on Discrimination in Software Development Careers](https://arxiv.org/abs/2510.22457)
*Shalini Chakraborty,Sebastian Baltes*

Main category: cs.SE

TL;DR: This study finds age- and gender-related discrimination are common in software engineering, but other forms (political, disability, caregiving) are also significant. Women and non-binary individuals experience more workplace issues, including mental health challenges. Future research should broaden its scope beyond just gender and race discrimination.


<details>
  <summary>Details</summary>
Motivation: Discussions on diversity in software engineering mainly focus on gender and race, overshadowing other prevalent forms of discrimination, such as age, political ideology, disability, and neurodivergence.

Method: Secondary analysis of 800 open-ended survey responses from 8,717 participants in the State of the Developer Nation 2025 survey. The analysis centered on patterns of perceived discrimination, related challenges, and negative impacts across multiple identity facets.

Result: Age- and gender-related discrimination were most frequently reported, with political and religious bias also notable. Female and non-binary participants experienced almost all workplace issues at higher rates than male counterparts. Intersectional discrimination—including race, age, political views, and caregiving responsibilities—surfaced. Gender biases led many to alter their appearance or behavior. Non-binary and women respondents reported higher rates of discrimination (35%) and mental health challenges (62%).

Conclusion: Discrimination in software engineering is multifaceted and not limited to gender and race. Researchers should consider and evaluate various identity facets, such as age, political perspective, disability, and caregiving responsibility, in future studies to more fully address workplace issues.

Abstract: Conversations around diversity and inclusion in software engineering often
focus on gender and racial disparities. However, the State of the Developer
Nation 2025 survey with 8,717 participants revealed that other forms of
discrimination are similarly prevalent but receive considerably less attention.
This includes discrimination based on age, political perspective, disabilities,
or cognitive differences such as neurodivergence. We conducted a secondary
analysis of 800 open-ended survey responses to examine patterns of perceived
discrimination, as well as related challenges and negative impacts. Our study
covers multiple identity facets, including age, gender, race, and disability.
We found that age- and gender-related discrimination was the most frequently
reported workplace issue, but discrimination based on political and religious
views emerged as further notable concerns. Most of the participants who
identified as female cited gender as the primary source of discrimination,
often accompanied by intersectional factors such as race, political views, age,
or sexual orientation. Discrimination related to caregiving responsibilities
was reported by all gender identities. Regarding the negative impacts of
workplace issues, many participants described modifying their appearance or
behavior in response to gender biases. Gender also appeared to influence
broader career challenges, as women and non-binary respondents reported
experiencing almost all workplace issues at higher rates, particularly
discrimination (35%) and mental health challenges (62%). Our goal is to raise
awareness in the research community that discrimination in software development
is multifaceted, and to encourage researchers to select and assess relevant
facets beyond age and gender when designing software engineering studies.

</details>


### [15] [Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL](https://arxiv.org/abs/2510.22530)
*Sungmin Kang,Sumi Yun,Jingun Hong,Shin Yoo,Gabin An*

Main category: cs.SE

TL;DR: AutoCrashFL, an LLM-based tool, localizes software crashes using only crash dumps and source code. It outperforms traditional methods, is scalable to industrial-scale software, and is especially effective for complex bugs.


<details>
  <summary>Details</summary>
Motivation: Traditional fault localization (FL) techniques are limited in industrial settings due to their reliance on dynamic analysis methods like coverage profiling or mutation testing, which are expensive and impractical for large-scale software.

Method: The paper introduces AutoCrashFL, an LLM (large language model) agent that localizes software crashes using only crash dumps and source code repositories, without requiring costly dynamic analysis.

Result: In experiments with SAP HANA (a 35-million-line industrial codebase), AutoCrashFL localized 30% of crashes at the top ranking, outperforming a baseline that localized only 17%. Additionally, it proved more effective for complex bugs and can indicate confidence in its findings.

Conclusion: AutoCrashFL is a practical and effective solution for large-scale, industrial fault localization, overcoming key limitations of traditional methods and enabling the scalable use of LLM agents in the software industry.

Abstract: Fault Localization (FL) aims to identify root causes of program failures. FL
typically targets failures observed from test executions, and as such, often
involves dynamic analyses to improve accuracy, such as coverage profiling or
mutation testing. However, for large industrial software, measuring coverage
for every execution is prohibitively expensive, making the use of such
techniques difficult. To address these issues and apply FL in an industrial
setting, this paper proposes AutoCrashFL, an LLM agent for the localization of
crashes that only requires the crashdump from the Program Under Test (PUT) and
access to the repository of the corresponding source code. We evaluate
AutoCrashFL against real-world crashes of SAP HANA, an industrial software
project consisting of more than 35 million lines of code. Experiments reveal
that AutoCrashFL is more effective in localization, as it identified 30%
crashes at the top, compared to 17% achieved by the baseline. Through thorough
analysis, we find that AutoCrashFL has attractive practical properties: it is
relatively more effective for complex bugs, and it can indicate confidence in
its results. Overall, these results show the practicality of LLM agent
deployment on an industrial scale.

</details>


### [16] [DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices](https://arxiv.org/abs/2510.22613)
*Songhan Zhang,Aoyang Fang,Yifan Yang,Ruiyi Cheng,Xiaoying Tang,Pinjia He*

Main category: cs.SE

TL;DR: DynaCausal is a new framework for diagnosing faults in cloud-native microservices. It smartly analyzes multiple data sources and prioritizes real causes, outperforming existing methods in both accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Cloud-native microservices are increasingly adopted for their scalability and rapid iteration, but they introduce complex and fast-changing dependencies that complicate diagnosing system failures. Existing root cause analysis (RCA) methods struggle with dynamic behaviors, noise, concept drift, and often misidentify root causes due to simplistic reliance on service deviations.

Method: The paper proposes DynaCausal, a dynamic causality-aware framework for RCA in distributed microservice systems. DynaCausal unifies multi-modal dynamic signals (from logs, traces, and metrics) and applies interaction-aware representation learning to model time-varying spatio-temporal dependencies. It employs a dynamic contrastive mechanism to separate real fault signals from noise and uses a causal-prioritized pairwise ranking objective to optimize causal attribution.

Result: DynaCausal was comprehensively evaluated on public benchmarks and outperformed state-of-the-art RCA approaches. It achieved an average AC@1 of 0.63, with notable absolute gains between 0.25 and 0.46, providing both accurate and interpretable fault diagnoses.

Conclusion: DynaCausal significantly improves root cause analysis in complex microservice environments by modeling dynamic causal relationships, effectively filtering noise, and prioritizing true causes, thus setting a new standard for accuracy and interpretability.

Abstract: Cloud-native microservices enable rapid iteration and scalable deployment but
also create complex, fast-evolving dependencies that challenge reliable
diagnosis. Existing root cause analysis (RCA) approaches, even with multi-modal
fusion of logs, traces, and metrics, remain limited in capturing dynamic
behaviors and shifting service relationships. Three critical challenges
persist: (i) inadequate modeling of cascading fault propagation, (ii)
vulnerability to noise interference and concept drift in normal service
behavior, and (iii) over-reliance on service deviation intensity that obscures
true root causes. To address these challenges, we propose DynaCausal, a dynamic
causality-aware framework for RCA in distributed microservice systems.
DynaCausal unifies multi-modal dynamic signals to capture time-varying
spatio-temporal dependencies through interaction-aware representation learning.
It further introduces a dynamic contrastive mechanism to disentangle true fault
indicators from contextual noise and adopts a causal-prioritized pairwise
ranking objective to explicitly optimize causal attribution. Comprehensive
evaluations on public benchmarks demonstrate that DynaCausal consistently
surpasses state-of-the-art methods, attaining an average AC@1 of 0.63 with
absolute gains from 0.25 to 0.46, and delivering both accurate and
interpretable diagnoses in highly dynamic microservice environments.

</details>


### [17] [Does In-IDE Calibration of Large Language Models work at Scale?](https://arxiv.org/abs/2510.22614)
*Roham Koohestani,Agnia Sergeyuk,David Gros,Claudio Spiess,Sergey Titov,Prem Devanbu,Maliheh Izadi*

Main category: cs.SE

TL;DR: Calibrating large language model confidence for code generation in IDEs generally doesn't improve reliability at scale, although user-specific adjustments help if enough interaction data is available. Developers best understand reliability through color-coded visual cues rather than numbers.


<details>
  <summary>Details</summary>
Motivation: Integrating large language models (LLMs) into integrated development environments (IDEs) is transforming software engineering, but AI-generated code often lacks reliability and usefulness for developers. Calibration of model confidence could potentially bridge this gap, yet there is insufficient large-scale evidence on its effectiveness and best practices for communicating model reliability to users.

Method: The study investigates calibration methods and user interface designs for LLMs in IDEs. Technically, the authors build a scalable and flexible calibration framework that uses Platt-scaling and can apply to any dataset, analyzing its effect on model confidence alignment with developer acceptance. Human-centered design principles are explored through scenario-based design, interviews, and surveys with designers and professional developers to determine optimal presentation of reliability signals.

Result: A large-scale analysis of 24 million developer interactions reveals that generic post-hoc calibration using Platt-scaling does not significantly improve confidence reliability on average. Personalized calibration can help, but only if extensive user interaction data is available. Developer studies show a clear preference for reliability signals delivered as non-numerical, color-coded indicators in the IDE.

Conclusion: Post-hoc calibration for LLMs in code generation does not reliably improve confidence measures for end users at scale, though personalization shows some promise if enough data exists. The presentational design of reliability signals is crucial—developers prefer color-coded, non-numerical indicators integrated into code editors.

Abstract: The introduction of large language models into integrated development
environments (IDEs) is revolutionizing software engineering, yet it poses
challenges to the usefulness and reliability of Artificial
Intelligence-generated code. Post-hoc calibration of internal model confidences
aims to align probabilities with an acceptability measure. Prior work suggests
calibration can improve alignment, but at-scale evidence is limited. In this
work, we investigate the feasibility of applying calibration of code models to
an in-IDE context. We study two aspects of the problem: (1) the technical
method for implementing confidence calibration and improving the reliability of
code generation models, and (2) the human-centered design principles for
effectively communicating reliability signal to developers. First, we develop a
scalable and flexible calibration framework which can be used to obtain
calibration weights for open-source models using any dataset, and evaluate
whether calibrators improve the alignment between model confidence and
developer acceptance behavior. Through a large-scale analysis of over 24
million real-world developer interactions across multiple programming
languages, we find that a general, post-hoc calibration model based on
Platt-scaling does not, on average, improve the reliability of model confidence
signals. We also find that while dynamically personalizing calibration to
individual users can be effective, its effectiveness is highly dependent on the
volume of user interaction data. Second, we conduct a multi-phase design study
with 3 expert designers and 153 professional developers, combining
scenario-based design, semi-structured interviews, and survey validation,
revealing a clear preference for presenting reliability signals via
non-numerical, color-coded indicators within the in-editor code generation
workflow.

</details>


### [18] [Collaborative LLM Agents for C4 Software Architecture Design Automation](https://arxiv.org/abs/2510.22787)
*Kamil Szczepanik,Jarosław A. Chudziak*

Main category: cs.SE

TL;DR: The paper presents an LLM-based system that automates C4 software architecture modeling by simulating expert dialogues and combines rule-based and LLM-driven evaluation to ensure model quality, showing promising results on several case studies.


<details>
  <summary>Details</summary>
Motivation: Designing software architecture using the C4 model is crucial but currently manual and labor-intensive. There is a need to automate and streamline this process to save time and improve efficiency.

Method: The authors propose a multi-agent system powered by large language models (LLMs). These agents simulate expert dialogues to analyze requirements and automatically generate C4 architectural views (Context, Container, Component). The generated architecture is evaluated using a hybrid framework combining deterministic, rule-based checks with qualitative, LLM-based scoring.

Result: When tested on five typical software system briefs, the system produced C4 models quickly, with high structural correctness and semantic alignment. Different LLMs showed varied strengths in architectural design tasks.

Conclusion: The study successfully automates C4 software architecture modeling using LLM-based agents and proposes robust evaluation methods. This advances the field of automated architecture design and its assessment.

Abstract: Software architecture design is a fundamental part of creating every software
system. Despite its importance, producing a C4 software architecture model, the
preferred notation for such architecture, remains manual and time-consuming. We
introduce an LLM-based multi-agent system that automates this task by
simulating a dialogue between role-specific experts who analyze requirements
and generate the Context, Container, and Component views of the C4 model.
Quality is assessed with a hybrid evaluation framework: deterministic checks
for structural and syntactic integrity and C4 rule consistency, plus semantic
and qualitative scoring via an LLM-as-a-Judge approach. Tested on five
canonical system briefs, the workflow demonstrates fast C4 model creation,
sustains high compilation success, and delivers semantic fidelity. A comparison
of four state-of-the-art LLMs shows different strengths relevant to
architectural design. This study contributes to automated software architecture
design and its evaluation methods.

</details>


### [19] [On the Freshness of Pinned Dependencies in Maven](https://arxiv.org/abs/2510.22815)
*Vasudev Vikram,Yuvraj Agarwal,Rohan Padhye*

Main category: cs.SE

TL;DR: Over 60% of Maven projects pin dependencies to outdated versions, posing security risks. The proposed Pin-Freshener tool uses crowdsourced tests to help developers confidently update dependencies, increasing test coverage and security.


<details>
  <summary>Details</summary>
Motivation: Developers pin library dependencies to ensure build reproducibility and avoid breaking changes, but this practice can lead to the use of outdated libraries with security vulnerabilities. The paper aims to understand how often this happens and presents a solution to encourage safer upgrade practices.

Method: The authors conducted an empirical study of Maven library usage, analyzing how frequently dependencies are pinned to outdated versions (explicitly defined as stale pins versus fresh pins). They also developed a tool called Pin-Freshener, which uses crowdsourced test results from peer projects to recommend safe upgrades and improve confidence in dependency freshness.

Result: The study found that more than 60% of consumers of popular Maven libraries use stale dependency pins, with some pins being over a year old. Upgrading to latest minor or patch versions could mitigate security issues in roughly 10% of cases. Pin-Freshener, by utilizing 1-5 additional crowdsourced test suites, increased coverage by 35-100%, and provided actionable upgrade confidence to thousands of projects.

Conclusion: Pinning dependencies, while useful for reproducibility, is widespread and leads to outdated, potentially vulnerable software. Pin-Freshener effectively leverages crowdsourced testing to safely encourage upgrading, offering developers practical tools and signals to improve ecosystem security and reliability.

Abstract: Library dependencies in software ecosystems play a crucial role in the
development of software. As newer releases of these libraries are published,
developers may opt to pin their dependencies to a particular version. While
pinning may have benefits in ensuring reproducible builds and avoiding breaking
changes, it bears larger risks in using outdated dependencies that may contain
bugs and security vulnerabilities. To understand the frequency and consequences
of dependency pinning, we first define the concepts of stale and fresh pins,
which are distinguished based on how outdated the dependency is relative to the
release date of the project. We conduct an empirical study to show that over
60% of consumers of popular Maven libraries contain stale pins to their
dependencies, with some outdated versions over a year old. These pinned
versions often miss out on security fixes; we find that 10% of all dependency
upgrades in our dataset to the latest minor or patch version would reduce
security vulnerabilities.
  We prototype an approach called Pin-Freshener that can encourage developers
to freshen their pins by leveraging the insight that crowdsourced tests of peer
projects can provide additional signal for the safety of an upgrade. Running
Pin-Freshener on dependency upgrades shows that just 1-5 additional test suites
can provide 35-100% more coverage of a dependency, compared to that of a single
consumer test suite. Our evaluation on real-world pins to the top 500 popular
libraries in Maven shows that Pin-Freshener can provide an additional signal of
at least 5 passing crowdsourced test suites to over 3,000 consumers to safely
perform an upgrade that reduces security vulnerabilities. Pin-Freshener can
provide practical confidence to developers by offering additional signal beyond
their own test suites, representing an improvement over current practices.

</details>


### [20] [CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs](https://arxiv.org/abs/2510.22986)
*Junjie Huang,Minghua He,Jinyang Liu,Yintong Huo,Domenico Bianculli,Michael R. Lyu*

Main category: cs.SE

TL;DR: CodeAD uses LLMs to automatically generate interpretable, efficient Python rules for log anomaly detection, outperforming existing methods in accuracy and speed, with low cost and high scalability.


<details>
  <summary>Details</summary>
Motivation: Log-based anomaly detection is essential for maintaining reliability in large-scale online systems. Existing methods such as machine learning, deep learning, and LLM-based approaches have advanced the field but suffer from low interpretability, high inference costs, and significant preprocessing, limiting their usefulness in real-time, high-volume scenarios. While rule-based systems are more efficient and transparent, they require much manual effort and are hard to scale.

Method: The paper introduces CodeAD, a framework that leverages LLMs to automatically generate lightweight Python rule functions for LogAD. CodeAD uses hierarchical clustering and anchor-grounded sampling to create representative log windows, enabling LLMs to identify anomaly patterns. It operates through an agentic workflow, iteratively generating, testing, repairing, and refining rules to ensure correctness and abstraction. The resulting rules run directly on raw logs, supporting efficient anomaly detection.

Result: Comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird) show that CodeAD achieves a 3.6% average absolute improvement in F1 score over existing state-of-the-art baselines, processes large datasets up to 4x faster, and incurs very low cost (under $4 per dataset for LLM invocation).

Conclusion: CodeAD offers a practical, scalable, interpretable, and efficient solution for online log anomaly detection, making automated LogAD feasible in real-world environments.

Abstract: Log-based anomaly detection (LogAD) is critical for maintaining the
reliability and availability of large-scale online service systems. While
machine learning, deep learning, and large language models (LLMs)-based methods
have advanced the LogAD, they often suffer from limited interpretability, high
inference costs, and extensive preprocessing requirements, limiting their
practicality for real-time, high-volume log analysis. In contrast, rule-based
systems offer efficiency and transparency, but require significant manual
effort and are difficult to scale across diverse and evolving environments. In
this paper, We present CodeAD, a novel framework that automatically synthesizes
lightweight Python rule functions for LogAD using LLMs. CodeAD introduces a
hierarchical clustering and anchor-grounded sampling strategy to construct
representative contrastive log windows, enabling LLMs to discern discriminative
anomaly patterns. To ensure robustness and generalizability, CodeAD employs an
agentic workflow that iteratively generates, tests, repairs, and refines the
rules until it meets correctness and abstraction requirements. The synthesized
rules are interpretable, lightweight, and directly executable on raw logs,
supporting efficient and transparent online anomaly detection. Our
comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird)
demonstrate that CodeAD achieves an average absolute improvement of 3.6% F1
score over the state-of-the-art baselines, while processing large datasets up
to 4x faster and at a fraction of the cost (total LLM invocation cost under 4
USD per dataset). These results highlight CodeAD as a practical and scalable
solution for online monitoring systems, enabling interpretable, efficient, and
automated LogAD in real-world environment.

</details>


### [21] [TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation](https://arxiv.org/abs/2510.23010)
*Ming-Tung Shen,Yuh-Jzer Joung*

Main category: cs.SE

TL;DR: This work proposes TALM, a dynamic tree-structured multi-agent framework with long-term memory, enhancing LLM code generation by flexible reasoning and efficient error correction. TALM shows strong performance and efficiency across standard coding benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) need to manage context and perform multi-step reasoning for agentic code generation, but existing multi-agent frameworks are rigid and inefficient in recovering from reasoning errors.

Method: TALM introduces a tree-structured multi-agent framework that uses structured task decomposition, localized re-reasoning, and incorporates a long-term memory module. It uses an extensible tree for collaboration, enabling efficient error correction and experience reuse via semantic querying of memory.

Result: Experiments on HumanEval, BigCodeBench, and ClassEval benchmarks show that TALM consistently achieves strong reasoning performance and high token efficiency.

Conclusion: TALM is robust and practically useful for complex code generation, overcoming limitations of prior multi-agent frameworks through flexible reasoning and long-term memory.

Abstract: Agentic code generation requires large language models (LLMs) capable of
complex context management and multi-step reasoning. Prior multi-agent
frameworks attempt to address these challenges through collaboration, yet they
often suffer from rigid workflows and high reasoning recovery costs. To
overcome these limitations, we propose TALM (Tree-Structured Multi-Agent
Framework with Long-Term Memory), a dynamic framework that integrates
structured task decomposition, localized re-reasoning, and long-term memory
mechanisms. TALM employs an extensible tree-based collaboration structure. The
parent-child relationships, when combined with a divide-and-conquer strategy,
enhance reasoning flexibility and enable efficient error correction across
diverse task scopes. Furthermore, a long-term memory module enables semantic
querying and integration of prior knowledge, supporting implicit
self-improvement through experience reuse. Experimental results on HumanEval,
BigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently
delivers strong reasoning performance and high token efficiency, highlighting
its robustness and practical utility in complex code generation tasks.

</details>


### [22] [From Online User Feedback to Requirements: Evaluating Large Language Models for Classification and Specification Tasks](https://arxiv.org/abs/2510.23055)
*Manjeshwar Aniruddh Mallya,Alessio Ferrari,Mohammad Amin Zadenoori,Jacek Dąbrowski*

Main category: cs.SE

TL;DR: Lightweight open-source LLMs can moderately automate the analysis and specification generation from user feedback for requirements engineering, showing reasonable accuracy and quality. The study offers empirical evidence, practical resources, and guidance on their use and limitations.


<details>
  <summary>Details</summary>
Motivation: Online user feedback is crucial for requirements engineering (RE), but it is difficult to analyze due to its large volume and noise. Large language models (LLMs) offer strong potential to automate feedback analysis and generate requirements specifications.

Method: The paper evaluates five lightweight open-source LLMs on three RE tasks: user request classification, non-functional requirement (NFR) classification, and requirements specification generation. Performance is measured on two datasets for classification and by human evaluation for specification quality.

Result: LLMs achieved moderate-to-high classification accuracy (F1 scores between 0.47 and 0.68) and moderately high specification quality (average rating around 3 out of 5).

Conclusion: Lightweight LLMs show promise for supporting feedback-driven requirements engineering tasks. The paper provides empirical evaluation, a replication package, and insights into the models' capabilities and limitations.

Abstract: [Context and Motivation] Online user feedback provides valuable information
to support requirements engineering (RE). However, analyzing online user
feedback is challenging due to its large volume and noise. Large language
models (LLMs) show strong potential to automate this process and outperform
previous techniques. They can also enable new tasks, such as generating
requirements specifications.
  [Question-Problem] Despite their potential, the use of LLMs to analyze user
feedback for RE remains underexplored. Existing studies offer limited empirical
evidence, lack thorough evaluation, and rarely provide replication packages,
undermining validity and reproducibility.
  [Principal Idea-Results] We evaluate five lightweight open-source LLMs on
three RE tasks: user request classification, NFR classification, and
requirements specification generation. Classification performance was measured
on two feedback datasets, and specification quality via human evaluation. LLMs
achieved moderate-to-high classification accuracy (F1 ~ 0.47-0.68) and
moderately high specification quality (mean ~ 3/5).
  [Contributions] We newly explore lightweight LLMs for feedback-driven
requirements development. Our contributions are: (i) an empirical evaluation of
lightweight LLMs on three RE tasks, (ii) a replication package, and (iii)
insights into their capabilities and limitations for RE.

</details>


### [23] [Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs](https://arxiv.org/abs/2510.23068)
*Ella Dodor,Cristina V. Lopes*

Main category: cs.SE

TL;DR: Checkstyle+ combines rule-based and LLM-driven analysis to better catch subtle code style violations that traditional linters miss, and demonstrates improved performance on real-world Java code.


<details>
  <summary>Details</summary>
Motivation: Although good code style is essential for software quality, traditional rule-based linters often miss more nuanced or complex style issues that require deeper code understanding.

Method: The authors introduce Checkstyle+, a hybrid linting tool that combines traditional rule-based checks (like those in Checkstyle) with large language model (LLM) capabilities to catch subtle style violations.

Result: When tested on 380 Java code files from Codeforces, Checkstyle+ outperformed standard Checkstyle in identifying violations of semantically nuanced style rules.

Conclusion: Hybridizing rule-based linting with LLMs enhances the detection of complex code style violations, improving code quality assurance tools.

Abstract: Good code style improves program readability, maintainability, and
collaboration, and is an integral component of software quality. Developers,
however, often cut corners when following style rules, leading to the wide
adoption of tools such as linters in professional software development
projects. Traditional linters like Checkstyle operate using rigid, rule-based
mechanisms that effectively detect many surface-level violations. However, in
most programming languages, there is a subset of style rules that require a
more nuanced understanding of code, and fall outside the scope of such static
analysis. In this paper, we propose Checkstyle+, a hybrid approach that
augments Checkstyle with large language model (LLM) capabilities, to identify
style violations that elude the conventional rule-based analysis. Checkstyle+
is evaluated on a sample of 380 Java code files, drawn from a broader dataset
of 30,800 real-world Java programs sourced from accepted Codeforces
submissions. The results show that Checkstyle+ achieves superior performance
over standard Checkstyle in detecting violations of the semantically nuanced
rules.

</details>


### [24] [Validating Formal Specifications with LLM-generated Test Cases](https://arxiv.org/abs/2510.23350)
*Alcino Cunha,Nuno Macedo*

Main category: cs.SE

TL;DR: Automated generation of Alloy test cases using GPT-5 and other LLMs shows promising results, making the validation process easier and more reliable by reducing manual effort and improving accuracy in detecting specification errors.


<details>
  <summary>Details</summary>
Motivation: Validating formal specifications is crucial but manually defining test cases is tedious and prone to errors, leading users to potentially skip this important step.

Method: The authors conducted an empirical evaluation, using pre-trained large language models (LLMs) to automatically generate test cases from natural language requirements. They focused on structural requirements for domain models specified in Alloy and compared GPT-5's effectiveness with other LLMs.

Result: GPT-5 is shown to be quite effective, generating syntactically correct positive and negative test cases that align with the requirements and help detect incorrect human-written specifications. Other LLMs were also evaluated, but GPT-5 was the primary focus.

Conclusion: Automating test case generation with LLMs, particularly GPT-5, significantly aids the validation of formal specifications, reducing manual effort and improving error detection.

Abstract: Validation is a central activity when developing formal specifications.
Similarly to coding, a possible validation technique is to define upfront test
cases or scenarios that a future specification should satisfy or not.
Unfortunately, specifying such test cases is burdensome and error prone, which
could cause users to skip this validation task. This paper reports the results
of an empirical evaluation of using pre-trained large language models (LLMs) to
automate the generation of test cases from natural language requirements. In
particular, we focus on test cases for structural requirements of simple domain
models formalized in the Alloy specification language. Our evaluation focuses
on the state-of-art GPT-5 model, but results from other closed- and open-source
LLMs are also reported. The results show that, in this context, GPT-5 is
already quite effective at generating positive (and negative) test cases that
are syntactically correct and that satisfy (or not) the given requirement, and
that can detect many wrong specifications written by humans.

</details>


### [25] [Floating-Point Neural Network Verification at the Software Level](https://arxiv.org/abs/2510.23389)
*Edoardo Manino,Bruno Farias,Rafael Sá Menezes,Fedor Shmarov,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: NeuroCodeBench 2.0 is a new C-based benchmark for verifying neural network correctness at the software level, highlighting that current tools largely fall short but are improving as a result of this benchmark.


<details>
  <summary>Details</summary>
Motivation: Neural networks are increasingly used in safety-critical systems, where correctness is vital. However, current verification techniques do not adequately guarantee software-level safety, particularly concerning floating-point implementations.

Method: The authors approach this by specifying and verifying neural networks—explicitly considering their floating-point implementations. They develop NeuroCodeBench 2.0, a benchmark suite in C compatible with SV-COMP, containing 912 examples across different neural network components. This suite enables comparison and evaluation of verification tools.

Result: The benchmark allows a rigorous evaluation of eight leading software verifiers on neural network code. The study finds that these tools can solve only about 11% of the benchmarks correctly and produce roughly 3% incorrect results. However, the release of NeuroCodeBench 2.0 has already spurred improvements in these verification techniques, as evidenced by a historical analysis.

Conclusion: NeuroCodeBench 2.0 fills a gap by providing a detailed, software-level neural network verification benchmark, revealing significant limitations in current verification tools but also driving progress in the field.

Abstract: The behaviour of neural network components must be proven correct before
deployment in safety-critical systems. Unfortunately, existing neural network
verification techniques cannot certify the absence of faults at the software
level. In this paper, we show how to specify and verify that neural networks
are safe, by explicitly reasoning about their floating-point implementation. In
doing so, we construct NeuroCodeBench 2.0, a benchmark comprising 912 neural
network verification examples that cover activation functions, common layers,
and full neural networks of up to 170K parameters. Our verification suite is
written in plain C and is compatible with the format of the International
Competition on Software Verification (SV-COMP). Thanks to it, we can conduct
the first rigorous evaluation of eight state-of-the-art software verifiers on
neural network code. The results show that existing automated verification
tools can correctly solve an average of 11% of our benchmark, while producing
around 3% incorrect verdicts. At the same time, a historical analysis reveals
that the release of our benchmark has already had a significantly positive
impact on the latter.

</details>


### [26] [Tracing Distribution Shifts with Causal System Maps](https://arxiv.org/abs/2510.23528)
*Joran Leest,Ilias Gerostathopoulos,Patricia Lago,Claudia Raibulet*

Main category: cs.SE

TL;DR: The paper introduces ML System Maps, a causal mapping approach to systematically trace and attribute the causes of distribution shifts in machine learning systems, moving beyond mere detection to support thorough root-cause analysis.


<details>
  <summary>Details</summary>
Motivation: Monitoring machine learning systems is challenging, particularly in identifying the causes of distribution shifts. Current methods focus on detecting the occurrence of shifts but not their root causes, often requiring manual tracing to determine if issues are due to software faults, data quality, or natural changes.

Method: The paper proposes 'ML System Maps,' which are causal maps featuring layered views that explicitly visualize the propagation paths linking the environment with the internal components of the ML system.

Result: By using ML System Maps, the approach allows for a more systematic attribution of distribution shifts, mapping them to potential causes across the system.

Conclusion: ML System Maps offer a structured and visual method to trace and attribute the causes of distribution shifts in ML systems, supporting better root-cause analysis. The paper presents a plan for further research and evaluation of this approach.

Abstract: Monitoring machine learning (ML) systems is hard, with standard practice
focusing on detecting distribution shifts rather than their causes. Root-cause
analysis often relies on manual tracing to determine whether a shift is caused
by software faults, data-quality issues, or natural change. We propose ML
System Maps -- causal maps that, through layered views, make explicit the
propagation paths between the environment and the ML system's internals,
enabling systematic attribution of distribution shifts. We outline the approach
and a research agenda for its development and evaluation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [27] [Linear effects, exceptions, and resource safety: a Curry-Howard correspondence for destructors](https://arxiv.org/abs/2510.23517)
*Sidney Congard,Guillaume Munch-Maccagnoni,Rémi Douence*

Main category: cs.PL

TL;DR: This paper creates type-theoretic and category-based models for programming languages, effectively combining resource-safety, side effects, and exception handling, capturing properties found in C++/Rust using concepts like destructors and move semantics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of combining linearity (managing resources safely), computational effects (side effects in programming), and exceptions in programming language models, and specifically to model resource-safety like that found in modern systems languages (e.g., C++/Rust).

Method: The authors analyze the strength of a monad $T(- \oplus E)$ within a linear setting, introducing the allocation monad to study resource-safety. They develop two effectful calculi: (1) a linear call-by-push-value language with allocation effects, and (2) an affine ordered variant with exceptions and destructors, using category-theoretic constructions inspired by C++/Rust.

Result: They establish resource-safety for a linear call-by-push-value calculus with allocation/deallocation effects, thanks to linear/ordered typing. They then show how exceptions and destructors can be integrated via default destruction, supporting side-effects during weakening and random-order resource release through move operations.

Conclusion: The paper offers abstract models that successfully combine linearity, effects, and exceptions, allowing the formalization and proof of resource-safety properties similar to those in languages like C++/Rust. The exchange rule enables flexible and safe resource management.

Abstract: We analyse the problem of combining linearity, effects, and exceptions, in
abstract models of programming languages, as the issue of providing some kind
of strength for a monad $T(- \oplus E)$ in a linear setting. We consider in
particular for $T$ the allocation monad, which we introduce to model and study
resource-safety properties. We apply these results to a series of two linear
effectful calculi for which we establish their resource-safety properties.
  The first calculus is a linear call-by-push-value language with two
allocation effects $\mathit{new}$ and $\mathit{delete}$. The resource-safety
properties follow from the linear (and even ordered) character of the typing
rules.
  We then explain how to integrate exceptions on top of linearity and effects
by adjoining default destruction actions to types, as inspired by C++/Rust
destructors. We see destructors as objects $\delta : A\rightarrow TI$ in the
slice category over $TI$. This construction gives rise to a second calculus, an
affine ordered call-by-push-value language with exceptions and destructors, in
which the weakening rule performs a side-effect. As in C++/Rust, a ``move''
operation is necessary to allow random-order release of resources, as opposed
to last-in-first-out order. Moving resources is modelled as an exchange rule
that performs a side-effect.

</details>
