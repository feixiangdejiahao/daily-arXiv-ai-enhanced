<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.PL](#cs.PL) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Hybrid Quantum-Classical Machine Learning with PennyLane: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2511.14786)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: PennyLane is introduced as a versatile Python toolkit enabling hybrid quantum-classical machine learning, with strong integration into existing ML frameworks. The paper provides use cases and practical examples, positioning PennyLane as a key reference for quantum-enhanced computational research.


<details>
  <summary>Details</summary>
Motivation: Quantum computing promises advantages for complex learning and optimization tasks, but practical implementation requires effective integration with classical machine learning techniques and software. There's a need for accessible tools that allow researchers to utilize hybrid quantum-classical workflows.

Method: Introduce and demonstrate PennyLane, an open-source Python framework, by showcasing use cases such as quantum kernel methods, variational quantum eigensolvers, portfolio optimization, and integration with popular classical ML tools (PyTorch, TensorFlow, JAX). Concrete code examples and integration with tools like scikit-learn, pandas, and matplotlib are provided.

Result: PennyLane efficiently supports construction of quantum circuits, automatic differentiation, and hybrid optimization tasks. It integrates well with classical machine learning workflows, broadening the accessibility and applicability of quantum algorithms. Concrete examples evidence its flexibility and ease of use for quantum-enhanced data science.

Conclusion: PennyLane establishes itself as an essential methodological tool for hybrid quantum-classical machine learning and optimization. It bridges foundational quantum concepts and practical ML applications, and is recommended as a standard reference for researchers implementing quantum workflows in Python.

Abstract: Hybrid quantum-classical machine learning represents a frontier in computational research, combining the potential advantages of quantum computing with established classical optimization techniques. PennyLane provides a Python framework that seamlessly bridges quantum circuits and classical machine learning, enabling researchers to build, optimize, and deploy variational quantum algorithms. This paper introduces PennyLane as a versatile tool for quantum machine learning, optimization, and quantum chemistry applications. We demonstrate use cases including quantum kernel methods, variational quantum eigensolvers, portfolio optimization, and integration with classical ML frameworks such as PyTorch, TensorFlow, and JAX. Through concrete Python examples with widely used libraries such as scikit-learn, pandas, and matplotlib, we show how PennyLane facilitates efficient quantum circuit construction, automatic differentiation, and hybrid optimization workflows. By situating PennyLane within the broader context of quantum computing and machine learning, we highlight its role as a methodological building block for quantum-enhanced data science. Our goal is to provide researchers and practitioners with a concise reference that bridges foundational quantum computing concepts and applied machine learning practice, making PennyLane a default citation for hybrid quantum-classical workflows in Python-based research.

</details>


### [2] [Enabling Predictive Maintenance in District Heating Substations: A Labelled Dataset and Fault Detection Evaluation Framework based on Service Data](https://arxiv.org/abs/2511.14791)
*Cyriana M. A. Roelofs,Edison Guevara Bastidas,Thomas Hugo,Stefan Faulstich,Anna Cadenbach*

Main category: cs.SE

TL;DR: This paper introduces an open source framework, including a labelled dataset, metrics, and baseline models, for early fault detection in district heating substations. The solution achieves high accuracy and earliness in identifying faults, enabling reproducible benchmarking and supporting consistent progress in this area.


<details>
  <summary>Details</summary>
Motivation: Early detection of faults in district heating substations is crucial for operational efficiency and reducing return temperatures, but development in the area is restricted by a lack of publicly available and labelled datasets for benchmarking and research.

Method: An open source framework is developed, featuring a public time-series dataset of 93 district heating substations with detailed annotations, a multi-metric evaluation method (Accuracy, Reliability/eventwise F-score, Earliness), baseline implementations using the EnergyFaultDetector Python library, and root cause analysis via ARCANA. Three use cases are demonstrated to show utility for operators.

Result: The models in the framework achieved a high normal-behaviour accuracy of 0.98, an eventwise F-score of 0.83 (for reliable fault detection), detected 60% of the faults before customer reports with an average advance notice of 3.9 days. The dataset and methodology allow for reproducible, meaningful benchmarking and comparison in the field.

Conclusion: This work provides the first comprehensive open-source dataset and benchmarking framework for fault detection in district heating substations, supporting consistent, reproducible, and operationally meaningful research and development. It substantially advances the foundation for future work in early fault detection and diagnosis in this domain.

Abstract: Early detection of faults in district heating substations is imperative to reduce return temperatures and enhance efficiency. However, progress in this domain has been hindered by the limited availability of public, labelled datasets. We present an open source framework combining a service report validated public dataset, an evaluation method based on Accuracy, Reliability, and Earliness, and baseline results implemented with EnergyFaultDetector, an open source Python framework.
  The dataset contains time series of operational data from 93 substations across two manufacturers, annotated with a list of disturbances due to faults and maintenance actions, a set of normal-event examples and detailed fault metadata. We evaluate the EnergyFaultDetector using three metrics: Accuracy for recognising normal behaviour, an eventwise F Score for reliable fault detection with few false alarms, and Earliness for early detection. The framework also supports root cause analysis using ARCANA. We demonstrate three use cases to assist operators in interpreting anomalies and identifying underlying faults. The models achieve high normal-behaviour accuracy (0.98) and eventwise F-score (beta=0.5) of 0.83, detecting 60% of the faults in the dataset before the customer reports a problem, with an average lead time of 3.9 days.
  Integrating an open dataset, metrics, open source code, and baselines establishes a reproducible, fault centric benchmark with operationally meaningful evaluation, enabling consistent comparison and development of early fault detection and diagnosis methods for district heating substations.

</details>


### [3] [irace-evo: Automatic Algorithm Configuration Extended With LLM-Based Code Evolution](https://arxiv.org/abs/2511.14794)
*Camilo Chacón Sartori,Christian Blum*

Main category: cs.SE

TL;DR: irace-evo combines LLM-driven code evolution with parameter tuning, discovering algorithm variants better than current solutions for bin packing, all at low cost and with minimal resource use.


<details>
  <summary>Details</summary>
Motivation: Traditional algorithm configuration tools like irace optimize only parameter values, not the underlying algorithm code. This limits their ability to discover new and improved algorithmic variants.

Method: The authors present irace-evo, an extension of irace. Irace-evo uses large language models (LLMs) to automatically evolve and modify algorithmic code, alongside tuning parameters. It features multi-language code support, efficient context management to minimize token use, and applies a principle to ensure evolution always starts from the original code for robustness.

Result: Evaluating irace-evo on the CMSA metaheuristic for the Variable-Sized Bin Packing Problem, the framework identified new algorithm variants outperforming the current state-of-the-art. This was achieved with lightweight LLMs and at low computational and monetary costs (less than 2 euros total).

Conclusion: Integrating LLM-powered code evolution with automated parameter tuning creates a powerful and economical method for developing improved metaheuristics, advancing the field of heuristic design.

Abstract: Automatic algorithm configuration tools such as irace efficiently tune parameter values but leave algorithmic code unchanged. This paper introduces a first version of irace-evo, an extension of irace that integrates code evolution through large language models (LLMs) to jointly explore parameter and code spaces. The proposed framework enables multi-language support (e.g., C++, Python), reduces token consumption via progressive context management, and employs the Always-From-Original principle to ensure robust and controlled code evolution. We evaluate irace-evo on the Construct, Merge, Solve & Adapt (CMSA) metaheuristic for the Variable-Sized Bin Packing Problem (VSBPP). Experimental results show that irace-evo can discover new algorithm variants that outperform the state-of-the-art CMSA implementation while maintaining low computational and monetary costs. Notably, irace-evo generates competitive algorithmic improvements using lightweight models (e.g., Claude Haiku 3.5) with a total usage cost under 2 euros. These results demonstrate that coupling automatic configuration with LLM-driven code evolution provides a powerful, cost-efficient avenue for advancing heuristic design and metaheuristic optimization.

</details>


### [4] [Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods](https://arxiv.org/abs/2511.14798)
*Ahmad Memon,Abdallah Mohamed*

Main category: cs.SE

TL;DR: This paper compares Direct and Reverse AI grading techniques for programming assignments. Direct is fast, Reverse assesses correction effort for finer grading. Both improve consistency, but require careful prompt engineering, especially for partial marks and logic errors. Synthetic code broadens evaluation, and hybrid AI-human systems promise further gains in CS grading.


<details>
  <summary>Details</summary>
Motivation: Manual grading of programming assignments is time-consuming and often inconsistent. Common automatic methods like unit testing do not effectively support partial credit or nuanced feedback. Advances in large language models create an opportunity for more effective, automated grading.

Method: The paper compares two AI-based grading techniques: Direct (AI applies rubric directly to student code) and Reverse (AI fixes errors and grades by analyzing the nature and number of fixes required). Both methods are tested on different grading scales and against tutor evaluations, with additional experiments on AI-generated synthetic student code to probe performance.

Result: Direct grading is faster and simpler, while Reverse grading produces finer assessments—especially for partial credit by emphasizing correction effort. Both methods benefit from careful prompt design and each has strengths and limitations. Synthetic code testing shows effectiveness across error types and difficulty levels.

Conclusion: AI-based grading can improve consistency, efficiency, and fairness in programming education, especially when combined with thoughtful prompt engineering and possibly hybrid human-AI systems. The Reverse technique, though slower, often yields more nuanced and accurate scoring.

Abstract: Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.
  This paper compares two AI-based grading techniques: \textit{Direct}, where the AI model applies a rubric directly to student code, and \textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.
  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.

</details>


### [5] [Scalable and Efficient Large-Scale Log Analysis with LLMs: An IT Software Support Case Study](https://arxiv.org/abs/2511.14803)
*Pranjal Gupta,Karan Bhukar,Harshit Kumar,Seema Nagar,Prateeti Mohapatra,Debanjana Kar*

Main category: cs.SE

TL;DR: A log analysis tool powered by LLMs, optimized for CPU deployment, dramatically increased efficiency and cut costs in IT support by automating log processing and issue diagnosis, as validated in a large-scale, real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of enormous volumes of log data in IT environments is impractical, creating the need for automated solutions for log analysis and issue diagnosis.

Method: The paper proposes a log analytics tool that utilizes Large Language Models (LLMs) for automated processing and diagnosis of log files. It introduces a novel method to efficiently run LLMs on CPUs, enabling the system to handle massive volumes of logs quickly without reducing result quality.

Result: The deployed tool has been in production since March 2024, scaling to 70 software products and processing over 2,000 support tickets. It provided significant operational improvements, saving 300+ man hours and an estimated $15,444 per month in manpower costs relative to traditional manual log analysis.

Conclusion: LLM-powered automated log analysis tools can substantially improve efficiency and cost-effectiveness in IT software support, especially when optimized for CPU-based processing at scale, as demonstrated in a real-world deployment.

Abstract: IT environments typically have logging mechanisms to monitor system health and detect issues. However, the huge volume of generated logs makes manual inspection impractical, highlighting the importance of automated log analysis in IT Software Support. In this paper, we propose a log analytics tool that leverages Large Language Models (LLMs) for log data processing and issue diagnosis, enabling the generation of automated insights and summaries. We further present a novel approach for efficiently running LLMs on CPUs to process massive log volumes in minimal time without compromising output quality. We share the insights and lessons learned from deployment of the tool - in production since March 2024 - scaled across 70 software products, processing over 2000 tickets for issue diagnosis, achieving a time savings of 300+ man hours and an estimated $15,444 per month in manpower costs compared to the traditional log analysis practices.

</details>


### [6] [Towards Continuous Assurance with Formal Verification and Assurance Cases](https://arxiv.org/abs/2511.14805)
*Dhaminda B. Abeywickrama,Michael Fisher,Frederic Wheeler,Louise Dennis*

Main category: cs.SE

TL;DR: This paper introduces a framework for continuous assurance of autonomous systems, merging design-time, runtime, and evolution assurance into a unified workflow with automatic traceability, demonstrated on a nuclear inspection robot.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems need sustained, justified confidence in their correctness and safety throughout their lifecycle. Traditional assurance separates design-time and runtime, leading to fragmented and less adaptable arguments, posing a major challenge for assured autonomy.

Method: The paper proposes a unified Continuous Assurance Framework integrating assurance processes across design-time, runtime, and system evolution using a traceable, model-driven workflow. Specifically, it uses RoboChart for formal verification of functional correctness and PRISM for probabilistic risk analysis in the design-time phase. An Eclipse plugin is developed to automate the regeneration of assurance arguments when specifications or verification results change.

Result: The approach is demonstrated on a nuclear inspection robot scenario, showing its effectiveness and traceability. The framework's alignment with Trilateral AI Principles is discussed, ensuring compliance with regulator-endorsed best practices.

Conclusion: Integrating design, runtime, and evolution-time assurance in a unified traceable workflow can improve adaptability and reliability of autonomous systems. Automated argument regeneration ensures ongoing traceability and compliance with best practices.

Abstract: Autonomous systems must sustain justified confidence in their correctness and safety across their operational lifecycle-from design and deployment through post-deployment evolution. Traditional assurance methods often separate development-time assurance from runtime assurance, yielding fragmented arguments that cannot adapt to runtime changes or system updates - a significant challenge for assured autonomy. Towards addressing this, we propose a unified Continuous Assurance Framework that integrates design-time, runtime, and evolution-time assurance within a traceable, model-driven workflow as a step towards assured autonomy. In this paper, we specifically instantiate the design-time phase of the framework using two formal verification methods: RoboChart for functional correctness and PRISM for probabilistic risk analysis. We also propose a model-driven transformation pipeline, implemented as an Eclipse plugin, that automatically regenerates structured assurance arguments whenever formal specifications or their verification results change, thereby ensuring traceability. We demonstrate our approach on a nuclear inspection robot scenario, and discuss its alignment with the Trilateral AI Principles, reflecting regulator-endorsed best practices.

</details>


### [7] [Automatic Pipeline Provisioning](https://arxiv.org/abs/2511.14825)
*Alexandre-Xavier Labonté-Lamoureux,Simon Boyer*

Main category: cs.SE

TL;DR: This paper investigates how automating the setup of software CI pipelines can improve deployment speed and efficiency, suggesting similar benefits for CD pipelines as well.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to streamline software engineering processes through faster pipeline deployments.

Method: The study explores and analyzes the application of automatic pipeline provisioning, with a primary focus on continuous integration (CI) pipelines.

Result: The research identifies benefits and practical applications of automatic pipeline provisioning in software engineering projects, specifically CI pipelines, and suggests possible parallels for continuous deployment (CD) pipelines.

Conclusion: Automatic pipeline provisioning can benefit software engineering by enabling faster and more efficient deployment of CI pipelines, with similar advantages expected for CD pipelines.

Abstract: The goal of this paper is to explore the benefits of automatic pipeline provisioning and identify how it can be applied. Automatic pipeline provisioning can be defined as a process of quickly deploying a pipeline for a software engineering project. This research will focus on CI pipelines, although the outcomes of this approach on CD pipelines will likely be similar.

</details>


### [8] [MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation](https://arxiv.org/abs/2511.14967)
*Basel Shbita,Farhan Ahmed,Chad DeLuca*

Main category: cs.SE

TL;DR: The paper introduces MermaidSeqBench, a benchmark to evaluate how well language models can generate Mermaid sequence diagrams from text. It uses both human and automated methods to build the dataset and judge outputs. Results show current LLMs have notable shortcomings, making this benchmark an important step for future research and evaluation improvements.


<details>
  <summary>Details</summary>
Motivation: There is a lack of benchmarks for evaluating large language models (LLMs) on their ability to generate accurate Mermaid sequence diagrams from textual descriptions, limiting progress and systematic evaluation in this area.

Method: The authors introduce MermaidSeqBench, a benchmark consisting of 132 human-verified and LLM-extended samples. The benchmark was built using a hybrid methodology—manual annotation, in-context LLM prompting, and rule-based variation. They use LLM-as-a-judge models to assess diagram generation across several fine-grained metrics such as syntax, activation handling, error handling, and usability. Multiple state-of-the-art LLMs and judge models are evaluated using this benchmark.

Result: Results show that there are significant differences in performance (capability gaps) across various LLMs and evaluation setups, highlighting room for improvement in structured diagram generation.

Conclusion: MermaidSeqBench provides a new foundation and tool for robust, rigorous evaluation of LLMs’ abilities to generate structured diagrams, facilitating future advancements and more granular assessment in this area.

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.

</details>


### [9] [FRIENDS GUI: A graphical user interface for data collection and visualization of vaping behavior from a passive vaping monitor](https://arxiv.org/abs/2511.15007)
*Shehan I Pranto,Brett Fassler,Md Rafi Islam,Ashley Schenkel,Larry W Hawk,Edward Sazonov*

Main category: cs.SE

TL;DR: The paper introduces an open-source system—a hardware device plus Python GUI—for recording and visualizing ENDS (vape) usage behavior. Validated with 24-hour data, the tool is easy to use and publicly available, potentially advancing research and regulatory science in this field.


<details>
  <summary>Details</summary>
Motivation: Understanding puffing topography is crucial for assessing ENDS use and related health risks, but effective tools for collecting and interpreting this behavioral data are lacking. Existing devices may lack accessibility or robustness for comprehensive research needs.

Method: The authors developed FRIENDS, an open-source hardware device for recording ENDS puffing behaviors. They also introduce a Python-based GUI software that extracts, decodes, and visualizes 24-hour puffing data from the device. Validation was performed with 24-hour experimental data to test the tool's accuracy and effectiveness.

Result: Validation showed accurate timestamp conversion, reliable decoding of events, and clear visualization of user behavior patterns over a 24-hour period. The FRIENDS software and hardware are accessible as open-source resources on GitHub.

Conclusion: The FRIENDS GUI makes ENDS behavioral data collection and analysis more accessible and interpretable for researchers, supporting better evaluation of ENDS use and potentially informing regulation.

Abstract: Understanding puffing topography (PT), which includes puff duration, intra puff interval, and puff count per session, is critical for evaluating Electronic Nicotine Delivery Systems (ENDS) use, toxicant exposure, and informing regulatory decisions. We developed FRIENDS (Flexible Robust Instrumentation of ENDS), an open-source device that records puffing and touch events of ENDS by attaching to it. This paper introduces the FRIENDS GUI that improves accessibility and interpretability of data collected by FRIENDS. The GUI is a Python-based open-source tool that extracts, decodes, and visualizes 24-hour puffing data from the FRIENDS device. Validation using 24-hour experimental data confirmed accurate timestamp conversion, reliable event decoding, and effective behavioral visualization. The software is freely available on GitHub for public use.

</details>


### [10] [Effective Code Membership Inference for Code Completion Models via Adversarial Prompts](https://arxiv.org/abs/2511.15107)
*Yuan Jiang,Zehao Li,Shan Huang,Christoph Treude,Xiaohong Su,Tiantian Wang*

Main category: cs.SE

TL;DR: AdvPrompt-MIA introduces an automated, deep learning-based MIA using adversarial code prompts, outperforming existing methods with up to 102% higher AUC, and works well across various models and datasets to better assess privacy risks.


<details>
  <summary>Details</summary>
Motivation: Current membership inference attacks (MIAs) for code completion models either use resource-intensive surrogate models or simple heuristics, which don't effectively capture the complex memorization behaviors of large code language models. This means privacy assessments are incomplete or inaccurate.

Method: The paper proposes AdvPrompt-MIA, which employs adversarially crafted code prompts to perturb model outputs. These variations are used to create feature vectors for a machine learning classifier that predicts whether a snippet was part of the model's training set. This approach leverages both adversarial techniques and deep learning to automatically learn nuanced memorization patterns.

Result: AdvPrompt-MIA achieves significantly higher accuracy than existing methods, with AUC gains up to 102% on leading code models like Code Llama 7B evaluated on the APPS and HumanEval benchmarks. The technique also demonstrates robustness, generalizing well across different models and datasets.

Conclusion: The proposed AdvPrompt-MIA method advances membership inference attacks for code completion models, offering an automated, accurate, and generalizable solution that highlights privacy risks more effectively than prior work.

Abstract: Membership inference attacks (MIAs) on code completion models offer an effective way to assess privacy risks by inferring whether a given code snippet was part of the training data. Existing black- and gray-box MIAs rely on expensive surrogate models or manually crafted heuristic rules, which limit their ability to capture the nuanced memorization patterns exhibited by over-parameterized code language models. To address these challenges, we propose AdvPrompt-MIA, a method specifically designed for code completion models, combining code-specific adversarial perturbations with deep learning. The core novelty of our method lies in designing a series of adversarial prompts that induce variations in the victim code model's output. By comparing these outputs with the ground-truth completion, we construct feature vectors to train a classifier that automatically distinguishes member from non-member samples. This design allows our method to capture richer memorization patterns and accurately infer training set membership. We conduct comprehensive evaluations on widely adopted models, such as Code Llama 7B, over the APPS and HumanEval benchmarks. The results show that our approach consistently outperforms state-of-the-art baselines, with AUC gains of up to 102%. In addition, our method exhibits strong transferability across different models and datasets, underscoring its practical utility and generalizability.

</details>


### [11] [MutDafny: A Mutation-Based Approach to Assess Dafny Specifications](https://arxiv.org/abs/2511.15403)
*Isabel Amaral,Alexandra Mendes,José Campos*

Main category: cs.SE

TL;DR: The paper presents MutDafny, a tool that applies mutation testing to detect weaknesses in Dafny formal specifications. By introducing faults and evaluating specification robustness, the tool uncovers real-world specification flaws in a large dataset, showing mutation testing's effectiveness in improving spec reliability.


<details>
  <summary>Details</summary>
Motivation: Specifications in formal verification languages like Dafny can contain errors that lead to verified programs deviating from their intended behavior. Despite their importance, these specifications are as susceptible to mistakes as program implementations. There is a need for tools to detect weaknesses in formal specifications to ensure program reliability.

Method: The authors develop MutDafny, a mutation testing tool for Dafny specifications. By introducing faults (mutations) into Dafny code and observing whether the mutated programs still verify against the specifications, MutDafny identifies potential weaknesses. The tool utilizes 32 mutation operators sourced from popular mutation testing tools and customized ones derived from bugfix commits in real-world Dafny projects.

Result: MutDafny was evaluated on a dataset of 794 real Dafny programs. Manual analysis of undetected mutants in the results uncovered five real-world specifications exhibiting weaknesses that could result in incorrect program verification. On average, these weak specifications were found at a rate of one per 241 lines of code.

Conclusion: Mutation testing can successfully reveal weaknesses in formal specifications written in Dafny. MutDafny, with its comprehensive mutation operator set, improves the reliability of specifications by helping developers identify and strengthen weak specs.

Abstract: This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.
  We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.

</details>


### [12] [Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework](https://arxiv.org/abs/2511.15168)
*Nguyen-Khang Le,Nguyen Hiep,Minh Nguyen,Son Luu,Trung Vo,Quan Bui,Nomura Shoshin,Le-Minh Nguyen*

Main category: cs.SE

TL;DR: This paper introduces datasets, benchmarks, and methods for training LLMs to generate high-quality Selenium scripts for web form testing, showing significant improvements over top existing models like GPT-4o and laying a foundation for future research in automated web testing with LLMs.


<details>
  <summary>Details</summary>
Motivation: Web form interaction is a crucial but underexplored component of automated web application testing with LLMs. There's a lack of public benchmarks and datasets specifically for evaluating LLMs on generating form interaction test cases, making systematic study and advancement challenging.

Method: The authors introduce a new method for training large language models (LLMs) to generate executable, syntactically correct Selenium scripts that target form interaction testing. They construct both synthetic and human-annotated datasets simulating diverse real-world form scenarios. Clear evaluation metrics—syntax correctness, script executability, and input field coverage—are defined.

Result: The proposed LLM-based approach significantly outperforms strong baselines including GPT-4o and other popular LLMs on all the defined evaluation metrics.

Conclusion: The paper establishes a systematic approach and benchmark for LLMs in web form interaction testing. It demonstrates the effectiveness of their method and provides datasets and metrics to facilitate future research in LLM-driven web application testing.

Abstract: Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.

</details>


### [13] [From Code Smells to Best Practices: Tackling Resource Leaks in PyTorch, TensorFlow, and Keras](https://arxiv.org/abs/2511.15229)
*Bashar Abdallah,Martyna E. Wojciechowska,Gustavo Santos,Edmand Yu,Maxime Lamothe,Alain Abran,Mohammad Hamdaqa*

Main category: cs.SE

TL;DR: This paper identifies and categorizes resource-leak-inducing code smells in PyTorch, TensorFlow, and Keras, and provides 50 best practices to mitigate these issues. Its findings help make ML applications more sustainable and efficient.


<details>
  <summary>Details</summary>
Motivation: Current ML research emphasizes performance metrics but overlooks resource efficiency and long-term sustainability, which are crucial for successful deployment. This study aims to bridge that gap by examining resource leaks in ML applications.

Method: The authors conducted an empirical investigation using developer discussions and code snippets from PyTorch, TensorFlow, and Keras. They identified resource-leak-inducing code smells and categorized them by root cause and framework characteristics. A three-phase validation process was used: independent analysis by three authors and consensus discussions.

Result: The study identified 30 code smells in PyTorch and 16 in TensorFlow/Keras that are linked to resource leaks. These were categorized and paired with at least one best practice each, resulting in 50 recommended coding patterns to mitigate resource leaks.

Conclusion: This is the first comprehensive study to systematically identify and categorize code smells causing resource leaks in ML frameworks, presenting actionable recommendations. The research advances resource-efficient, sustainable ML development and provides structured insights into the causes of resource leaks.

Abstract: Much of the existing ML research focuses on model performance metrics, leaving limited attention to the long-term sustainability and resource efficiency of ML applications. While high performance is essential, ensuring efficient resource management is equally critical for robust deployment. This study addresses this gap by systematically identifying code smells that lead to resource leaks in ML applications. We conducted an empirical investigation of developer discussions and real-world code snippets from PyTorch, TensorFlow, and Keras. The analysis identified 30 PyTorch-related smells and 16 TensorFlow/Keras smells linked to resource leaks. These smells were categorized in two ways: (1) based on their root causes, and (2) as general ML smells with framework-specific characteristics. For each smell, we derived at least one best practice, resulting in 50 recommended coding patterns aimed at reducing resource leakage and improving efficiency. To ensure the validity of our findings, we employed a three-phase validation process involving independent analysis by three authors followed by consensus discussions. This is the first comprehensive study to examine resource-leak-inducing code smells across major ML frameworks and to present actionable best practices for mitigating them. The contributions support developers in building more efficient and sustainable ML applications and offer a structured view of the underlying causes of resource leaks.

</details>


### [14] [M, Toolchain and Language for Reusable Model Compilation](https://arxiv.org/abs/2511.15257)
*Hiep Hong Trinh,Federico Ciccozzi,Abu Naser Masud,Marjan Sirjani,Mikael Sjödin*

Main category: cs.SE

TL;DR: This paper presents M, a new modeling language and toolchain designed to enable easy, systematic compilation of complex system models into various specialized targets (like simulation and verification). M addresses deficiencies in existing languages by supporting multi-target compilation and providing an integration point for other languages.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of deriving multiple specialized models from a high-level system model for different engineering tasks (simulation, deployment, verification), given that most current modeling languages are narrowly focused and don't support multi-target compilation efficiently.

Method: The paper introduces M, a textual, grammar-driven modeling language based on the actor model with discrete-event scheduling semantics. It provides constructs for entity modeling, message-based interactions, and time-/state-triggered reactions. The initiative includes a toolchain that allows systematic multi-target compilation, preserving semantic conformance.

Result: M enables the generation of diverse target artifacts from a single source model, supporting multi-target compilation in model-driven engineering of complex, concurrent, and time-aware systems. M can also act as a middle language to integrate other modeling languages into its compilation framework.

Conclusion: The proposed M language and toolchain significantly improve the efficiency and reliability of developing complex software-driven systems by supporting systematic, multi-target model compilation and semantic preservation. It also facilitates integration across different modeling languages.

Abstract: Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.

</details>


### [15] [A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development](https://arxiv.org/abs/2511.15293)
*Jia Li,Zhi Jin,Kechi Zhang,Huangzhao Zhang,Jiaru Qian,Tiankuo Zhao*

Main category: cs.SE

TL;DR: The paper introduces AutoSW, an iterative AI-driven software development paradigm that converts natural language requirements into executable code with minimal human input. Prototype results show successful end-to-end automation, marking a feasible path towards fully automated software development.


<details>
  <summary>Details</summary>
Motivation: Researchers are seeking ways to achieve greater automation in software development, leveraging advancements in AI to reduce human involvement and improve efficiency. The paper is motivated by the convergence of current AI-assistant-driven approaches and fully automated coding ('vibe coding').

Method: The authors propose a new paradigm called AutoSW, which features an iterative 'analyze-plan-implement-deliver' loop, positioning AI systems as first-class actors in translating human natural language requirements into executable software. They developed a lightweight prototype to demonstrate this paradigm and ran several representative case studies.

Result: Initial results show that AutoSW can deliver executable software from natural language inputs, demonstrating feasibility for end-to-end automated software development with minimal human intervention.

Conclusion: AutoSW is a promising direction for fully automated software development, where AI and humans collaborate in a loop to transform intentions into functioning software systems. This paradigm paves the way for future research and practical applications in automating the entire software lifecycle.

Abstract: Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.

</details>


### [16] [From Machine Learning Documentation to Requirements: Bridging Processes with Requirements Languages](https://arxiv.org/abs/2511.15340)
*Yi Peng,Hans-Martin Heyn,Jennifer Horkoff*

Main category: cs.SE

TL;DR: The paper investigates whether ML documentation (ModelCards, DataSheets) contains useful requirements information for ML-enabled systems, and whether existing RE frameworks can organize this information. The results show that ML documentation provides valuable requirements details, and current RE templates can help structure it for software engineering purposes.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of specifying requirements for ML components in software engineering, recognizing that traditional requirements engineering approaches face new obstacles when applied to ML-enabled systems. The authors note that ML documentation (like ModelCards and DataSheets) might be an underutilized resource for requirements engineering-relevant information.

Method: The authors first analyze 20 publicly available ModelCards and DataSheets to investigate the amount and nature of requirements engineering-relevant information. They then evaluate the effectiveness of three established requirements engineering representations (EARS, Rupp's template, and Volere) to structure this information into formal requirements.

Result: The study finds that ModelCards and DataSheets contain a significant amount of potentially requirements engineering-relevant information. Furthermore, the authors show that ML-specific knowledge from these documents can be effectively transformed into structured requirements using existing RE representations.

Conclusion: ML documentation such as ModelCards and DataSheets can be leveraged to specify requirements for ML components in software engineering processes. There is a feasible pathway for incorporating ML documentation into RE for ML-enabled systems, helping to bridge the gap faced by traditional RE approaches.

Abstract: In software engineering processes for machine learning (ML)-enabled systems, integrating and verifying ML components is a major challenge. A prerequisite is the specification of ML component requirements, including models and data, an area where traditional requirements engineering (RE) processes face new obstacles. An underexplored source of RE-relevant information in this context is ML documentation such as ModelCards and DataSheets. However, it is uncertain to what extent RE-relevant information can be extracted from these documents. This study first investigates the amount and nature of RE-relevant information in 20 publicly available ModelCards and DataSheets. We show that these documents contain a significant amount of potentially RE-relevant information. Next, we evaluate how effectively three established RE representations (EARS, Rupp's template, and Volere) can structure this knowledge into requirements. Our results demonstrate that there is a pathway to transform ML-specific knowledge into structured requirements, incorporating ML documentation in software engineering processes for ML systems.

</details>


### [17] [EPSO: A Caching-Based Efficient Superoptimizer for BPF Bytecode](https://arxiv.org/abs/2511.15589)
*Qian Zhu,Yuxuan Liu,Ziyuan Zhu,Shangqing Liu,Lei Bu*

Main category: cs.SE

TL;DR: EPSO is a new caching superoptimizer for eBPF, enabling much better optimizations than current compilers or optimizers with minimal overhead, by reusing rewrite rules discovered offline. It significantly reduces program size and improves performance for high-impact Linux projects.


<details>
  <summary>Details</summary>
Motivation: Optimizing eBPF programs is critical due to stringent verifier safety constraints and high-performance needs, but current compilers and manual optimization rules are limited in effectiveness, while traditional superoptimization is too computationally expensive.

Method: EPSO, a caching-based superoptimizer, is proposed. It uses offline superoptimization to discover rewrite rules and then reuses these rules to efficiently optimize eBPF programs during compilation, minimizing runtime overhead.

Result: EPSO found 795 rewrite rules and reduced program size by up to 68.87 percent (average 24.37 percent) compared to Clang, outperforming state-of-the-art optimizers K2 and Merlin on nearly all benchmarks. EPSO also reduced program runtime by an average of 6.60 percent, enhancing network application throughput and latency.

Conclusion: EPSO overcomes the limitations of rule-based and traditional superoptimization approaches, delivering significantly improved optimization for eBPF programs with little runtime cost, thus benefiting real-world eBPF-based projects and the Linux kernel ecosystem.

Abstract: Extended Berkeley Packet Filter (eBPF) allows developers to extend Linux kernel functionality without modifying its source code. To ensure system safety, an in-kernel safety checker, the verifier, enforces strict safety constraints (for example, a limited program size) on eBPF programs loaded into the kernel. These constraints, combined with eBPF's performance-critical use cases, make effective optimization essential. However, existing compilers (such as Clang) offer limited optimization support, and many semantics-preserving transformations are rejected by the verifier, which makes handcrafted optimization rule design both challenging and limited in effectiveness. Superoptimization overcomes the limitations of rule-based methods by automatically discovering optimal transformations, but its high computational cost limits scalability. To address this, we propose EPSO, a caching-based superoptimizer that discovers rewrite rules via offline superoptimization and reuses them to achieve high-quality optimizations with minimal runtime overhead. We evaluate EPSO on benchmarks from the Linux kernel and several eBPF-based projects, including Cilium, Katran, hXDP, Sysdig, Tetragon, and Tracee. EPSO discovers 795 rewrite rules and achieves up to 68.87 percent (average 24.37 percent) reduction in program size compared to Clang's output, outperforming the state-of-the-art BPF optimizer K2 on all benchmarks and Merlin on 92.68 percent of them. Additionally, EPSO reduces program runtime by an average of 6.60 percent, improving throughput and lowering latency in network applications.

</details>


### [18] [Quantum-Guided Test Case Minimization for LLM-Based Code Generation](https://arxiv.org/abs/2511.15665)
*Huixiang Zhang,Mahzabeen Emu*

Main category: cs.SE

TL;DR: This paper presents a novel framework combining LLM-driven test generation with quantum optimization for code efficiency, significantly reducing computational resources and improving code quality.


<details>
  <summary>Details</summary>
Motivation: Effectively controlling LLMs to produce concise and efficient code remains challenging in software engineering, necessitating improved methods for code generation and code quality leveraging new AI and optimization techniques.

Method: The authors propose a framework that integrates Test-Driven Development (TDD) principles with combinatorial optimization. Specifically, an LLM is prompted to generate test suites, and the problem of Test Case Minimization (TCM) is formulated as a Quadratic Unconstrained Binary Optimization (QUBO) model. The framework leverages both classical and quantum solvers, notably quantum annealers, for efficient problem solving.

Result: Quantum annealing outperforms simulated annealing by solving the TCM task 16 times faster. The overall framework reduces token consumption by 36.5% and delivers marked improvements in code quality.

Conclusion: Combining generative AI and combinatorial optimization, particularly via precise model formulation and quantum optimization hardware, can significantly advance code generation and software engineering outcomes.

Abstract: Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Compiling to recurrent neurons](https://arxiv.org/abs/2511.14953)
*Joey Velez-Ginorio,Nada Amin,Konrad Kording,Steve Zdancewic*

Main category: cs.PL

TL;DR: Discrete algorithms can be made differentiable and integrated into neural networks using a new programming language that compiles conditionals and iteration into neural network structures, resulting in faster and more efficient learning.


<details>
  <summary>Details</summary>
Motivation: Differentiable programming currently struggles to integrate discrete structures, such as conditionals and iteration, since these break the derivatives required for gradient-based learning, limiting the kinds of algorithms that can be directly expressed. The authors aim to overcome this limitation.

Method: The authors introduce a minimal, typed, higher-order and linear programming language called Cajal, which supports iteration and is designed to compile programs into linear recurrent neurons. They prove compilation correctness and conduct experiments by integrating these neurons with neural networks for iterative image transformation tasks.

Result: The results show that neural networks linked with recurrent neurons from Cajal learn faster and with greater data-efficiency compared to networks programmed without first-class iteration. This demonstrates that their method can bring discrete algorithmic structures into differentiable programs.

Conclusion: Integrating discrete structures like iteration as first-class elements in differentiable programming is possible and beneficial. Their programming language, Cajal, enables discrete algorithms to be compiled into a differentiable form compatible with gradient-based learning, enriching the combination between learning and traditional programming constructs.

Abstract: Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\textsf{Cajal}\scriptstyle(\mathbb{\multimap}, \mathbb{2}, \mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.

</details>


### [20] [Compiling Set Queries into Work-Efficient Tree Traversals](https://arxiv.org/abs/2511.15000)
*Alexander J Root,Christophe Gyurgyik,Purvi Goel,Kayvon Fatahalian,Jonathan Ragan-Kelley,Andrew Adams,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: The paper presents a generalized automated system for optimizing tree-based queries using metadata-driven pruning, symbolic interval analysis, and query fusion; it matches manual expert implementations and can lead to faster results, especially for complex or non-standard queries.


<details>
  <summary>Details</summary>
Motivation: Tree data structures help speed up queries by allowing branches to be ignored or included based on stored metadata, but pruning logic is often tailored manually for each use case.

Method: The authors generalize and mechanize the logic for subtree pruning and inclusion. Their method uses symbolic interval analysis (extended to handle geometric predicates) to automatically produce pruning conditions from node metadata. They also introduce a compiler that can fuse compound queries into single traversals.

Result: They can automatically derive complex tree joins that go beyond basic equality and range predicates. The runtime behavior matches hand-crafted expert solutions and can be more efficient than fallback linear or nested-loop approaches.

Conclusion: The approach enables automatic optimization of tree-based query processing, simplifying development and potentially improving performance for a wider class of queries.

Abstract: Trees can accelerate queries that search or aggregate values over large collections. They achieve this by storing metadata that enables quick pruning (or inclusion) of subtrees when predicates on that metadata can prove that none (or all) of the data in a subtree affect the query result. Existing systems implement this pruning logic manually for each query predicate and data structure. We generalize and mechanize this class of optimization. Our method derives conditions for when subtrees can be pruned (or included wholesale), expressed in terms of the metadata available at each node. We efficiently generate these conditions using symbolic interval analysis, extended with new rules to handle geometric predicates (e.g., intersection, containment). Additionally, our compiler fuses compound queries (e.g., reductions on filters) into a single tree traversal. These techniques enable the automatic derivation of generalized single-index and dual-index tree joins that support a wide class of join predicates beyond standard equality and range predicates. The generated traversals match the behavior of expert-written code that implements query-specific traversals, and can asymptotically outperform the linear scans and nested-loop joins that existing systems fall back to when hand-written cases do not apply.

</details>


### [21] [Data Layout Polymorphism for Bounding Volume Hierarchies](https://arxiv.org/abs/2511.15028)
*Christophe Gyurgyik,Alexander J Root,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: Scion is a new domain-specific language that lets developers specify data layouts for bounding volume hierarchies separately from their traversal logic. This flexibility leads to better performance and portability, and enables discovery of optimal layouts tailored to specific use cases and hardware.


<details>
  <summary>Details</summary>
Motivation: Existing bounding volume hierarchy (BVH) data structures often tie data layout tightly to traversal logic, making it hard to independently optimize for performance (cache usage, memory bandwidth, vectorization) and portability. This restricts developers and enforces trade-offs between efficiency and reusability.

Method: The authors introduce Scion, a domain-specific language (DSL) and compiler that allows the specification of data layouts for BVHs, decoupled from their traversal algorithms. They evaluate Scion's expressiveness and performance by experimenting with various layout optimizations on different algorithms, architectures, and workloads.

Result: Scion can represent a wide range of high-performance computing layout optimizations in an architecture-agnostic way. Empirical results show that the best-performing and most memory-efficient (Pareto-optimal) layouts vary significantly depending on algorithm, hardware, and workload properties. Additionally, they discover a new BVH layout particularly effective for ray tracing, attaining Pareto-optimality across multiple contexts.

Conclusion: Scion empowers developers to independently optimize BVH data layouts and traversal logic, promoting both performance and portability. Its systematic exploration identifies and enables superior, context-sensitive layout solutions previously obscured by traditional coupling. This approach resolves the trade-off between efficiency and reuse in high-performance computing.

Abstract: Bounding volume hierarchies are ubiquitous acceleration structures in graphics, scientific computing, and data analytics. Their performance depends critically on data layout choices that affect cache utilization, memory bandwidth, and vectorization -- increasingly dominant factors in modern computing. Yet, in most programming systems, these layout choices are hopelessly entangled with the traversal logic. This entanglement prevents developers from independently optimizing data layouts and algorithms across different contexts, perpetuating a false dichotomy between performance and portability. We introduce Scion, a domain-specific language and compiler for specifying the data layouts of bounding volume hierarchies independent of tree traversal algorithms. We show that Scion can express a broad spectrum of layout optimizations used in high performance computing while remaining architecture-agnostic. We demonstrate empirically that Pareto-optimal layouts (along performance and memory footprint axes) vary across algorithms, architectures, and workload characteristics. Through systematic design exploration, we also identify a novel ray tracing layout that combines optimization techniques from prior work, achieving Pareto-optimality across diverse architectures and scenes.

</details>


### [22] [Cement2: Temporal Hardware Transactions for High-Level and Efficient FPGA Programming](https://arxiv.org/abs/2511.15073)
*Youwei Xiao,Zizhang Luo,Weijie Peng,Yuyang Zou,Yun Liang*

Main category: cs.PL

TL;DR: Cement2, a new Rust-based HDL, introduces cycle-aware, transactional abstractions that simplify and generalize FPGA design. It lets developers write efficient, high-level hardware code with multi-cycle timing semantics. Benchmarking shows Cement2 matches hand-crafted RTL in performance and resource usage, while greatly improving design productivity and flexibility.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of current hardware design methodologies. RTL provides fine control but poor behavioral guarantees and low abstraction; HLS and transactional HDLs improve abstraction but have unpredictable overhead or lack multi-cycle timing constructs, reducing productivity and applicability for complex FPGA design.

Method: The authors introduce 'temporal hardware transactions,' which add cycle-level, timing-aware abstractions to transactional hardware description languages. They embed these features in Cement2 (in Rust), allowing designers to succinctly express multi-cycle and intra-cycle hardware behaviors. Cement2 also employs multiple synthesis and optimization stages to generate efficient hardware from these high-level descriptions.

Result: Using Cement2, the authors successfully implemented various hardware modules, including a RISC-V processor and accelerators, demonstrating improved design productivity. Performance and resource usage remained on par with traditional, hand-coded RTL implementations, validating the new abstraction's efficiency and versatility.

Conclusion: Temporal hardware transactions in Cement2 enable FPGA designers to write high-level, composable, timing-aware hardware code that does not compromise performance or resource efficiency, thus boosting productivity and broadening HDL applicability for general hardware design.

Abstract: Hardware design faces a fundamental challenge: raising abstraction to improve productivity while maintaining control over low-level details like cycle accuracy. Traditional RTL design in languages like SystemVerilog composes modules through wiring-style connections that provide weak guarantees for behavioral correctness. While high-level synthesis (HLS) and emerging abstractions attempt to address this, they either introduce unpredictable overhead or restrict design generality. Although transactional HDLs provide a promising foundation by lifting design abstraction to atomic and composable rules, they solely model intra-cycle behavior and do not reflect the native temporal design characteristics, hindering applicability and productivity for FPGA programming scenarios.
  We propose temporal hardware transactions, a new abstraction that brings cycle-level timing awareness to designers at the transactional language level. Our approach models temporal relationships between rules and supports the description of rules whose actions span multiple clock cycles, providing intuitive abstraction to describe multi-cycle architectural behavior. We implement this in Cement2, a transactional HDL embedded in Rust, enabling programming hardware constructors to build both intra-cycle and temporal transactions. Cement2's synthesis framework lowers description abstraction through multiple analysis and optimization phases, generating efficient hardware. With Cement2's abstraction, we program a RISC-V soft-core processor, custom CPU instructions, linear algebra kernels, and systolic array accelerators, leveraging the high-level abstraction for boosted productivity. Evaluation shows that Cement2 does not sacrifice performance and resources compared to hand-coded RTL designs, demonstrating the high applicability for general FPGA design tasks.

</details>


### [23] [SkyEgg: Joint Implementation Selection and Scheduling for Hardware Synthesis using E-graphs](https://arxiv.org/abs/2511.15323)
*Youwei Xiao,Yuyang Zou,Yun Liang*

Main category: cs.PL

TL;DR: SkyEgg is a new hardware synthesis framework that jointly optimizes implementation selection and scheduling using e-graphs and MILP. It surpasses existing HLS tools, showing up to 5.22x speedup on FPGAs by better leveraging advanced features.


<details>
  <summary>Details</summary>
Motivation: Current hardware synthesis techniques, including advanced high-level synthesis (HLS) tools, separate implementation selection from scheduling, leading to suboptimal designs. This disconnect prevents efficient utilization of modern FPGA features, as implementation choices are made without considering scheduling consequences, resulting in missed optimization opportunities.

Method: SkyEgg jointly optimizes implementation selection and scheduling using an e-graph data structure. It uniformly represents algebraic transformations and hardware implementation choices as rewrite rules within the e-graph. The framework constructs an e-graph from the input program, applies rewrites via equality saturation, and formulates the optimization as a mixed-integer linear programming (MILP) problem, with both exact MILP solving and a scalable ASAP heuristic.

Result: SkyEgg was evaluated on a range of benchmarks targeting Xilinx Kintex UltraScale+ FPGAs. It showed an average speedup of 3.01x over Vitis HLS, with improvements up to 5.22x for complex expressions.

Conclusion: SkyEgg demonstrates significant advances in hardware synthesis by jointly optimizing implementation selection and scheduling. The approach fully exploits FPGA features, leading to notably superior designs and performance compared to current HLS tools.

Abstract: Hardware synthesis from high-level descriptions remains fundamentally limited by the sequential optimization of interdependent design decisions. Current methodologies, including state-of-the-art high-level synthesis (HLS) tools, artificially separate implementation selection from scheduling, leading to suboptimal designs that cannot fully exploit modern FPGA heterogeneous architectures. Implementation selection is typically performed by ad-hoc pattern matching on operations, a process that does not consider the impact on scheduling. Subsequently, scheduling algorithms operate on fixed selection solutions with inaccurate delay estimates, which misses critical optimization opportunities from appropriately configured FPGA blocks like DSP slices.
  We present SkyEgg, a novel hardware synthesis framework that jointly optimizes implementation selection and scheduling using the e-graph data structure. Our key insight is that both algebraic transformations and hardware implementation choices can be uniformly represented as rewrite rules within an e-graph, modeling the complete design space of implementation candidates to be selected and scheduled together. First, SkyEgg constructs an e-graph from the input program. It then applies both algebraic and implementation rewrites through equality saturation. Finally, it formulates the joint optimization as a mixed-integer linear programming (MILP) problem on the saturated e-graph. We provide both exact MILP solving and an efficient ASAP heuristic for scalable synthesis. Our evaluation on benchmarks from diverse applications targeting Xilinx Kintex UltraScale+ FPGAs demonstrates that SkyEgg achieves an average speedup of 3.01x over Vitis HLS, with improvements up to 5.22x for complex expressions.

</details>


### [24] [Graph Rewriting Language as a Platform for Quantum Diagrammatic Calculi](https://arxiv.org/abs/2511.15581)
*Kayo Tei,Haruto Mishina,Naoki Yamamoto,Kazunori Ueda*

Main category: cs.PL

TL;DR: This paper introduces a new platform based on LMNtal for quantum circuit optimization using graph rewriting, providing enhanced transformation, verification, and visualization capabilities that complement existing domain-specific tools.


<details>
  <summary>Details</summary>
Motivation: Quantum circuit optimization is challenging and current graph-based methods like ZX-calculus need improved tools for transformation and verification.

Method: The paper proposes using LMNtal, a general-purpose hierarchical graph rewriting language, to build a platform for quantum circuit transformation and verification, leveraging native graph transformation, quantified pattern matching (QLMNtal), and interactive state space exploration.

Result: Case studies show the framework helps to understand optimization paths and supports the design of new algorithms and strategies for quantum circuit simplification.

Conclusion: The LMNtal language and its toolchain provide a novel, complementary platform for quantum circuit transformation and optimization, offering unique perspectives compared to domain-specific tools.

Abstract: Systematic discovery of optimization paths in quantum circuit simplification remains a challenge. Today, ZX-calculus, a computing model for quantum circuit transformation, is attracting attention for its highly abstract graph-based approach. Whereas existing tools such as PyZX and Quantomatic offer domain-specific support for quantum circuit optimization, visualization and theorem-proving, we present a complementary approach using LMNtal, a general-purpose hierarchical graph rewriting language, to establish a diagrammatic transformation and verification platform with model checking. Our methodology shows three advantages: (1) manipulation of ZX-diagrams through native graph transformation rules, enabling direct implementation of basic rules; (2) quantified pattern matching via QLMNtal extensions, greatly simplifying rule specification; and (3) interactive visualization and validation of optimization paths through state space exploration. Through case studies, we demonstrate how our framework helps understand optimization paths and design new algorithms and strategies. This suggests that the declarative language LMNtal and its toolchain could serve as a new platform to investigate quantum circuit transformation from a different perspective.

</details>
