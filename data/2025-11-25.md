<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 33]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The Software Engineering Simulations Lab: Agentic AI for RE Quality Simulations](https://arxiv.org/abs/2511.17762)
*Henning Femmer,Ivan Esau*

Main category: cs.SE

TL;DR: This paper addresses the challenge of evaluating requirements quality in software engineering by introducing Agentic AI simulations. These simulations, performed by AI agents, offer a fast and simple way to study the effects of quality defects—even in environments where requirements are increasingly consumed by AI. Initial feasibility tests demonstrate potential, but more work is needed to refine the approach and address its limitations, especially around human behavior replication.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of empirical and systematic models to evaluate requirements quality in software engineering, especially as requirements are increasingly consumed by AI agents, not just humans.

Method: The authors propose to use Agentic AI simulations, which replicate software engineering processes with standardized AI agents in qualitative, event-driven simulations.

Result: Initial feasibility study shows that even a basic implementation of Agentic AI simulation can produce executable simulations, encouraging further technical development and wider adoption in requirements engineering research.

Conclusion: Agentic AI simulations are a promising tool for enriching empirical research into requirements quality, offering speed and simplicity; however, limitations in replicating human behaviors must be further examined.

Abstract: Context and motivation. Quality in Requirements Engineering (RE) is still predominantly anecdotal and intuition-driven. Creating a solid requirements quality model requires broad sets of empirical evidence to evaluate quality factors and their context. Problem. However, empirical data on the detailed effects of requirements quality defects is scarce, since it is costly to obtain. Furthermore, with the advent of AI-based development, the requirements quality factors may change: Requirements are no longer only consumed by humans, but increasingly also by AI agents, which might lead to a different efficient and effective requirements style. Principal ideas. We propose to extend the RE research toolbox with Agentic AI simulations, in which software engineering (SE) processes are replicated by standardized agents in stochastic, dynamic, event-driven, qualitative simulations. We argue that their speed and simplicity makes them a valuable addition to RE research, although limitations in replicating human behavior need to be studied and understood. Contribution. This paper contributes a first concept, a research roadmap, a prototype, and a first feasibility study for RE simulations with agentic AI. Study results indicate that even a naive implementation leads to executable simulations, encouraging technical improvements along with broader application in RE research.

</details>


### [2] [Validating API Design Requirements for Interoperability: A Static Analysis Approach Using OpenAPI](https://arxiv.org/abs/2511.17836)
*Edwin Sundberg,Thea Ekmark,Workneh Yilma Ayele*

Main category: cs.SE

TL;DR: The paper introduces S.E.O.R.A, a tool that automates early-stage quality checks for RESTful API design using customizable rules, making API validation more consistent and efficient. This enhances interoperability, governance, and alignment with enterprise requirements.


<details>
  <summary>Details</summary>
Motivation: RESTful APIs are widely used in enterprise software, but evaluating the quality of their design is still a mostly manual and inconsistent process, especially during early development. There is a need for systematic, automated approaches to ensure API quality, interoperability, and governance across organizations.

Method: The authors used the Design Science Research (DSR) methodology. They gathered user needs, conducted a literature review to identify 75 API design rules, and built a configurable rule engine to detect structural violations in OpenAPI specifications. The tool was evaluated through structured experiments and thematic analysis with industry experts.

Result: The tool (S.E.O.R.A) enables early validation of non-functional API requirements, provides actionable and traceable feedback, and integrates well with requirements elicitation and quality assurance processes. It automates API design checks, supporting consistent and reusable conformance practices.

Conclusion: S.E.O.R.A improves API design quality by automating the validation of design rules, operationalizing design principles into verifiable constraints, and embedding them in a practical tool. The approach enhances requirements engineering, supports system interoperability and governance, and can adapt to organizational needs.

Abstract: RESTful APIs are central in developing interoperable, modular, and maintainable software systems in enterprises today. Also, it is essential to support system evolution, service interoperability, and governance across organizational boundaries to ensure good quality and consistency of these APIs. However, evaluating API design quality, which is part of non-functional requirement tasks, remains a largely manual and ad hoc process, particularly during early development. Using a Design Science Research (DSR) methodology, we elicited user needs, identified 75 API design rules using a literature review, and implemented a configurable rule engine to detect structural violations in OpenAPI specifications. The proposed tool supports organizational adaptability by allowing rules to be customized, enabled, or disabled, enabling integration of domain-specific standards. The evaluation was conducted through structured experiments and thematic analysis involving industry experts. API quality validation contributes to aligning technical designs with requirements and enterprise architecture by strengthening interoperability and governance between enterprise systems. The results show that S.E.O.R.A facilitates early validation of non-functional API requirements, provides actionable and traceable feedback, and aligns well with requirements elicitation and quality assurance processes. It improves the API design process by automating checks that would otherwise require manual inspection, thus supporting consistent and reusable conformance practices. This work contributes to requirements engineering by operationalizing design principles as verifiable constraints and embedding them into a practical validation tool. Future directions include IDE integration, expanded rule coverage, and real-world deployment to support continuous compliance in agile API development lifecycles.

</details>


### [3] [SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning](https://arxiv.org/abs/2511.19422)
*David Jiahao Fu,Aryan Gupta,Aaron Councilman,David Grove,Yu-Xiong Wang,Vikram Adve*

Main category: cs.SE

TL;DR: SLMFix uses a small RL-finetuned language model to fix errors in LLM-generated code, outperforming traditional finetuning approaches and achieving high correctness across various specialized programming languages.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) perform well in code generation but still produce syntactic errors, especially in low-resource programming languages (LRPLs). Finetuning LLMs is computationally expensive and often infeasible in resource-constrained settings.

Method: The authors propose SLMFix, a code generation pipeline using a small language model (SLM) finetuned with reinforcement learning (RL) to repair syntactic errors in LLM-generated programs. RL training is guided by rewards from a static validator and semantic similarity metric.

Result: Experimental results show that SLMFix achieves over 95% pass rate on the static validator across multiple domain-specific languages (DSLs). It substantially improves over the base LLM and outperforms supervised finetuning approaches, even for 7B models on LRPLs.

Conclusion: SLMFix is an effective and generalizable solution for improving code generation in domain-specific and low-resource languages where traditional finetuning is impractical. It offers a computationally efficient alternative to standard LLM finetuning methods.

Abstract: Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.

</details>


### [4] [A Low-Code Methodology for Developing AI Kiosks: a Case Study with the DIZEST Platform](https://arxiv.org/abs/2511.17853)
*SunMin Moon,Jangwon Gim,Chaerin Kim,Yeeun Kim,YoungJoo Kim,Kang Choi*

Main category: cs.SE

TL;DR: The paper proposes DIZEST, a low-code platform for kiosk systems, showing it performs better than others like Jupyter Notebook and Orange3 and is validated in a photo kiosk case study, leading to better integration, user experience, and flexibility.


<details>
  <summary>Details</summary>
Motivation: Modern kiosk systems suffer from integration issues, structural rigidity, low performance, and a lack of collaborative frameworks. The motivation is to address these limitations with a more flexible, high-performing, and collaborative solution.

Method: The study introduces DIZEST, a specialized low-code architecture for kiosk systems, and validates its effectiveness through comparative analysis against platforms like Jupyter Notebook, ComfyUI, and Orange3, along with a practical photo kiosk case study.

Result: DIZEST outperforms existing low-code platforms in key metrics and proves its effectiveness in a real-world photo kiosk scenario by enhancing core aspects: interoperability, user experience, and deployment flexibility.

Conclusion: The DIZEST-based low-code platform effectively enhances kiosk systems by improving interoperability, user experience, and deployment flexibility compared to existing platforms.

Abstract: This paper presents a comprehensive study on enhancing kiosk systems through a low-code architecture, with a focus on AI-based implementations. Modern kiosk systems are confronted with significant challenges, including a lack of integration, structural rigidity, performance bottlenecks, and the absence of collaborative frameworks. To overcome these limitations, we propose a DIZEST-based approach methodology, a specialized low-code platform that enables intuitive workflow design and seamless AI integration. Through a comparative analysis with existing platforms, including Jupyter Notebook, ComfyUI, and Orange3, we demonstrate that DIZEST delivers superior performance across key evaluation criteria. Our photo kiosk case study further validates the effectiveness of this approach in improving interoperability, enhancing user experience, and increasing deployment flexibility.

</details>


### [5] [Synthesizing Precise Protocol Specs from Natural Language for Effective Test Generation](https://arxiv.org/abs/2511.17977)
*Kuangxiangzi Liu,Dhiman Chakraborty,Alexander Liggesmeyer,Andreas Zeller*

Main category: cs.SE

TL;DR: This paper introduces AUTOSPEC, a two-stage LLM-based pipeline for converting natural language protocol specifications into formal, human-readable ones, which facilitates extensive automated testing. The approach is shown to be accurate and practical on multiple real internet protocols.


<details>
  <summary>Details</summary>
Motivation: Testing safety- and security-critical systems is crucial but current manual, natural language-based test case derivation is slow, error-prone, and hard to scale. Formal specifications support automation but are difficult to produce by hand. Bridging this gap is necessary for more efficient, reliable testing.

Method: The authors propose a two-stage pipeline: (1) Use large language models (LLMs) to extract protocol elements from natural language specifications; (2) Synthesize and refine formal protocol specifications with these elements and an implementation. The generated specification can then be used for large-scale, automated testing without further need for LLMs.

Result: The prototype system, AUTOSPEC, was evaluated on five internet protocol RFCs and achieved high accuracy: recovering 92.8% of client and 80.2% of server message types, and 81.5% message acceptance in various real-world systems.

Conclusion: A structured, LLM-assisted approach can effectively automate the transition from natural language protocol descriptions to formal specifications, enabling efficient, scalable automated testing systems. The process also produces reusable, human-interpretable specifications.

Abstract: Safety- and security-critical systems have to be thoroughly tested against their specifications. The state of practice is to have _natural language_ specifications, from which test cases are derived manually - a process that is slow, error-prone, and difficult to scale. _Formal_ specifications, on the other hand, are well-suited for automated test generation, but are tedious to write and maintain. In this work, we propose a two-stage pipeline that uses large language models (LLMs) to bridge the gap: First, we extract _protocol elements_ from natural-language specifications; second, leveraging a protocol implementation, we synthesize and refine a formal _protocol specification_ from these elements, which we can then use to massively test further implementations.
  We see this two-stage approach to be superior to end-to-end LLM-based test generation, as 1. it produces an _inspectable specification_ that preserves traceability to the original text; 2. the generation of actual test cases _no longer requires an LLM_; 3. the resulting formal specs are _human-readable_, and can be reviewed, version-controlled, and incrementally refined; and 4. over time, we can build a _corpus_ of natural-language-to-formal-specification mappings that can be used to further train and refine LLMs for more automatic translations.
  Our prototype, AUTOSPEC, successfully demonstrated the feasibility of our approach on five widely used _internet protocols_ (SMTP, POP3, IMAP, FTP, and ManageSieve) by applying its methods on their _RFC specifications_ written in natural-language, and the recent _I/O grammar_ formalism for protocol specification and fuzzing. In its evaluation, AUTOSPEC recovers on average 92.8% of client and 80.2% of server message types, and achieves 81.5% message acceptance across diverse, real-world systems.

</details>


### [6] [Enhancing Automated Program Repair via Faulty Token Localization and Quality-Aware Patch Refinement](https://arxiv.org/abs/2511.18001)
*Jiaolong Kong,Xiaofei Xie,Yiheng Xiong,Yuekun Wang,Jian Wang*

Main category: cs.SE

TL;DR: TokenRepair enhances LLM-based program repair by targeting suspicious code tokens using internal reflection and external feedback, significantly improving bug-fixing performance over prior techniques.


<details>
  <summary>Details</summary>
Motivation: Large language models are promising for automated program repair, but existing techniques mainly use coarse external feedback and lack fine-grained internal indications of errors, leading to less efficient and effective repairs.

Method: TokenRepair combines fine-grained internal reflection (analyzing token-level uncertainty to identify suspicious code tokens) with external feedback. It uses Chain-of-Thought guided rewriting to refine only problematic tokens and filters candidates using quality-aware feedback.

Result: TokenRepair achieved new state-of-the-art performance, fixing 88 bugs on Defects4J 1.2 and 139 on HumanEval-Java, with improvements up to 34.9% over existing models.

Conclusion: TokenRepair’s integration of token-level uncertainty and external feedback enables more efficient and effective automated program repair, outperforming previous LLM-based approaches.

Abstract: Large language models (LLMs) have recently demonstrated strong potential for automated program repair (APR). However, existing LLM-based techniques primarily rely on coarse-grained external feedback (e.g.,test results) to guide iterative patch generation, while lacking fine-grained internal signals that reveal why a patch fails or which parts of the generated code are likely incorrect. This limitation often leads to inefficient refinement, error propagation, and suboptimal repair performance. In this work, we propose TokenRepair, a novel two-level refinement framework that enhances APR by integrating internal reflection for localizing potentially faulty tokens with external feedback for quality-aware patch refinement. Specifically, TokenRepair first performs internal reflection by analyzing context-aware token-level uncertainty fluctuations to identify suspicious or low-confidence tokens within a patch. It then applies Chain-of-Thought guided rewriting to refine only these localized tokens, enabling targeted and fine-grained correction. To further stabilize the iterative repair loop, TokenRepair incorporates a quality-aware external feedback mechanism that evaluates patch quality and filters out low-quality candidates before refinement. Experimental results show that TokenRepair achieves new state-of-the-art repair performance, correctly fixing 88 bugs on Defects4J 1.2 and 139 bugs on HumanEval-Java, demonstrating substantial improvements ranging from 8.2% to 34.9% across all models on Defects4J 1.2 and from 3.3% to 16.1% on HumanEval-Java.

</details>


### [7] [MASTEST: A LLM-Based Multi-Agent System For RESTful API Tests](https://arxiv.org/abs/2511.18038)
*Xiaoke Han,Hong Zhu*

Main category: cs.SE

TL;DR: MASTEST automates RESTful API testing using a mix of LLM and programmed agents, showing high performance with GPT-4o and DeepSeek on various public APIs. It covers scenario generation, script creation, execution, and result analysis, requiring little manual correction. DeepSeek is better in detecting types and status codes, while GPT-4o covers more API operations. MASTEST proves practical and effective for cloud-native API testing.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for automated and accurate RESTful API testing in cloud-native applications. Traditional methods are often time-consuming, while recent advances in large language models (LLMs) offer promise for automating testing workflows. However, integrating LLMs into a comprehensive testing toolchain, ensuring quality and correctness while maintaining human oversight, remains an open problem.

Method: The authors propose MASTEST, a multi-agent system that combines LLM-based and conventional programmed agents to automate the full API testing workflow. This includes generating test scenarios and scripts from OpenAPI specifications, executing those scripts, and analyzing responses for correctness and coverage. The system allows human testers to review and correct the LLM-generated artifacts. MASTEST was evaluated using two LLMs (GPT-4o and DeepSeek V3.1 Reasoner) on five public APIs, measuring various dimensions of test quality and effectiveness.

Result: Both LLMs demonstrated high overall performance in API testing tasks. DeepSeek outperformed in data type correctness and status code detection, while GPT-4o excelled in API operation coverage. Both maintained 100% script syntax correctness, only needing minimal manual edits for full semantic correctness. This indicates that MASTEST is effective and that LLMs are practical for automating many aspects of RESTful API testing.

Conclusion: MASTEST is an effective multi-agent system for automating RESTful API testing using LLMs, with empirical results showing high test coverage, correctness, and script quality. The system significantly reduces manual intervention needed for testing while supporting human oversight for critical quality assurance.

Abstract: Testing RESTful API is increasingly important in quality assurance of cloud-native applications. Recent advances in machine learning (ML) techniques have demonstrated that various testing activities can be performed automatically by large language models (LLMs) with reasonable accuracy. This paper develops a multi-agent system called MASTEST that combines LLM-based and programmed agents to form a complete tool chain that covers the whole workflow of API test starting from generating unit and system test scenarios from API specification in the OpenAPI Swagger format, to generating of Pytest test scripts, executing test scripts to interact with web services, to analysing web service response messages to determine test correctness and calculate test coverage. The system also supports the incorporation of human testers in reviewing and correcting LLM generated test artefacts to ensure the quality of testing activities. MASTEST system is evaluated on two LLMs, GPT-4o and DeepSeek V3.1 Reasoner with five public APIs. The performances of LLMs on various testing activities are measured by a wide range of metrics, including unit and system test scenario coverage and API operation coverage for the quality of generated test scenarios, data type correctness, status code coverage and script syntax correctness for the quality of LLM generated test scripts, as well as bug detection ability and usability of LLM generated test scenarios and scripts. Experiment results demonstrated that both DeepSeek and GPT-4o achieved a high overall performance. DeepSeek excels in data type correctness and status code detection, while GPT-4o performs best in API operation coverage. For both models, LLM generated test scripts maintained 100\% syntax correctness and only required minimal manual edits for semantic correctness. These findings indicate the effectiveness and feasibility of MASTEST.

</details>


### [8] [Event-Chain Analysis for Automated Driving and ADAS Systems: Ensuring Safety and Meeting Regulatory Timing Requirements](https://arxiv.org/abs/2511.18092)
*Sebastian Dingler,Philip Rehkop,Florian Mayer,Ralf Muenzenberger*

Main category: cs.SE

TL;DR: This paper introduces a transparent, structured White-Box methodology using Event-Chain Modeling to address strict timing requirements in automated driving systems. It offers clear timing analysis at the architectural level, ensures early regulatory compliance, and enables system optimization, as demonstrated by a practical case study.


<details>
  <summary>Details</summary>
Motivation: Automated Driving Systems (ADS) and Advanced Driver Assistance Systems (ADAS) must comply with strict timing constraints imposed by international regulations and standards to ensure safety. Existing regulatory frameworks require transparency in demonstrating compliance, making timing analysis both critical and challenging.

Method: The paper introduces a structured, White-Box methodology using Event-Chain Modeling. This approach provides transparent, component-level timing analysis for ADS/ADAS architectures, allowing clear derivation, modeling, and validation of timing constraints. It is complemented with early verification through simulation and demonstrated via a detailed case study.

Result: The Event-Chain-centric method enhances regulatory compliance, improves system design, and enables model-based safety analysis. The case study confirms early detection of compliance issues, enables systematic parameter optimization, and provides quantitative, probabilistic analysis-based evidence for compliance dossiers.

Conclusion: The White-Box Event-Chain Modeling methodology offers significant advantages for timing analysis in ADS/ADAS, facilitating regulatory compliance, early verification, and system optimization. It supports requirements of international regulations by providing transparent evidence of architectural suitability and timing validation.

Abstract: Automated Driving Systems (ADS), including Advanced Driver Assistance Systems (ADAS), must fulfill not only high functional expectations but also stringent timing constraints mandated by international regulations and standards. Regulatory frameworks such as UN regulations, NCAP standards, ISO norms, and NHTSA guidelines impose strict bounds on system reaction times to ensure safe vehicle operation. This paper presents a structured, White-Box methodology based on Event-Chain Modeling to address these timing challenges. Unlike Black-Box approaches, Event-Chain Analysis offers transparent insights into the timing behavior of each functional component - from perception and planning to actuation and human interaction. This perspective is also aligned with multiple regulations, which require that homologation dossiers provide evidence that the chosen system architecture is suitable to ensure compliance with the specified requirements. Our methodology enables the derivation, modeling, and validation of end-to-end timing constraints at the architectural level and facilitates early verification through simulation. Through a detailed case study, we demonstrate how this Event-Chain-centric approach enhances regulatory compliance, optimizes system design, and supports model-based safety analysis techniques, with results showing early identification of compliance issues, systematic parameter optimization, and quantitative evidence generation through probabilistic analysis.

</details>


### [9] [Towards a General Framework for HTN Modeling with LLMs](https://arxiv.org/abs/2511.18165)
*Israel Puerta-Merino,Carlos Núñez-Molina,Pablo Mesejo,Juan Fernández-Olivares*

Main category: cs.SE

TL;DR: The paper introduces an LLM-based tool for Hierarchical Planning model generation and shows that, while LLMs can parse both AP and HP cases comparably, their syntactic validity for HP is far lower, indicating substantial challenges ahead.


<details>
  <summary>Details</summary>
Motivation: While large language models (LLMs) have been successfully used for generating Automated Planning (AP) models, their ability to generate Hierarchical Planning (HP) models is less developed. There is a notable gap in sophistication between hierarchical and non-hierarchical architectures in this context.

Method: The authors introduce L2HP, an extension of the L2P library, which enables LLM-driven generation of HP models. The design focuses on generality and extensibility. They conduct experiments using their framework to compare LLM modeling capabilities for both AP and HP models, utilizing the PlanBench dataset.

Result: LLMs achieve similar (around 36%) parsing success rates in both AP and HP settings. However, syntactic validity is much lower for HP models (1%) compared to AP models (20%), showing that HP modeling presents greater challenges for LLMs.

Conclusion: The results highlight the unique difficulties that LLMs face when generating HP models as opposed to AP models, suggesting that further research and development are necessary to improve LLM-driven HP model generation.

Abstract: The use of Large Language Models (LLMs) for generating Automated Planning (AP) models has been widely explored; however, their application to Hierarchical Planning (HP) is still far from reaching the level of sophistication observed in non-hierarchical architectures. In this work, we try to address this gap. We present two main contributions. First, we propose L2HP, an extension of L2P (a library to LLM-driven PDDL models generation) that support HP model generation and follows a design philosophy of generality and extensibility. Second, we apply our framework to perform experiments where we compare the modeling capabilities of LLMs for AP and HP. On the PlanBench dataset, results show that parsing success is limited but comparable in both settings (around 36\%), while syntactic validity is substantially lower in the hierarchical case (1\% vs. 20\% of instances). These findings underscore the unique challenges HP presents for LLMs, highlighting the need for further research to improve the quality of generated HP models.

</details>


### [10] [Establishing Traceability Links between Release Notes & Software Artifacts: Practitioners' Perspectives](https://arxiv.org/abs/2511.18187)
*Sristy Sumana Nath,Banani Roy,Munima Jahan*

Main category: cs.SE

TL;DR: LLM-based approaches effectively recover traceability links between release notes and development artifacts in open-source projects—reducing errors and manual effort, with strong support from practitioners.


<details>
  <summary>Details</summary>
Motivation: Maintaining traceability between software release notes and development artifacts is crucial for technical debt management and maintainability, but is challenging in open-source projects with remote, asynchronous contributors.

Method: Empirical analysis of GitHub repositories to measure traceability gaps, curation of a benchmark dataset (3,500 instances), and application of LLM-based methods augmented with time proximity features to recover traceability links.

Result: 47% of release artifacts lacked traceability links and 12% had broken links. The LLM-based approach, specifically Gemini 1.5 Pro, achieved Precision@1 of 0.73 for PR traceability recovery. Practitioner survey showed 84% find traceability maintenance at least somewhat important.

Conclusion: LLM-based methods, when combined with contextual features, can significantly improve traceability link recovery between release notes and development artifacts, addressing a major pain point in open-source project management.

Abstract: Maintaining traceability links between software release notes and corresponding development artifacts, e.g., pull requests (PRs), commits, and issues, is essential for managing technical debt and ensuring maintainability. However, in open-source environments where contributors work remotely and asynchronously, establishing and maintaining these links is often error-prone, time-consuming, and frequently overlooked. Our empirical study of GitHub repositories revealed that 47% of release artifacts lacked traceability links, and 12% contained broken links. To address this gap, we first analyzed release notes to identify their What, Why, and How information and assessed how these align with PRs, commits, and issues. We curated a benchmark dataset consisting of 3,500 filtered and validated traceability link instances. Then, we implemented LLM-based approaches to automatically establish traceability links of three pairs between release note contents & PRs, release note contents & PRs and release note contents & issues. By combining the time proximity feature, the LLM-based approach, e.g., Gemini 1.5 Pro, achieved a high Precision@1 value of 0.73 for PR traceability recovery. To evaluate the usability and adoption potential of this approach, we conducted an online survey involving 33 open-source practitioners. 16% of respondents rated as very important, and 68% as somewhat important for traceability maintenance.

</details>


### [11] [LLM Assisted Coding with Metamorphic Specification Mutation Agent](https://arxiv.org/abs/2511.18249)
*Mostafijur Rahman Akhond,Gias Uddin*

Main category: cs.SE

TL;DR: The paper introduces a new LLM-based agent that uses metamorphic relations to refine task instructions and generate high-quality test cases, leading to substantial accuracy and coverage improvements in code generation tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the reliability problems of LLMs in software engineering, which arise from ambiguous or inconsistent user specifications.

Method: The paper proposes CodeMetaAgent (CMA), an LLM agent driven by metamorphic relations (MRs). CMA systematically refines task specifications and generates semantically constrained test cases. Unlike traditional post-validation use of MRs, CMA leverages MRs in the generative process to guide LLMs.

Result: Through experiments on HumanEval-Pro, MBPP-Pro, and SWE-Bench_Lite using multiple LLMs (GPT-4o, Mistral Large, GPT-OSS, Qwen3-Coder), CMA improved code generation accuracy by up to 17% and achieved code coverage gains up to 99.81%.

Conclusion: MRs, when used to guide LLM-based software development rather than just post-validation, can greatly enhance consistency and reliability in code generation.

Abstract: Metamorphic Relations (MRs) serve as a foundational mechanism for generating semantically equivalent mutations. Software engineering has advanced significantly in recent years with the advent of Large Language Models (LLMs). However, the reliability of LLMs in software engineering is often compromised by ambiguities and inconsistencies due to improper user specification. To address this challenge, we present CodeMetaAgent (CMA), a metamorphic relation-driven LLM agent that systematically refines task specifications and generates semantically constrained test cases. Our proposed framework uses MRs with LLMs to improve generation consistency and reduce variability caused by specifications, unlike the traditional use of MRs as post validations. Our framework has been evaluated on the HumanEval-Pro, MBPP-Pro, and SWE-Bench_Lite datasets using the GPT-4o, Mistral Large, GPT-OSS, and Qwen3-Coder models. It improved code generation accuracy by up to 17% and achieved code coverage gains of up to 99.81%. These results show that metamorphic relations can be a simple but effective guide in assisting LLM-based software development.

</details>


### [12] [Can Large Language Models Solve Path Constraints in Symbolic Execution?](https://arxiv.org/abs/2511.18288)
*Wenhan Wang,Kaibo Liu,Zeyu Sun,An Ran Chen,Ge Li,Gang Huang,Lei Ma*

Main category: cs.SE

TL;DR: LLMs can solve some symbolic execution limitations by generating test inputs and classifying path constraints more effectively than traditional solvers, especially for complex real-world code. This paves the way for wider use of symbolic execution in practice.


<details>
  <summary>Details</summary>
Motivation: Symbolic execution helps with software testing and debugging but struggles with complex data structures and external API calls due to the limitations of traditional SMT solvers. Addressing this limitation could greatly improve the applicability of symbolic execution to real-world software.

Method: The authors investigate using large language models (LLMs) as an alternative to SMT solvers for path constraint solving within symbolic execution. They perform an empirical study on two tasks—test input generation and path satisfiability classification—using new evaluation pipelines and benchmarks, with data from both competition-level and real-world codebases.

Result: State-of-the-art LLMs succeed in generating correct test cases for 60% of execution paths and improve test coverage beyond what traditional symbolic execution tools achieve, especially in scenarios involving complex structures or external calls. LLMs also prove capable at classifying path satisfiability.

Conclusion: LLMs present a promising avenue to support and extend symbolic execution, overcoming some critical limitations of SMT solvers and enhancing the generalizability of symbolic execution for real-world software analysis.

Abstract: Symbolic execution is an important software analysis technique which benefits downstream tasks such as software testing and debugging. However, several limitations hinder symbolic execution from application on real-world software. One of the limitations is the inability to solve diverse execution path constraints: traditional symbolic execution based on SMT solvers is difficult to handle execution paths with complex data structures or external API calls. In this paper, we focus on investigating the possibility of adopting large language models (LLM) for path constraint solving instead of traditional solver-based techniques in symbolic execution. We conduct an empirical study to evaluate the ability of LLMs in two types of path constraint solving: generating test inputs to facilitate an execution path, and determining whether a given execution path can be satisfied without triggering any bugs. We build new evaluation pipelines and benchmarks for two tasks: test case generation and path classification, which include data sources from both competition-level programs and real-world repositories. Our experiment results show that state-of-the-art LLMs are able to solve path constraints in both generation and classification tasks, with 60% of generated test cases that accurately cover the given execution path. Moreover, LLMs are capable of improving test coverage by covering execution paths in real-world repositories where traditional symbolic execution tools cannot be applied. These findings highlight the possibility of extending symbolic execution techniques with LLMs in the future to improve the ability and generalizability of symbolic execution.

</details>


### [13] [A Needle in a Haystack: Intent-driven Reusable Artifacts Recommendation with LLMs](https://arxiv.org/abs/2511.18343)
*Dongming Jin,Zhi Jin,Xiaohong Chen,Zheng Fang,Linyu Li,Yuanpeng He,Jia Li,Yirang Zhang,Yingtao Fang*

Main category: cs.SE

TL;DR: This paper benchmarks artifact recommendation techniques in open source software, showing LLMs outperform traditional methods but need improvement. The proposed TreeRec framework uses hierarchical semantic trees to guide recommendations, boosting accuracy and efficiency for LLMs and showing promise for real-world application.


<details>
  <summary>Details</summary>
Motivation: Developers struggle to find suitable reusable artifacts among many options in open source software development, leading to inefficiency. Current recommendation techniques, including LLMs, have shown promise but their effectiveness needs thorough evaluation.

Method: The paper introduces IntentRecBench, a benchmarking dataset spanning three open source ecosystems, and compares the performance of five LLMs and six traditional artifact recommendation methods. It also proposes TreeRec, a feature tree-guided recommendation framework that organizes artifacts into a hierarchical semantic tree using LLM-based abstraction for better intent alignment and reduced reasoning time.

Result: LLMs outperform traditional recommendation approaches but still suffer from low precision and high inference cost. TreeRec improves precision and efficiency for various LLMs across ecosystems, demonstrating better performance and generalizability.

Conclusion: TreeRec presents an effective way to address low precision and high inference cost in LLM-based artifact recommendation by semantically organizing artifacts. It offers a scalable solution for practical deployment.

Abstract: In open source software development, the reuse of existing artifacts has been widely adopted to avoid redundant implementation work. Reusable artifacts are considered more efficient and reliable than developing software components from scratch. However, when faced with a large number of reusable artifacts, developers often struggle to find artifacts that can meet their expected needs. To reduce this burden, retrieval-based and learning-based techniques have been proposed to automate artifact recommendations. Recently, Large Language Models (LLMs) have shown the potential to understand intentions, perform semantic alignment, and recommend usable artifacts. Nevertheless, their effectiveness has not been thoroughly explored. To fill this gap, we construct an intent-driven artifact recommendation benchmark named IntentRecBench, covering three representative open source ecosystems. Using IntentRecBench, we conduct a comprehensive comparative study of five popular LLMs and six traditional approaches in terms of precision and efficiency. Our results show that although LLMs outperform traditional methods, they still suffer from low precision and high inference cost due to the large candidate space. Inspired by the ontology-based semantic organization in software engineering, we propose TreeRec, a feature tree-guided recommendation framework to mitigate these issues. TreeRec leverages LLM-based semantic abstraction to organize artifacts into a hierarchical semantic tree, enabling intent and function alignment and reducing reasoning time. Extensive experiments demonstrate that TreeRec consistently improves the performance of diverse LLMs across ecosystems, highlighting its generalizability and potential for practical deployment.

</details>


### [14] [Evaluating perturbation robustnessof generative systems that use COBOL code inputs](https://arxiv.org/abs/2511.18488)
*Samuel Ackerman,Wesam Ibraheem,Orna Raz,Marcel Zalmanovici*

Main category: cs.SE

TL;DR: A robust evaluation approach for LLM systems using COBOL code is presented, with methods to perturb input programs, measure robustness, and visualize findings to support debugging and system enhancement.


<details>
  <summary>Details</summary>
Motivation: LLM-based systems, especially those that process COBOL code, are overly sensitive to minor, meaning-preserving variations in input. This sensitivity undermines reliability, which is critical given COBOL's role in many business-critical, legacy applications.

Method: They design COBOL code perturbation techniques (from paragraph-level to whole programs), expand datasets with these variants, run LLM-based translation tasks, measure performance changes, and visualize results via interactive dashboards to aid analysis and debugging.

Result: The authors introduce a framework for measuring and visualizing robustness in LLM-based systems handling COBOL code. They provide a set of perturbation methods, dataset variants, performance metrics, and dashboards that pinpoint sources of sensitivity and facilitate debugging and improvement.

Conclusion: The proposed tools and framework enable better robustness assessment and targeted system improvements for LLM applications involving COBOL code, with extensibility to related tasks such as code generation and explanation.

Abstract: Systems incorporating large language models (LLMs) as a component are known to be sensitive (i.e., non-robust) to minor input variations that do not change the meaning of the input; such sensitivity may reduce the system's usefulness. Here, we present a framework to evaluate robustness of systems using COBOL code as input; our application is translation between COBOL and Java programming languages, but the approach extends to other tasks such as code generation or explanation. Targeting robustness of systems with COBOL as input is essential yet challenging. Many business-critical applications are written in COBOL, yet these are typically proprietary legacy applications and their code is unavailable to LLMs for training. We develop a library of COBOL paragraph and full-program perturbation methods, and create variant-expanded versions of a benchmark dataset of examples for a specific task. The robustness of the LLM-based system is evaluated by measuring changes in values of individual and aggregate metrics calculated on the system's outputs. Finally, we present a series of dynamic table and chart visualization dashboards that assist in debugging the system's outputs, and monitoring and understanding root causes of the system's sensitivity to input variation. These tools can be further used to improve the system by, for instance, indicating variations that should be handled by pre-processing steps.

</details>


### [15] [HQPEF-Py: Metrics, Python Patterns, and Guidance for Evaluating Hybrid Quantum Programs](https://arxiv.org/abs/2511.18506)
*Michael Adjei Osei,Sidney Shapiro*

Main category: cs.SE

TL;DR: The paper proposes and demonstrates new workflow-oriented metrics and audit methods for rigorously evaluating hybrid quantum programs, emphasizing reproducibility and practical comparisons between quantum and classical workflows.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for evaluating hybrid quantum programs as comprehensive workflows rather than focusing only on isolated components like devices or algorithms. This approach is motivated by the growing complexity of quantum-classical systems and the necessity for practical, comparative, and reproducible performance assessment.

Method: The authors formalize new evaluation metrics (QRL, normalized UQ), introduce an audit process for pipeline timing and drift, and offer Python reference code examples to validate and instantiate their approach using classical and quantum computing frameworks.

Result: The authors introduce a Quantum Readiness Level (QRL) score, a normalized utility-of-quantumness (UQ) metric under quality constraints, and a timing-and-drift audit process. They also present Python reference implementations exemplifying how these metrics and audits can be applied to both classical and quantum solvers, ensuring budget-matched and reproducible evaluations.

Conclusion: Workflow-aware metrics and audit procedures are essential for credible evaluation of hybrid quantum programs. The proposed framework and reference implementations provide a standardized and reproducible way to assess quantum utility in real-world scenarios.

Abstract: We study how to evaluate hybrid quantum programs as end-to-end workflows rather than as isolated devices or algorithms. Building on the Hybrid Quantum Program Evaluation Framework (HQPEF), we formalize a workflow-aware Quantum Readiness Level (QRL) score; define a normalized speedup under quality constraints for the Utility of Quantumness (UQ); and provide a timing-and-drift audit for hybrid pipelines. We complement these definitions with concise Python reference implementations that illustrate how to instantiate the metrics and audit procedures with state-of-the-art classical and quantum solvers (e.g., via Qiskit or PennyLane), while preserving matched-budget discipline and reproducibility.

</details>


### [16] [End-to-End Automated Logging via Multi-Agent Framework](https://arxiv.org/abs/2511.18528)
*Renyi Zhong,Yintong Huo,Wenwei Gu,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: Autologger is a new hybrid framework that automates the full software logging process. It uses a classifier to decide if logging is needed, then agents to determine where and what to log. Tested on open-source projects, Autologger strongly outperforms current tools, offering a clear advance for software observability and LLM-driven automation.


<details>
  <summary>Details</summary>
Motivation: There is a persistent challenge in software engineering: developers either overlog (leading to high costs and noise) or underlog (sacrificing crucial system observability and debugging telemetry). Existing automated logging tools do not adequately address the core question of whether logging is necessary and struggle with the multifaceted complexity of effective logging.

Method: Autologger is a hybrid framework that encompasses the entire logging pipeline. It uses a fine-tuned classifier (“Judger”) to determine whether a method should have a logging statement. If so, two specialized agents—Locator (where to log) and Generator (what to log)—work together, utilizing program analysis and retrieval tools, to insert or improve logging.

Result: Autologger is evaluated on three large open-source projects and outperforms state-of-the-art baselines. It achieves 96.63% F1-score on the whether-to-log decision and improves the quality of logging statements by 16.13% in end-to-end performance. Autologger has demonstrated generalizability across multiple backbone LLMs, boosting their performance.

Conclusion: Autologger effectively addresses the end-to-end challenge of automated logging, outperforming existing solutions both in decision-making and quality of generated statements, while being adaptable to other LLMs.

Abstract: Software logging is critical for system observability, yet developers face a dual crisis of costly overlogging and risky underlogging. Existing automated logging tools often overlook the fundamental whether-to-log decision and struggle with the composite nature of logging. In this paper, we propose Autologger, a novel hybrid framework that addresses the complete the end-to-end logging pipeline. Autologger first employs a fine-tuned classifier, the Judger, to accurately determine if a method requires new logging statements. If logging is needed, a multi-agent system is activated. The system includes specialized agents: a Locator dedicated to determining where to log, and a Generator focused on what to log. These agents work together, utilizing our designed program analysis and retrieval tools. We evaluate Autologger on a large corpus from three mature open-source projects against state-of-the-art baselines. Our results show that Autologger achieves 96.63\% F1-score on the crucial whether-to-log decision. In an end-to-end setting, Autologger improves the overall quality of generated logging statements by 16.13\% over the strongest baseline, as measured by an LLM-as-a-judge score. We also demonstrate that our framework is generalizable, consistently boosting the performance of various backbone LLMs.

</details>


### [17] [From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence](https://arxiv.org/abs/2511.18538)
*Jian Yang,Wei Zhang,Shark Liu,Jiajun Wu,Shawn Guo,Yizhi Li*

Main category: cs.SE

TL;DR: This work reviews and experiments with both general and specialized code LLMs, evaluating their data pipelines, training methods, and practical effectiveness, and matches academic research directions with industry challenges in automated code generation.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the rapid evolution and mainstream adoption of code generation LLMs, the shift in accuracy paradigms, and the need to bridge academic benchmarks with actual industrial deployment challenges.

Method: The authors use a combination of analytic and probing experiments, comprehensive literature review, and empirical tests across different model architectures, training paradigms (pre-training, fine-tuning, RL), and dataset comparisons.

Result: The study critically evaluates state-of-the-art general and code-specialized LLMs, identifies design choices and trade-offs, pinpoints deficiencies in current research related to industry needs (e.g., correctness, security, context-awareness), and suggests future research directions. Experimental results illuminate impacts of scales, frameworks, and hyperparameters on model performance.

Conclusion: This paper delivers an in-depth analysis and synthesis of code-centric large language models (LLMs), mapping their development, strengths, and practical adoption while highlighting research gaps related to real-world software engineering needs.

Abstract: Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.

</details>


### [18] [Strategic Decision Framework for Enterprise LLM Adoption](https://arxiv.org/abs/2511.18589)
*Michael Trusov,Minha Hwang,Zainab Jamal,Swarup Chandra*

Main category: cs.SE

TL;DR: This paper provides a six-step framework for organizations to adopt LLMs. It gives practical advice—based on interviews and real-world cases—for addressing application selection, security, development, and deployment, ensuring informed decisions and secure integration.


<details>
  <summary>Details</summary>
Motivation: Organizations are keen to transform operations using LLMs but lack structured guidance for adoption and implementation, facing concerns over data security, development processes, infrastructure, and compliance.

Method: The paper introduces a systematic six-step decision framework, drawn from interviews and case analyses of real LLM deployments (both successful and unsuccessful) across industry contexts.

Result: The framework offers actionable steps for organizations, guiding them from LLM application selection through deployment, bolstered by examples from healthcare, finance, and software.

Conclusion: With this decision framework, business leaders can align LLM capabilities safely and efficiently to core objectives, overcoming adoption challenges and facilitating secure integration across use cases.

Abstract: Organizations are rapidly adopting Large Language Models (LLMs) to transform their operations, yet they lack clear guidance on key decisions for adoption and implementation. While LLMs offer powerful capabilities in content generation, assisted coding, and process automation, businesses face critical challenges in data security, LLM solution development approach, infrastructure requirements, and deployment strategies. Healthcare providers must protect patient data while leveraging LLMs for medical analysis, financial institutions need to balance automated customer service with regulatory compliance, and software companies seek to enhance development productivity while maintaining code security.
  This article presents a systematic six-step decision framework for LLM adoption, helping organizations navigate from initial application selection to final deployment. Based on extensive interviews and analysis of successful and failed implementations, our framework provides practical guidance for business leaders to align technological capabilities with business objectives. Through key decision points and real-world examples from both B2B and B2C contexts, organizations can make informed decisions about LLM adoption while ensuring secure and efficient integration across various use cases, from customer service automation to content creation and advanced analytics.

</details>


### [19] [From Reviewers' Lens: Understanding Bug Bounty Report Invalid Reasons with LLMs](https://arxiv.org/abs/2511.18608)
*Jiangrui Zheng,Yingming Zhou,Ali Abdullah Ahmad,Hanqing Yao,Xueqing Liu*

Main category: cs.SE

TL;DR: Identifying invalid bug bounty reports is challenging for current language models due to a bias toward accepting reports. Enhancing these models with structured taxonomies via a RAG framework improves detection and consistency. The findings also reveal that reviewer bias towards reputable reporters affects review outcomes, highlighting transparency and fairness challenges.


<details>
  <summary>Details</summary>
Motivation: With the increase in AI-generated bug reports on bug bounty platforms, there is little support for bug hunters to understand why many reports are labeled invalid, which impacts report quality and reviewer workload. The paper seeks to improve the prediction and interpretation of invalid reports.

Method: The authors conduct an empirical study, collecting 9,942 bug bounty reports (including 1,400 invalid ones), and evaluate several large language models (including GPT-5, DeepSeek, and fine-tuned RoBERTa) on their ability to detect invalid reports. They develop a taxonomy for rejection reasons in Information Disclosure vulnerabilities and integrate this taxonomy into a retrieval-augmented generation framework, then analyze the impact of reporter reputation on review outcomes.

Result: While state-of-the-art models achieve high overall accuracy, they struggle to consistently identify invalid reports, often over-accepting reports as valid. Incorporating a structured taxonomy into a RAG framework significantly improves invalid report detection and consistency. Additionally, reviewers appear biased in favor of reporters with higher reputations.

Conclusion: Combining LLMs with structured reviewer knowledge via a taxonomy-based approach can improve invalid bug report detection and makes the review process more consistent and transparent. Reviewer judgments are also influenced by reporter reputation, indicating the need to address such biases to ensure fairness.

Abstract: Bug bounty platforms (e.g., HackerOne, BugCrowd) leverage crowd-sourced vulnerability discovery to improve continuous coverage, reduce the cost of discovery, and serve as an integral complement to internal red teams. With the rise of AI-generated bug reports, little work exists to help bug hunters understand why these reports are labeled as invalid. To improve report quality and reduce reviewers' burden, it is critical to predict invalid reports and interpret invalid reasons.
  In this work, we conduct an empirical study with the purpose of helping bug hunters understand the validity of reports. We collect a dataset of 9,942 disclosed bug bounty reports, including 1,400 invalid reports, and evaluate whether state-of-the-art large language models can identify invalid reports. While models such as GPT-5, DeepSeek, and a fine-tuned RoBERTa achieve strong overall accuracy, they consistently struggle to detect invalid cases, showing a tendency to over-accept reports. To improve invalidity detection, we build a taxonomy of rejection reasons for Information Disclosure vulnerabilities and incorporate it into a retrieval-augmented generation (RAG) framework. This approach substantially improves classification consistency and reduces bias. We also examine whether reviewer decisions may be influenced by factors beyond the content of the report. Our analysis shows that reporters with higher reputations tend to receive more favorable outcomes in borderline cases, suggesting that perceived expertise can influence review judgments.
  Overall, our findings highlight the challenges of invalid report identification and show that combining LLMs with structured reviewer knowledge can support more transparent and consistent vulnerability report review.

</details>


### [20] [Leveraging Discrete Choice Experiments for User-Centric Requirements Prioritization in mHealth Applications](https://arxiv.org/abs/2511.18625)
*Wei Wang,Hourieh Khalajzadeh,John Grundy,Anuradha Madugalla,Humphrey O. Obie*

Main category: cs.SE

TL;DR: This paper investigates what drives user preferences and trade-offs in adaptive interfaces for mobile health apps, using a quantitative experiment with chronic disease patients.


<details>
  <summary>Details</summary>
Motivation: To address persistent usability and accessibility challenges for diverse users of mHealth apps by understanding barriers to the adoption of adaptive user interfaces.

Method: A Discrete Choice Experiment (DCE) with 186 chronic disease mHealth app users, analyzing preferences across six key adaptation attributes using a mixed logit model; included subgroup analysis by demographics and coping mechanisms.

Result: User preferences favor adaptive interfaces that maintain usability and give users control over app adaptations; frequent, large changes and involving caregivers may decrease acceptance, with differences observed across demographic and behavioral subgroups.

Conclusion: High usability, user control, infrequent and small adaptations increase adoption of AUIs in mHealth apps; frequent functions and caregiver involvement may reduce value for users.

Abstract: Mobile health (mHealth) applications are widely used for chronic disease management, but usability and accessibility challenges persist due to the diverse needs of users. Adaptive User Interfaces (AUIs) offer a personalized solution to enhance user experience, yet barriers to adoption remain. Understanding user preferences and trade-offs is essential to ensure widespread acceptance of adaptation designs. This study identifies key factors influencing user preferences and trade-offs in mHealth adaptation design. A Discrete Choice Experiment (DCE) was conducted with 186 participants who have chronic diseases and use mHealth applications. Participants were asked to select preferred adaptation designs from choices featuring six attributes with varying levels. A mixed logit model was used to analyze preference heterogeneity and determine the factors most likely influencing adoption. Additionally, subgroup analyses were performed to explore differences by age, gender, health conditions, and coping mechanisms. Maintaining usability while ensuring controllability over adaptations, infrequent adaptations, and small-scale changes are key factors that facilitate the adoption of adaptive mHealth app designs. In contrast, frequently used functions and caregiver involvement can diminish the perceived value of such adaptations. This study employs a data-driven approach to quantify user preferences, identify key trade-offs, and reveal variations across demographic and behavioral subgroups through preference heterogeneity modeling. Furthermore, our results offer valuable guidance for developing future adaptive mHealth applications and lay the groundwork for continued exploration into requirements prioritization within the field of software engineering.

</details>


### [21] [ChroniUXMag: A Persona-Driven Framework for Inclusive mHealth Requirements Engineering](https://arxiv.org/abs/2511.18634)
*Wei Wang,Devi Karolita,Hourieh Khalajzadeh,John Grundy,Anuradha Madugalla,Humphrey O. Obie*

Main category: cs.SE

TL;DR: ChroniUXMag is a new framework to help design more inclusive mobile health apps for chronic disease management by identifying and addressing user needs often missed by standard design and evaluation methods. It uses personas and evidence from broad research to guide its approach.


<details>
  <summary>Details</summary>
Motivation: Mobile health (mHealth) apps are widely used in chronic disease management but struggle with accessibility, inclusivity, and sustained user engagement. Patients have evolving and unique needs due to their health progression, treatment adherence, and support systems, which traditional requirements engineering often fails to capture.

Method: The study developed the ChroniUXMag framework, inspired by InclusiveMag and GenderMag, through two phases: (1) identifying inclusivity facets via literature review, focus groups, interviews, and a large-scale survey; (2) synthesizing these facets into diverse personas and embedding them into a cognitive walkthrough tool.

Result: Thirteen socio-technical inclusivity facets (e.g., trust, digital literacy, dependency, cultural context) were identified, enabling persona-driven evaluations to uncover mHealth inclusivity barriers missed by standard usability approaches.

Conclusion: ChroniUXMag provides an evidence-based, reproducible method for embedding inclusivity into mHealth requirements engineering processes. It systematically addresses barriers overlooked by traditional assessments and will be evaluated further in practical design contexts.

Abstract: Mobile health (mHealth) applications are increasingly adopted for chronic disease management, yet they face persistent challenges related to accessibility, inclusivity, and sustained engagement. Patients' needs evolve dynamically with their health progression, adherence, and caregiver support, creating unique requirements engineering (RE) challenges that traditional approaches often overlook. This study introduces ChroniUXMag, a framework for eliciting and analysing inclusivity requirements in mHealth design. Building on InclusiveMag and GenderMag principles, the framework aims to help researchers and practitioners systematically capture and evaluate factors that influence how individuals with chronic conditions perceive, trust, and interact with mHealth systems. The framework was developed through two stages of the InclusiveMag process. In the first stage, inclusivity facets were identified through a systematic literature review, focus groups, interviews, and a large-scale survey. In the second stage, these facets were synthesised into personas representing diverse health situations, attitudes, and digital practices, and integrated into an adapted cognitive walkthrough form. Thirteen facets were identified that capture the socio-technical complexity of mHealth use, including trust, digital literacy, dependency, and cultural context. These facets support structured, persona-driven evaluations that reveal inclusivity barriers often missed by traditional usability assessments. ChroniUXMag contributes to RE by offering a reproducible, evidence-based approach for embedding inclusivity into mHealth requirements. Future work will extend the third stage Apply through practitioner-led evaluation in real-world design contexts.

</details>


### [22] [Summary-Mediated Repair: Can LLMs use code summarisation as a tool for program repair?](https://arxiv.org/abs/2511.18782)
*Lukas Twist*

Main category: cs.SE

TL;DR: Inserting a code summarization step before automated code repair can help LLMs fix more subtle bugs, but the improvement is moderate and depends on the LLM; summaries are best used as supportive diagnostic tools, not standalone solutions.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) often write code that appears correct but contains subtle bugs that are hard for the LLMs to detect. Interestingly, when summarizing code, LLMs can often understand and articulate high-level intent while missing low-level mistakes. This observation motivates the exploration of using code summarization as an intermediate step in automated code repair.

Method: The authors propose and implement 'summary-mediated repair,' a prompt-based pipeline that explicitly uses natural-language code summarization between code input and code repair. They systematically evaluate this approach across eight LLMs using two established function-level code benchmarks (HumanEvalPack and MBPP). They compare various styles of summaries, with a focus on error-aware diagnostic summaries, against traditional direct repair baselines.

Result: Error-aware diagnostic summaries consistently lead to the largest improvements in code repair rates across LLMs, fixing up to 65% of previously unseen errors. These gains are about 5% higher on average compared to direct repair, but the overall improvements are modest and vary depending on the model.

Conclusion: Leveraging code summaries—especially error-aware ones—as an intermediary diagnostic step can improve the effectiveness of LLM-based code repair pipelines. However, while summaries help, they are not a complete solution and work best when integrated as part of a broader repair process.

Abstract: Large Language Models (LLMs) often produce code with subtle implementation-level bugs despite strong benchmark performance. These errors are hard for LLMs to spot and can have large behavioural effects; yet when asked to summarise code, LLMs can frequently surface high-level intent and sometimes overlook this low-level noise. Motivated by this, we propose summary-mediated repair, a prompt-only pipeline for program repair that leverages natural-language code summarisation as an explicit intermediate step, extending previous work that has already shown code summarisation to be a useful intermediary for downstream tasks. We evaluate our method across eight production-grade LLMs on two function level benchmarks (HumanEvalPack and MBPP), comparing several summary styles against a direct repair baseline. Error-aware diagnostic summaries consistently yield the largest gains - repairing up to 65% of unseen errors, on average of 5% more than the baseline - though overall improvements are modest and LLM-dependent. Our results position summaries as a cheap, human-interpretable diagnostic artefact that can be integrated into program-repair pipelines rather than a stand-alone fix-all.

</details>


### [23] [Optimizing LLM Code Suggestions: Feedback-Driven Timing with Lightweight State Bounds](https://arxiv.org/abs/2511.18842)
*Mohammad Nour Al Awad,Sergey Ivanov,Olga Tikhonova*

Main category: cs.SE

TL;DR: This paper introduces an adaptive timing system for LLM code suggestions, using real-time developer feedback and cognitive state modeling. It boosts suggestion acceptance rates and slashes wasted inference, yielding more efficient and practical code assistants.


<details>
  <summary>Details</summary>
Motivation: LLM-driven code auto-completion often interrupts developers or wastes computational resources due to poorly timed suggestions. Existing systems lack adaptive mechanisms for when suggestions should appear, leading to inefficiencies and poor user experience.

Method: The authors propose an adaptive timing framework that dynamically chooses the optimal delay for code completion suggestions. This mechanism leverages real-time developer feedback, logistic transformation of recent suggestion acceptance rates, a bounded delay range, and binary predictions of the developer's cognitive state.

Result: In a two-month field study with professional developers, static delays improved suggestion acceptance rate from 4.9% to 15.4%. The proposed adaptive timing mechanism further increased acceptance rate to 18.6%, while reducing blind rejections dramatically from 8.3% to 0.36%, and cutting down redundant inference calls by 75%.

Conclusion: Adapting the timing of LLM code suggestions based on developer feedback and cognitive state significantly enhances both acceptance rates and efficiency, making code assistants more valuable and cost-effective.

Abstract: Large Language Models (LLMs) have transformed code auto-completion by generating context-aware suggestions. Yet, deciding when to present these suggestions remains underexplored, often leading to interruptions or wasted inference calls. We propose an adaptive timing mechanism that dynamically adjusts the delay before offering a suggestion based on real-time developer feedback. Our suggested method combines a logistic transform of recent acceptance rates with a bounded delay range, anchored by a high-level binary prediction of the developer's cognitive state. In a two-month deployment with professional developers, our system improved suggestion acceptance from 4.9% with no delay to 15.4% with static delays, and to 18.6% with adaptive timing-while reducing blind rejections (rejections without being read) from 8.3% to 0.36%. Together, these improvements increase acceptance and substantially reduce wasted inference calls by 75%, making LLM-based code assistants more efficient and cost-effective in practice.

</details>


### [24] [Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming](https://arxiv.org/abs/2511.18849)
*Mohammad Nour Al Awad,Sergey Ivanov,Olga Tikhonova*

Main category: cs.SE

TL;DR: A privacy-preserving filter using real-time developer behavior predicts when LLM suggestions in code editors are likely to be accepted, doubling acceptance rates and reducing wasted computation, all without inspecting code or prompts.


<details>
  <summary>Details</summary>
Motivation: Many AI code suggestions offered by LLMs are ignored in code editors, leading to wasted computation, higher latency, and user interruptions. There is a need to improve the efficiency and usefulness of LLM-driven suggestions.

Method: The paper presents a lightweight pre-filtering model that uses real-time developer telemetry (such as typing speed, navigation, and editing activity) to predict the likelihood that a suggestion will be accepted, before invoking the LLM. This is implemented in a Visual Studio Code plugin and deployed in a production setting for four months.

Result: The proposed filter almost doubled the acceptance rate of AI suggestions (from 18.4% to 34.2%) and reduced low-value LLM calls by 35%. The system achieves these improvements using only editor telemetry and without inspecting code or prompts.

Conclusion: Behavioral signals can effectively improve user experience and system efficiency in LLM-assisted programming. Timing-aware and privacy-preserving adaptation mechanisms are valuable for integrating AI-powered assistance in code editors.

Abstract: Large Language Models (LLMs) are increasingly integrated into code editors to provide AI-powered code suggestions. Yet many of these suggestions are ignored, resulting in wasted computation, increased latency, and unnecessary interruptions. We introduce a lightweight pre-filtering model that predicts the likelihood of suggestion acceptance before invoking the LLM, using only real-time developer telemetry such as typing speed, file navigation, and editing activity. Deployed in a production-grade Visual Studio Code plugin over four months of naturalistic use, our approach nearly doubled acceptance rates (18.4% -> 34.2%) while suppressing 35% of low-value LLM calls. These findings demonstrate that behavioral signals alone can meaningfully improve both user experience and system efficiency in LLM-assisted programming, highlighting the value of timing-aware, privacy-preserving adaptation mechanisms. The filter operates solely on pre-invocation editor telemetry and never inspects code or prompts.

</details>


### [25] [Time Travel: LLM-Assisted Semantic Behavior Localization with Git Bisect](https://arxiv.org/abs/2511.18854)
*Yujing Wang,Weize Hong*

Main category: cs.SE

TL;DR: This paper proposes a new framework that uses large language models and chain of thought reasoning to improve semantic fault localization in git bisect processes, especially under noisy and non-deterministic conditions. Fine-tuning and weak supervision approaches further increase efficiency, resulting in notable improvements in fault localization success rates and traversal times across open source projects.


<details>
  <summary>Details</summary>
Motivation: Traditional git bisecting methods are limited by deterministic and binary assumptions, which do not hold in modern development environments where tests are flaky and failure states are nonmonotonic. This makes fault localization challenging and motivates the integration of LLMs to enhance bisect processes.

Method: The paper introduces a framework that incorporates LLMs into git bisect, leveraging chain of thought reasoning for commit-level analysis. DeepSeekCoderV2 is fine-tuned using QLoRA on a dataset of semantically labeled diffs. Weak supervision, human-in-the-loop correction, and self-consistency filtering are used to manage annotation overhead. Multiple LLMs are evaluated for suitability.

Result: Experiments on various open source projects demonstrate a 6.4 percentage point improvement in success rate for fault localization, from 74.2% to 80.6%. The enhanced system also achieves up to a 2x reduction in average bisect traversal time and significantly fewer failed traversals.

Conclusion: Integrating LLMs with git bisect using structured reasoning and specialized finetuning leads to substantial improvements in semantic fault localization efficiency and reliability. The approach is effective even under noisy and nonmonotonic conditions, and the paper discusses strategies for prompt design and temporal reasoning. 

Abstract: We present a novel framework that integrates Large Language Models (LLMs) into the Git bisect process for semantic fault localization. Traditional bisect assumes deterministic predicates and binary failure states assumptions often violated in modern software development due to flaky tests, nonmonotonic regressions, and semantic divergence from upstream repositories. Our system augments bisect traversal with structured chain of thought reasoning, enabling commit by commit analysis under noisy conditions. We evaluate multiple open source and proprietary LLMs for their suitability and fine tune DeepSeekCoderV2 using QLoRA on a curated dataset of semantically labeled diffs. We adopt a weak supervision workflow to reduce annotation overhead, incorporating human in the loop corrections and self consistency filtering. Experiments across multiple open source projects show a 6.4 point absolute gain in success rate from 74.2 to 80.6 percent, leading to significantly fewer failed traversals and by experiment up to 2x reduction in average bisect time. We conclude with discussions on temporal reasoning, prompt design, and finetuning strategies tailored for commit level behavior analysis.

</details>


### [26] [VecIntrinBench: Benchmarking Cross-Architecture Intrinsic Code Migration for RISC-V Vector](https://arxiv.org/abs/2511.18867)
*Liutong Han,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: VecIntrinBench is the first benchmark suite supporting RISC-V Vector (RVV) intrinsic migration. It enables evaluation of both rule-based and LLM-based code migration methods, with results showing LLMs to be as effective as, and sometimes better than, rule-based approaches. The benchmark is open-sourced for widespread use.


<details>
  <summary>Details</summary>
Motivation: There is a growing demand for migrating and adapting algorithm libraries to the RISC-V software ecosystem. However, current benchmarks focus on mainstream SIMD intrinsics, lacking comprehensive support for RISC-V Vector (RVV) extensions, making it challenging to evaluate and improve code migration methods for RVV.

Method: The authors propose VecIntrinBench, a new benchmark suite that encompasses the RVV extension. VecIntrinBench contains 50 function-level tasks implemented in scalar, RVV intrinsics, Arm Neon intrinsics, and x86 intrinsics. The benchmark includes extensive functional and performance tests. Various code migration approaches, including rule-based mapping and LLM-based code generation, are systematically evaluated using VecIntrinBench.

Result: Results show that advanced LLM-based code generation methods match the effectiveness of rule-based mapping approaches for RISC-V code migration, while also offering better performance. The benchmark provides novel insights into code migration methodologies and LLM development for intrinsic translation.

Conclusion: VecIntrinBench fills the gap in RVV benchmark support, enabling comprehensive evaluation of intrinsic migration methods for RISC-V. Advanced LLMs show significant promise in code migration, potentially guiding future research in the area. The benchmark is open-sourced for community benefit.

Abstract: Intrinsic functions are specialized functions provided by the compiler that efficiently operate on architecture-specific hardware, allowing programmers to write optimized code in a high-level language that fully exploits hardware features. Using intrinsics to vectorize core code blocks is a standard optimization method in high-performance libraries, often requiring specific vector optimization implementations for multiple mainstream architectures. The promising RISC-V software ecosystem has a significant demand for algorithm library migration and adaptation. Translating existing intrinsic functions to RISC-V Vector (RVV) intrinsic functions across architectures is currently a mainstream approach. Rule-based intrinsic mapping methods and LLM-based code generation can help developers address the code migration challenge. However, existing intrinsic code benchmarks focus on mainstream SIMD intrinsics and lack support for the emerging RISC-V architecture. There is currently no benchmark that comprehensively evaluates the intrinsic migration capabilities for the RVV extension. To fill this gap, we propose VecIntrinBench, the first intrinsic benchmark encompassing RVV extensions. It includes 50 function-level tasks from open source repositories, implemented as scalars, RVV intrinsics, Arm Neon intrinsics, and x86 intrinsics, along with comprehensive functional and performance test cases. We systematically evaluated various code migration approaches on VecIntrinBench, yielding a series of insightful findings. The results demonstrate that advanced Large Language Models (LLMs) achieve a similar effect as rule-based mapping approaches for RISC-V code migration, while also delivering superior performance. We further analyze the reasons and identify future directions for LLM development in the code migration field. The VecIntrinBench is open-sourced to benefit the broader community and developers.

</details>


### [27] [Optimization-Aware Test Generation for Deep Learning Compilers](https://arxiv.org/abs/2511.18918)
*Qingchao Shen,Zan Wang,Haoyang Ma,Yongqiang Tian,Lili Huang,Zibo Xiao,Junjie Chen,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: OATest is a new approach for generating optimization-aware tests for deep learning compilers. It outperforms existing methods by detecting more bugs and covering more code, revealing 58 previously unknown bugs in popular compilers.


<details>
  <summary>Details</summary>
Motivation: Ensuring the reliability and security of DL compilers is critical, but current testing approaches struggle at the optimization stage due to difficulty in generating suitable tests.

Method: OATest synthesizes optimization-aware computational graphs using patterns from documented tests, integrates them into seed graphs, applies an edge reusing strategy for strong context-pattern connections, and uses an auxiliary layers addition strategy to maintain graph validity. Differential testing is employed on TVM and ONNXRuntime compilers using two test oracles.

Result: OATest detected more bugs and achieved higher code coverage compared to previous methods, uncovering 58 previously unknown bugs (36 confirmed or fixed) in TVM and ONNXRuntime.

Conclusion: OATest proves to be an effective tool for discovering bugs and improving code coverage in DL compilers, outperforming existing methods and uncovering numerous previously unknown bugs.

Abstract: Deep Learning (DL) compilers have been widely utilized to optimize DL models for efficient deployment across various hardware. Due to their vital role in the DL ecosystem, ensuring their reliability and security is critical. However, existing approaches have limitations in testing optimization stages, which is the core functionality of DL compilers, due to the difficulty in generating optimization-aware tests. In this paper, we proposed OATest, a novel approach for synthesizing optimization-aware computational graphs. The approach combines patterns extracted from documented tests for optimization and incorporates them into seed computational graphs, enabling broader exploration of optimization paths. To guarantee the optimization-awareness of generated graphs, OATest introduces the edges reusing strategy to establish strong connections between patterns and contexts. Additionally, to solve the validity challenge for the generated graphs, OATest employs an auxiliary layers addition strategy to resolve broken constraints. Equipped with two distinct test oracles, OATest applies differential testing to evaluate the two widely used DL compilers (i.e., TVM and ONNXRuntime). Our experimental results show that OATest outperforms the state-of-the-art method by detecting more bugs and achieving higher code coverage in TVM and ONNXRutimes. Additionally, OATest uncovers 58 previously unknown bugs, 36 of which have been confirmed or fixed by developers.

</details>


### [28] [LLM-Driven Kernel Evolution: Automating Driver Updates in Linux](https://arxiv.org/abs/2511.18924)
*Arina Kharlamova,Jiawen Liu,Tianyi Zhang,Xinrui Yang,Humaid Alqasimi,Youcheng Sun,Chun Jason Xue*

Main category: cs.SE

TL;DR: The paper introduces DRIVEBENCH and AUTODRIVER, tools for automating Linux driver updates using LLMs. The system demonstrates moderate success in generating functionally correct, kernel-compliant driver patches, facilitating reproducible and safer driver maintenance.


<details>
  <summary>Details</summary>
Motivation: Linux kernel evolution regularly breaks device drivers due to changes in APIs/ABI, semantics, and security updates. Manual driver maintenance is costly and error-prone, motivating automation to maintain driver compatibility efficiently and safely as the kernel evolves.

Method: The authors propose DRIVEBENCH, an executable corpus of Linux kernel-to-driver co-evolution cases, and AUTODRIVER, a system powered by large language models (LLMs) for automated driver maintenance. AUTODRIVER combines prompt engineering, collaboration among multiple agents, static code analysis, and iterative validation to produce reliable driver patches.

Result: The DRIVEBENCH corpus covers kernel versions v5.10 to v6.10, containing 235 validated co-evolution cases. In their evaluation (55 cases), AUTODRIVER achieved a 56.4% compilation success rate, with QEMU-based experiments confirming that most generated patches preserved driver initialization.

Conclusion: DRIVEBENCH and AUTODRIVER represent significant progress toward automating and systematizing Linux driver maintenance. By releasing their dataset and tooling, the authors support further reproducible research and pave the way for continuously safe driver and kernel co-evolution.

Abstract: Linux kernel evolution breaks drivers through API/ABI changes, semantic shifts, and security-hardening updates. We introduce DRIVEBENCH, an executable corpus of kernel$\rightarrow$driver co-evolution cases, and AUTODRIVER, a closed-loop, LLM-driven system for automating driver maintenance. The system integrates prompt engineering, multi-agent collaboration, static analysis, and iterative validation to ensure that generated patches are not only syntactically correct but also functionally and semantically consistent with kernel conventions. The corpus spans v5.10-v6.10 with 235 validated cases drawn from 612 candidates. In evaluation across 55 cases, AUTODRIVER achieves 56.4% compilation success; QEMU-based boot verification indicates that compiled patches preserve driver initialization in most instances. By releasing DRIVEBENCH and tooling, we enable reproducible research and a practical route to continuous, safe co-evolution of drivers with the Linux kernel.

</details>


### [29] [LLMAID: Identifying AI Capabilities in Android Apps with LLMs](https://arxiv.org/abs/2511.19059)
*Pei Liu,Terry Zhuo,Jiawei Deng,Thong James,Shidong Pan,Sherry Xu,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhang*

Main category: cs.SE

TL;DR: LLMAID uses large language models to automatically and accurately detect AI capabilities in mobile apps, outperforming manual and rule-based methods, with most identified AI functionalities relating to computer vision.


<details>
  <summary>Details</summary>
Motivation: Current manual and rule-based AI detection approaches are costly, inefficient, and unable to adapt to advances in AI techniques, creating a need for automated tools that can reliably identify and summarize AI capabilities in mobile software.

Method: LLMAID performs four tasks: candidate extraction, knowledge base interaction, AI capability detection, and summarization. Its performance was evaluated on 4,201 Android apps, compared to rule-based baselines, via quantitative metrics and a user study.

Result: LLMAID, a tool based on large language models (LLMs), was proposed and evaluated for discovering and analyzing AI capabilities in mobile apps, specifically Android applications. LLMAID automates AI capability detection and provides summaries of AI services, outperforming traditional rule-based methods both in comprehensiveness and accuracy. It was applied to 4,201 apps and demonstrated significant improvements in identifying AI-enabled applications. Furthermore, its generated summaries were deemed more informative by developers. Empirical analysis showed most AI functionalities are concentrated in computer vision tasks.

Conclusion: LLMAID is effective for detecting and summarizing AI functionalities in mobile applications, offering significant advantages over prior approaches in both precision and recall, and providing more useful service summaries for developers.

Abstract: Recent advancements in artificial intelligence (AI) and its widespread integration into mobile software applications have received significant attention, highlighting the growing prominence of AI capabilities in modern software systems. However, the inherent hallucination and reliability issues of AI continue to raise persistent concerns. Consequently, application users and regulators increasingly ask critical questions such as: Does the application incorporate AI capabilities? and What specific types of AI functionalities are embedded? Preliminary efforts have been made to identify AI capabilities in mobile software; however, existing approaches mainly rely on manual inspection and rule-based heuristics. These methods are not only costly and time-consuming but also struggle to adapt advanced AI techniques.
  To address the limitations of existing methods, we propose LLMAID (Large Language Model for AI Discovery). LLMAID includes four main tasks: (1) candidate extraction, (2) knowledge base interaction, (3) AI capability analysis and detection, and (4) AI service summarization. We apply LLMAID to a dataset of 4,201 Android applications and demonstrate that it identifies 242% more real-world AI apps than state-of-the-art rule-based approaches. Our experiments show that LLM4AID achieves high precision and recall, both exceeding 90%, in detecting AI-related components. Additionally, a user study indicates that developers find the AI service summaries generated by LLMAID to be more informative and preferable to the original app descriptions. Finally, we leverage LLMAID to perform an empirical analysis of AI capabilities across Android apps. The results reveal a strong concentration of AI functionality in computer vision (54.80%), with object detection emerging as the most common task (25.19%).

</details>


### [30] [Can LLMs Recover Program Semantics? A Systematic Evaluation with Symbolic Execution](https://arxiv.org/abs/2511.19130)
*Rong Feng,Suman Saha*

Main category: cs.SE

TL;DR: Fine-tuning LLMs with symbolic execution artifacts helps them better deobfuscate code. GPT-4.1-mini performed best, and supplemental training data improved all models. This approach strengthens software engineering tasks affected by code obfuscation.


<details>
  <summary>Details</summary>
Motivation: Obfuscation in software makes key engineering tasks such as comprehension, maintenance, testing, and vulnerability detection more difficult. Typical analysis tools and LLMs are not able to effectively recover original program semantics from obfuscated code.

Method: The authors fine-tune large language models (LLMs) with symbolic execution artifacts (e.g., SMT constraints, path statistics, test cases) to help them deobfuscate programs. They create a benchmark using four common obfuscation techniques and evaluate several state-of-the-art LLMs under standard and enhanced (with KLEE artifacts) training approaches.

Result: GPT-4.1-mini demonstrated the best overall deobfuscation performance. Incorporating symbolic execution artifacts significantly improved both semantic preservation and compilation success across all tested LLMs.

Conclusion: Deobfuscation is a broad software engineering issue. Fine-tuning LLMs with symbolic execution artifacts can enhance automated testing, static analysis, and program comprehension when dealing with obfuscated code.

Abstract: Obfuscation poses a persistent challenge for software engineering tasks such as program comprehension, maintenance, testing, and vulnerability detection. While compiler optimizations and third-party code often introduce transformations that obscure program intent, existing analysis tools and large language models (LLMs) struggle to recover the original semantics. In this work, we investigate whether LLMs, when fine-tuned with symbolic execution artifacts, can effectively deobfuscate programs and restore analyzability. We construct a benchmark by applying four widely studied transformations-control-flow flattening, opaque predicates, arithmetic encoding, and branch encoding-across diverse C programs from TUM Obfuscation Benchmarks, the LLVM test suite, and algorithmic repositories. We then compare three state-of-the-art LLMs under two training configurations: baseline fine-tuning on obfuscated/original code pairs, and enhanced fine-tuning with additional KLEE artifacts such as SMT constraints, path statistics, and test cases. Our evaluation examines syntactic correctness (compilation success), semantic fidelity (behavioral equivalence under symbolic execution), and code quality (readability and structure). Results show that GPT-4.1-mini achieves the strongest deobfuscation overall, and that incorporating KLEE artifacts consistently improves semantic preservation and compilation success across models. These findings highlight deobfuscation as a broader software engineering concern, demonstrating that combining LLMs with symbolic execution can strengthen automated testing, static analysis, and program comprehension in the presence of obfuscation.

</details>


### [31] [LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation](https://arxiv.org/abs/2511.19132)
*Mohammad Abboush,Ahmad Hatahet,Andreas Rausch*

Main category: cs.SE

TL;DR: Manual fault injection test case creation is costly for automotive systems. This paper demonstrates LLMs, especially gpt-4o, can automate test generation with high accuracy, speeding up testing, reducing costs, and improving safety evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional fault injection (FI) testing for automotive software systems is manual, demanding extensive labor to identify where, when, and what faults to inject. As systems grow more complex, this manual process becomes increasingly costly and inefficient.

Method: The authors propose using Large Language Models (LLMs), specifically gpt-4o, to automate the generation of fault test cases (TCs) from functional safety requirements (FSRs). They focus on representativeness and coverage criteria, validate various LLMs, and execute the generated TCs in real time on a hardware-in-the-loop automotive system model.

Result: The LLM-assisted approach showed superior FSR classification and fault TC generation performance compared to other models, achieving F1-scores of 88% and 97.5%, respectively. The automated method was successfully deployed for real-time testing on a high-fidelity automotive system.

Conclusion: Automating fault injection test case generation with LLMs optimizes real-time automotive software safety testing. This reduces costs, minimizes manual effort, and enhances safety evaluation for complex safety-critical systems, with gpt-4o outperforming other state-of-the-art models.

Abstract: A well-known testing method for the safety evaluation and real-time validation of automotive software systems (ASSs) is Fault Injection (FI). In accordance with the ISO 26262 standard, the faults are introduced artificially for the purpose of analyzing the safety properties and verifying the safety mechanisms during the development phase. However, the current FI method and tools have a significant limitation in that they require manual identification of FI attributes, including fault type, location and time. The more complex the system, the more expensive, time-consuming and labour-intensive the process. To address the aforementioned challenge, a novel Large Language Models (LLMs)-assisted fault test cases (TCs) generation approach for utilization during real-time FI tests is proposed in this paper. To this end, considering the representativeness and coverage criteria, the applicability of various LLMs to create fault TCs from the functional safety requirements (FSRs) has been investigated. Through the validation results of LLMs, the superiority of the proposed approach utilizing gpt-4o in comparison to other state-of-the-art models has been demonstrated. Specifically, the proposed approach exhibits high performance in terms of FSRs classification and fault TCs generation with F1-score of 88% and 97.5%, respectively. To illustrate the proposed approach, the generated fault TCs were executed in real time on a hardware-in-the-loop system, where a high-fidelity automotive system model served as a case study. This novel approach offers a means of optimizing the real-time testing process, thereby reducing costs while simultaneously enhancing the safety properties of complex safety-critical ASSs.

</details>


### [32] [Synthesizing Test Cases for Narrowing Specification Candidates](https://arxiv.org/abs/2511.19177)
*Alcino Cunha,Nuno Macedo*

Main category: cs.SE

TL;DR: The paper introduces and evaluates algorithms for generating test cases to compare and select between alternative formal specifications, providing a practical tool to automate specification choice through systematic testing.


<details>
  <summary>Details</summary>
Motivation: Selecting the most appropriate formal specification among several candidates is challenging and currently lacks automated or systematic support.

Method: The paper proposes two solver-based algorithms for generating test suites that differentiate between alternative formal specifications. One algorithm produces minimal test suites, while the other does not guarantee minimality but is more scalable. Both are implemented in a prototype for Alloy specifications.

Result: Evaluation demonstrates that the minimal (optimal) algorithm is efficient for practical solutions, and the non-minimal (non-optimal) algorithm handles larger sets with reasonable test suite sizes.

Conclusion: Their approach systematically narrows down candidate formal specifications using user-classified test results, making the selection process more efficient and scalable.

Abstract: This paper proposes a technique to help choose the best formal specification candidate among a set of alternatives. Given a set of specifications, our technique generates a suite of test cases that, once classified by the user as desirable or not, narrows down the set of candidates to at most one specification. Two alternative solver-based algorithms are proposed, one that generates a minimal test suite, and another that does not ensure minimality. Both algorithms were implemented in a prototype that can be used generate test suites to help choose among alternative Alloy specifications. Our evaluation of this prototype against a large set of problems showed that the optimal algorithm is efficient enough for many practical problems, and that the non-optimal algorithm can scale up to dozens of candidate specifications while still generating reasonably sized test suites.

</details>


### [33] [Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering](https://arxiv.org/abs/2511.19427)
*Jayanaka L. Dantanarayana,Savini Kashmira,Thakee Nathees,Zichen Zhang,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.SE

TL;DR: Semantic Engineering lets developers annotate code with natural-language context, improving automated prompt creation for LLMs. In the Jac language, this approach matches the effectiveness of manual Prompt Engineering while reducing developer effort, thus advancing AI-Integrated programming.


<details>
  <summary>Details</summary>
Motivation: Current LLM programming approaches like Meaning Typed Programming (MTP) automate prompt creation using code semantics. However, real-world applications rely on context, developer intent, and domain knowledge not captured by static code semantics.

Method: They propose Semantic Engineering, which lets programmers enrich code with natural-language context using Semantic Context Annotations (SemTexts), integrated in the Jac language. This enables LLMs to better interpret developer intent, automating high-quality prompt generation. A benchmark suite is also introduced for evaluation.

Result: Semantic Engineering, with SemTexts, improves the fidelity of prompts generated for LLM-based systems. The results demonstrate performance on par with manual Prompt Engineering but with much less required developer effort.

Conclusion: By allowing developers to embed semantic context directly into code, Semantic Engineering bridges the gap between code semantics and developer intent, resulting in more effective AI-Integrated programming without manual prompt design.

Abstract: AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [34] [TensorRight: Automated Verification of Tensor Graph Rewrites](https://arxiv.org/abs/2511.17838)
*Jai Arora,Sirui Lu,Devansh Jain,Tianfan Xu,Farzin Houshmand,Phitchaya Mangpo Phothilimthana,Mohsen Lesani,Praveen Narayanan,Karthik Srinivasa Murthy,Rastislav Bodik,Amit Sabne,Charith Mendis*

Main category: cs.PL

TL;DR: The paper introduces TensorRight, an automated system for verifying tensor graph rewrites over tensors of any rank or size. It uses a novel DSL and algorithm to reduce unbounded verification to finitely many bounded checks, enabling it to generalize correctness guarantees far beyond prior systems. TensorRight successfully verifies 115 rewrite rules from XLA, a significant improvement over earlier approaches.


<details>
  <summary>Details</summary>
Motivation: Current tensor compilers rely on graph rewrites for optimization, but previous verification methods only cover fixed-rank tensors, leaving rewrites for arbitrary ranks unproven. There is a need for an automated verification system that guarantees the soundness of these general rewrites.

Method: The authors introduce TensorRight DSL, a core language that uses aggregated-axis definitions to model rewrite rules over tensors with unbounded axes. The system determines a computable rank bound; then, it generates finite proof obligations from these bounded cases, which are verified automatically using symbolic execution and SMT solving.

Result: In practice, TensorRight is able to verify the correctness of 115 out of 175 tensor rewrite rules from XLA's algebraic simplifier, compared to only 18 rules expressible by previous automatic methods.

Conclusion: TensorRight successfully verifies tensor graph rewrite rules for arbitrary tensor ranks and sizes, addressing a longstanding gap in the automated verification of tensor compiler optimizations.

Abstract: Tensor compilers, essential for generating efficient code for deep learning models across various applications, employ tensor graph rewrites as one of the key optimizations. These rewrites optimize tensor computational graphs with the expectation of preserving semantics for tensors of arbitrary rank and size. Despite this expectation, to the best of our knowledge, there does not exist a fully automated verification system to prove the soundness of these rewrites for tensors of arbitrary rank and size. Previous works, while successful in verifying rewrites with tensors of concrete rank, do not provide guarantees in the unbounded setting.
  To fill this gap, we introduce TensorRight, the first automatic verification system that can verify tensor graph rewrites for input tensors of arbitrary rank and size. We introduce a core language, TensorRight DSL, to represent rewrite rules using a novel axis definition, called aggregated-axis, which allows us to reason about an unbounded number of axes. We achieve unbounded verification by proving that there exists a bound on tensor ranks, under which bounded verification of all instances implies the correctness of the rewrite rule in the unbounded setting. We derive an algorithm to compute this rank using the denotational semantics of TensorRight DSL. TensorRight employs this algorithm to generate a finite number of bounded-verification proof obligations, which are then dispatched to an SMT solver using symbolic execution to automatically verify the correctness of the rewrite rules. We evaluate TensorRight's verification capabilities by implementing rewrite rules present in XLA's algebraic simplifier. The results demonstrate that TensorRight can prove the correctness of 115 out of 175 rules in their full generality, while the closest automatic, bounded-verification system can express only 18 of these rules.

</details>
