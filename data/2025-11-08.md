<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: Using better English in prompts leads LLMs to generate more correct code. Developers should use advanced natural language to get more reliable AI-generated programming solutions.


<details>
  <summary>Details</summary>
Motivation: While much of the existing research focuses on prompt structure when interacting with Large Language Models (LLMs), the impact of the natural language proficiency in the prompts on the generated code’s quality was not explored.

Method: The authors used the HumanEval dataset and systematically varied the English language proficiency of prompts from basic to advanced across 164 programming tasks. They then measured the proficiency and correctness of the code generated by different LLMs in response to these prompts.

Result: LLMs generally responded best to prompts written at an intermediate (B2) English level, but higher-proficiency (more advanced) prompts consistently resulted in more correct code outputs across all tested models. The effect on code proficiency was dependent on the specific model used.

Conclusion: Natural language proficiency in prompts is a significant factor in controlling and improving the quality and correctness of code generated by LLMs. Developers can use higher-proficiency language in their prompts to obtain more reliable code generation from AI tools.

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [2] [Tutorial Debriefing: Applied Statistical Causal Inference in Requirements Engineering](https://arxiv.org/abs/2511.03875)
*Julian Frattini,Hans-Martin Heyn,Robert Feldt,Richard Torkar*

Main category: cs.SE

TL;DR: Software engineering research aims to improve practice, but proving that new tools or guidelines actually help often requires evidence of cause and effect. While randomized experiments are best, they're not always possible, so strong statistical methods for analyzing observational data are essential.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure that software engineering research genuinely benefits software producers and consumers by transferring scientific findings into practical tools, processes, or guidelines that improve outcomes.

Method: The paper discusses two primary approaches to establishing causal claims in software engineering research: (1) randomized controlled trials (RCTs), where participants are randomly assigned to intervention or control groups, and (2) statistical causal inference (SCI) from observational data when RCTs aren't feasible.

Result: The abstract suggests that while RCTs are ideal for demonstrating causality, they are often not practical. Therefore, it emphasizes the necessity of developing and relying on robust statistical methods for causal inference from observational data instead.

Conclusion: The paper concludes that when RCTs are impractical, the software engineering community must adopt reliable statistical methods for causal inference to establish causal relationships between interventions and outcomes based on observational data.

Abstract: As any scientific discipline, the software engineering (SE) research
community strives to contribute to the betterment of the target population of
our research: software producers and consumers. We will only achieve this
betterment if we manage to transfer the knowledge acquired during research into
practice. This transferal of knowledge may come in the form of tools,
processes, and guidelines for software developers. However, the value of these
contributions hinges on the assumption that applying them causes an improvement
of the development process, user experience, or other performance metrics. Such
a promise requires evidence of causal relationships between an exposure or
intervention (i.e., the contributed tool, process or guideline) and an outcome
(i.e., performance metrics). A straight-forward approach to obtaining this
evidence is via controlled experiments in which a sample of a population is
randomly divided into a group exposed to the new tool, process, or guideline,
and a control group. However, such randomized control trials may not be
legally, ethically, or logistically feasible. In these cases, we need a
reliable process for statistical causal inference (SCI) from observational
data.

</details>


### [3] [Collaborative Agents for Automated Program Repair in Ruby](https://arxiv.org/abs/2511.03925)
*Nikta Akbarpour,Mahdieh Sadat Benis,Fatemeh Hendijani Fard,Ali Ouni,Mohamed Aymen Saied*

Main category: cs.SE

TL;DR: RAMP is a novel, lightweight iterative program repair framework for Ruby using collaborative agents, test generation, and self-reflection. It is efficient, database-free, and outperforms previous methods on standard benchmarks, offering new directions in LLM-based debugging for less-studied languages.


<details>
  <summary>Details</summary>
Motivation: Most APR methods are focused on a few languages and are computationally expensive; Ruby, although popular, is understudied in APR.

Method: RAMP, a lightweight, collaborative agent framework, formulates program repair as an iterative, feedback-driven process using targeted test generation and self-reflection. It operates via prompting and test feedback directly in Ruby, without requiring large multilingual databases or costly model fine-tuning.

Result: RAMP achieves 67% pass@1 on the Ruby XCodeEval benchmark, converges in fewer than five iterations, and ablation studies show test generation and self-reflection are critical.

Conclusion: RAMP significantly outperforms previous approaches in Ruby program repair, especially for wrong answers, compilation, and runtime errors, and provides insight into multi-agent strategies for LLM-driven APR in under-studied languages.

Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language
Models (LLMs), but most existing methods remain computationally expensive, and
focused on a small set of languages. Ruby, despite its widespread use in web
development and the persistent challenges faced by its developers, has received
little attention in APR research. In this paper, we introduce RAMP, a novel
lightweight framework that formulates program repair as a feedback-driven,
iterative process for Ruby. RAMP employs a team of collaborative agents that
generate targeted tests, reflect on errors, and refine candidate fixes until a
correct solution is found. Unlike prior approaches, RAMP is designed to avoid
reliance on large multilingual repair databases or costly fine-tuning, instead
operating directly on Ruby through lightweight prompting and test-driven
feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a
pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly
within five iterations, and ablation studies confirm that test generation and
self-reflection are key drivers of its performance. Further analysis shows that
RAMP is particularly effective at repairing wrong answers, compilation errors,
and runtime errors. Our approach provides new insights into multi-agent repair
strategies, and establishes a foundation for extending LLM-based debugging
tools to under-studied languages.

</details>


### [4] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: This paper presents a multi-agent workflow that uses specialized LLMs and simulation tools, with a unique self-correcting feedback system, to automatically generate RTL code from natural language. It achieves higher accuracy and efficiency than previous approaches, validated on open datasets, and narrows the gap between different LLMs.


<details>
  <summary>Details</summary>
Motivation: Automatic RTL generation from natural language is a complex task typically requiring human expertise and multiple iterations. Current methods face limitations in error handling, adaptability, and performance, especially when using specialized LLMs and hardware tools.

Method: The paper introduces an agentic workflow with multiple agents that combine specialized large language models (LLMs) and hardware simulation tools. The agents use a progressive error feedback system (PEFA), which iteratively corrects errors and increases solution complexity, enabling adaptive RTL code generation. The workflow validates generated RTL for compilation, functional correctness, and synthesizability.

Result: The proposed agentic flow demonstrates improved performance on two open-source natural language-to-RTL datasets, achieving state-of-the-art pass rates. It also shows efficiency in token usage and effectively narrows the performance gap between open- and closed-source LLMs.

Conclusion: The agentic multi-agent framework with progressive error feedback enables efficient and accurate automatic RTL generation, setting new benchmarks in the field and reducing reliance on closed-source LLMs.

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [5] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: PSD2Code presents a new pipeline that systematically parses design files and aligns assets for automated, high-quality frontend code generation using large language models, outperforming previous solutions in consistency, fidelity, and readiness for real-world deployment.


<details>
  <summary>Details</summary>
Motivation: There is a gap between design prototypes (e.g., Photoshop documents) and deployable frontend code. Existing methods for design-to-code often struggle with translating visual designs into consistent, production-ready frontend code due to issues like misalignment and structural inconsistencies.

Method: The paper proposes PSD2Code, a novel multi-modal approach using PSD file parsing and asset alignment. It introduces the ParseAlignGenerate pipeline, which extracts hierarchical and semantic information from design files, then combines this with large language model capabilities through structured prompt construction and constraint-based alignment strategies to generate React+SCSS code.

Result: PSD2Code significantly outperforms existing design-to-code methods in code similarity, visual fidelity, and production readiness. It also works well across different large language models, showing strong model independence and validating the integration of structured design data for industrial-grade code automation.

Conclusion: Integrating structured design information with multimodal large language models via the ParseAlignGenerate pipeline enables production-ready frontend code generation, overcoming key limitations of prior design-to-code approaches and advancing automated frontend development.

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, existing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi-modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGenerate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relationships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates significant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [6] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: VulInstruct helps LLMs detect code vulnerabilities by explicitly teaching them security expectations derived from past patches and domain-specific problems. This approach leads to large gains over existing methods—improving accuracy, recall, and unique vulnerability discovery, including new high-severity flaws.


<details>
  <summary>Details</summary>
Motivation: LLMs are good at code understanding, but struggle with vulnerability detection, partly because they lack explicit knowledge of security specifications that define safe code behavior. Such knowledge is rarely present in training data, limiting their reasoning about potential vulnerabilities.

Method: The authors propose VulInstruct, a specification-guided method that builds a knowledge base of security specifications from historical vulnerabilities. This database covers both general safe coding practices (from patches across multiple projects) and domain-specific behaviors (from repeated issues in target repositories). VulInstruct uses these to guide LLMs in vulnerability detection, encouraging reasoning based on expected safe behaviors.

Result: On the PrimeVul benchmark, VulInstruct significantly outperforms baselines with a 45.0% F1-score (32.7% higher), 37.7% recall (50.8% higher), and uniquely detects 24.3% of vulnerabilities (2.4x more than baselines). It also found a previously unknown high-severity vulnerability in production code, highlighting its real-world value.

Conclusion: Incorporating explicit security specifications enables LLMs to better understand and detect code vulnerabilities, bridging the gap in security reasoning previously lacking in LLM-based code analysis, and providing practical improvements in real-world vulnerability detection.

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [7] [LLM-Driven Adaptive Source-Sink Identification and False Positive Mitigation for Static Analysis](https://arxiv.org/abs/2511.04023)
*Shiyin Lin*

Main category: cs.SE

TL;DR: AdaTaint uses LLMs and symbolic reasoning together to find software vulnerabilities more accurately, reducing false positives and improving recall compared to leading tools, making static analysis both adaptable and reliable.


<details>
  <summary>Details</summary>
Motivation: Static analysis for software vulnerabilities often faces two major problems: incomplete specifications of sources and sinks, and a high rate of false positives. Existing approaches, especially those relying solely on LLMs, lack grounding in program semantics and can be unreliable.

Method: The proposed method, AdaTaint, is a framework that integrates large language model (LLM) suggestions with symbolic/static program analysis. It adaptively infers source/sink specifications and filters spurious alerts by validating LLM hypotheses with program facts and constraints, blending neural and symbolic reasoning for determinism.

Result: AdaTaint was evaluated on Juliet 1.3, SV-COMP-style C benchmarks, and three large real-world projects. It achieved a reduction in false positives by 43.7% and improved recall by 11.2% on average compared to state-of-the-art baselines including CodeQL, Joern, and LLM-only pipelines.

Conclusion: Combining LLM inference with symbolic validation yields a more accurate and reliable static analysis framework for discovering software vulnerabilities, supporting both practical adaptability and rigorous validation.

Abstract: Static analysis is effective for discovering software vulnerabilities but
notoriously suffers from incomplete source--sink specifications and excessive
false positives (FPs). We present \textsc{AdaTaint}, an LLM-driven taint
analysis framework that adaptively infers source/sink specifications and
filters spurious alerts through neuro-symbolic reasoning. Unlike LLM-only
detectors, \textsc{AdaTaint} grounds model suggestions in program facts and
constraint validation, ensuring both adaptability and determinism.
  We evaluate \textsc{AdaTaint} on Juliet 1.3, SV-COMP-style C benchmarks, and
three large real-world projects. Results show that \textsc{AdaTaint} reduces
false positives by \textbf{43.7\%} on average and improves recall by
\textbf{11.2\%} compared to state-of-the-art baselines (CodeQL, Joern, and
LLM-only pipelines), while maintaining competitive runtime overhead. These
findings demonstrate that combining LLM inference with symbolic validation
offers a practical path toward more accurate and reliable static vulnerability
analysis.

</details>


### [8] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: Existing evaluations of LLM software agents are limited by weak benchmarks and unstandardized implementations. The authors present a stronger benchmark and hybrid evaluation framework, showing that agents meet only half of realistic requirements, mainly due to issues in requirement handling and self-checking. Enhancing these abilities is crucial for progress.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for evaluating LLM-based autonomous software development agents are simplistic, and comparisons between different agent architectures are confounded by varying implementation details, making scientific evaluation challenging.

Method: The authors first develop E2EDevBench, a challenging and dynamically updated benchmark that simulates realistic software development scenarios. Then, they introduce a hybrid evaluation framework that combines test-case-based functional assessment and LLM-driven, fine-grained requirement verification. They conduct a controlled study on three agent architectures using a unified implementation to fairly assess differences due to workflow design.

Result: State-of-the-art software development agents satisfy about 50% of requirements on the new benchmark. Performance depends heavily on the agents' strategies for task decomposition and collaboration. Major bottlenecks identified are missing requirements and poor self-verification.

Conclusion: The work provides the community with a realistic benchmark and a robust evaluation methodology, offering insights on the limitations of current agents. The key areas needing improvement are requirement comprehension and planning for better agent performance.

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [9] [Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment](https://arxiv.org/abs/2511.04157)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.SE

TL;DR: This paper finds that LLMs used in software engineering generally align better with AI practitioner values than the general public, but inconsistencies between what they say and how they act suggest risks if used uncritically. Human oversight and systematic benchmarking are recommended.


<details>
  <summary>Details</summary>
Motivation: With the growing use of LLMs in software engineering tasks, there is increasing concern about whether these models make value judgments that match human expectations for responsible AI and ethical principles.

Method: The study evaluates 23 LLMs over four distinct tasks related to responsible AI values: (T1) choosing core values, (T2) rating their relevance in contexts, (T3) handling trade-offs between values, and (T4) prioritizing software requirements that reflect these values. Two human groups (US-representative sample and AI practitioners) are used for comparison.

Result: LLMs tend to agree more with AI practitioners than the broader US public, prioritizing values like fairness, privacy, transparency, safety, and accountability. However, there is noticeable inconsistency between what LLMs claim to value and how they act when prioritizing requirements, raising concerns about their faithfulness.

Conclusion: While LLMs show promise in mirroring certain responsible AI values, discrepancies between stated values and decision-making behavior highlight risks if used without human oversight. There is a clear need for rigorous evaluation and monitoring of their value alignment in software development.

Abstract: Large Language Models (LLMs) are increasingly employed in software
engineering tasks such as requirements elicitation, design, and evaluation,
raising critical questions regarding their alignment with human judgments on
responsible AI values. This study investigates how closely LLMs' value
preferences align with those of two human groups: a US-representative sample
and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key
responsible AI values, (T2) rating their importance in specific contexts, (T3)
resolving trade-offs between competing values, and (T4) prioritizing software
requirements that embody those values. The results show that LLMs generally
align more closely with AI practitioners than with the US-representative
sample, emphasizing fairness, privacy, transparency, safety, and
accountability. However, inconsistencies appear between the values that LLMs
claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4),
revealing gaps in faithfulness between stated and applied behavior. These
findings highlight the practical risk of relying on LLMs in requirements
engineering without human oversight and motivate the need for systematic
approaches to benchmark, interpret, and monitor value alignment in AI-assisted
software development.

</details>


### [10] [Explaining Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2511.04179)
*Oshando Johnson,Alexandra Fomina,Ranjith Krishnamurthy,Vaibhav Chaudhari,Rohith Kumar Shanmuganathan,Eric Bodden*

Main category: cs.SE

TL;DR: SAFE, an IDE plugin using GPT-4o, enhances SAST tool usability by providing clearer vulnerability explanations, effectively helping beginner to intermediate developers address security issues.


<details>
  <summary>Details</summary>
Motivation: SAST tools are commonly used to detect security vulnerabilities, but their generic warning messages often fail to communicate essential information clearly to developers. This leads to misunderstandings or the overlooking of critical security issues.

Method: The paper introduces SAFE, an IDE plugin that integrates GPT-4o to generate detailed explanations about the causes, impacts, and mitigation strategies of vulnerabilities flagged by SAST tools. The effectiveness of SAFE was evaluated through an expert user study.

Result: The user study showed that the explanations produced by SAFE significantly help beginner and intermediate developers better understand and address security vulnerabilities.

Conclusion: Using LLMs to enhance the explainability of SAST tool warnings through the SAFE plugin makes these tools more user-friendly and effective, particularly for less experienced developers.

Abstract: The prevalence of security vulnerabilities has prompted companies to adopt
static application security testing (SAST) tools for vulnerability detection.
Nevertheless, these tools frequently exhibit usability limitations, as their
generic warning messages do not sufficiently communicate important information
to developers, resulting in misunderstandings or oversight of critical
findings. In light of recent developments in Large Language Models (LLMs) and
their text generation capabilities, our work investigates a hybrid approach
that uses LLMs to tackle the SAST explainability challenges. In this paper, we
present SAFE, an Integrated Development Environment (IDE) plugin that leverages
GPT-4o to explain the causes, impacts, and mitigation strategies of
vulnerabilities detected by SAST tools. Our expert user study findings indicate
that the explanations generated by SAFE can significantly assist beginner to
intermediate developers in understanding and addressing security
vulnerabilities, thereby improving the overall usability of SAST tools.

</details>


### [11] [GITER: A Git-Based Declarative Exchange Model Using Kubernetes-Style Custom Resources](https://arxiv.org/abs/2511.04182)
*Christos Tranoris*

Main category: cs.SE

TL;DR: This paper proposes using Git as a communication layer instead of APIs or message brokers for distributed systems, making integrations more transparent, auditable, and loosely coupled, especially useful for cross-domain and air-gapped scenarios, but with some trade-offs in complexity and real-time capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional API and message broker-based integrations for distributed system communication can be complex, lack transparency, and are less suited for air-gapped and cross-domain scenarios. The authors aim to simplify asynchronous information exchange between entities while ensuring auditability and loose system coupling.

Method: They propose using Git as a coordination medium. Distributed entities (Publishers and Consumers) interact through a shared Git repository, using GitOps principles and Kubernetes-style resources (with 'spec' and 'status' fields). Git features like commits and access control ensure traceability and security.

Result: The method enables auditable, version-controlled, and transparent asynchronous communication, extending GitOps patterns from infrastructure management to broader integration challenges, such as inter-organizational and air-gapped collaboration. Architectural details and practical trade-offs are discussed.

Conclusion: Git can serve as a viable, lightweight alternative to traditional integration approaches for certain asynchronous scenarios, offering reproducibility and autonomy, though there are trade-offs compared to RESTful and broker-based systems.

Abstract: This paper introduces a lightweight and auditable method for asynchronous
information exchange between distributed entities using Git as the coordination
medium. The proposed approach replaces traditional APIs and message brokers
with a Git-based communication model built on the principles of Kubernetes
Operators and Custom Resources (CRs). Each participating entity, designated as
a Publisher or Consumer, interacts through a shared repository that serves as a
single source of truth, where the spec field captures the desired state and the
status field reflects the observed outcome. This pattern extends GitOps beyond
infrastructure management to support cross-domain, inter-organizational, and
air-gapped collaboration scenarios. By leveraging Git native features
(versioning, commit signing, and access control) the model ensures
transparency, traceability, and reproducibility while preserving loose coupling
and autonomy between systems. The paper discusses architectural principles,
implementation considerations, and comparisons with RESTful and broker-based
integrations, highlighting both the advantages and trade-offs of adopting Git
as a declarative communication substrate.

</details>


### [12] [A Tool for Benchmarking Large Language Models' Robustness in Assessing the Realism of Driving Scenarios](https://arxiv.org/abs/2511.04267)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali*

Main category: cs.SE

TL;DR: The paper introduces DriveRLR, a benchmarking tool that uses automatically generated scenario variants to assess how well different LLMs can judge the realism of driving scenarios, revealing strengths and weaknesses of popular models and helping improve safety testing for autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: Ensuring the safety of autonomous driving systems is challenging, especially when relying on simulation-based scenario testing, as measuring the realism of these simulated scenarios is difficult. There is a need for effective, scalable assessment methods.

Method: The paper proposes DriveRLR, a benchmark tool designed to assess the robustness of Large Language Models (LLMs) in evaluating the realism of driving scenarios. DriveRLR operates by generating mutated scenario variants and constructing corresponding prompts, which are used to test the LLMs' ability to assess scenario realism. The practical evaluation is conducted using the DeepScenario dataset and three state-of-the-art LLMs: GPT-5, Llama 4 Maverick, and Mistral Small 3.2.

Result: DriveRLR successfully highlights differences in the robustness of various LLMs regarding scenario realism assessment. The tool demonstrates effectiveness in distinguishing the capabilities of LLMs and presents potential as a guiding metric for scenario generation in ADS testing workflows.

Conclusion: DriveRLR is an effective benchmarking tool for evaluating both the robustness of LLMs in scenario realism assessment and the practical utility of LLM-driven evaluation methods in simulation-based autonomous driving system testing.

Abstract: In recent years, autonomous driving systems have made significant progress,
yet ensuring their safety remains a key challenge. To this end, scenario-based
testing offers a practical solution, and simulation-based methods have gained
traction due to the high cost and risk of real-world testing. However,
evaluating the realism of simulated scenarios remains difficult, creating
demand for effective assessment methods. Recent advances show that Large
Language Models (LLMs) possess strong reasoning and generalization
capabilities, suggesting their potential in assessing scenario realism through
scenario-related textual prompts. Motivated by this, we propose DriveRLR, a
benchmark tool to assess the robustness of LLMs in evaluating the realism of
driving scenarios. DriveRLR generates mutated scenario variants, constructs
prompts, which are then used to assess a given LLM's ability and robustness in
determining the realism of driving scenarios. We validate DriveRLR on the
DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4
Maverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals
differences in the robustness of various LLMs, demonstrating its effectiveness
and practical value in scenario realism assessment. Beyond LLM robustness
evaluation, DriveRLR can serve as a practical component in applications such as
an objective function to guide scenario generation, supporting simulation-based
ADS testing workflows.

</details>


### [13] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: Instead of just ranking LLMs by benchmark scores, this paper analyzes which code tasks LLMs consistently fail at, why they fail (including code complexity), and finds four main areas of weakness, helping direct future improvements in code-generating AI.


<details>
  <summary>Details</summary>
Motivation: Current LLM code generation benchmarks mainly score overall performance, but don't reveal specific recurring failures or weaknesses, making it hard to target future improvements.

Method: Examined four major code generation benchmarks for tasks consistently unsolved by leading LLMs. Explored if solution code complexity impacts failures, and conducted a systematic examination of 114 persistently failed tasks.

Result: Identified four common patterns of weaknesses among LLMs and highlighted frequently occurring complications in benchmark tasks that cause failure.

Conclusion: Understanding and cataloging these consistent failures and their causes is crucial to guide the development of more robust code generation models.

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [14] [Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development](https://arxiv.org/abs/2511.04427)
*Hao He,Courtney Miller,Shyam Agarwal,Christian Kästner,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: Adopting LLM agents such as Cursor briefly speeds up software development but consistently increases code complexity and warnings, leading to slower progress over time.


<details>
  <summary>Details</summary>
Motivation: There have been widespread claims that large language model (LLM) agents can greatly increase productivity in software engineering, but there is limited empirical evidence to support these claims.

Method: The authors use a state-of-the-art difference-in-differences design, comparing GitHub projects that adopted the Cursor LLM agent with a matched control group of similar projects that did not adopt Cursor. They also employ panel generalized method of moments estimation to further analyze results.

Result: The adoption of Cursor results in a significant, large, but temporary boost in development velocity. However, this comes with a significant and lasting rise in static analysis warnings and code complexity. Over time, the increase in warnings and complexity contributes to a slowdown in development velocity.

Conclusion: While LLM agent adoption like Cursor can offer an initial productivity boost for software engineering projects, it also increases code quality issues and complexity, which negatively impacts long-term velocity. These findings have important implications for practitioners, tool designers, and researchers in the field.

Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize
the field of software engineering. Among other things, LLM agents are rapidly
gaining momentum in their application to software development, with
practitioners claiming a multifold productivity increase after adoption. Yet,
empirical evidence is lacking around these claims. In this paper, we estimate
the causal effect of adopting a widely popular LLM agent assistant, namely
Cursor, on development velocity and software quality. The estimation is enabled
by a state-of-the-art difference-in-differences design comparing
Cursor-adopting GitHub projects with a matched control group of similar GitHub
projects that do not use Cursor. We find that the adoption of Cursor leads to a
significant, large, but transient increase in project-level development
velocity, along with a significant and persistent increase in static analysis
warnings and code complexity. Further panel generalized method of moments
estimation reveals that the increase in static analysis warnings and code
complexity acts as a major factor causing long-term velocity slowdown. Our
study carries implications for software engineering practitioners, LLM agent
assistant designers, and researchers.

</details>


### [15] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: EDIT-Bench is a new benchmark for LLM code editing capabilities, based on real-world data. Evaluating 40 models, most struggled, underscoring the need for realistic tests that include code context and user instructions. EDIT-Bench sets a higher standard for future AI coding assistant evaluation.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for code editing using LLMs do not adequately reflect real-world usage, often using artificial datasets. This limits the assessment of LLMs' true capabilities in assisting developers.

Method: The authors introduce EDIT-Bench, a benchmark dataset consisting of 545 code editing problems sourced from real-world developer interactions. These problems span multiple natural and programming languages, and varying use-cases, incorporating contextual elements such as cursor position and highlighted code to mimic actual editing environments. They evaluate 40 different LLMs using this benchmark.

Result: EDIT-Bench proves to be challenging, with only 5 out of 40 models scoring over 60%. Performance varies notably depending on the category of user instruction and the level of contextual information provided, with a difference of up to 11% in success rates.

Conclusion: EDIT-Bench provides a more realistic and challenging assessment of LLM code editing capabilities, highlighting gaps in current model performance and the importance of realistic evaluation environments.

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>


### [16] [Microservices Is Dying, A New Method for Module Division Based on Universal Interfaces](https://arxiv.org/abs/2511.04548)
*Qing Wang,Yong Zhang*

Main category: cs.SE

TL;DR: The paper introduces a new system design and methodology that aims to eliminate module dependencies by ensuring true module independence through universal interfaces, exemplified by the EIGHT architecture, which allows dynamic modification within monolithic applications. This approach challenges existing microservice and monolithic paradigms and offers a promising route for building scalable, adaptable systems.


<details>
  <summary>Details</summary>
Motivation: Microservice architectures, despite physically isolating modules, still suffer from unwanted dependency propagation and coupling between modules. The motivation is to address and resolve this persistent issue and seek a new system design paradigm beyond traditional microservice and monolithic architectures.

Method: The paper proposes a conceptual method for calculating module independence, derives necessary conditions for true module independence, and introduces a system design philosophy and software engineering methodology that seeks to eliminate module dependencies. It describes a design pattern for universal interfaces as boundaries between modules and applies this approach to build a platform architecture named EIGHT.

Result: The method is applied to build the EIGHT platform architecture, which demonstrates that, with guaranteed module independence, even monolithic applications can support dynamic runtime modification, loading, and unloading of modules. This shows the feasibility and advantages of the proposed methodology.

Conclusion: The paper concludes that the proposed architecture and methodology offer a new direction for designing complex systems, effectively addressing limitations of both microservices and monolithic architectures by focusing on true module independence and dynamic adaptability.

Abstract: Although microservices have physically isolated modules, they have failed to
prevent the propagation and diffusion of dependencies. To trace the root cause
of the inter-module coupling, this paper, starting from the impact assessment
approach for module changes, proposes a conceptual method for calculating
module independence and utilizes this method to derive the necessary conditions
for module independence. Then, a new system design philosophy and software
engineering methodology is proposed, aimed at eliminating dependencies between
modules. A specific pattern is employed to design a set of universal
interfaces, serving as a universal boundary between modules. Subsequently, this
method is used to implement a platform architecture named EIGHT, demonstrating
that, as long as module independence is guaranteed, even a monolithic
application within a single process can dynamically load, unload, or modify any
part at runtime. Finally, the paper concludes that this architecture aims to
explore a novel path for increasingly complex systems, beyond microservice and
monolithic architectures.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [Modular abstract syntax trees (MAST): substitution tensors with second-class sorts](https://arxiv.org/abs/2511.03946)
*Marcelo P. Fiore,Ohad Kammar,Georg Moser,Sam Staton*

Main category: cs.PL

TL;DR: The paper adapts abstract syntax frameworks to handle languages with restricted sorts, using advanced categorical structures, and proves useful substitution properties for CBV.


<details>
  <summary>Details</summary>
Motivation: The motivation is to generalize the treatment of abstract syntax, especially for languages where certain sorts are restricted (second-class sorts), which is a situation common in calculi like CBV and CBPV.

Method: The paper adapts and generalizes existing frameworks (Fiore, Plotkin, Turi) using actegories instead of monoidal categories and employs bicategorical arguments for the development.

Result: The developed theory is applied to prove substitution lemmas for several varieties of Call-by-Value lambda calculus.

Conclusion: The paper succeeds in accounting for second-class sorts in binding languages, shifting the mathematical foundation to actegories, and demonstrates the utility of the theory by proving relevant lemmas for CBV.

Abstract: We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with
binding, substitution, and holes to account for languages with second-class
sorts. These situations include programming calculi such as the Call-by-Value
lambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting
second-class sorts from appearing in variable contexts changes the
characterisation of the abstract syntax from monoids in monoidal categories to
actions in actegories. We reproduce much of the development through
bicategorical arguments. We apply the resulting theory by proving substitution
lemmata for varieties of CBV.

</details>
