{"id": "2511.04824", "categories": ["cs.SE", "D.2.7"], "pdf": "https://arxiv.org/pdf/2511.04824", "abs": "https://arxiv.org/abs/2511.04824", "authors": ["Kosei Horikawa", "Hao Li", "Yutaro Kashiwa", "Bram Adams", "Hajimu Iida", "Ahmed E. Hassan"], "title": "Agentic Refactoring: An Empirical Study of AI Coding Agents", "comment": "23 pages, 7 Tables, 5 Figuress, Submitted to ACM Transactions on\n  Software Engineering and Methodology(TOSEM)", "summary": "Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are\ntransforming the software engineering landscape. These AI-powered systems\nfunction as autonomous teammates capable of planning and executing complex\ndevelopment tasks. Agents have become active participants in refactoring, a\ncornerstone of sustainable software development aimed at improving internal\ncode quality without altering observable behavior. Despite their increasing\nadoption, there is a critical lack of empirical understanding regarding how\nagentic refactoring is utilized in practice, how it compares to human-driven\nrefactoring, and what impact it has on code quality. To address this empirical\ngap, we present a large-scale study of AI agent-generated refactorings in\nreal-world open-source Java projects, analyzing 15,451 refactoring instances\nacross 12,256 pull requests and 14,988 commits derived from the AIDev dataset.\nOur empirical analysis shows that refactoring is a common and intentional\nactivity in this development paradigm, with agents explicitly targeting\nrefactoring in 26.1% of commits. Analysis of refactoring types reveals that\nagentic efforts are dominated by low-level, consistency-oriented edits, such as\nChange Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable\n(8.5%), reflecting a preference for localized improvements over the high-level\ndesign changes common in human refactoring. Additionally, the motivations\nbehind agentic refactoring focus overwhelmingly on internal quality concerns,\nwith maintainability (52.5%) and readability (28.1%). Furthermore, quantitative\nevaluation of code quality metrics shows that agentic refactoring yields small\nbut statistically significant improvements in structural metrics, particularly\nfor medium-level changes, reducing class size and complexity (e.g., Class LOC\nmedian $\\Delta$ = -15.25).", "AI": {"tldr": "AI coding agents (like Codex, Claude Code, Cursor) frequently perform refactoring in open-source Java projects, mainly focusing on small consistency changes rather than big design shifts. Their edits aim to boost maintainability and readability, making measurable but modest improvements to code structure. These agents are becoming regular contributors to software quality via frequent, targeted code enhancements.", "motivation": "The motivation is to empirically understand how agentic (AI agent-driven) refactoring is used in real-world software projects, how it compares to human-led refactoring, and its impact on code quality, since there's a lack of evidence in current research.", "method": "The paper conducts a large-scale empirical study, analyzing 15,451 refactoring actions from 12,256 pull requests and 14,988 commits within open-source Java projects, using data from the AIDev dataset.", "result": "Refactoring is a common activity in AI agent-supported development, with agents explicitly responsible for refactoring in 26.1% of commits. Most agentic refactoring involves low-level, consistency-oriented edits, such as changing variable types and renaming parameters, favoring localized rather than high-level design improvements. The top motivations are maintainability (52.5%) and readability (28.1%). Agentic refactoring leads to small but statistically significant structural improvements, especially reducing class size and complexity.", "conclusion": "AI agents act as active refactoring participants, generally focusing on consistency and internal code quality improvements. While the improvements are modest, this demonstrates a meaningful positive impact of agentic refactoring on code structure, highlighting agents\u2019 emerging role in sustainable software development."}}
{"id": "2511.04849", "categories": ["cs.SE", "cs.AI", "I.2.6; I.2.7; D.2.3"], "pdf": "https://arxiv.org/pdf/2511.04849", "abs": "https://arxiv.org/abs/2511.04849", "authors": ["Quang-Dung Nguyen", "Tri-Dung Tran", "Thanh-Hieu Chu", "Hoang-Loc Tran", "Xiangwei Cheng", "Dirk Slama"], "title": "Software Defined Vehicle Code Generation: A Few-Shot Prompting Approach", "comment": "6 pages, 3 figures", "summary": "The emergence of Software-Defined Vehicles (SDVs) marks a paradigm shift in\nthe automotive industry, where software now plays a pivotal role in defining\nvehicle functionality, enabling rapid innovation of modern vehicles. Developing\nSDV-specific applications demands advanced tools to streamline code generation\nand improve development efficiency. In recent years, general-purpose large\nlanguage models (LLMs) have demonstrated transformative potential across\ndomains. Still, restricted access to proprietary model architectures hinders\ntheir adaption to specific tasks like SDV code generation. In this study, we\npropose using prompts, a common and basic strategy to interact with LLMs and\nredirect their responses. Using only system prompts with an appropriate and\nefficient prompt structure designed using advanced prompt engineering\ntechniques, LLMs can be crafted without requiring a training session or access\nto their base design. This research investigates the extensive experiments on\ndifferent models by applying various prompting techniques, including bare\nmodels, using a benchmark specifically created to evaluate LLMs' performance in\ngenerating SDV code. The results reveal that the model with a few-shot\nprompting strategy outperforms the others in adjusting the LLM answers to match\nthe expected outcomes based on quantitative metrics.", "AI": {"tldr": "The paper shows that advanced prompting strategies, especially few-shot prompting, can effectively adapt large language models for Software-Defined Vehicle code generation without requiring retraining or access to the underlying models. This has practical implications for faster and more efficient automotive software development.", "motivation": "The automotive industry is undergoing a transformation with the rise of Software-Defined Vehicles (SDVs), making software a central component for vehicle innovation and functionality. However, developing applications for SDVs requires more effective tools for code generation, and adapting general-purpose large language models (LLMs) for this purpose is limited by their proprietary nature.", "method": "The study utilizes prompt engineering techniques, specifically system prompts with efficient structures, to interact with LLMs for SDV code generation. Multiple prompting strategies, including bare models and few-shot prompting, were tested across different models using a specially designed benchmark that evaluates LLMs' ability to generate SDV-specific code.", "result": "Results from extensive experimentation show that the few-shot prompting strategy enables LLMs to generate code for SDVs more accurately and efficiently than other strategies, according to quantitative metrics.", "conclusion": "Prompt engineering, especially few-shot prompting, offers an effective way to tailor general-purpose LLMs for SDV code generation tasks without needing access to the proprietary model architecture or retraining. This approach can significantly improve development efficiency in the automotive software domain."}}
{"id": "2511.04986", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.04986", "abs": "https://arxiv.org/abs/2511.04986", "authors": ["Mohammadreza Saeidi", "Ethan Thoma", "Raula Gaikovina Kula", "Gema Rodr\u00edguez-P\u00e9rez"], "title": "What About Our Bug? A Study on the Responsiveness of NPM Package Maintainers", "comment": null, "summary": "Background: Widespread use of third-party libraries makes ecosystems like\nNode Package Manager (npm) critical to modern software development. However,\nthis interconnected chain of dependencies also creates challenges: bugs in one\nlibrary can propagate downstream, potentially impacting many other libraries\nthat rely on it. We hypothesize that maintainers may not always decide to fix a\nbug, especially if the maintainer decides it falls out of their responsibility\nwithin the chain of dependencies. Aims: To confirm this hypothesis, we\ninvestigate the responsiveness of 30,340 bug reports across 500 of the most\ndepended-upon npm packages. Method: We adopt a mixed-method approach to mine\nrepository issue data and perform qualitative open coding to analyze reasons\nbehind unaddressed bug reports. Results: Our findings show that maintainers are\ngenerally responsive, with a median project-level responsiveness of 70% (IQR:\n55%-89%), reflecting their commitment to support downstream developers.\nConclusions: We present a taxonomy of the reasons some bugs remain unresolved.\nThe taxonomy includes contribution practices, dependency constraints, and\nlibrary-specific standards as reasons for not being responsive. Understanding\nmaintainer behavior can inform practices that promote a more robust and\nresponsive open-source ecosystem that benefits the entire community.", "AI": {"tldr": "Most popular npm package maintainers are highly responsive to bug reports, but some bugs remain unresolved due to factors like contribution norms, dependency issues, and library-specific standards. The study offers insights for creating more robust open-source practices.", "motivation": "Modern software heavily relies on third-party libraries, such as those in the npm ecosystem. This interconnectedness means bugs can spread downstream and affect many dependent projects. The paper hypothesizes that library maintainers may not always fix bugs if they feel it's not their direct responsibility, leading to unresolved issues.", "method": "A mixed-method approach is used: mining repository issue data and employing qualitative open coding to analyze why certain bug reports in popular npm packages remain unresolved.", "result": "Maintainers show high overall responsiveness, with a median project-level responsiveness of 70% (interquartile range: 55%-89%). The study also identifies reasons for unresolved bugs, such as contribution practices, dependency constraints, and standards unique to specific libraries.", "conclusion": "The paper presents a taxonomy explaining why some bugs are left unresolved, such as community contribution practices and dependency-related responsibilities. This understanding is crucial for developing strategies to make the open-source ecosystem more reliable and responsive."}}
{"id": "2511.05165", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05165", "abs": "https://arxiv.org/abs/2511.05165", "authors": ["Ahmad Hatahet", "Christoph Knieke", "Andreas Rausch"], "title": "Generating Software Architecture Description from Source Code using Reverse Engineering and Large Language Model", "comment": null, "summary": "Software Architecture Descriptions (SADs) are essential for managing the\ninherent complexity of modern software systems. They enable high-level\narchitectural reasoning, guide design decisions, and facilitate effective\ncommunication among diverse stakeholders. However, in practice, SADs are often\nmissing, outdated, or poorly aligned with the system's actual implementation.\nConsequently, developers are compelled to derive architectural insights\ndirectly from source code-a time-intensive process that increases cognitive\nload, slows new developer onboarding, and contributes to the gradual\ndegradation of clarity over the system's lifetime. To address these issues, we\npropose a semi-automated generation of SADs from source code by integrating\nreverse engineering (RE) techniques with a Large Language Model (LLM). Our\napproach recovers both static and behavioral architectural views by extracting\na comprehensive component diagram, filtering architecturally significant\nelements (core components) via prompt engineering, and generating state machine\ndiagrams to model component behavior based on underlying code logic with\nfew-shots prompting. This resulting views representation offer a scalable and\nmaintainable alternative to traditional manual architectural documentation.\nThis methodology, demonstrated using C++ examples, highlights the potent\ncapability of LLMs to: 1) abstract the component diagram, thereby reducing the\nreliance on human expert involvement, and 2) accurately represent complex\nsoftware behaviors, especially when enriched with domain-specific knowledge\nthrough few-shot prompting. These findings suggest a viable path toward\nsignificantly reducing manual effort while enhancing system understanding and\nlong-term maintainability.", "AI": {"tldr": "Manual software architecture documentation is often missing or outdated. By combining reverse engineering and large language models, this paper offers a semi-automated, scalable way to generate essential architectural views from source code, improving system understanding and long-term maintainability.", "motivation": "Modern software systems are highly complex, necessitating high-quality architectural documentation (SADs) for management, communication, and design reasoning. In reality, SADs are often incomplete or outdated, forcing developers to rely on source code for architectural understanding, which is inefficient and increases cognitive burden.", "method": "The paper proposes a semi-automated method that combines reverse engineering (RE) techniques with Large Language Models (LLMs) to generate SADs from source code. The approach extracts component diagrams and filters significant elements via prompt engineering, then models component behaviors using state machine diagrams generated with few-shots prompting. The methodology is demonstrated using C++ programs.", "result": "The approach successfully recovers static and behavioral architectural views, reducing the need for manual expert intervention. LLMs abstract component diagrams and accurately capture complex software behaviors, especially when supplemented with domain-specific knowledge, thereby streamlining documentation and system comprehension.", "conclusion": "The integration of LLMs with reverse engineering renders architectural documentation generation more scalable and maintainable. The approach reduces manual effort, improves developer onboarding, and sustains understanding and clarity as systems evolve."}}
{"id": "2511.05205", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05205", "abs": "https://arxiv.org/abs/2511.05205", "authors": ["Huimin Hu", "Michael Pradel"], "title": "CodeMapper: A Language-Agnostic Approach to Mapping Code Regions Across Commits", "comment": null, "summary": "During software evolution, developers commonly face the problem of mapping a\nspecific code region from one commit to another. For example, they may want to\ndetermine how the condition of an if-statement, a specific line in a\nconfiguration file, or the definition of a function changes. We call this the\ncode mapping problem. Existing techniques, such as git diff, address this\nproblem only insufficiently because they show all changes made to a file\ninstead of focusing on a code region of the developer's choice. Other\ntechniques focus on specific code elements and programming languages (e.g.,\nmethods in Java), limiting their applicability. This paper introduces\nCodeMapper, an approach to address the code mapping problem in a way that is\nindependent of specific program elements and programming languages. Given a\ncode region in one commit, CodeMapper finds the corresponding region in another\ncommit. The approach consists of two phases: (i) computing candidate regions by\nanalyzing diffs, detecting code movements, and searching for specific code\nfragments, and (ii) selecting the most likely target region by calculating\nsimilarities. Our evaluation applies CodeMapper to four datasets, including two\nnew hand-annotated datasets containing code region pairs in ten popular\nprogramming languages. CodeMapper correctly identifies the expected target\nregion in 71.0%--94.5% of all cases, improving over the best available\nbaselines by 1.5--58.8 absolute percent points.", "AI": {"tldr": "CodeMapper is a general-purpose tool to track how specific code regions change across software commits, working better than current tools and supporting multiple languages and code elements.", "motivation": "Developers often need to trace the evolution of specific code regions across software commits, such as tracking changes in a function definition or configuration line. Existing tools (e.g., git diff) typically show all changes in a file rather than focusing on a selected code region, which is inefficient for this need. Other approaches are limited to certain languages or code elements.", "method": "The paper introduces CodeMapper, a language-independent approach for mapping a code region from one commit to its corresponding region in another. CodeMapper works in two phases: (1) computes candidate regions using diffs, code movement detection, and code fragment searching; (2) selects the most likely corresponding region based on similarity calculations.", "result": "In evaluations across four datasets (including two new, hand-annotated multilingual datasets), CodeMapper correctly mapped code regions in 71.0%-94.5% of cases, outperforming best existing methods by 1.5 to 58.8 absolute percentage points.", "conclusion": "CodeMapper effectively addresses the code mapping problem during software evolution and can be applied across different programming languages and code elements, improving accuracy over existing tools."}}
{"id": "2511.05297", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05297", "abs": "https://arxiv.org/abs/2511.05297", "authors": ["Mohammed Hilel", "Yannis Karmim", "Jean De Bodinat", "Reda Sarehane", "Antoine Gillon"], "title": "Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation", "comment": null, "summary": "Digital Adoption Platforms (DAPs) have become essential tools for helping\nemployees navigate complex enterprise software such as CRM, ERP, or HRMS\nsystems. Companies like LemonLearning have shown how digital guidance can\nreduce training costs and accelerate onboarding. However, building and\nmaintaining these interactive guides still requires extensive manual effort.\nLeveraging Large Language Models as virtual assistants is an appealing\nalternative, yet without a structured understanding of the target software,\nLLMs often hallucinate and produce unreliable answers. Moreover, most\nproduction-grade LLMs are black-box APIs, making fine-tuning impractical due to\nthe lack of access to model weights. In this work, we introduce a Graph-based\nRetrieval-Augmented Generation framework that automatically converts enterprise\nweb applications into state-action knowledge graphs, enabling LLMs to generate\ngrounded and context-aware assistance. The framework was co-developed with the\nAI enterprise RAKAM, in collaboration with Lemon Learning. We detail the\nengineering pipeline that extracts and structures software interfaces, the\ndesign of the graph-based retrieval process, and the integration of our\napproach into production DAP workflows. Finally, we discuss scalability,\nrobustness, and deployment lessons learned from industrial use cases.", "AI": {"tldr": "The paper introduces a framework that constructs knowledge graphs from enterprise software interfaces to support reliable, context-aware LLM-based digital assistance. It automates guide creation, outperforms manual efforts, and improves scalability, as demonstrated in industrial deployments.", "motivation": "Manual creation and maintenance of interactive guides for enterprise software are labor-intensive. While LLMs could automate guidance, current approaches suffer from unreliability and hallucinations due to lack of structured understanding and impracticality of fine-tuning black-box models.", "method": "The authors propose a Graph-based Retrieval-Augmented Generation framework that automatically transforms enterprise web applications into state-action knowledge graphs. This facilitates the use of LLMs to provide reliable, context-aware assistance. The method encompasses engineering pipelines for extracting software interfaces, graph-based retrieval systems, and integration into production DAP workflows.", "result": "The framework was developed in collaboration with industry partners and successfully integrated into existing DAP workflows. The paper reports on its scalability, robustness, and deployment, drawing lessons from real-world industrial cases.", "conclusion": "By grounding LLM assistance in structured knowledge graphs derived from software interfaces, the proposed approach overcomes major reliability and scalability barriers inherent in conventional manual and LLM-based guidance methods. This leads to more trustworthy and maintainable digital guidance in enterprise settings."}}
{"id": "2511.05302", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.05302", "abs": "https://arxiv.org/abs/2511.05302", "authors": ["Qianru Meng", "Xiao Zhang", "Zhaochen Ren", "Joost Visser"], "title": "Code Review Automation using Retrieval Augmented Generation", "comment": null, "summary": "Code review is essential for maintaining software quality but is\nlabor-intensive. Automated code review generation offers a promising solution\nto this challenge. Both deep learning-based generative techniques and\nretrieval-based methods have demonstrated strong performance in this task.\nHowever, despite these advancements, there are still some limitations where\ngenerated reviews can be either off-point or overly general. To address these\nissues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages\nRetrieval-Augmented Generation (RAG) to combine retrieval-based and generative\nmethods, explicitly incorporating external domain knowledge into the code\nreview process. RARe uses a dense retriever to select the most relevant reviews\nfrom the codebase, which then enrich the input for a neural generator,\nutilizing the contextual learning capacity of large language models (LLMs), to\nproduce the final review. RARe outperforms state-of-the-art methods on two\nbenchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.\nIts effectiveness is further validated through a detailed human evaluation and\na case study using an interpretability tool, demonstrating its practical\nutility and reliability.", "AI": {"tldr": "RARe, a Retrieval-Augmented Generation model, combines past code reviews and language models to produce more relevant and detailed automated reviews, outperforming previous methods in both benchmarks and human studies.", "motivation": "Code review is critical for ensuring software quality but is a time-consuming manual process. While deep learning and retrieval-based methods help automate reviews, they can still generate irrelevant or generic comments. There is a need for more accurate and contextually relevant automated code review techniques.", "method": "The authors propose RARe (Retrieval-Augmented Reviewer), a model that uses Retrieval-Augmented Generation (RAG). RARe combines a dense retriever to select relevant past reviews and a neural generator (leveraging large language models) to produce enriched, context-aware code reviews, explicitly incorporating external domain knowledge.", "result": "RARe surpasses current state-of-the-art automated review systems on two benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96. Its superiority is confirmed via human evaluation and a case study using an interpretability tool, showing practical effectiveness and reliability.", "conclusion": "Integrating retrieval-based and generative models with external knowledge substantially improves the relevance and specificity of automated code reviews, and RARe establishes a new standard for the task."}}
{"id": "2511.05459", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05459", "abs": "https://arxiv.org/abs/2511.05459", "authors": ["Jingxuan Xu", "Ken Deng", "Weihao Li", "Songwei Yu", "Huaixi Tang", "Haoyang Huang", "Zhiyi Lai", "Zizheng Zhan", "Yanan Wu", "Chenchen Zhang", "Kepeng Lei", "Yifan Yao", "Xinping Lei", "Wenqiang Zhu", "Zongxian Feng", "Han Li", "Junqi Xiong", "Dailin Li", "Zuchen Gao", "Kun Wu", "Wen Xiang", "Ziqi Zhan", "Yuanxing Zhang", "Wuxuan Gong", "Ziyuan Gao", "Guanxiang Wang", "Yirong Xue", "Xiaojiang Zhang", "Jinghui Wang", "Huiming Wang", "Wenhao Zhuang", "Zhaoxiang Zhang", "Yuqun Zhang", "Haotian Zhang", "Bin Chen", "Jiaheng Liu"], "title": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models", "comment": null, "summary": "Evaluating large language models (LLMs) for software engineering has been\nlimited by narrow task coverage, language bias, and insufficient alignment with\nreal-world developer workflows. Existing benchmarks often focus on algorithmic\nproblems or Python-centric bug fixing, leaving critical dimensions of software\nengineering underexplored. To address these gaps, we introduce SWE-Compass1, a\ncomprehensive benchmark that unifies heterogeneous code-related evaluations\ninto a structured and production-aligned framework. SWE-Compass spans 8 task\ntypes, 8 programming scenarios, and 10 programming languages, with 2000\nhigh-quality instances curated from authentic GitHub pull requests and refined\nthrough systematic filtering and validation. We benchmark ten state-of-the-art\nLLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear\nhierarchy of difficulty across task types, languages, and scenarios. Moreover,\nby aligning evaluation with real-world developer practices, SWE-Compass\nprovides a rigorous and reproducible foundation for diagnosing and advancing\nagentic coding capabilities in large language models.", "AI": {"tldr": "This paper introduces SWE-Compass, a large and realistic benchmark for testing LLMs on diverse software engineering tasks, revealing strengths and weaknesses across scenarios and providing a better basis for future model development.", "motivation": "Current LLM benchmarks for software engineering lack broad task coverage, show language bias, and aren't aligned with real developer workflows. Existing evaluations often focus on limited areas, neglecting the diversity and complexity of real-world software engineering tasks.", "method": "The authors introduced SWE-Compass, a benchmark made up of 2000 curated instances from real GitHub pull requests across 8 task types, 8 programming scenarios, and 10 programming languages. The data was systematically filtered and validated. Ten state-of-the-art LLMs were then evaluated using two agentic frameworks: SWE-Agent and Claude Code.", "result": "The results showed a clear hierarchy of difficulty among different task types, languages, and scenarios when LLMs are evaluated. The benchmark highlighted how existing models perform across a spectrum of realistic coding tasks.", "conclusion": "SWE-Compass offers a rigorous, comprehensive, and reproducible benchmark for evaluating LLMs on software engineering tasks, better aligning with real-world developer practices and advancing the understanding of LLM coding capabilities."}}
{"id": "2511.05476", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.05476", "abs": "https://arxiv.org/abs/2511.05476", "authors": ["Md. Abdul Awal", "Mrigank Rochan", "Chanchal K. Roy"], "title": "A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?", "comment": "The paper is currently under review at a peer-reviewed journal", "summary": "Transformer-based language models of code have achieved state-of-the-art\nperformance across a wide range of software analytics tasks, but their\npractical deployment remains limited due to high computational costs, slow\ninference speeds, and significant environmental impact. To address these\nchallenges, recent research has increasingly explored knowledge distillation as\na method for compressing a large language model of code (the teacher) into a\nsmaller model (the student) while maintaining performance. However, the degree\nto which a student model deeply mimics the predictive behavior and internal\nrepresentations of its teacher remains largely unexplored, as current\naccuracy-based evaluation provides only a surface-level view of model quality\nand often fails to capture more profound discrepancies in behavioral fidelity\nbetween the teacher and student models. To address this gap, we empirically\nshow that the student model often fails to deeply mimic the teacher model,\nresulting in up to 285% greater performance drop under adversarial attacks,\nwhich is not captured by traditional accuracy-based evaluation. Therefore, we\npropose MetaCompress, a metamorphic testing framework that systematically\nevaluates behavioral fidelity by comparing the outputs of teacher and student\nmodels under a set of behavior-preserving metamorphic relations. We evaluate\nMetaCompress on two widely studied tasks, using compressed versions of popular\nlanguage models of code, obtained via three different knowledge distillation\ntechniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress\nidentifies up to 62% behavioral discrepancies in student models, underscoring\nthe need for behavioral fidelity evaluation within the knowledge distillation\npipeline and establishing MetaCompress as a practical framework for testing\ncompressed language models of code derived through knowledge distillation.", "AI": {"tldr": "Knowledge-distilled code models may mimic the teacher's accuracy but can fail in deep behavioral fidelity, suffering under adversarial attacks. MetaCompress offers a robust testing framework that exposes these hidden discrepancies, emphasizing the importance of evaluating behavioral fidelity rather than relying solely on accuracy.", "motivation": "Transformer-based language models for code tasks are very powerful but are limited in practical deployment due to their high computational costs, slow inference, and environmental concerns. Knowledge distillation is often used to compress these models, but evaluation methods typically focus on accuracy, overlooking deeper behavioral fidelity between the original (teacher) and compressed (student) models.", "method": "The authors introduce MetaCompress, a metamorphic testing framework for systematically evaluating behavioral fidelity by comparing outputs from teacher and student models using behavior-preserving metamorphic relations. They evaluate this framework on compressed versions of popular code models obtained via three knowledge distillation techniques: Compressor, AVATAR, and MORPH.", "result": "MetaCompress was able to identify up to 62% behavioral discrepancies in student models. It also showed that students could exhibit up to 285% higher performance drop under adversarial conditions compared to teachers, a fact not captured by traditional accuracy metrics.", "conclusion": "Accuracy-based evaluations are insufficient for assessing the true quality and reliability of distilled (compressed) code models. MetaCompress provides a necessary and practical solution for systematically testing and revealing behavioral fidelity issues, making it a valuable addition to the knowledge distillation pipeline."}}
