{"id": "2512.23742", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23742", "abs": "https://arxiv.org/abs/2512.23742", "authors": ["Guangxi Fan", "Tianliang Ma", "Xuguang Sun", "Xun Wang", "Kain Lu Low", "Leilai Shao"], "title": "AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization", "comment": "7 pages, 7 figures, 2 tables", "summary": "With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools."}
{"id": "2512.23743", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23743", "abs": "https://arxiv.org/abs/2512.23743", "authors": ["Yunguo Yu"], "title": "Hybrid-Code: A Privacy-Preserving, Redundant Multi-Agent Framework for Reliable Local Clinical Coding", "comment": "18 pages, 1 figure, original research paper", "summary": "Clinical coding automation using cloud-based Large Language Models (LLMs) poses privacy risks and latency bottlenecks, rendering them unsuitable for on-premise healthcare deployment. We introduce Hybrid-Code, a hybrid neuro-symbolic multi-agent framework for local clinical coding that ensures production reliability through redundancy and verification. Our system comprises two agents: a Coder that attempts language model-based semantic reasoning using BioMistral-7B but falls back to deterministic keyword matching when model output is unreliable, ensuring pipeline completion; and an Auditor that verifies codes against a 257-code knowledge base and clinical evidence. Evaluating on 1,000 MIMIC-III discharge summaries, we demonstrate no hallucinated codes among accepted outputs within the knowledge base, 24.47% verification rate, and 34.11% coverage (95% CI: 31.2%--37.0%) with 86%+ language model utilization. The Auditor filtered invalid format codes and provided evidence-based quality control (75.53% rejection rate) while ensuring no patient data leaves the hospital firewall. The hybrid architecture -- combining language model semantic understanding (when successful), deterministic fallback (when the model fails), and symbolic verification (always active) -- ensures both reliability and privacy preservation, addressing critical barriers to AI adoption in healthcare. Our key finding is that reliability through redundancy is more valuable than pure model performance in production healthcare systems, where system failures are unacceptable."}
{"id": "2512.23746", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23746", "abs": "https://arxiv.org/abs/2512.23746", "authors": ["Wei Li", "Yan Zou", "Yixin Liang", "José Moura", "Shawn Blanton"], "title": "DEFT: Differentiable Automatic Test Pattern Generation", "comment": null, "summary": "Modern IC complexity drives test pattern growth, with the majority of patterns targeting a small set of hard-to-detect (HTD) faults. This motivates new ATPG algorithms to improve test effectiveness specifically for HTD faults. This paper presents DEFT (Differentiable Automatic Test Pattern Generation), a new ATPG approach that reformulates the discrete ATPG problem as a continuous optimization task. DEFT introduces a mathematically grounded reparameterization that aligns the expected continuous objective with discrete fault-detection semantics, enabling reliable gradient-based pattern generation. To ensure scalability and stability on deep circuit graphs, DEFT integrates a custom CUDA kernel for efficient forward-backward propagation and applies gradient normalization to mitigate vanishing gradients. Compared to a leading commercial tool on two industrial benchmarks, DEFT improves HTD fault detection by 21.1% and 48.9% on average under the same pattern budget and comparable runtime. DEFT also supports practical ATPG settings such as partial assignment pattern generation, producing patterns with 19.3% fewer 0/1 bits while still detecting 35% more faults. These results indicate DEFT is a promising and effective ATPG engine, offering a valuable complement to existing heuristic."}
{"id": "2512.23747", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.23747", "abs": "https://arxiv.org/abs/2512.23747", "authors": ["Abhinav Parmar", "Abhisek Panigrahi", "Abhishek Kumar Dwivedi", "Abhishek Bhattacharya", "Adarsh Ramachandra", "Aditya Choudhary", "Aditya Garg", "Aditya Raj", "Alankrit Bhatt", "Alpesh Yadav", "Anant Vishnu", "Ananthu Pillai", "Ankush Kumar", "Aryan Patnaik", "Aswatha Narayanan S", "Avanish Raj Singh", "Bhavya Shree Gadda", "Brijesh Pankajbhai Kachhadiya", "Buggala Jahnavi", "Chidurala Nithin Krishna", "Chintan Shah", "Chunduru Akshaya", "Debarshi Banerjee", "Debrup Dey", "Deepa R.", "Deepika B G", "Faiz ur Rahman", "Gagan Gayari", "Gudhi Jagadeesh Kumar Naidu", "Gursimar Singh", "Harshal Tyagi", "Harshini K", "James Mani Vathalloor", "Jayarama Nettar", "Jayashree Gajjam", "Joe Walter Sugil George", "Kamalakara Sri Krishna Tadepalli", "Kamalkumar Rathinasamy", "Karan Chaurasia", "Karthikeyan S", "Kashish Arora", "Kaushal Desai", "Khushboo Buwade", "Kiran Manjrekar", "Malikireddy Venkata Sai Likhitha", "Manjunath A", "Mitali Mahavir Bedmutha", "Mohammed Rafee Tarafdar", "Nikhil Tiwari", "Nikitha K Gigi", "Pavan Ravikumar", "Pendyala Swarnanjali", "Piyush Anand", "Prakash Chandrasekar", "Prasanna Bhalchandra Gawade", "Prasanth Sivan", "Preeti Khurana", "Priyanshi Babbar", "Rajab Ali Mondal", "Rajesh Kumar Vissapragada", "Rajeshwari Ganesan", "Rajeswari Koppisetti", "Ramjee R.", "Ramkumar Thiruppathisamy", "Rani G. S.", "S Reka", "Samarth Gupta", "Sandeep Reddy Kothakota", "Sarathy K", "Sathyanarayana Sampath Kumar", "Saurabh Kumar", "Shashank Khasare", "Shenbaga Devi Venkatesh Kumar", "Shiva Rama Krishna Parvatham", "Shoeb Shaikh", "Shrishanmathi A", "Shubham Pathak", "Sree Samhita Koppaka", "Sreenivasa Raghavan K S", "Sreeram Venkatasubramanian", "Suprabha Desai Bojja", "Swetha R", "Syed Ahmed", "Chinmai Harshitha Thota", "Tushar Yadav", "Veeravelly Kusumitha", "V V S S Prasanth Patnaik", "Vidya Sri Sesetti", "Vijayakeerthi K", "Vikram Raj Bakshi", "Vinay K K", "Vinoth Kumar Loganathan", "Vipin Tiwari", "Vivek Kumar Shrivastav", "V Venkata Sri Datta Charan", "Wasim Akhtar Khan"], "title": "State-of-the-art Small Language Coder Model: Mify-Coder", "comment": null, "summary": "We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware."}
{"id": "2512.23738", "categories": ["cs.PL", "cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.23738", "abs": "https://arxiv.org/abs/2512.23738", "authors": ["Adharsh Kamath", "Sishen Zhang", "Calvin Xu", "Shubham Ugare", "Gagandeep Singh", "Sasa Misailovic"], "title": "Enforcing Temporal Constraints for LLM Agents", "comment": null, "summary": "LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning."}
{"id": "2512.23769", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.23769", "abs": "https://arxiv.org/abs/2512.23769", "authors": ["Ranit Debnath Akash", "Ashish Kumar", "Verya Monjezi", "Ashutosh Trivedi", "Gang", "Tan", "Saeid Tizpaz-Niari"], "title": "Uncovering Discrimination Clusters: Quantifying and Explaining Systematic Fairness Violations", "comment": "In 40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)", "summary": "Fairness in algorithmic decision-making is often framed in terms of individual fairness, which requires that similar individuals receive similar outcomes. A system violates individual fairness if there exists a pair of inputs differing only in protected attributes (such as race or gender) that lead to significantly different outcomes-for example, one favorable and the other unfavorable. While this notion highlights isolated instances of unfairness, it fails to capture broader patterns of systematic or clustered discrimination that may affect entire subgroups. We introduce and motivate the concept of discrimination clustering, a generalization of individual fairness violations. Rather than detecting single counterfactual disparities, we seek to uncover regions of the input space where small perturbations in protected features lead to k-significantly distinct clusters of outcomes. That is, for a given input, we identify a local neighborhood-differing only in protected attributes-whose members' outputs separate into many distinct clusters. These clusters reveal significant arbitrariness in treatment solely based on protected attributes that help expose patterns of algorithmic bias that elude pairwise fairness checks. We present HyFair, a hybrid technique that combines formal symbolic analysis (via SMT and MILP solvers) to certify individual fairness with randomized search to discover discriminatory clusters. This combination enables both formal guarantees-when no counterexamples exist-and the detection of severe violations that are computationally challenging for symbolic methods alone. Given a set of inputs exhibiting high k-unfairness, we introduce a novel explanation method to generate interpretable, decision-tree-style artifacts. Our experiments demonstrate that HyFair outperforms state-of-the-art fairness verification and local explanation methods."}
{"id": "2512.23740", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23740", "abs": "https://arxiv.org/abs/2512.23740", "authors": ["Ole Fenske", "Maximilian Popko", "Sebastian Bader", "Thomas Kirste"], "title": "Towards representation agnostic probabilistic programming", "comment": "Accepted at LAFI@POPL25", "summary": "Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express."}
{"id": "2512.23780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23780", "abs": "https://arxiv.org/abs/2512.23780", "authors": ["Denesa Zyberaj", "Pascal Hirmer", "Marco Aiello", "Stefan Wagner"], "title": "Test Case Specification Techniques and System Testing Tools in the Automotive Industry: A Review", "comment": "This is the author accepted manuscript (AAM) of a paper accepted for publication in The Journal of Systems and Software (Elsevier). The final published version will be available via the journal", "summary": "The automotive domain is shifting to software-centric development to meet regulation, market pressure, and feature velocity. This shift increases embedded systems' complexity and strains testing capacity. Despite relevant standards, a coherent system-testing methodology that spans heterogeneous, legacy-constrained toolchains remains elusive, and practice often depends on individual expertise rather than a systematic strategy. We derive challenges and requirements from a systematic literature review (SLR), complemented by industry experience and practice. We map them to test case specification techniques and testing tools, evaluating their suitability for automotive testing using PRISMA. Our contribution is a curated catalog that supports technique/tool selection and can inform future testing frameworks and improvements. We synthesize nine recurring challenge areas across the life cycle, such as requirements quality and traceability, variability management, and toolchain fragmentation. We then provide a prioritized criteria catalog that recommends model-based planning, interoperable and traceable toolchains, requirements uplift, pragmatic automation and virtualization, targeted AI and formal methods, actionable metrics, and lightweight organizational practices."}
{"id": "2512.23768", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.23768", "abs": "https://arxiv.org/abs/2512.23768", "authors": ["Abdulla M"], "title": "VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution", "comment": "30 pages, 5 figures. Primary category cs.PL, secondary cs.DC", "summary": "The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments."}
{"id": "2512.23782", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.23782", "abs": "https://arxiv.org/abs/2512.23782", "authors": ["Kessia Nepomuceno", "Fabio Petrillo"], "title": "A Systematic Mapping on Software Fairness: Focus, Trends and Industrial Context", "comment": null, "summary": "Context: Fairness in systems has emerged as a critical concern in software engineering, garnering increasing attention as the field has advanced in recent years. While several guidelines have been proposed to address fairness, achieving a comprehensive understanding of research solutions for ensuring fairness in software systems remains challenging. Objectives: This paper presents a systematic literature mapping to explore and categorize current advancements in fairness solutions within software engineering, focusing on three key dimensions: research trends, research focus, and viability in industrial contexts. Methods: We develop a classification framework to organize research on software fairness from a fresh perspective, applying it to 95 selected studies and analyzing their potential for industrial adoption. Results: Our findings reveal that software fairness research is expanding, yet it remains heavily focused on methods and algorithms. It primarily focuses on post-processing and group fairness, with less emphasis on early-stage interventions, individual fairness metrics, and understanding bias root causes. Additionally fairness research remains largely academic, with limited industry collaboration and low to medium Technology Readiness Level (TRL), indicating that industrial transferability remains distant. Conclusion: Our results underscore the need to incorporate fairness considerations across all stages of the software development life-cycle and to foster greater collaboration between academia and industry. This analysis provides a comprehensive overview of the field, offering a foundation to guide future research and practical applications of fairness in software systems."}
{"id": "2512.23996", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.23996", "abs": "https://arxiv.org/abs/2512.23996", "authors": ["A. R. Balasubramanian", "Mohammad Hossein Khoshechin Jorshari", "Rupak Majumdar", "Umang Mathur", "Minjian Zhang"], "title": "State Space Estimation for DPOR-based Model Checkers", "comment": "paper under review", "summary": "We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources."}
{"id": "2512.23844", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23844", "abs": "https://arxiv.org/abs/2512.23844", "authors": ["Tao Dong", "Harini Sampath", "Ja Young Lee", "Sherry Y. Shi", "Andrew Macvean"], "title": "From Correctness to Collaboration: Toward a Human-Centered Framework for Evaluating AI Agent Behavior in Software Engineering", "comment": null, "summary": "As Large Language Models (LLMs) evolve from code generators into collaborative partners for software engineers, our methods for evaluation are lagging. Current benchmarks, focused on code correctness, fail to capture the nuanced, interactive behaviors essential for successful human-AI partnership. To bridge this evaluation gap, this paper makes two core contributions. First, we present a foundational taxonomy of desirable agent behaviors for enterprise software engineering, derived from an analysis of 91 sets of user-defined agent rules. This taxonomy defines four key expectations of agent behavior: Adhere to Standards and Processes, Ensure Code Quality and Reliability, Solving Problems Effectively, and Collaborating with the User.\n  Second, recognizing that these expectations are not static, we introduce the Context-Adaptive Behavior (CAB) Framework. This emerging framework reveals how behavioral expectations shift along two empirically-derived axes: the Time Horizon (from immediate needs to future ideals), established through interviews with 15 expert engineers, and the Type of Work (from enterprise production to rapid prototyping, for example), identified through a prompt analysis of a prototyping agent. Together, these contributions offer a human-centered foundation for designing and evaluating the next generation of AI agents, moving the field's focus from the correctness of generated code toward the dynamics of true collaborative intelligence."}
{"id": "2512.23875", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23875", "abs": "https://arxiv.org/abs/2512.23875", "authors": ["Mohsen Hesamolhokama", "Behnam Rohani", "Amirahmad Shafiee", "MohammadAmin Fazli", "Jafar Habibi"], "title": "From Illusion to Insight: Change-Aware File-Level Software Defect Prediction Using Agentic AI", "comment": null, "summary": "Much of the reported progress in file-level software defect prediction (SDP) is, in reality, nothing but an illusion of accuracy. Over the last decades, machine learning and deep learning models have reported increasing performance across software versions. However, since most files persist across releases and retain their defect labels, standard evaluation rewards label-persistence bias rather than reasoning about code changes. To address this issue, we reformulate SDP as a change-aware prediction task, in which models reason over code changes of a file within successive project versions, rather than relying on static file snapshots. Building on this formulation, we propose an LLM-driven, change-aware, multi-agent debate framework. Our experiments on multiple PROMISE projects show that traditional models achieve inflated F1, while failing on rare but critical defect-transition cases. In contrast, our change-aware reasoning and multi-agent debate framework yields more balanced performance across evolution subsets and significantly improves sensitivity to defect introductions. These results highlight fundamental flaws in current SDP evaluation practices and emphasize the need for change-aware reasoning in practical defect prediction. The source code is publicly available."}
{"id": "2512.23982", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23982", "abs": "https://arxiv.org/abs/2512.23982", "authors": ["Hung-Fu Chang", "MohammadShokrolah Shirazi", "Lizhou Cao", "Supannika Koolmanojwong Mobasser"], "title": "Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education", "comment": "21 pages, 5 figures", "summary": "Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities."}
{"id": "2512.24159", "categories": ["cs.SE", "cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2512.24159", "abs": "https://arxiv.org/abs/2512.24159", "authors": ["Natalia Garanina", "Vladimir Zyubin", "Igor Anureev"], "title": "Developing controlled natural language for formal specification patterns using AI assistants", "comment": null, "summary": "Using an AI assistant, we developed a method for systematically constructing controlled natural language for requirements based on formal specification patterns containing logical attributes. The method involves three stages: 1) compiling a generalized natural language requirement pattern that utilizes all attributes of the formal specification template; 2) generating, using the AI assistant, a corpus of natural language requirement patterns, reduced by partially evaluating attributes (the developed prompt utilizes the generalized template, attribute definitions, and specific formal semantics of the requirement patterns); and 3) formalizing the syntax of the controlled natural language based on an analysis of the grammatical structure of the resulting patterns. The method has been tested for event-driven temporal requirements."}
{"id": "2512.24183", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24183", "abs": "https://arxiv.org/abs/2512.24183", "authors": ["Nan Jia", "Wangchao Sang", "Pengfei Lin", "Xiangping Chen", "Yuan Huang", "Yi Liu", "Mingliang Li"], "title": "CoHalLo: code hallucination localization via probing hidden layer vector", "comment": null, "summary": "The localization of code hallucinations aims to identify specific lines of code containing hallucinations, helping developers to improve the reliability of AI-generated code more efficiently. Although recent studies have adopted several methods to detect code hallucination, most of these approaches remain limited to coarse-grained detection and lack specialized techniques for fine-grained hallucination localization. This study introduces a novel method, called CoHalLo, which achieves line-level code hallucination localization by probing the hidden-layer vectors from hallucination detection models. CoHalLo uncovers the key syntactic information driving the model's hallucination judgments and locates the hallucinating code lines accordingly. Specifically, we first fine-tune the hallucination detection model on manually annotated datasets to ensure that it learns features pertinent to code syntactic information. Subsequently, we designed a probe network that projects high-dimensional latent vectors onto a low-dimensional syntactic subspace, generating vector tuples and reconstructing the predicted abstract syntax tree (P-AST). By comparing P-AST with the original abstract syntax tree (O-AST) extracted from the input AI-generated code, we identify the key syntactic structures associated with hallucinations. This information is then used to pinpoint hallucinated code lines. To evaluate CoHalLo's performance, we manually collected a dataset of code hallucinations. The experimental results show that CoHalLo achieves a Top-1 accuracy of 0.4253, Top-3 accuracy of 0.6149, Top-5 accuracy of 0.7356, Top-10 accuracy of 0.8333, IFA of 5.73, Recall@1% Effort of 0.052721, and Effort@20% Recall of 0.155269, which outperforms the baseline methods."}
{"id": "2512.24462", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24462", "abs": "https://arxiv.org/abs/2512.24462", "authors": ["Yoonha Cha", "Victoria Jackson", "Lauren Shu", "Stacy Branham", "André van der Hoek"], "title": "\"Game Changer\" or \"Overenthusiastic Drunk Acquaintance\"? Generative AI Use by Blind and Low Vision Software Professionals in the Workplace", "comment": "13 pages", "summary": "The software development workplace poses numerous technical and collaborative accessibility challenges for blind and low vision software professionals (BLVSPs). Though Generative AI (GenAI) is increasingly adopted within the software development industry and has been a rapidly growing topic of interest in research, to date, the unique perspectives of BLVSPs have yet to be consulted. We report on a qualitative study involving 39 semi-structured interviews with BLVSPs about what the introduction of GenAI has meant for their work. We found that BLVSPs used GenAI for many software development tasks, resulting in benefits such as increased productivity and accessibility. However, significant costs were also accompanied by GenAI use as they were more vulnerable to hallucinations than their sighted colleagues. Sometimes, organizational policies prevented use. Based on our findings, we discuss the higher-risks and higher-returns that BLVSPs had to carefully weigh when deciding whether and when to use GenAI tools for work."}
{"id": "2512.24530", "categories": ["cs.SE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.24530", "abs": "https://arxiv.org/abs/2512.24530", "authors": ["Nikolaos Mavrogeorgis", "Christos Vasiladiotis", "Pei Mu", "Amir Khordadi", "Björn Franke", "Antonio Barbalace"], "title": "A Magnified View into Heterogeneous-ISA Thread Migration Performance without State Transformation", "comment": null, "summary": "Heterogeneous-ISA processor designs have attracted considerable research interest. However, unlike their homogeneous-ISA counterparts, explicit software support for bridging ISA heterogeneity is required. The lack of a compilation toolchain ready to support heterogeneous-ISA targets has been a major factor hindering research in this exciting emerging area. For any such compiler, \"getting right\" the mechanics involved in state transformation upon migration and doing this efficiently is of critical importance. In particular, any runtime conversion of the current program stack from one architecture to another would be prohibitively expensive. In this paper, we design and develop Unifico, a new multi-ISA compiler that generates binaries that maintain the same stack layout during their execution on either architecture. Unifico avoids the need for runtime stack transformation, thus eliminating overheads associated with ISA migration. Additional responsibilities of the Unifico compiler backend include maintenance of a uniform ABI and virtual address space across ISAs. Unifico is implemented using the LLVM compiler infrastructure, and we are currently targeting the x86-64 and ARMv8 ISAs. We have evaluated Unifico across a range of compute-intensive NAS benchmarks and show its minimal impact on overall execution time, where less than 6% (10%) overhead is introduced on average for high-end (low-end) processors. We also analyze the performance impact of Unifico's key design features and demonstrate that they can be further optimized to mitigate this impact. When compared against the state-of-the-art Popcorn compiler, Unifico reduces binary size overhead from ~200% to ~10%, whilst eliminating the stack transformation overhead during ISA migration."}
{"id": "2512.24560", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24560", "abs": "https://arxiv.org/abs/2512.24560", "authors": ["David Gros", "Prem Devanbu"], "title": "Localized Calibrated Uncertainty in Code Language Models", "comment": null, "summary": "Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of \"Minimal Intent Aligning Patches\" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed."}
{"id": "2512.24570", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24570", "abs": "https://arxiv.org/abs/2512.24570", "authors": ["Shiqi Kuang", "Zhao Tian", "Tao Xiao", "Dong Wang", "Junjie Chen"], "title": "On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation."}
{"id": "2512.24594", "categories": ["cs.SE", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.24594", "abs": "https://arxiv.org/abs/2512.24594", "authors": ["Zhongyi Wang", "Tengjie Lin", "Mingshuai Chen", "Haokun Li", "Mingqi Yang", "Xiao Yi", "Shengchao Qin", "Yixing Luo", "Xiaofeng Li", "Bin Gu", "Liqiang Lu", "Jianwei Yin"], "title": "A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs", "comment": "Accepted at OOPSLA 2026", "summary": "Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort."}
{"id": "2512.24630", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24630", "abs": "https://arxiv.org/abs/2512.24630", "authors": ["Md Nahidul Islam Opu", "Shahidul Islam", "Muhammad Asaduzzaman", "Shaiful Chowdhury"], "title": "How Do Agentic AI Systems Address Performance Optimizations? A BERTopic-Based Analysis of Pull Requests", "comment": null, "summary": "LLM-based software engineering is influencing modern software development. In addition to correctness, prior studies have also examined the performance of software artifacts generated by AI agents. However, it is unclear how exactly the agentic AI systems address performance concerns in practice. In this paper, we present an empirical study of performance-related pull requests generated by AI agents. Using LLM-assisted detection and BERTopic-based topic modeling, we identified 52 performance-related topics grouped into 10 higher-level categories. Our results show that AI agents apply performance optimizations across diverse layers of the software stack and that the type of optimization significantly affects pull request acceptance rates and review times. We also found that performance optimization by AI agents primarily occurs during the development phase, with less focus on the maintenance phase. Our findings provide empirical evidence that can support the evaluation and improvement of agentic AI systems with respect to their performance optimization behaviors and review outcomes."}
{"id": "2512.24635", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.24635", "abs": "https://arxiv.org/abs/2512.24635", "authors": ["Zhili Huang", "Ling Xu", "Chao Liu", "Weifeng Sun", "Xu Zhang", "Yan Lei", "Meng Yan", "Hongyu Zhang"], "title": "DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information", "comment": "22 pages, 7 figures, preprint version", "summary": "Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.\n  To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs."}
{"id": "2512.24636", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24636", "abs": "https://arxiv.org/abs/2512.24636", "authors": ["Tanjum Motin Mitul", "Md. Masud Mazumder", "Md Nahidul Islam Opu", "Shaiful Chowdhury"], "title": "How Do Agentic AI Systems Deal With Software Energy Concerns? A Pull Request-Based Study", "comment": null, "summary": "As Software Engineering enters its new era (SE 3.0), AI coding agents increasingly automate software development workflows. However, it remains unclear how exactly these agents recognize and address software energy concerns-an issue growing in importance due to large-scale data centers, energy-hungry language models, and battery-constrained devices. In this paper, we examined the energy awareness of agent-authored pull requests (PRs) using a publicly available dataset. We identified 216 energy-explicit PRs and conducted a thematic analysis, deriving a taxonomy of energy-aware work. Our further analysis of the applied optimization techniques shows that most align with established research recommendations. Although building and running these agents is highly energy intensive, encouragingly, the results indicate that they exhibit energy awareness when generating software artifacts. However, optimization-related PRs are accepted less frequently than others, largely due to their negative impact on maintainability."}
{"id": "2512.24656", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24656", "abs": "https://arxiv.org/abs/2512.24656", "authors": ["Mir Mohammad Yousuf", "Shabir Ahmad Sofi"], "title": "Characterizing Bugs and Quality Attributes in Quantum Software: A Large-Scale Empirical Study", "comment": null, "summary": "Quantum Software Engineering (QSE) is essential for ensuring the reliability and maintainability of hybrid quantum-classical systems, yet empirical evidence on how bugs emerge and affect quality in real-world quantum projects remains limited. This study presents the first ecosystem-scale longitudinal analysis of software defects across 123 open source quantum repositories from 2012 to 2024, spanning eight functional categories, including full-stack libraries, simulators, annealing, algorithms, compilers, assembly, cryptography, and experimental computing. Using a mixed method approach combining repository mining, static code analysis, issue metadata extraction, and a validated rule-based classification framework, we analyze 32,296 verified bug reports. Results show that full-stack libraries and compilers are the most defect-prone categories due to circuit, gate, and transpilation-related issues, while simulators are mainly affected by measurement and noise modeling errors. Classical bugs primarily impact usability and interoperability, whereas quantum-specific bugs disproportionately degrade performance, maintainability, and reliability. Longitudinal analysis indicates ecosystem maturation, with defect densities peaking between 2017 and 2021 and declining thereafter. High-severity defects cluster in cryptography, experimental computing, and compiler toolchains. Repositories employing automated testing detect more defects and resolve issues faster. A negative binomial regression further shows that automated testing is associated with an approximate 60 percent reduction in expected defect incidence. Overall, this work provides the first large-scale data-driven characterization of quantum software defects and offers empirical guidance for improving testing, documentation, and maintainability practices in QSE."}
{"id": "2512.24858", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24858", "abs": "https://arxiv.org/abs/2512.24858", "authors": ["Ke Ma", "Jianjun Huang", "Wei You", "Bin Liang", "Jingzheng Wu", "Yanjun Wu", "Yuanjun Gong"], "title": "Feature Slice Matching for Precise Bug Detection", "comment": "Accepted by FSE2026", "summary": "Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs."}
{"id": "2512.24941", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.24941", "abs": "https://arxiv.org/abs/2512.24941", "authors": ["Zhiyong Zhang", "Xiaoyan Zhang", "Xiaoqi Li"], "title": "Securing High-Concurrency Ticket Sales: A Framework Based on Microservice", "comment": null, "summary": "The railway ticketing system is one of the most important public service infrastructure. In peak periods such as holidays, it is often faced with the challenge of high concurrency scenarios because of a large number of users accessing at the same time. The traditional aggregation architecture can not meet the peak user requirements because of its insufficient fault tolerance and low ability. Therefore, the system needs to use microservice architecture for development, and add multiple security methods to ensure that the system can have good stability and data consistency under high concurrency scenarios, and can respond quickly to user requests. This paper introduces the use of B/S architecture and Spring Cloud to design and develop a railway ticket purchase system that can maintain stability and reliability under high concurrency scenarios, and formulate multiple security design methods for the system. This system integrates a range of functions, such as real-time train inquiries, dynamic seat updates, online seat selection, and ticket purchasing, effectively addressing common problems associated with offline ticket purchasing, such as long queues and delayed information. It enables a complete online process from inquiry and booking to payment and refunds. Furthermore, the \"add passenger\" function allows users to purchase tickets for others, extending the convenience of online ticketing to people with limited internet access. The system design prioritizes security and stability, while also focusing on high performance, and achieves these goals through a carefully designed architecture and the integration of multiple middleware components. After the completion of the system development, the core interface of the system is tested, and then the results are analyzed. The test data proves that the system has good ability and stability under high concurrency."}
