{"id": "2511.19521", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.19521", "abs": "https://arxiv.org/abs/2511.19521", "authors": ["Tesla Zhang", "Asher Kornfeld", "Rui Li", "Sonya Simkin", "Yue Yao", "Stephanie Balzer"], "title": "Mechanizing a Proof-Relevant Logical Relation for Timed Message-Passing Protocols", "comment": "15 pages, 9 figures", "summary": "Semantic typing has become a powerful tool for program verification, applying the technique of logical relations as not only a proof method, but also a device for prescribing program behavior. In recent work, Yao et al. scaled semantic typing to the verification of timed message-passing protocols, which are prevalent in, e.g., IoT and real-time systems applications. The appeal of semantic typing in this context is precisely because of its ability to support typed and untyped program components alike -- including physical objects -- which caters to the heterogeneity of these applications. Another demand inherent to these applications is timing: constraining the time or time window within which a message exchange must happen. Yao et al. equipped their logical relation not only with temporal predicates, but also with computable trajectories, to supply the evidence that an inhabitant can step from one time point to another one. While Yao et al. provide the formalization for such a verification tool, it lacks a mechanization. Mechanizing the system would not only provide a machine proof for it, but also facilitate scalability for future extensions and applications.\n  This paper tackles the challenge of mechanizing the resulting proof-relevant logical relation in a proof assistant. allowing trajectories to be interleaved, partitioned, and concatenated, while the intended equality on trajectories is the equality of their graphs when seen as processes indexed by time. Unfortunately, proof assistants based on intensional type theory only have modest support for such equations, forcing a prolific use of transports. This paper reports on the process of mechanizing Yao et al.'s results, comprising the logical relation, the algebra of computable trajectories with supporting lemmas, and the fundamental theorem of the logical relation, in the Rocq theorem prover.", "AI": {"tldr": "The paper mechanizes a proof-relevant logical relation for verifying timed message-passing protocols, enhancing the formalism by implementing it in a proof assistant. This allows for machine-checked, scalable proofs in heterogeneous systems, tackling challenges specific to trajectory operations and type theory limitations.", "motivation": "Semantic typing is effective for program verification, especially in verifying heterogeneous, timed message-passing protocols like those used in IoT and real-time systems. However, previous work by Yao et al. provided only a formalization without mechanization, limiting its practical scalability and automation.", "method": "This paper mechanizes the proof-relevant logical relation from Yao et al. in the Rocq proof assistant. It implements the logical relation, algebra of computable trajectories, supporting lemmas, and the fundamental theorem, overcoming challenges related to handling trajectory equality in intensional type theory.", "result": "Successful mechanization of the formal system, logical relation, and supporting algebra in the Rocq theorem prover. The work handles complex operations with trajectories (interleaving, partitioning, concatenating) and manages the limitations of intensional type theory using transports.", "conclusion": "The mechanization bridges the gap between formal theory and practical proof in semantic typing for timed protocols, enabling machine-checked proofs and extending scalability for future applications. The paper demonstrates the feasibility and benefits of automated verification for these systems."}}
{"id": "2511.19764", "categories": ["cs.PL", "cs.AR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19764", "abs": "https://arxiv.org/abs/2511.19764", "authors": ["Ayaka Yorihiro", "Griffin Berlstein", "Pedro Pontes Garc\u00eda", "Kevin Laeufer", "Adrian Sampson"], "title": "Understanding Accelerator Compilers via Performance Profiling", "comment": null, "summary": "Accelerator design languages (ADLs), high-level languages that compile to hardware units, help domain experts quickly design efficient application-specific hardware. ADL compilers optimize datapaths and convert software-like control flow constructs into control paths. Such compilers are necessarily complex and often unpredictable: they must bridge the wide semantic gap between high-level semantics and cycle-level schedules, and they typically rely on advanced heuristics to optimize circuits. The resulting performance can be difficult to control, requiring guesswork to find and resolve performance problems in the generated hardware. We conjecture that ADL compilers will never be perfect: some performance unpredictability is endemic to the problem they solve.\n  In lieu of compiler perfection, we argue for compiler understanding tools that give ADL programmers insight into how the compiler's decisions affect performance. We introduce Petal, a cycle-level Petal for the Calyx intermediate language (IL). Petal instruments the Calyx code with probes and then analyzes the trace from a register-transfer-level simulation. It maps the events in the trace back to high-level control constructs in the Calyx code to track the clock cycles when each construct was active. Using case studies, we demonstrate that Petal's cycle-level profiles can identify performance problems in existing accelerator designs. We show that these insights can also guide developers toward optimizations that the compiler was unable to perform automatically, including a reduction by 46.9\\% of total cycles for one application.", "AI": {"tldr": "Petal is a tool for profiling hardware generated from high-level languages like Calyx, helping developers understand and improve the performance of their designs through cycle-level analysis and targeted optimizations.", "motivation": "ADL compilers are complex and unpredictable due to the semantic gap between high-level languages and hardware, making performance difficult to analyze and optimize. Tools are needed to help programmers understand and improve compiler decisions.", "method": "Petal instruments Calyx intermediate language code with probes, analyzes RTL simulation traces, and maps trace events back to high-level control constructs to provide cycle-level profiling.", "result": "Petal identifies performance problems in accelerator designs and facilitates optimizations, demonstrating a 46.9% reduction in total cycles for one application in case studies.", "conclusion": "Petal's cycle-level profiling provides valuable insights into the performance of hardware generated by ADL compilers, helping developers identify and optimize bottlenecks that the compiler cannot resolve automatically."}}
{"id": "2511.20369", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.20369", "abs": "https://arxiv.org/abs/2511.20369", "authors": ["Frank Sch\u00fcssele", "Matthias Zumkeller", "Miriam Lagunes-Rochin", "Dominik Klumpp"], "title": "The Ghosts of Empires: Extracting Modularity from Interleaving-Based Proofs (Extended Version)", "comment": "39 pages, 10 figures, 1 table. Extended version with proofs of the paper published at POPL'2026 (https://doi.org/10.1145/3776684)", "summary": "Implementation bugs threaten the soundness of algorithmic software verifiers. Generating correctness certificates for correct programs allows for efficient independent validation of verification results, and thus helps to reveal such bugs. Automatic generation of small, compact correctness proofs for concurrent programs is challenging, as the correctness arguments may depend on the particular interleaving, which can lead to exponential explosion. We present an approach that converts an interleaving-based correctness proof, as generated by many algorithmic verifiers, into a thread-modular correctness proof in the style of Owicki and Gries. We automatically synthesize ghost variables that capture the relevant interleaving information, and abstract away irrelevant details. Our evaluation shows that the approach is efficient in practice and generates compact proofs, compared to a baseline.", "AI": {"tldr": "This paper presents a method to efficiently and automatically generate compact correctness proofs for concurrent programs by converting interleaving-based proofs into thread-modular ones, helping to independently validate verifier results and find implementation bugs.", "motivation": "Implementation bugs in software verifiers undermine trust in their results. Independent validation via automatically generated correctness certificates for concurrent programs is desirable but challenging due to complex interleaving scenarios leading to proof explosion.", "method": "The method involves converting interleaving-based correctness proofs into thread-modular proofs using Owicki-Gries style reasoning, and automatically synthesizing ghost variables to capture relevant interleaving information while abstracting unnecessary details.", "result": "The approach demonstrated practical efficiency and generated more compact proofs than a baseline method during evaluation.", "conclusion": "The paper concludes that the proposed approach efficiently generates compact thread-modular correctness proofs for concurrent programs, enabling independent validation and exposing potential verifier bugs."}}
{"id": "2511.20617", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.20617", "abs": "https://arxiv.org/abs/2511.20617", "authors": ["Saman Dehghan", "Tianran Sun", "Tianxiang Wu", "Zihan Li", "Reyhaneh Jabbarvand"], "title": "Translating Large-Scale C Repositories to Idiomatic Rust", "comment": "21 pages, 14 figures", "summary": "Existing C to Rust translation techniques fail to balance quality and scalability: transpilation-based approaches scale to large projects but produce code with poor safety, idiomaticity, and readability. In contrast, LLM-based techniques are prohibitively expensive due to their reliance on frontier models (without which they cannot reliably generate compilable translations), thus limiting scalability. This paper proposes Rustine, a fully automated pipeline for effective and efficient repository-level C to idiomatic safe Rust translation. Evaluating on a diverse set of 23 C programs, ranging from 27 to 13,200 lines of code, Rustine can generate fully compilable Rust code for all and achieve 87% functional equivalence (passing 1,063,099 assertions out of 1,221,192 in test suites with average function and line coverage of 74.7% and 72.2%). Compared to six prior repository-level C to Rust translation techniques, the translations by Rustine are overall safer (fewer raw pointers, pointer arithmetic, and unsafe constructs), more idiomatic (fewer Rust linter violations), and more readable. When the translations cannot pass all tests to fulfill functional equivalence, human developers were able to complete the task in 4.5 hours, on average, using Rustine as debugging support.", "AI": {"tldr": "Rustine is a scalable and automated tool for translating C code to safe and idiomatic Rust, outperforming existing techniques in quality and supporting developers to efficiently complete translations when full automation falls short.", "motivation": "Current C to Rust translation methods struggle to achieve both scalability and high code quality; existing approaches are either scalable but produce low-quality code, or generate high-quality code but are not scalable due to reliance on expensive, large language models.", "method": "The paper introduces Rustine, a fully automated pipeline that translates entire C repositories to idiomatic and safe Rust, aiming to provide both scalability and code quality. Rustine is evaluated on a diverse set of 23 C programs, assessing compilation, functional equivalence, code quality metrics, and usability in aiding developers.", "result": "Rustine successfully produces fully compilable Rust code for all evaluated projects, achieving 87% functional equivalence and good code coverage. Compared to six other techniques, its output is safer, more idiomatic, and more readable. In cases where full functional equivalence isn't achieved automatically, developers can quickly complete the task with Rustine's assistance.", "conclusion": "Rustine enhances repository-level C-to-Rust translation by delivering both scalable and high-quality output. It outperforms existing methods in code safety, idiomaticity, and readability, and provides practical utility in aiding human developers to complete translations efficiently when needed. It thus represents a significant advance in the automation of programming language migration."}}
{"id": "2511.19477", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19477", "abs": "https://arxiv.org/abs/2511.19477", "authors": ["Aram Vardanyan"], "title": "Building Browser Agents: Architecture, Security, and Practical Solutions", "comment": "30 pages, 22 figures. Production architecture and benchmark evaluation of browser agents", "summary": "Browser agents enable autonomous web interaction but face critical reliability and security challenges in production. This paper presents findings from building and operating a production browser agent. The analysis examines where current approaches fail and what prevents safe autonomous operation. The fundamental insight: model capability does not limit agent performance; architectural decisions determine success or failure. Security analysis of real-world incidents reveals prompt injection attacks make general-purpose autonomous operation fundamentally unsafe. The paper argues against developing general browsing intelligence in favor of specialized tools with programmatic constraints, where safety boundaries are enforced through code instead of large language model (LLM) reasoning. Through hybrid context management combining accessibility tree snapshots with selective vision, comprehensive browser tooling matching human interaction capabilities, and intelligent prompt engineering, the agent achieved approximately 85% success rate on the WebGames benchmark across 53 diverse challenges (compared to approximately 50% reported for prior browser agents and 95.7% human baseline).", "AI": {"tldr": "Production browser agents fail due to architectural and security limitations, especially prompt injection vulnerabilities. The paper demonstrates that shifting from general-purpose LLM-driven agents to specialized, code-constrained tools boosts reliability and achieves a much higher success rate on a real-world benchmark, though some gap with human performance remains.", "motivation": "Browser agents often struggle with reliability and security issues, particularly when deployed in real-world production environments, limiting their practical utility. Existing solutions are insufficient to prevent safety failures such as prompt injection attacks.", "method": "The paper reports on the practical experience of building and operating a production browser agent, employing security incident analysis, architectural evaluation, hybrid context management leveraging accessibility tree snapshots with selective vision, and advanced prompt engineering. The agent is benchmarked on WebGames challenges.", "result": "The browser agent achieved about 85% success on the WebGames benchmark covering 53 challenges, a notable improvement from the ~50% success rate of previous agents but still below the human baseline of 95.7%.", "conclusion": "General-purpose autonomous browser agents are fundamentally unsafe due to vulnerabilities like prompt injection attacks. Success hinges on architectural decisions, not just model capability. The authors recommend specialized agents with programmatic safety constraints over general browsing intelligence enforced solely through LLM reasoning."}}
{"id": "2511.19483", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19483", "abs": "https://arxiv.org/abs/2511.19483", "authors": ["Qingsong He", "Jing Nan", "Jiayu Jiao", "Liangjie Tang", "Xiaodong Xu", "Mengmeng Sun", "Qingyao Wang", "Minghui Yan"], "title": "Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation", "comment": null, "summary": "Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\\% while achieving a 92\\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.", "AI": {"tldr": "Z-Space is a multi-agent framework for efficient tool invocation in MCP systems, using advanced intent parsing and fine-grained tool filtering, and achieves high accuracy and resource savings in large-scale, real-world deployment.", "motivation": "Efficiently and accurately matching functionalities across thousands of heterogeneous tools in enterprise-scale environments is challenging. Existing solutions suffer from high latency, context inflation, and semantic disconnections. A more scalable and precise approach is needed for practical, large-scale deployments.", "method": "The paper introduces a multi-agent collaborative architecture with three core innovations: (1) intent parsing for structured semantic understanding, (2) a fused subspace weighted tool filtering module (FSWW) for fine-grained semantic alignment, and (3) an inference execution agent for dynamic and fault-tolerant task execution. The framework is deployed and evaluated in real-world scenarios.", "result": "The system reduced average token consumption in tool inference by 96.26% and achieved a 92% tool invocation accuracy in production, serving multiple large-scale business units, thereby greatly improving intelligent test data generation.", "conclusion": "The Z-Space framework significantly improves the efficiency and reliability of intelligent test data generation by providing a scalable, accurate, and low-latency tool invocation mechanism in enterprise-scale MCP services. It reduces token consumption and boosts tool invocation accuracy."}}
{"id": "2511.19484", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19484", "abs": "https://arxiv.org/abs/2511.19484", "authors": ["Randall Balestriero", "Hugues Van Assel", "Sami BuGhanem", "Lucas Maes"], "title": "stable-pretraining-v1: Foundation Model Research Made Simple", "comment": null, "summary": "Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.", "AI": {"tldr": "Stable-pretraining is a modular, performance-focused library for foundation model and SSL research, streamlining essential tools and logging to ease experimentation, debugging, and reproducibility, thus accelerating research and enabling scalable, novel studies.", "motivation": "Existing research in foundation models and SSL is slowed by complicated codebases, redundant effort, and the difficulty of scaling experiments. A streamlined, extensible framework can help accelerate research and lower entry barriers.", "method": "They introduce a new library, stable-pretraining, built on PyTorch, Lightning, Hugging Face, and TorchMetrics. The library includes unified SSL utilities such as probes, collapse detection, augmentation pipelines, and extensible evaluation routines. Everything is logged for debugging, monitoring, and reproducibility.", "result": "Stable-pretraining was validated by enabling new research insights with minimal overhead, such as depthwise representation probing and analysis of CLIP degradation under synthetic data finetuning. It remains scalable to large experiments.", "conclusion": "Stable-pretraining facilitates foundation model and SSL research by providing a flexible, modular, and extensively logged library, reducing engineering barriers and enabling new insights with minimal overhead."}}
{"id": "2511.19489", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19489", "abs": "https://arxiv.org/abs/2511.19489", "authors": ["Zhe Zhao", "Yuheng Yang", "Haibin Wen", "Xiaojie Qiu", "Zaixi Zhang", "Qingfu Zhang"], "title": "Evolution without an Oracle: Driving Effective Evolution with LLM Judges", "comment": "14 pages, 5 figures", "summary": "The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through \"Problem Specification.\" By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing \"computable metrics\" to \"describable qualities,\" thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.", "AI": {"tldr": "The paper shows that using LLMs alongside a new framework (MADE) breaks the dependency of Evolutionary Computation on objective fitness functions, enabling optimization in subjective, complex domains and greatly outperforming existing methods.", "motivation": "Traditionally, Evolutionary Computation relies on objective, computable fitness functions (Oracles), which limits its use in domains where fitness is subjective or non-computable. The authors aim to remove this restriction in order to enable optimization in open-ended domains where ground truth is not available.", "method": "The paper introduces MADE (Multi-Agent Decomposed Evolution), a framework that leverages Large Language Models (LLMs) as subjective judges. MADE decomposes vague instructions into specific, verifiable sub-requirements, using 'Problem Specification' to convert high-variance LLM feedback into reliable selection pressure for evolutionary processes.", "result": "Across complex benchmarks such as DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (from 39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following tasks.", "conclusion": "The work demonstrates that evolutionary optimization can be effectively guided by 'describable qualities' judged by LLMs instead of traditional, machine-computable metrics. This represents a paradigm shift, enabling optimization in subjective, open-ended domains."}}
{"id": "2511.19510", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19510", "abs": "https://arxiv.org/abs/2511.19510", "authors": ["Asif Zaman", "Kallol Naha", "Khalid Belhajjame", "Hasan M. Jamil"], "title": "CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem", "comment": "9 pages, 4 figures", "summary": "Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.", "AI": {"tldr": "This paper introduces CodeR$^3$, an AI-driven system to migrate and revive decayed scientific workflows from legacy platforms to modern ones. While automation eases workflow recovery, critical steps still need human expertise, leading to a proposed collaborative platform for community-driven revival and validation.", "motivation": "Many published scientific workflows decay over time due to system/software obsolescence, especially in legacy platforms like Taverna, causing loss of valuable computational expertise. The paper aims to address the challenge of reviving and reusing these workflows efficiently.", "method": "The authors developed CodeR$^3$, an AI-powered legacy workflow migration system that analyzes decayed workflows, converts them to modern systems, provides visualization, automates service replacement, and includes human-in-the-loop validation, validated via Taverna workflow case studies.", "result": "Case studies show the system reduces manual labor in workflow parsing and service identification, but tasks like service substitution and data validation still need expert intervention. The authors propose a crowdsourcing platform for collaborative workflow revival and validation.", "conclusion": "Automation can significantly aid in reviving legacy scientific workflows, but expert human oversight is essential for tasks like service substitution and data validation. A balanced approach maximizes efficiency and maintains correctness."}}
{"id": "2511.19635", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19635", "abs": "https://arxiv.org/abs/2511.19635", "authors": ["Abhi Chivukula", "Jay Somasundaram", "Vijay Somasundaram"], "title": "Agint: Agentic Graph Compilation for Software Engineering Agents", "comment": "18 pages, 5 figures, NeurIPS 2025: Deep Learning for Code in the Agentic Era", "summary": "LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.", "AI": {"tldr": "Agint is a new toolchain that converts natural language instructions into reliable, reproducible, and scalable code workflow DAGs using a hybrid LLM-based approach. It improves reliability, concurrency, and productivity for both developers and non-developers, supporting rapid prototyping and seamless refinement of agentic workflows at scale.", "motivation": "Existing LLM-based coding agents struggle with reliability, context management, reproducibility, and scalability. There is a need for a system that enables efficient, interpretable, and robust code generation and editing that works seamlessly for both teams and individual users.", "method": "Agint leverages a hierarchical, incremental transformation process, converting natural-language instructions through explicit type floors into typed, effect-aware code DAGs using a hybrid LLM and function-based JIT runtime. This supports dynamic refinement, reproducibility, speculative execution, and parallel generation.", "result": "Agint improves code reliability with typed graph bindings, enables scalable and efficient development using smaller models with lower latency, and provides composable tools for both developers and non-tech users. The system allows for concurrent, reproducible, and optimizable code and dataflow generation, supporting a continuous team co-creation model.", "conclusion": "Agint bridges the gap between natural language instructions and reliable, scalable code generation, enabling higher developer productivity, reproducibility, and composability for both technical and non-technical users."}}
{"id": "2511.19875", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19875", "abs": "https://arxiv.org/abs/2511.19875", "authors": ["Qingyu Zhang", "Puzhuo Liu", "Peng Di", "Chenxiong Qian"], "title": "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection", "comment": null, "summary": "Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level \"purpose\" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.", "AI": {"tldr": "The paper introduces CODEFUSE-COMMITEVAL, a new LLM benchmark for detecting message-code inconsistencies in commit messages. It reveals strengths and weaknesses of current models and techniques, especially for nuanced \u2018purpose\u2019 mismatches, and provides a basis for future improvements in code review reliability.", "motivation": "Message-code inconsistency (MCI) in version control systems reduces the quality and reliability of code review, maintenance, and research datasets, yet has no dedicated benchmark for detection. The paper aims to address this gap.", "method": "The authors introduce CODEFUSE-COMMITEVAL, a specialized benchmark for MCI detection using large language models. Diverse inconsistent commits are created by rule-based mutations and validated through two-fold verification, generating labeled message-diff datasets. Six open-source LLMs are evaluated with vanilla inference and three augmentation strategies (few-shot prompting, chain-of-thought, extended context), with detailed performance metrics and type-wise analysis.", "result": "LLMs are generally better at detecting inconsistencies than consistencies, especially for component, file-path, and operation types; intent-level inconsistencies remain challenging and costly. GPT-OSS-20B achieves the best results but is less efficient. Augmentation methods show varying utility: context aids larger models, few-shot prompting improves token usage but may introduce errors, and chain-of-thought boosts precision at a cost. The new benchmark enables rigorous evaluation and reveals gaps in current model performance.", "conclusion": "CODEFUSE-COMMITEVAL sets a new standard for benchmarking MCI detection, demonstrating promising LLM performance but underscoring the need for richer context and more balanced data, particularly for semantic and intent-based inconsistencies."}}
{"id": "2511.20403", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20403", "abs": "https://arxiv.org/abs/2511.20403", "authors": ["Andrea Lops", "Fedelucio Narducci", "Azzurra Ragone", "Michelantonio Trizio", "Claudio Barto"], "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework", "comment": "Accepted at 40th IEEE/ACM International Conference on Automated Software Engineering", "summary": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.", "AI": {"tldr": "AgoneTest is an automated framework for evaluating Java unit tests generated by large language models, using real-world datasets and advanced metrics. Results show LLM-generated tests can rival human ones in coverage and defect detection, especially with improved prompting. The framework is a valuable tool for further research and development in LLM-based software testing.", "motivation": "Unit testing is crucial but costly in software development, and evaluating LLM-generated unit tests in a standardized way is lacking. The motivation is to provide a framework to facilitate objective comparison of LLMs and prompting strategies for unit test generation.", "method": "The paper presents AgoneTest, an automated evaluation framework, with the Classes2Test dataset mapping Java classes to their test classes. The framework uses metrics like mutation score and test smells for comprehensive assessment. It enables end-to-end evaluation of LLM-generated Java unit tests under realistic conditions.", "result": "Experimental results reveal that, for tests that successfully compile, LLM-generated tests can achieve comparable or superior coverage and defect detection compared to human-written tests. Enhanced prompting strategies also improve test quality.", "conclusion": "AgoneTest demonstrates the capability of LLMs in software test generation, encourages adoption of advanced prompting strategies, and offers a foundation for model, prompt, and testing practice improvements."}}
