<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Agentic Refactoring: An Empirical Study of AI Coding Agents](https://arxiv.org/abs/2511.04824)
*Kosei Horikawa,Hao Li,Yutaro Kashiwa,Bram Adams,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: AI coding agents (like Codex, Claude Code, Cursor) frequently perform refactoring in open-source Java projects, mainly focusing on small consistency changes rather than big design shifts. Their edits aim to boost maintainability and readability, making measurable but modest improvements to code structure. These agents are becoming regular contributors to software quality via frequent, targeted code enhancements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to empirically understand how agentic (AI agent-driven) refactoring is used in real-world software projects, how it compares to human-led refactoring, and its impact on code quality, since there's a lack of evidence in current research.

Method: The paper conducts a large-scale empirical study, analyzing 15,451 refactoring actions from 12,256 pull requests and 14,988 commits within open-source Java projects, using data from the AIDev dataset.

Result: Refactoring is a common activity in AI agent-supported development, with agents explicitly responsible for refactoring in 26.1% of commits. Most agentic refactoring involves low-level, consistency-oriented edits, such as changing variable types and renaming parameters, favoring localized rather than high-level design improvements. The top motivations are maintainability (52.5%) and readability (28.1%). Agentic refactoring leads to small but statistically significant structural improvements, especially reducing class size and complexity.

Conclusion: AI agents act as active refactoring participants, generally focusing on consistency and internal code quality improvements. While the improvements are modest, this demonstrates a meaningful positive impact of agentic refactoring on code structure, highlighting agents’ emerging role in sustainable software development.

Abstract: Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are
transforming the software engineering landscape. These AI-powered systems
function as autonomous teammates capable of planning and executing complex
development tasks. Agents have become active participants in refactoring, a
cornerstone of sustainable software development aimed at improving internal
code quality without altering observable behavior. Despite their increasing
adoption, there is a critical lack of empirical understanding regarding how
agentic refactoring is utilized in practice, how it compares to human-driven
refactoring, and what impact it has on code quality. To address this empirical
gap, we present a large-scale study of AI agent-generated refactorings in
real-world open-source Java projects, analyzing 15,451 refactoring instances
across 12,256 pull requests and 14,988 commits derived from the AIDev dataset.
Our empirical analysis shows that refactoring is a common and intentional
activity in this development paradigm, with agents explicitly targeting
refactoring in 26.1% of commits. Analysis of refactoring types reveals that
agentic efforts are dominated by low-level, consistency-oriented edits, such as
Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable
(8.5%), reflecting a preference for localized improvements over the high-level
design changes common in human refactoring. Additionally, the motivations
behind agentic refactoring focus overwhelmingly on internal quality concerns,
with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative
evaluation of code quality metrics shows that agentic refactoring yields small
but statistically significant improvements in structural metrics, particularly
for medium-level changes, reducing class size and complexity (e.g., Class LOC
median $\Delta$ = -15.25).

</details>


### [2] [Software Defined Vehicle Code Generation: A Few-Shot Prompting Approach](https://arxiv.org/abs/2511.04849)
*Quang-Dung Nguyen,Tri-Dung Tran,Thanh-Hieu Chu,Hoang-Loc Tran,Xiangwei Cheng,Dirk Slama*

Main category: cs.SE

TL;DR: The paper shows that advanced prompting strategies, especially few-shot prompting, can effectively adapt large language models for Software-Defined Vehicle code generation without requiring retraining or access to the underlying models. This has practical implications for faster and more efficient automotive software development.


<details>
  <summary>Details</summary>
Motivation: The automotive industry is undergoing a transformation with the rise of Software-Defined Vehicles (SDVs), making software a central component for vehicle innovation and functionality. However, developing applications for SDVs requires more effective tools for code generation, and adapting general-purpose large language models (LLMs) for this purpose is limited by their proprietary nature.

Method: The study utilizes prompt engineering techniques, specifically system prompts with efficient structures, to interact with LLMs for SDV code generation. Multiple prompting strategies, including bare models and few-shot prompting, were tested across different models using a specially designed benchmark that evaluates LLMs' ability to generate SDV-specific code.

Result: Results from extensive experimentation show that the few-shot prompting strategy enables LLMs to generate code for SDVs more accurately and efficiently than other strategies, according to quantitative metrics.

Conclusion: Prompt engineering, especially few-shot prompting, offers an effective way to tailor general-purpose LLMs for SDV code generation tasks without needing access to the proprietary model architecture or retraining. This approach can significantly improve development efficiency in the automotive software domain.

Abstract: The emergence of Software-Defined Vehicles (SDVs) marks a paradigm shift in
the automotive industry, where software now plays a pivotal role in defining
vehicle functionality, enabling rapid innovation of modern vehicles. Developing
SDV-specific applications demands advanced tools to streamline code generation
and improve development efficiency. In recent years, general-purpose large
language models (LLMs) have demonstrated transformative potential across
domains. Still, restricted access to proprietary model architectures hinders
their adaption to specific tasks like SDV code generation. In this study, we
propose using prompts, a common and basic strategy to interact with LLMs and
redirect their responses. Using only system prompts with an appropriate and
efficient prompt structure designed using advanced prompt engineering
techniques, LLMs can be crafted without requiring a training session or access
to their base design. This research investigates the extensive experiments on
different models by applying various prompting techniques, including bare
models, using a benchmark specifically created to evaluate LLMs' performance in
generating SDV code. The results reveal that the model with a few-shot
prompting strategy outperforms the others in adjusting the LLM answers to match
the expected outcomes based on quantitative metrics.

</details>


### [3] [What About Our Bug? A Study on the Responsiveness of NPM Package Maintainers](https://arxiv.org/abs/2511.04986)
*Mohammadreza Saeidi,Ethan Thoma,Raula Gaikovina Kula,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: Most popular npm package maintainers are highly responsive to bug reports, but some bugs remain unresolved due to factors like contribution norms, dependency issues, and library-specific standards. The study offers insights for creating more robust open-source practices.


<details>
  <summary>Details</summary>
Motivation: Modern software heavily relies on third-party libraries, such as those in the npm ecosystem. This interconnectedness means bugs can spread downstream and affect many dependent projects. The paper hypothesizes that library maintainers may not always fix bugs if they feel it's not their direct responsibility, leading to unresolved issues.

Method: A mixed-method approach is used: mining repository issue data and employing qualitative open coding to analyze why certain bug reports in popular npm packages remain unresolved.

Result: Maintainers show high overall responsiveness, with a median project-level responsiveness of 70% (interquartile range: 55%-89%). The study also identifies reasons for unresolved bugs, such as contribution practices, dependency constraints, and standards unique to specific libraries.

Conclusion: The paper presents a taxonomy explaining why some bugs are left unresolved, such as community contribution practices and dependency-related responsibilities. This understanding is crucial for developing strategies to make the open-source ecosystem more reliable and responsive.

Abstract: Background: Widespread use of third-party libraries makes ecosystems like
Node Package Manager (npm) critical to modern software development. However,
this interconnected chain of dependencies also creates challenges: bugs in one
library can propagate downstream, potentially impacting many other libraries
that rely on it. We hypothesize that maintainers may not always decide to fix a
bug, especially if the maintainer decides it falls out of their responsibility
within the chain of dependencies. Aims: To confirm this hypothesis, we
investigate the responsiveness of 30,340 bug reports across 500 of the most
depended-upon npm packages. Method: We adopt a mixed-method approach to mine
repository issue data and perform qualitative open coding to analyze reasons
behind unaddressed bug reports. Results: Our findings show that maintainers are
generally responsive, with a median project-level responsiveness of 70% (IQR:
55%-89%), reflecting their commitment to support downstream developers.
Conclusions: We present a taxonomy of the reasons some bugs remain unresolved.
The taxonomy includes contribution practices, dependency constraints, and
library-specific standards as reasons for not being responsive. Understanding
maintainer behavior can inform practices that promote a more robust and
responsive open-source ecosystem that benefits the entire community.

</details>


### [4] [Generating Software Architecture Description from Source Code using Reverse Engineering and Large Language Model](https://arxiv.org/abs/2511.05165)
*Ahmad Hatahet,Christoph Knieke,Andreas Rausch*

Main category: cs.SE

TL;DR: Manual software architecture documentation is often missing or outdated. By combining reverse engineering and large language models, this paper offers a semi-automated, scalable way to generate essential architectural views from source code, improving system understanding and long-term maintainability.


<details>
  <summary>Details</summary>
Motivation: Modern software systems are highly complex, necessitating high-quality architectural documentation (SADs) for management, communication, and design reasoning. In reality, SADs are often incomplete or outdated, forcing developers to rely on source code for architectural understanding, which is inefficient and increases cognitive burden.

Method: The paper proposes a semi-automated method that combines reverse engineering (RE) techniques with Large Language Models (LLMs) to generate SADs from source code. The approach extracts component diagrams and filters significant elements via prompt engineering, then models component behaviors using state machine diagrams generated with few-shots prompting. The methodology is demonstrated using C++ programs.

Result: The approach successfully recovers static and behavioral architectural views, reducing the need for manual expert intervention. LLMs abstract component diagrams and accurately capture complex software behaviors, especially when supplemented with domain-specific knowledge, thereby streamlining documentation and system comprehension.

Conclusion: The integration of LLMs with reverse engineering renders architectural documentation generation more scalable and maintainable. The approach reduces manual effort, improves developer onboarding, and sustains understanding and clarity as systems evolve.

Abstract: Software Architecture Descriptions (SADs) are essential for managing the
inherent complexity of modern software systems. They enable high-level
architectural reasoning, guide design decisions, and facilitate effective
communication among diverse stakeholders. However, in practice, SADs are often
missing, outdated, or poorly aligned with the system's actual implementation.
Consequently, developers are compelled to derive architectural insights
directly from source code-a time-intensive process that increases cognitive
load, slows new developer onboarding, and contributes to the gradual
degradation of clarity over the system's lifetime. To address these issues, we
propose a semi-automated generation of SADs from source code by integrating
reverse engineering (RE) techniques with a Large Language Model (LLM). Our
approach recovers both static and behavioral architectural views by extracting
a comprehensive component diagram, filtering architecturally significant
elements (core components) via prompt engineering, and generating state machine
diagrams to model component behavior based on underlying code logic with
few-shots prompting. This resulting views representation offer a scalable and
maintainable alternative to traditional manual architectural documentation.
This methodology, demonstrated using C++ examples, highlights the potent
capability of LLMs to: 1) abstract the component diagram, thereby reducing the
reliance on human expert involvement, and 2) accurately represent complex
software behaviors, especially when enriched with domain-specific knowledge
through few-shot prompting. These findings suggest a viable path toward
significantly reducing manual effort while enhancing system understanding and
long-term maintainability.

</details>


### [5] [CodeMapper: A Language-Agnostic Approach to Mapping Code Regions Across Commits](https://arxiv.org/abs/2511.05205)
*Huimin Hu,Michael Pradel*

Main category: cs.SE

TL;DR: CodeMapper is a general-purpose tool to track how specific code regions change across software commits, working better than current tools and supporting multiple languages and code elements.


<details>
  <summary>Details</summary>
Motivation: Developers often need to trace the evolution of specific code regions across software commits, such as tracking changes in a function definition or configuration line. Existing tools (e.g., git diff) typically show all changes in a file rather than focusing on a selected code region, which is inefficient for this need. Other approaches are limited to certain languages or code elements.

Method: The paper introduces CodeMapper, a language-independent approach for mapping a code region from one commit to its corresponding region in another. CodeMapper works in two phases: (1) computes candidate regions using diffs, code movement detection, and code fragment searching; (2) selects the most likely corresponding region based on similarity calculations.

Result: In evaluations across four datasets (including two new, hand-annotated multilingual datasets), CodeMapper correctly mapped code regions in 71.0%-94.5% of cases, outperforming best existing methods by 1.5 to 58.8 absolute percentage points.

Conclusion: CodeMapper effectively addresses the code mapping problem during software evolution and can be applied across different programming languages and code elements, improving accuracy over existing tools.

Abstract: During software evolution, developers commonly face the problem of mapping a
specific code region from one commit to another. For example, they may want to
determine how the condition of an if-statement, a specific line in a
configuration file, or the definition of a function changes. We call this the
code mapping problem. Existing techniques, such as git diff, address this
problem only insufficiently because they show all changes made to a file
instead of focusing on a code region of the developer's choice. Other
techniques focus on specific code elements and programming languages (e.g.,
methods in Java), limiting their applicability. This paper introduces
CodeMapper, an approach to address the code mapping problem in a way that is
independent of specific program elements and programming languages. Given a
code region in one commit, CodeMapper finds the corresponding region in another
commit. The approach consists of two phases: (i) computing candidate regions by
analyzing diffs, detecting code movements, and searching for specific code
fragments, and (ii) selecting the most likely target region by calculating
similarities. Our evaluation applies CodeMapper to four datasets, including two
new hand-annotated datasets containing code region pairs in ten popular
programming languages. CodeMapper correctly identifies the expected target
region in 71.0%--94.5% of all cases, improving over the best available
baselines by 1.5--58.8 absolute percent points.

</details>


### [6] [Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2511.05297)
*Mohammed Hilel,Yannis Karmim,Jean De Bodinat,Reda Sarehane,Antoine Gillon*

Main category: cs.SE

TL;DR: The paper introduces a framework that constructs knowledge graphs from enterprise software interfaces to support reliable, context-aware LLM-based digital assistance. It automates guide creation, outperforms manual efforts, and improves scalability, as demonstrated in industrial deployments.


<details>
  <summary>Details</summary>
Motivation: Manual creation and maintenance of interactive guides for enterprise software are labor-intensive. While LLMs could automate guidance, current approaches suffer from unreliability and hallucinations due to lack of structured understanding and impracticality of fine-tuning black-box models.

Method: The authors propose a Graph-based Retrieval-Augmented Generation framework that automatically transforms enterprise web applications into state-action knowledge graphs. This facilitates the use of LLMs to provide reliable, context-aware assistance. The method encompasses engineering pipelines for extracting software interfaces, graph-based retrieval systems, and integration into production DAP workflows.

Result: The framework was developed in collaboration with industry partners and successfully integrated into existing DAP workflows. The paper reports on its scalability, robustness, and deployment, drawing lessons from real-world industrial cases.

Conclusion: By grounding LLM assistance in structured knowledge graphs derived from software interfaces, the proposed approach overcomes major reliability and scalability barriers inherent in conventional manual and LLM-based guidance methods. This leads to more trustworthy and maintainable digital guidance in enterprise settings.

Abstract: Digital Adoption Platforms (DAPs) have become essential tools for helping
employees navigate complex enterprise software such as CRM, ERP, or HRMS
systems. Companies like LemonLearning have shown how digital guidance can
reduce training costs and accelerate onboarding. However, building and
maintaining these interactive guides still requires extensive manual effort.
Leveraging Large Language Models as virtual assistants is an appealing
alternative, yet without a structured understanding of the target software,
LLMs often hallucinate and produce unreliable answers. Moreover, most
production-grade LLMs are black-box APIs, making fine-tuning impractical due to
the lack of access to model weights. In this work, we introduce a Graph-based
Retrieval-Augmented Generation framework that automatically converts enterprise
web applications into state-action knowledge graphs, enabling LLMs to generate
grounded and context-aware assistance. The framework was co-developed with the
AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the
engineering pipeline that extracts and structures software interfaces, the
design of the graph-based retrieval process, and the integration of our
approach into production DAP workflows. Finally, we discuss scalability,
robustness, and deployment lessons learned from industrial use cases.

</details>


### [7] [Code Review Automation using Retrieval Augmented Generation](https://arxiv.org/abs/2511.05302)
*Qianru Meng,Xiao Zhang,Zhaochen Ren,Joost Visser*

Main category: cs.SE

TL;DR: RARe, a Retrieval-Augmented Generation model, combines past code reviews and language models to produce more relevant and detailed automated reviews, outperforming previous methods in both benchmarks and human studies.


<details>
  <summary>Details</summary>
Motivation: Code review is critical for ensuring software quality but is a time-consuming manual process. While deep learning and retrieval-based methods help automate reviews, they can still generate irrelevant or generic comments. There is a need for more accurate and contextually relevant automated code review techniques.

Method: The authors propose RARe (Retrieval-Augmented Reviewer), a model that uses Retrieval-Augmented Generation (RAG). RARe combines a dense retriever to select relevant past reviews and a neural generator (leveraging large language models) to produce enriched, context-aware code reviews, explicitly incorporating external domain knowledge.

Result: RARe surpasses current state-of-the-art automated review systems on two benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96. Its superiority is confirmed via human evaluation and a case study using an interpretability tool, showing practical effectiveness and reliability.

Conclusion: Integrating retrieval-based and generative models with external knowledge substantially improves the relevance and specificity of automated code reviews, and RARe establishes a new standard for the task.

Abstract: Code review is essential for maintaining software quality but is
labor-intensive. Automated code review generation offers a promising solution
to this challenge. Both deep learning-based generative techniques and
retrieval-based methods have demonstrated strong performance in this task.
However, despite these advancements, there are still some limitations where
generated reviews can be either off-point or overly general. To address these
issues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages
Retrieval-Augmented Generation (RAG) to combine retrieval-based and generative
methods, explicitly incorporating external domain knowledge into the code
review process. RARe uses a dense retriever to select the most relevant reviews
from the codebase, which then enrich the input for a neural generator,
utilizing the contextual learning capacity of large language models (LLMs), to
produce the final review. RARe outperforms state-of-the-art methods on two
benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.
Its effectiveness is further validated through a detailed human evaluation and
a case study using an interpretability tool, demonstrating its practical
utility and reliability.

</details>


### [8] [SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models](https://arxiv.org/abs/2511.05459)
*Jingxuan Xu,Ken Deng,Weihao Li,Songwei Yu,Huaixi Tang,Haoyang Huang,Zhiyi Lai,Zizheng Zhan,Yanan Wu,Chenchen Zhang,Kepeng Lei,Yifan Yao,Xinping Lei,Wenqiang Zhu,Zongxian Feng,Han Li,Junqi Xiong,Dailin Li,Zuchen Gao,Kun Wu,Wen Xiang,Ziqi Zhan,Yuanxing Zhang,Wuxuan Gong,Ziyuan Gao,Guanxiang Wang,Yirong Xue,Xiaojiang Zhang,Jinghui Wang,Huiming Wang,Wenhao Zhuang,Zhaoxiang Zhang,Yuqun Zhang,Haotian Zhang,Bin Chen,Jiaheng Liu*

Main category: cs.SE

TL;DR: This paper introduces SWE-Compass, a large and realistic benchmark for testing LLMs on diverse software engineering tasks, revealing strengths and weaknesses across scenarios and providing a better basis for future model development.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks for software engineering lack broad task coverage, show language bias, and aren't aligned with real developer workflows. Existing evaluations often focus on limited areas, neglecting the diversity and complexity of real-world software engineering tasks.

Method: The authors introduced SWE-Compass, a benchmark made up of 2000 curated instances from real GitHub pull requests across 8 task types, 8 programming scenarios, and 10 programming languages. The data was systematically filtered and validated. Ten state-of-the-art LLMs were then evaluated using two agentic frameworks: SWE-Agent and Claude Code.

Result: The results showed a clear hierarchy of difficulty among different task types, languages, and scenarios when LLMs are evaluated. The benchmark highlighted how existing models perform across a spectrum of realistic coding tasks.

Conclusion: SWE-Compass offers a rigorous, comprehensive, and reproducible benchmark for evaluating LLMs on software engineering tasks, better aligning with real-world developer practices and advancing the understanding of LLM coding capabilities.

Abstract: Evaluating large language models (LLMs) for software engineering has been
limited by narrow task coverage, language bias, and insufficient alignment with
real-world developer workflows. Existing benchmarks often focus on algorithmic
problems or Python-centric bug fixing, leaving critical dimensions of software
engineering underexplored. To address these gaps, we introduce SWE-Compass1, a
comprehensive benchmark that unifies heterogeneous code-related evaluations
into a structured and production-aligned framework. SWE-Compass spans 8 task
types, 8 programming scenarios, and 10 programming languages, with 2000
high-quality instances curated from authentic GitHub pull requests and refined
through systematic filtering and validation. We benchmark ten state-of-the-art
LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear
hierarchy of difficulty across task types, languages, and scenarios. Moreover,
by aligning evaluation with real-world developer practices, SWE-Compass
provides a rigorous and reproducible foundation for diagnosing and advancing
agentic coding capabilities in large language models.

</details>


### [9] [A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?](https://arxiv.org/abs/2511.05476)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: Knowledge-distilled code models may mimic the teacher's accuracy but can fail in deep behavioral fidelity, suffering under adversarial attacks. MetaCompress offers a robust testing framework that exposes these hidden discrepancies, emphasizing the importance of evaluating behavioral fidelity rather than relying solely on accuracy.


<details>
  <summary>Details</summary>
Motivation: Transformer-based language models for code tasks are very powerful but are limited in practical deployment due to their high computational costs, slow inference, and environmental concerns. Knowledge distillation is often used to compress these models, but evaluation methods typically focus on accuracy, overlooking deeper behavioral fidelity between the original (teacher) and compressed (student) models.

Method: The authors introduce MetaCompress, a metamorphic testing framework for systematically evaluating behavioral fidelity by comparing outputs from teacher and student models using behavior-preserving metamorphic relations. They evaluate this framework on compressed versions of popular code models obtained via three knowledge distillation techniques: Compressor, AVATAR, and MORPH.

Result: MetaCompress was able to identify up to 62% behavioral discrepancies in student models. It also showed that students could exhibit up to 285% higher performance drop under adversarial conditions compared to teachers, a fact not captured by traditional accuracy metrics.

Conclusion: Accuracy-based evaluations are insufficient for assessing the true quality and reliability of distilled (compressed) code models. MetaCompress provides a necessary and practical solution for systematically testing and revealing behavioral fidelity issues, making it a valuable addition to the knowledge distillation pipeline.

Abstract: Transformer-based language models of code have achieved state-of-the-art
performance across a wide range of software analytics tasks, but their
practical deployment remains limited due to high computational costs, slow
inference speeds, and significant environmental impact. To address these
challenges, recent research has increasingly explored knowledge distillation as
a method for compressing a large language model of code (the teacher) into a
smaller model (the student) while maintaining performance. However, the degree
to which a student model deeply mimics the predictive behavior and internal
representations of its teacher remains largely unexplored, as current
accuracy-based evaluation provides only a surface-level view of model quality
and often fails to capture more profound discrepancies in behavioral fidelity
between the teacher and student models. To address this gap, we empirically
show that the student model often fails to deeply mimic the teacher model,
resulting in up to 285% greater performance drop under adversarial attacks,
which is not captured by traditional accuracy-based evaluation. Therefore, we
propose MetaCompress, a metamorphic testing framework that systematically
evaluates behavioral fidelity by comparing the outputs of teacher and student
models under a set of behavior-preserving metamorphic relations. We evaluate
MetaCompress on two widely studied tasks, using compressed versions of popular
language models of code, obtained via three different knowledge distillation
techniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress
identifies up to 62% behavioral discrepancies in student models, underscoring
the need for behavioral fidelity evaluation within the knowledge distillation
pipeline and establishing MetaCompress as a practical framework for testing
compressed language models of code derived through knowledge distillation.

</details>
