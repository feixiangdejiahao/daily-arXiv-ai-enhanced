<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Triage in Software Engineering: A Systematic Review of Research and Practice](https://arxiv.org/abs/2511.08607)
*Yongxin Zhao,Shenglin Zhang,Yujia Wu,Yuxin Sun,Yongqian Sun,Dan Pei,Chetan Bansal,Minghua Ma*

Main category: cs.SE

TL;DR: This paper systematically reviews 234 works on automated triage in software systems, highlighting the evolution, challenges, and gaps between research and real-world adoption. It offers resources for evaluation and pinpoints directions for future research to enhance practical impact.


<details>
  <summary>Details</summary>
Motivation: The complexity and volume of heterogeneous data in modern software systems has made issue triage a crucial yet challenging process, necessitating automated solutions for reliable and maintainable system operations.

Method: The paper conducts a comprehensive survey and systematic review of 234 publications from 2004 onwards. It examines fundamental concepts, system architectures, and the specific problems addressed by triage automation. The authors compare academic versus industrial research goals, analyze empirical studies of industrial practice, and catalog open-source datasets and evaluation metrics.

Result: The survey identifies major obstacles to practical adoption of automated triage systems, summarizes key evaluation methodologies, and provides a unified perspective for assessing triage effectiveness. It also highlights a gap between academic research and industrial application and suggests future directions for tighter integration.

Conclusion: A thorough review of the past two decades of triage automation research is presented, mapping out current challenges and offering resources and recommendations to advance both research and practice. The survey fosters better evaluation techniques and encourages collaboration between academia and industry.

Abstract: As modern software systems continue to grow in complexity, triage has become a fundamental process in system operations and maintenance. Triage aims to efficiently prioritize, assign, and assess issues to ensure the reliability of complex environments. The vast amount of heterogeneous data generated by software systems has made effective triage indispensable for maintaining reliability, facilitating maintainability, and enabling rapid issue response. Motivated by these challenges, researchers have devoted extensive effort to advancing triage automation and have achieved significant progress over the past two decades. This survey provides a comprehensive review of 234 papers from 2004 to the present, offering an in-depth examination of the fundamental concepts, system architecture, and problem statement. By comparing the distinct goals of academic and industrial research and by analyzing empirical studies of industrial practices, we identify the major obstacles that limit the practical deployment of triage systems. To assist practitioners in method selection and performance evaluation, we summarize widely adopted open-source datasets and evaluation metrics, providing a unified perspective on the measurement of triage effectiveness. Finally, we outline potential future directions and emerging opportunities to foster a closer integration between academic innovation and industrial application. All reviewed papers and projects are available at https://github.com/AIOps-Lab-NKU/TriageSurvey.

</details>


### [2] [Energy Consumption of Dataframe Libraries for End-to-End Deep Learning Pipelines:A Comparative Analysis](https://arxiv.org/abs/2511.08644)
*Punit Kumar,Asif Imran,Tevfik Kosar*

Main category: cs.SE

TL;DR: This paper compares Pandas, Polars, and Dask in real deep learning pipelines, measuring their runtime, memory, disk usage, and energy consumption under GPU-intensive workloads, providing actionable insights for machine learning practitioners choosing data manipulation libraries.


<details>
  <summary>Details</summary>
Motivation: There's a gap in literature regarding how Python data manipulation libraries perform within complete deep learning pipelines, especially when interacting with GPU workloads during crucial phases.

Method: A detailed comparative analysis was performed on Pandas, Polars, and Dask by embedding them into deep learning pipelines and measuring KPI metrics (runtime, memory, disk, energy) across various models and datasets.

Result: Key performance indicators such as runtime, memory usage, disk usage, and energy consumption (CPU & GPU) were measured for each library in various pipeline scenarios.

Conclusion: The comparative results reveal strengths and weaknesses of Pandas, Polars, and Dask in real deep learning applications, informing choices for data pipeline construction in ML workflows.

Abstract: This paper presents a detailed comparative analysis of the performance of three major Python data manipulation libraries - Pandas, Polars, and Dask - specifically when embedded within complete deep learning (DL) training and inference pipelines. The research bridges a gap in existing literature by studying how these libraries interact with substantial GPU workloads during critical phases like data loading, preprocessing, and batch feeding. The authors measured key performance indicators including runtime, memory usage, disk usage, and energy consumption (both CPU and GPU) across various machine learning models and datasets.

</details>


### [3] [An insight into the technical debt-fix trade off in software backporting](https://arxiv.org/abs/2511.09000)
*Jarin Tasnim,Debasish Chakroborti,Chanchal K. Roy,Kevin A. Schneider*

Main category: cs.SE

TL;DR: The paper studied over 100,000 backport commits in 87 repositories and found that about 4.3% introduce new technical debt, mainly influenced by the project phase, ecosystem differences, and developer characteristics such as experience and ownership.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to understand how technical debt is introduced during the process of backporting software updates to older, stable versions. Given the ongoing nature of software maintenance, identifying when and why technical debt emerges during backporting can help improve software stability and maintenance practices.

Method: The study analyzed 105,396 commits originating from 31,076 backport sources in 87 repositories across three major ecosystems: Apache, Eclipse, and Python. It focused on identifying the circumstances and factors leading to the introduction of technical debt during backporting activities.

Result: Approximately 4.3% of backports were found to introduce new technical debt. Apache had the most absolute instances, while Python and Eclipse had higher debt-to-commit ratios. Debt in Apache was linked to feature migrations early in the release lifecycle, while Python and Eclipse accumulated debt mainly in the middle phase. The study also found that inexperienced developers, those under high workload, and non-owners were more likely to introduce technical debt.

Conclusion: Backporting fixes and features to older software versions often introduces technical debt, with debt patterns varying across ecosystems and phase of release cycles. Developer experience, workload, and code ownership are significant factors influencing technical debt introduction during backporting.

Abstract: Maintaining software is an ongoing process that stretches beyond the initial release. Stable software versions continuously evolve to fix bugs, add improvements, address security issues, and ensure compatibility. This ongoing support involves Backporting, which means taking a fix or update from a newer version and applying it to an older version of the same software. As software versions evolve, new technical debt can arise during backport maintenance activities. This study examines the technical debt involved in fixing 105,396 commits from 31,076 backport sources across 87 repositories in three software ecosystems (Apache, Eclipse, and Python). The goal is to identify when and why new technical debt arises during backporting in stable source code. Our results indicate that approximately 4.3% of backports introduce new technical debt. Apache contributes the most absolute instances, while Python and Eclipse exhibit nearly three times higher debt-to-commit ratios than Apache. Feature migrations make older Apache releases debt-prone in the early phase, whereas Python and Eclipse releases tend to accumulate technical debt mostly during the middle phase of their release cycles. Additionally, developers who are inexperienced, under high workloads, or non-owners are more likely to introduce technical debt during backporting.

</details>


### [4] [Test Plan Generation for Live Testing of Cloud Services](https://arxiv.org/abs/2511.09038)
*Oussama Jebbar,Ferhat Khendek,Maria Toeroe*

Main category: cs.SE

TL;DR: This paper introduces an automated system for generating test plans during live testing in production environments. By streamlining tasks such as configuration selection, deployment planning, and interference mitigation, the approach minimizes service disruptions and operational errors, as shown in a case study.


<details>
  <summary>Details</summary>
Motivation: Managing live testing in a production environment is challenging due to the risk of disturbing production traffic. Manually designing test plans becomes increasingly tedious and error-prone as system complexity grows.

Method: The paper proposes an automated approach to generating test plans. This system encompasses tasks like selecting/generating test configurations, planning their deployment, scheduling test runs, and choosing mitigation strategies to reduce interference with production traffic. The approach is demonstrated using a case study.

Result: The automated approach for test plan generation simplifies the planning process and is expected to reduce service disruption from testing in live environments. The case study demonstrates its application and benefits.

Conclusion: Automating the generation of test plans in live environments greatly improves efficiency, reduces errors, and minimizes disruptions to production traffic, especially for large, complex systems.

Abstract: Live testing is performed in the production environment ideally without causing unacceptable disturbance to the production traffic. Thus, test activities have to be orchestrated properly to avoid interferences with the production traffic. A test plan is the road map that specifies how the test activities need to be orchestrated. Developing a test plan includes tasks such as test configuration selection/generation, test configuration deployment planning, creating the test runs schedule, choosing strategies to mitigate the risk of interferences, etc. The manual design of a test plan is tedious and error prone. This task becomes harder especially when the systems are large and complex. In this paper we propose an approach for automating test plans generation. With this approach we aim at reducing service disruption that may be induced by the testing activities in production. We illustrate our approach with a case study and discuss its different aspects.

</details>


### [5] [Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation](https://arxiv.org/abs/2511.09122)
*Joschka Kersting,Michael Rummel,Gesa Benndorf*

Main category: cs.SE

TL;DR: This paper presents a local coding assistant for industrial Programmable Logic Controllers, using small model fine-tuning, prompt engineering, and Retrieval-Augmented Generation to achieve high code quality without large datasets or cloud dependence; validated with compilation and user feedback.


<details>
  <summary>Details</summary>
Motivation: Training coding assistants for Programmable Logic Controllers (PLCs) is difficult because PLC code uses proprietary dialects and tools, and standard LLMs lack knowledge of domain-specific function blocks and project structures. Additionally, companies like Mitsubishi Electric have data privacy concerns with cloud-based solutions; hence, a local, specialized coding assistant is needed.

Method: The authors developed a coding assistant for industrial PLC code using a low-data domain approach. They avoided fine-tuning large language models and instead fine-tuned smaller, local models suitable for edge device deployment. Their solution pits multiple AI models against each other, employs reasoning, fixes bugs automatically, and checks code validity via direct compilation in a chat interface. Extensive prompt engineering and Retrieval-Augmented Generation (RAG) were used to adapt the assistant to the specific industrial context.

Result: Their solution successfully generated high-quality PLC code without large model fine-tuning. The approach was validated using code compilation statistics and user ratings, demonstrating effective code generation in low-data domains. The RAG-based assistant benefited greatly from targeted prompt engineering and retrieval techniques.

Conclusion: A local, low-data, RAG-supported coding assistant can reliably generate industrial PLC code, even for proprietary dialects, through smart use of prompt engineering, retrieval, and model competition. Fine-tuning small models, rather than large ones, makes the approach practical, secure, and effective for edge deployment.

Abstract: Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.

</details>


### [6] [Leveraging Self-Paced Learning for Software Vulnerability Detection](https://arxiv.org/abs/2511.09212)
*Zeru Cheng,Yanjing Yang,He Zhang,Lanxin Yang,Jinghao Hu,Jinwei Xu,Bohan Liu,Haifeng Shen*

Main category: cs.SE

TL;DR: SPLVD uses self-paced learning to dynamically select training data, improving software vulnerability detection. It achieves superior accuracy on standard benchmarks and unseen projects, outperforming current state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for software vulnerability detection struggle with accuracy, primarily due to poor quality training data (source code) that hampers learning.

Method: SPLVD (Self-Paced Learning for Software Vulnerability Detection) is introduced, utilizing a dynamic data selection process. The approach simulates human learning by starting with easy examples and gradually including harder ones. It uses a specialized data selector to assess and choose source code based on their difficulty level before every training epoch.

Result: SPLVD was evaluated on three benchmark datasets (over 239K source code samples, with 25K vulnerabilities) and outperformed state-of-the-art approaches with highest F1 scores (89.2%, 68.7%, 43.5%). On unseen OpenHarmony projects, SPLVD achieved the highest precision of 90.9%.

Conclusion: SPLVD's self-paced learning and adaptive data selection significantly enhance vulnerability detection accuracy compared to existing methods, proving its robustness and practical applicability.

Abstract: Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.

</details>


### [7] [AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews](https://arxiv.org/abs/2511.09223)
*Panya Trakoolgerntong,Tao Xiao,Masanari Kondo,Chaiyong Ragkhitwetsagul,Morakot Choetkiertikul,Pattaraporn Sangaroonsilp,Yasutaka Kamei*

Main category: cs.SE

TL;DR: The paper introduces AILINKPREVIEWER, an LLM-based tool for generating link previews in pull requests. Although contextual summaries rate higher on standard metrics, users preferred simpler, non-contextual previews, highlighting a usability versus performance trade-off. The findings suggest that LLM-powered link previews could improve code review workflows.


<details>
  <summary>Details</summary>
Motivation: Traditional code reviews in software engineering often include links to issues and resources in pull requests, but these links are usually ignored by automated tools, restricting the amount of rich information available and increasing the cognitive load on reviewers.

Method: The authors developed a tool called AILINKPREVIEWER, which uses Large Language Models (LLMs) to automatically generate link previews in PRs by processing metadata such as PR titles, descriptions, comments, and the linked content itself. The tool was evaluated on 50 GitHub repositories using three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. Performance was compared using BLEU, BERTScore, and compression ratio metrics, alongside a user study involving seven participants.

Result: Contextual LLM-generated summaries outperformed non-contextual and metadata-based previews on automatic evaluation metrics. However, user preferences leaned toward non-contextual summaries, indicating a trade-off between objective metrics and subjective usability.

Conclusion: LLM-driven link previews can enrich PR code reviews by providing streamlined contextual information, enhancing reviewer efficiency. There is, however, a balance to be struck between metric excellence and user preference for usability.

Abstract: Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering.
  The video demo is available at https://www.youtube.com/watch?v=h2qH4RtrB3E, and the tool and its source code can be found at https://github.com/c4rtune/AILinkPreviewer.

</details>


### [8] [Leveraging Large Language Models for Use Case Model Generation from Software Requirements](https://arxiv.org/abs/2511.09231)
*Tobias Eisenreich,Nicholas Friedlaender,Stefan Wagner*

Main category: cs.SE

TL;DR: This study demonstrates that open-weight LLMs can slash the time required for creating use case models by 60% compared to manual methods, all while maintaining quality and offering useful guidance for engineers.


<details>
  <summary>Details</summary>
Motivation: Manual creation of use case models is time-consuming and often skipped, despite their importance in achieving stakeholder consensus. The authors aim to address this bottleneck.

Method: The paper proposes integrating an open-weight Large Language Model (LLM) with advanced prompt engineering to automatically extract actors and use cases from software requirements.

Result: In comparison with traditional manual modeling, the LLM-based approach reduced modeling time by 60% without sacrificing model quality. Engineers found the method efficient and helpful.

Conclusion: LLMs can substantially accelerate the use case modeling process while preserving quality, making them a valuable tool for practitioners.

Abstract: Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.

</details>


### [9] [Decoding the Configuration of AI Coding Agents: Insights from Claude Code Projects](https://arxiv.org/abs/2511.09268)
*Helio Victor F. Santos,Vitor Costa,Joao Eduardo Montandon,Marco Tulio Valente*

Main category: cs.SE

TL;DR: This paper analyzes 328 configuration files from agentic code assistant Claude Code. It finds that these files are essential for specifying software engineering concerns and practices, especially architectural guidelines, highlighting their critical role in the effectiveness of agentic coding systems.


<details>
  <summary>Details</summary>
Motivation: Agentic code assistants depend heavily on configuration files that guide their software engineering practices, but little is known about the structure and content of these files.

Method: Empirical study analyzing 328 configuration files from public Claude Code projects to identify specified software engineering concerns and practices, and their co-occurrence.

Result: Configuration files emphasize the importance of specifying diverse software engineering concerns and practices, with a strong focus on architectural guidelines.

Conclusion: Defining a broad range of software engineering practices in configuration files is crucial for agentic code assistants, especially regarding architectural constraints.

Abstract: Agentic code assistants are a new generation of AI systems capable of performing end-to-end software engineering tasks. While these systems promise unprecedented productivity gains, their behavior and effectiveness depend heavily on configuration files that define architectural constraints, coding practices, and tool usage policies. However, little is known about the structure and content of these configuration artifacts. This paper presents an empirical study of the configuration ecosystem of Claude Code, one of the most widely used agentic coding systems. We collected and analyzed 328 configuration files from public Claude Code projects to identify (i) the software engineering concerns and practices they specify and (ii) how these concerns co-occur within individual files. The results highlight the importance of defining a wide range of concerns and practices in agent configuration files, with particular emphasis on specifying the architecture the agent should follow.

</details>


### [10] [Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks](https://arxiv.org/abs/2511.09373)
*Adam Štorek,Vikas Upadhyay,Marianne Menglin Liu,Daniel W. Peterson,Anshul Mittal,Sujeeth Bharadwaj,Fahad Shah,Dan Roth*

Main category: cs.SE

TL;DR: Routesplain is a new router for software-related tasks that uses interpretable concepts to assign user queries to the right LLM. It improves accuracy and cost compared to individual models and black-box routers, while offering transparent, rational routing decisions.


<details>
  <summary>Details</summary>
Motivation: There is significant variability in how large language models (LLMs) perform across different software-related tasks. Efficiently routing user queries to the most appropriate LLM could improve result quality and reduce costs, but existing routers are general-purpose and use opaque, black-box decision processes.

Method: The authors introduce Routesplain, a router specifically designed for software-oriented tasks. Routesplain analyzes each query to extract interpretable concepts such as task type, domain, and reasoning complexity, and uses only these concepts to determine routing—providing clear and faithful explanations for its decisions.

Result: Routesplain is tested on 16 state-of-the-art LLMs and 8 different software-related tasks, including code generation/repair, input/output prediction, and QA. It consistently achieves better accuracy and lower costs than any individual model and matches or exceeds all existing black-box routing baselines. Its concept-level analysis also reveals further opportunities for optimization.

Conclusion: Routesplain introduces a novel, interpretable approach to LLM routing in the software domain, offering strong performance improvements and greater transparency over traditional black-box routers.

Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Soteria: Efficient Symbolic Execution as a Functional Library](https://arxiv.org/abs/2511.08729)
*Sacha-Élie Ayoun,Opale Sjöstedt,Azalea Raad*

Main category: cs.PL

TL;DR: Intermediate language-based symbolic execution tools have significant trade-offs. Soteria is a lightweight OCaml library that enables direct, source-language symbolic execution engines, achieving better or comparable results to leading tools for C and Rust without sacrificing performance or accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing symbolic execution (SE) tools use intermediate languages (ILs) to support multiple programming languages, aiming for reusability and efficiency. However, this introduces trade-offs in performance, accuracy, and feature support.

Method: The authors propose Soteria, a lightweight OCaml library designed to facilitate writing SE engines directly for each source language in a functional style. Soteria enables SE engines to work directly with source-language semantics and supports configurability, compositional reasoning, and ease of implementation.

Result: Using Soteria, the authors developed Soteria$^{Rust}$ (supporting Rust's Tree Borrows aliasing model) and Soteria$^{C}$ (for compositional SE in C). Both tools were found to be competitive with or outperforming state-of-the-art tools like Kani, Pulse, CBMC, and Gillian-C in both performance and the number of bugs detected.

Conclusion: Building SE engines directly for each source language using Soteria avoids the compromises of IL-based approaches, achieving sound, efficient, accurate, and expressive SE with formalized theoretical foundations and proven soundness.

Abstract: Symbolic execution (SE) tools often rely on intermediate languages (ILs) to support multiple programming languages, promising reusability and efficiency. In practice, this approach introduces trade-offs between performance, accuracy, and language feature support. We argue that building SE engines \emph{directly} for each source language is both simpler and more effective. We present Soteria, a lightweight OCaml library for writing SE engines in a functional style, without compromising on performance, accuracy or feature support. Soteria enables developers to construct SE engines that operate directly over source-language semantics, offering \emph{configurability}, compositional reasoning, and ease of implementation. Using Soteria, we develop Soteria$^{\text{Rust}}$, the \emph{first} Rust SE engine supporting Tree Borrows (the intricate aliasing model of Rust), and Soteria$^{\text{C}}$, a compositional SE engine for C. Both tools are competitive with or outperform state-of-the-art tools such as Kani, Pulse, CBMC and Gillian-C in performance and the number of bugs detected. We formalise the theoretical foundations of Soteria and prove its soundness, demonstrating that sound, efficient, accurate, and expressive SE can be achieved without the compromises of ILs.

</details>


### [12] [Galois Slicing as Automatic Differentiation](https://arxiv.org/abs/2511.09203)
*Robert Atkey,Roly Perera*

Main category: cs.PL

TL;DR: The paper links Galois slicing—a provenance-tracking program analysis—to automatic differentiation using categorical semantics, yielding new insights, clarifying design choices, and enabling extensions to quantitative analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to deepen the understanding of Galois slicing, a program analysis technique for explaining executions, and to explore connections with differentiable programming, potentially enriching its theory and application.

Method: The authors use the CHAD approach to automatic differentiation and recast Galois slicing within a categorical semantics framework. This reformulation allows the authors to examine analogy and extensions, particularly into quantitative interval analysis, and clarifies implicit design choices in previous work.

Result: The result is a new categorical semantics for Galois slicing inspired by differentiable programming and automatic differentiation, along with a deeper analysis of its structure, potential generalizations, and the nature of choices in existing methods.

Conclusion: By connecting Galois slicing to automatic differentiation via categorical semantics, the paper advances the theoretical foundation and points toward both extensions and refined applications in program provenance analysis.

Abstract: Galois slicing is a technique for program slicing for provenance, developed by Perera and collaborators. Galois slicing aims to explain program executions by demonstrating how to track approximations of the input and output forwards and backwards along a particular execution. In this paper, we explore an analogy between Galois slicing and differentiable programming, seeing the implementation of forwards and backwards slicing as a kind of automatic differentiation. Using the CHAD approach to automatic differentiation due to Vákár and collaborators, we reformulate Galois slicing via a categorical semantics. In doing so, we are able to explore extensions of the Galois slicing idea to quantitative interval analysis, and to clarify the implicit choices made in existing instantiations of this approach.

</details>
