<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: This paper presents a high-quality, emotion-annotated German sentiment dataset for software engineering, addressing a key resource gap. The dataset shows strong annotation reliability and reveals the need for more domain-specific sentiment analysis tools in German.


<details>
  <summary>Details</summary>
Motivation: Current sentiment analysis tools for software engineering are mainly based on English or non-German data, lacking resources for the German-speaking community. There is a need for domain-specific datasets to strengthen sentiment analysis capabilities within German developer teams.

Method: The authors created a dataset of 5,949 unique developer statements from the German forum Android-Hilfe.de. Each statement was annotated with one of six basic emotions by four German-speaking computer science students, following Shaver et al.'s emotion model. The annotation process was quantitatively evaluated for interrater agreement and reliability.

Result: The annotation process delivered high interrater agreement and reliability, demonstrating that the dataset is valid and robust. When existing German sentiment analysis tools were evaluated on the dataset, they confirmed the lack of domain-specific solutions for software engineering. The study also provides insights on optimizing annotation and proposes additional use cases for the dataset.

Conclusion: The newly introduced German sentiment dataset fills a significant gap for the German-speaking software engineering community, offering a robust resource for sentiment analysis. The dataset's reliability supports its use for research and practical applications, and future work can further optimize and extend its use cases.

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [2] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: An automated tool was developed to extract explainability requirements and explanations from user reviews. While AI-generated explanations are often clear and stylistically preferred, their relevance and correctness need improvement, making human validation important. A curated dataset is released for further research.


<details>
  <summary>Details</summary>
Motivation: There is an increasing demand for explainability in software to ensure transparency, build trust, and meet regulatory requirements. However, there is a lack of systematic methods to derive explainability requirements and corresponding explanations from user feedback.

Method: The authors developed a tool-supported, automated approach that extracts explainability requirements and generates explanations from user reviews. They evaluated their approach by collaborating with an industrial automation company, using a dataset of 58 annotated user reviews with corresponding hand-crafted requirements and explanations.

Result: The AI-generated requirements were often less relevant and accurate compared to human-generated ones, but the AI-generated explanations were frequently preferred for their clarity and style. However, correctness issues still exist in AI-generated outputs, emphasizing the need for human validation.

Conclusion: This work presents an automated method to derive explainability requirements and explanations from user reviews, empirically highlights the current strengths (clarity, style) and weaknesses (correctness) of AI-generated artifacts, and provides a curated dataset for future research.

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [3] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: The paper proposes integrating AAS with BPMN in a distributed copy-on-write infrastructure, improving automation, security, collaboration, efficiency, and traceability in engineering workflows.


<details>
  <summary>Details</summary>
Motivation: To address the need for automation and optimization in plant and process engineering by leveraging Industry 4.0 technologies, particularly the Asset Administration Shell (AAS) and interoperable Digital Twins.

Method: The paper explores the integration of AAS in engineering workflows, combines it with BPMN for defining structured automated processes, proposes a distributed AAS copy-on-write infrastructure, and introduces a workflow management prototype that automates AAS operations.

Result: A distributed AAS infrastructure is implemented that enhances security, scalability, and cross-organizational collaboration. A workflow management prototype is provided to automate operations, improving efficiency and traceability.

Conclusion: Integrating AAS with BPMN and distributed infrastructure can significantly enhance automation, collaboration, and efficiency in engineering workflows.

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [4] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: Despite LLMs' advanced capabilities, developers must still manually convert abstract requirements into detailed programming tasks supplemented with design and architecture info, indicating software engineering fundamentals remain necessary.


<details>
  <summary>Details</summary>
Motivation: There is growing speculation that advanced generative LLMs could replace traditional software engineering by generating quality code directly from requirements. However, it is unclear how developers currently adapt and incorporate requirements into LLM prompts for code generation.

Method: The researchers conducted interviews with 18 practitioners from 14 different companies to investigate how they utilize requirements and related design artifacts when leveraging LLMs for coding tasks.

Result: The study found that standard requirements are too abstract to be provided directly to LLMs. Developers need to manually break down requirements into concrete tasks and supplement them with detailed design decisions and constraints before using them with LLMs.

Conclusion: Manual refinement of requirements into actionable programming tasks is still essential, even when using LLMs for code generation. As such, foundational requirements engineering work remains crucial, and full automation of the process is not currently feasible.

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [5] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: This paper systematically reviews prompt engineering for requirements engineering, creates a hybrid taxonomy linking PE techniques to RE tasks, maps research coverage and gaps, and proposes a roadmap for reliable, practitioner-friendly PE workflows in RE.


<details>
  <summary>Details</summary>
Motivation: Prompt engineering techniques for large language models (LLMs) show promise for requirements engineering (RE) tasks, but a lack of clarity and controllability in LLMs limits their reliable application in RE. There is no systematic guidance for using LLMs in this domain.

Method: The authors conducted a systematic literature review following Kitchenham's and Petersen's protocols, searching six digital libraries, screening 867 records, and analyzing 35 primary studies relevant to prompt engineering for requirements engineering (PE4RE).

Result: The study introduces a hybrid taxonomy connecting prompt engineering patterns (like few-shot and chain-of-thought) to specific RE tasks (such as elicitation, validation, and traceability). Mapping existing work, it identifies task coverage, LLM families, prompt types used, and uncovers current limitations and research gaps.

Conclusion: The paper provides a roadmap to help advance prompt engineering for requirements engineering from prototype use toward systematic, reproducible workflows suitable for practitioners, addressing fragmentation and promoting trust in LLMs for RE.

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [6] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: The paper explores how AI, specifically Retrieval-Augmented Generation models, can help automate and improve requirements engineering in the space industry, making it more accessible for smaller players. Early tests show reduced manual work and better compliance, suggesting a promising path for broader AI integration.


<details>
  <summary>Details</summary>
Motivation: Requirements engineering in the space industry is challenging due to the complexity, the need for precision, strict compliance standards, and adaptation to specific mission needs. Smaller organizations or newcomers particularly face difficulties extracting actionable requirements from large, unstructured documents.

Method: The paper proposes a modular, AI-driven framework that utilizes Retrieval-Augmented Generation (RAG) models. This approach preprocesses mission documents, categorizes them semantically, retrieves relevant content from standards, and uses large language models (LLMs) to synthesize draft requirements. The method is applied to a real-world mission document for demonstration and initial evaluation.

Result: Preliminary results demonstrate that the approach can reduce manual requirements engineering efforts, increase relevant requirements coverage, and aid in compliance alignment. The paper shows feasibility with a real case study in collaboration with an industry partner.

Conclusion: The AI-supported and semi-automated approach to requirements engineering in the space sector can democratize participation, especially for smaller organizations, by lowering barriers and improving efficiency. The authors propose a future roadmap for integrating AI more deeply into RE workflows.

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: General program equivalence is undecidable, but by using abstractions like propositional equivalence and mathematical frameworks such as (G)KAT, equivalence checking becomes both practical and feasible.


<details>
  <summary>Details</summary>
Motivation: Program equivalence is generally undecidable, meaning it's impossible to always determine whether two arbitrary programs are equivalent. However, by abstracting away the semantics of statements, the problem can be transformed into one that is both theoretically and practically solvable. The motivation is to find practical equivalence notions and methods for programs where full equivalence is too difficult.

Method: The paper focuses on studying a form of program equivalence known as propositional equivalence, specifically using the algebraic framework called (Guarded) Kleene Algebra with Tests (G(K)AT). This involves formalizing and reasoning about program equivalence at the propositional level, allowing for effective checking of certain equivalences.

Result: Recent developments in using (G)KAT for propositional program equivalence are summarized, illustrating that this framework offers powerful tools for deciding equivalence at an abstract level. It is shown that many intuitive program transformations can be justified formally using (G)KAT.

Conclusion: By working at the level of propositional equivalence and leveraging (G)KAT, it is possible to efficiently decide equivalence between many programs that would otherwise be too difficult to compare due to the undecidability of general program equivalence.

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>
